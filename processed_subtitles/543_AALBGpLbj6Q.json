{
  "text": "one of the most interesting parts of\nmachine learning is generative machine\nlearning in this video we're going to be\nusing that exact technique specifically\na generative adversarial neural network\nto get started with generative machine\nlearning let's do it\n[Music]\n[Music]\n[Music]\nso in this video we're going to be\ncovering a bunch of stuff but\nspecifically we are going to be focusing\non generative machine learning using a\ngenerative adversarial neural network\nnow we're going to be using this to\nbuild and design our very own fashion\nline but this could be just about\nanything you could generate synthetic\nimages of a whole range of different\ntypes of fields or spheres or\nobjects who knows\nnow we're going to take this step by\nstep and code it up all the way from\nscratch so this is going to be\ninstalling our dependencies getting our\ndata visualizing our data then we're\ngoing to get to the good bit and this is\nwhere we actually build a generator\nmodel so this is the actual neural\ncomponent that is able to generate new\nimages or new objects we'll then take it\na little bit further and design our\ndiscriminator model think of this as a\nart critic which is trying to detect a\nfake from a real image and we'll wrap it\nall up in a custom training loop and\nthen visualize our results ready to do\nit let's get to it\nhey nick what are some use cases for\ngans well basic gans are great at\ngenerating synthetic data more advanced\nmodels allow you to do things like super\nresolution smoke removal and style\ntransfers wait\nthink you could build us a simple game\nand maybe\ngenerate some fashion sure let's get to\nit first thing we need to do is set up\nour environment welcome on back to the\nbreakdown board\ni really wanted to do that it's been a\nwhile okay so in this video we're going\nto be building again but not just any\ngame we're going to be building ghana\ngenerate or again to generate fashion so\nthe first thing that we're going to be\nneeding to do is get some images of\nfashion now in order to do that we're\ngoing to leverage the tensorflow data\nset api\naka tfds\nand we are specifically going to be\nusing the fashion mnist data set so this\nis going to allow us to get images of\nthings like t-shirts\nshoes let's get draw a boot or something\nand things like bags as well so like\nit's got a whole bunch of random images\nthat we can actually use\nfor our particular data set so\nthe thing that we're then going to do is\nwe're actually going to have to build up\na generator so generator is going to\ntake in a bunch of random numbers\nand this is commonly referred to as a\nlatent dimension so it might be like 1\n23 42\n71\nso on\nand what that is actually going to\noutput is it's going to take in those\nrandom numbers and it's going to output\na set of values which represent an image\nso what we're actually going to do so\nlet's say for example the images that\nwe're getting back from tfds\nare going to be in the shape 28 by 28 by\n1.\nwe might take in some random values into\nour generator\nlet's say in our particular case we'll\ntake in 128 random values\nthose 128 values will be reshaped into a\nformat that allows us to output a set of\nvalues which are 28 by 28 by 1. so we'll\nactually take those 128 values and we'll\nbuild up our generator so our generator\nwill be comprised of a number of\nconvolutional neural network layers\nas well as some up sampling layers\nand eventually what we are going to get\nprobably connect these hey\nwe're going to output an image which is\n28\nby 28\nby one which is effectively a generated\nimage that is the crux of again now this\nis where the discriminator comes in\nwe'll actually take the generated image\nand pass it through a discriminator\nneural network and that is again going\nto have a number of convolutional neural\nnetwork layers\nbut the final output for that neural\nnetwork is going to be\na zero\nor a one\nwhich represents whether or not it's a\ntrue or a fake image so we've set it up\nso that zero represents true\nand 1 represents fake now other\nimplementations might do this a little\nbit differently but this is definitely\ngoing to work so we'll then take that\ngenerated image\nand bang it into our discriminator at\nthe same time we'll also take real\nimages\nall the way from tfds i'm not going to\nlet's just\nmake an uh entry so\none will be going here\nso that will come from our tfts model\nand we'll be able to train our\ndiscriminator as to what is real and\nwhat is true now this is where the fun\nbit comes in the custom training loop\ni've drawn uh\nsome dude lifting weights here but you\nkind of get the idea we actually need to\nbalance off the training between our\ngenerator and our discriminator and this\nis a key step when it comes to actually\nbuilding guns we actually train\na discriminator\nand it gets rewarded if it effectively\npicks out fakes\nwhereas our generator gets rewarded if\nit's able to fool the generator or fool\nthe discriminate up so generator\nits goal is to fool\nthe discriminator\nnow the cool thing about keras and\ntensorflow which is going to be the deep\nlearning framework that we're actually\ngoing to use is that we can actually\ncreate a custom train step\nand through here we can actually build\nthat custom training loop\nwhich allows us to build a generator now\nfinally when it comes to actually\ntesting this model we can actually take\nthe generator model specifically discard\nthe discriminator because we don't\nreally need it anymore it's purely\nneeded for training we can then pass\nthrough those 128 random variables to\nour generator\nlet's say that this is our generator is\nrepresented as a pink model\ncan mod\nand then what that is going to do is\nit's going to output our different\nimages of fashion so we might get a\nt-shirt\nwe might get a boot\nso on and so forth now there's a whole\nbunch of applications for this again you\ncan use it for\ngenerating other types of stuff you can\neven go to a slightly more advanced\nmodel called a conditional gan which\ngives you a little bit more control as\nto what it is that you actually generate\nfor now we're going to keep it simple\nlet's get to it alrighty so the first\nthing that we told our client that we\nwere going to do in order to build up\nagain is set up our environment now in\norder to go through and complete our\ngame there's a bunch of stuff that we're\ngoing to do but we'll set up our\nenvironment first so once we've done\nthat we're going to visualize and build\nour data set we're gonna take a look at\nbuilding our neural network so we'll\nbuild up our generator our discriminator\nwhich i'll talk about more lately or\nlater\nand then we're gonna construct our\ntraining loop which is pretty involved\nbut we'll walk through it and then we'll\ntest out our generator but first things\nfirst let's go on ahead\nand set up our environment now the first\nthing that we need to do is install some\ndependencies so the main dependencies\nthat we're going to have around here are\ntensorflow matplotlib so let's go ahead\nand install those and then we'll be able\nto kick things off\nokay so those are all of our\ndependencies that we're going to need so\ni've written one line of code there and\nthe full line is exclamation mark pip\ninstall tensorflow tensorflow gpu map\nplot lib tensorflow dash data sets and\nipi widgets so tensorflow and tensorflow\ngpu are going to be the core\ndependencies that we're going to need to\nactually build up our deep neural\nnetwork matplotlib is largely going to\nbe used in the visualization component\nso when we actually go and visualize our\ndata sets in the next step after we've\nhad a chat to our client we're going to\nbe using that tensorflow datasets is the\nactual data set that we're actually\ngoing to be using so if you go to\ntensorflow data sets\nand actually take a look so there's a\nbunch of data sets available now we're\njust going to be using the fashion data\nset so i think we can go into the\ncatalog\nand search for fashion\ni think it's in here yeah so we're going\nto be using fashion mnist so\nthere's a bunch of images from zalando's\narticle images so they're all 28 by 28\ngrayscale so this is just going to make\nbuilding our deep neural network a\nlittle bit faster rather than using\nmonster images like i've done for the\nsuper resolution tutorial which will be\ncoming around kind of soonish\ncool all right so those are data sets\nand then ipi widgets is just a\ndependency whenever you're downloading\ntensorflow data sets if we go and run\nthat i'm pretty sure i've got all of\nthese installed already so it should run\nrelatively quickly\nand it has\nlet's double check we've got no errors\nso we've got a warning saying we need to\nupgrade pip that's perfectly fine we can\nignore that now\ni always get the comments nick show us\nwhat versions you're using so i'm just\ngoing to run a quick pip list and show\nyou what versions of stuff i'm using\nso the core things that we need to take\na look at are tense flow so over here\nyou can see i'm using tensorflow 2.8.0\nand tensorflow gpu 2.8.0 tensorflow data\nsets we're using 4.5.2\nwhat else did we have we installed\nmatplotlib let's take a look at that\nmatplotlib we're using 3.5.1 so that\nshould hopefully\nget you across the board if you're\ntrying to work out what versions of\nstuff to install keep in mind that the\nversions of tensorflow that i'm using\nare namely because i'm matching the\nversion of\ncuda and cu dnn that i've got installed\nso\nif you i've probably shown this a\nbillion times but i want to show you\njust once more so if you actually type\nin tensorflow gpu\ninto google and you actually take a look\nat\nuse gpu uh there should be\ni'm not sure if it's on this page but\nthere's actually a no it's not on this\npage\nso there's actually a compatibility\nguide here we go it's down here so over\nhere on the left hand side build from\nsource actually shows you so if you\nscroll all the way down this actually\ngives you the the configurations that\nyou're going to need so i'm using 2.8\n2.9 is now out um so i'm using 2.8 which\nmeans that i need to have and i'm\nrunning this on windows which means that\ni need to have microsoft visual studio\ncode 2019\nbazel 4.2.1 and then these are the two\nmost important bits in order to enable\ngpu acceleration we need cu dnn 8.1 or\nku and kuda 11.2 not optional you need\nboth\njust keep key thing to keep in mind as\nto why i use specific versions okay\nso that is our set of dependencies now\ninstalled the next thing that we need to\ndo is first up what we are going to do\nis limit the gpu consumption so if i\nactually show you my little gpu at the\nmoment so right now you can see that\nwe've kind of spun up some stuff so we\nmight need to shut down some existing\nkernels but what we want to do is we\nwant to limit the virtual ram growth and\nthis is a key thing whenever using\ntensorflow it'll expand to use all of\nthat ram so you want to constrain that a\nlittle bit so you don't get a ton of out\nof memory errors so we can do this using\nuh the gpu i think it's set memory\ngrowth equal to true so i'm going to\nshow you how to write this command and\nthen we'll take a look so let's do it\nokay that is our memory growth now set\nso we've gone and written four lines of\ncode and you've probably seen me bring\nthis one in a ton of times before so the\nfirst line that we've written is import\ntense flow as tf so this is going to\nallow us to leverage tensorflow so uh\nbringing\nin tense flow\nand then if you've ever watched any of\nmy other deep learning tutorials these\nthree lines are going to be super common\nso first up we go and grab all of our\ngpus and the full line there is gpus\nequals tf dot\nconfig.experimental.list underscore\nphysical underscore devices and then\nwe're going to pick up all of our gpus\nso if i actually type in gpus\nyou can see i've only got one there so\ni've got a 2070 super on this deep\nlearning machine and then what we're\ndoing is we are looping through each one\nof those so if we actually did have\nmultiple then we'd actually go and loop\nthrough every single one of them this\njust sort of sets you up for scaling if\nyou were to use multiple gpus later on\nso the next line is for gpu in gpu so\nit's effectively looping through this so\nfor gpu and gpus\nprint gpu\nso you can see it's going to print out\neach physical device then we're actually\nsetting that memory growth so remember i\nsort of mentioned that that function so\ntf.config.experimental.set\nunderscore memory underscore growth\nwe're passing our gpu so that particular\ngpu is part of our iterator and we're\nsetting that growth equal to true so\nthat means we're not going to blow out\nour vram when it comes to actually\nbuilding up our deep neural network now\nthe last thing or the second last thing\nthat we need to do is actually import\nthe rest of our dependencies so we've so\nfar installed a bunch we've taken a look\nat our versions and we've limited our\nmemory growth we now need to bring in\nthe rest of our dependencies and then\nbring in our data so first up let's\nbring in the rest of our dependencies\nokay those are our next two dependencies\nnow brought in so i've gone and written\ntwo lines there so the first thing that\nwe're doing is bringing in\ntense flow data sets\nor fashion\nm nist\nso that line there is import tensorflow\nunderscore data sets as tfds so this is\ngoing to allow us to use this fashion\nmnist data set and so we're going to\neffectively be able to generate images\nthat kind of look like this which could\nbe scaled out to be able to generate\ncolor images could be scatter to\ngenerate bigger images there's a ton of\nstuff that you could do\nso\nwe are bringing in tensorflow data sets\nand then the next line that we're\nbringing in is bringing in matplotlib\nmap plot lib for vis stuff\nso the line there is from matplotlib\nimport pi plot as plt so really just two\ncore lines of code there to bring in our\ndependencies now the next thing that we\nneed to do is actually\ndownload and import our data set so\nlet's actually do that\nokay and that is our data set now loaded\nso we've actually gone and used the\ntensorflow data sets api\nso use the tensorflow data sets\napi to bring in the data source\nso we've written one line of code there\nso it's ds equals\ntfds.load so you can see that there so\ntfds.load\nand then we've specified what dataset we\nactually want to bring in which in this\ncase is fashion mnist so it's this one\nover here and the cool thing about the\ntensorflow data sets catalog is that you\ncan kind of just reference it as that\nand bring in a ton of different types of\ndata sets and i think i've talked a\nlittle bit about how to work with the\ntensorflow data set api previously not\ngetting it from the tensorflow data sets\nbut building your own but this is how we\ngo and download a pre-existing one so\ntfds.load is your friend and then you\npass through which data set you want so\nlet's say for example you wanted i don't\nknow images of satellites you could\nactually pass through eurosat and you\nwould get satellite images it doesn't\nactually show up there but that's sort\nof the cool idea behind it that you can\niterate really quick and then we specify\nthat we only want the training partition\nso if specified split equals train so\nthe full line is ds equals tfds.load and\nthen we're passing through fashion mnist\nas the first uh positional argument\nwhich is just a string and then we're\npassing through a keyword argument to\nsplit equals train so we now actually\nhave our data set so if we actually went\nand took a look so ds\ndot as\nnumpy iterator let's take actually let's\ntake a look at the type first\nokay so this if we take a look this\nlooks like it's a prefetch component so\nit's part of our tensorflow datasets api\nso if we went and typed in dot\nas numpy\niterator\ndot next we should be able to get a\narray there you go so what we're\nactually getting back is a dictionary so\nyou can see the first component is\na the image itself and then if we scroll\non down let's actually take a look at\nthe keys\nso we've got an image and then we've got\na label so if we actually went and\ngrabbed the image\nright so this is just an image so we're\nactually going to take a look at how to\nvisualize this in a sec but just know\nfor now that we've got an image and this\nis obviously a classification data set\nso we can take a look at the labels as\nwell in that particular case alrighty\nthat is the first component now done so\nwe've now gone and imported our\ndependencies in our data so remember we\nwent and installed a bunch of stuff\nspecifically tensorflow tensorflow gpu\nmatplotlib tensorflow data sets and ipi\nwidgets we then went and limited our\nmemory growth we went and brought in a\ncouple of dependencies so tensorflow\ndata sets and map plot lib and then we\nwent and downloaded our data set that\nwe're going to use for again from the\ntensorflow data sets catalog and we're\nbringing fashion mnists to be able to\ngenerate our own fashion components\nand then we also took a look at ad\nbriefly look at our data set but we're\ngoing to dive into that a bit more in a\nsecond all right let's jump back on over\nto our client and have a chat\nso\nthat's all done it's done well what next\nit's always good practice to visualize\nour data in this case we've got some\npretty small images because it's just a\nprototype but it's still worthwhile\ntaking a look with matplotlib\nnonetheless so as i mentioned it's good\npractice to visualize your data set\nbefore you try to do anything with it\nbecause otherwise you're sort of just\nwalking in the dark which can be a\nlittle bit problematic\nwhen you're building machine learning\nthings so what we're going to do now is\nwe're actually going to take a look and\nvisualize our data set now in order to\ndo that first thing that we're going to\ndo is bring in numpy so import numpy as\nnp\nand we're specifically going to use\nnumpy to do do some data\ntransformation\nwhich we'll see in a second so now that\nwe've actually gone and grabbed or\nbrought in numpy the next thing that we\nactually want to do is build an iterator\nso you would have seen here before i was\nactually creating\na loop to be able to continuously grab\ndata\nout of our tensorflow datasets pipeline\nso what we've actually got here so by\ndownloading our tensorflow or fashion\nmnist data set using the tensorflow data\nset source we've actually got a pipeline\ndon't think of it as you've loaded in\nall of your images into memory we've got\na set of repeatable calls that we can\nmake to this pipeline to bring data back\nright\nnow in order to bring that data back we\nfirst up need to actually set up an\niterator so this is think of this like\nalmost like a connection right we can\nthen call next to bring back the next\nbatch of data so by setting up the\niterator and then going next it'll bring\nback one batch or one image depending on\nhow big the batch size is then if we go\nnext again and bring back another one\nnext again it'll continuously bring back\nmore data so what we're going to do is\nfirst up set up an iterator\nwhich is effectively just going to look\nlike this right\nand we're going to call it data iterator\nset up let's call it connection setup\nconnection\naka iterator\nso then what we can actually do is just\ncall the iterator and type in dot next\nand this will continuously bring back a\nbatch traditionally you'll batch up the\ndata set so that you've actually got\nmultiple images per batch or multiple\nsamples per batch so if we type in data\niterator\nderator dot next\nso you can see that that's one image\nthen if i call another one that's\nactually another image so watch these\nnumbers down here right they're actually\ngoing to change so you can see that that\nchanged changed\nchange so we're actually bringing back\nnew data sets each and every time\nboom boom boom boom boom boom boom so\nwhat we're actually doing is we're\ncalling out to that pipeline and\nbringing back a new batch every single\ntime so it's not loading it all into\nmemory to begin with it's actually\nmaking a call and bringing back data as\nand when we need so this helps preserve\nsome of the memory on your computer\nparticularly when you're doing hardcore\ndeep learning okay so now that we've\ntaken a look at how to get data out of a\npipeline\nso getting\ndata\nout of the pipeline\nboom all right and then we can uh cool\nthing about jupiter lab i don't know if\nyou've seen this is that if you actually\nright click on an output cell you can\nactually dis or enable scrolling for\noutput so then rather than me having to\ngo all the way down to find the next\ncell or go to the next bit it actually\nallows me to condense the output so if i\nenable scrolling for outputs you can see\nthat much cleaner now so i can actually\nsee what the hell i'm doing um hopefully\nthat that makes your life a little bit\neasier as well okay so what have we done\nbrought in numpy set up our iterator aka\nour data connection and then i'm showing\nyou how to get data out of the pipeline\nthe next thing that we actually want to\ndo is do a little bit of is so the first\nthing that we're going to do is create\nsome subplots so we're going to use\nplot.subplots to actually create some\nsubplots using matplotlib and then we'll\nplot out four images so let's do it\nokay there we go so we've actually gone\nand visualized some of our images now so\nlet's take this step by step and let me\nshow you what i've actually written\nthere so i've written one two three five\ndifferent lines of code so the first\nline is actually establishing our think\nof the way the matplotlibs is almost\nlike setting up the\nformat and then you plugging in images\nparticularly when you're working in\nsubplots so first thing that we're doing\nis we're setting up the format so set up\nthe subplot\nformatting\nand all we're really saying here is that\nwe want four columns and we want the\ntotal figure which is the total plot to\nbe 20 by 20 pixels in terms of size\nso then the entire figure is referenced\nas figure so this is the fig is actually\nthe whole thing ax is each one of these\nindividual subplots right so ax could\njust as easily be called subplot one\nsubplot two subplot three so if we\nactually take a look at ax right\nit's just four different components so\nthat's the first subplot that's the\nsecond subplot that's the third that is\nthe fourth right\nand then we can actually visualize\nsomething in each one of those\nindividual little subplots which is\nexactly what we've done the first thing\nwe're doing is we're setting up the\nsubplots and the return values that\nwe're going to get back are the entire\nplot the whole thing and the axes which\nis one two three four because we've\nspecified end calls equals to four\nthen we're looping four times so we're\ngoing and grabbing four individual\nimages from our data iterator which is\nover here so we're just calling dot next\nfour times which is exactly this\nso for idx in range so let's say we're\ngoing to loop\nfour times\nand get images\nso for idx in range four first thing\nthat we're going to do is we're going to\nget a batch now a batch is effectively\njust a sample in this case i could just\nas easily call this sample\nbecause it's not really i don't believe\nit's been batched up right so we're\ngetting one sample by calling data\niterator.next and remember that sample\nis going to be comprised of two parts\nthe image which is part of a dictionary\nand then the label so the first thing\nthat we're doing is we're using the axes\nand we're using the i am show function\nwhich is just the image show function\navailable inside of matplotlib and then\nwe're passing through our image now\nwe're squeezing it because it's actually\ninside of a set of subarrays so if we\nactually go and grab an image\ndot shape\nso what we actually want to do is we\nwant to condense this down so we've got\none a single value down the bottom so if\nwe actually type in\nnp dot squeeze\nget rid of that\nso you can see that we're now collapsing\nit down so if we type in dot shape now\nit's just 28 by 28 which allows us to\nvisualize it a whole heap easier\nokay so what are we doing so we're first\nup getting the axes that we want to\nvisualize so we're going to grab the\nfirst index or the first axis so let's\nactually take a look so\nax\nso we're effectively going all right\ngrab index 0\nand then what we're going to do is we're\ngoing to type in dot\ni am show and then pass through the\nimage which is exactly what we've done\nthere so mp.squeeze we're grabbing our\nsample image and then we're passing it\nto that so that is what actually does\nthe visualization so let's actually\nwrite some comments so grab an\nimage and label\nplot the image\nusing a specific\naxis\nactually we'll call it subplot\nand then the next thing that we're doing\nis we're just setting the title this is\npurely optional like if i commented that\nout it'd just get rid of the numbers at\nthe top now the numbers at the top are\nnot all that descriptive because we\nhaven't actually gone and taken a look\nat what each one of those labels are but\npresumably\nlabel 5 is going to be some sort of high\nheel label 8 is going to be a bag label\n6 is going to be a jumper because you\ncan see those two there so this is just\nprinting out\nor appending\nthe image\nlabel as the plot title\nand to do that we've written ax and then\nwe're passing through the index so we're\ngrabbing this specific\nsubplot at a point in time and then\nwe're using dot title so we're grabbing\nthe dot title attribute and we're using\nthe dot set text function to be able to\nset the text above that so you can see\nthose little fives let me zoom in on\nthat so you can see that five there you\ncan see that 8 there\nso on and so forth right so that makes\nit a little bit easier to see what we're\nactually plotting out so again we can\nkeep plotting we're going to see\ndifferent images so at 4 might be a\nmen's jumper 5 might be a shoe one might\nbe some pants\nnine\na boot for a men's jumper nine a boot\nagain three a singlet or something like\nthat uh you sort of get the idea right\nso we can actually visualize each of\nthose images and because we're using the\ndot next function we can actually keep\ngetting different sets of images back\npretty cool right\nso we're getting a whole bunch of\ndifferent images\nperformance jumper nine boot eight bags\nsix what is that singlet jumper whatever\nyou sort of get the idea but we've got\nour fashion images now we're actually\ngoing to use again to be able to\ngenerate these types of images now if\nyou think about the capabilities of this\nwe could actually use it to be able to\ngenerate synthetic data sets we could\nuse it to be able to generate faces\nlater on because all we'd really need to\ndo is sub in different types of images\nand use a similar pipeline to do this\ntype of thing\nokay\nso that is our\nvisualization now done so so far what\nwe've gone and done is we've done a\nlittle bit of data transformation using\nnumpy because remember we used\nnp.squeeze to collapse our array we've\ntaken a look at how we can set up a\nconnection to our data set how we can go\nand get data out of the pipeline and\nwe've taken a look at how we can go in\nahead and visualize it and guys as per\nusual all this code is going to be\navailable via github so i'll include the\nlink in the description below i'm just\ngoing to clean this up a bit\nalrighty cool now the next thing that we\nactually need to do is do a little bit\nof data processing so right now these\nimages are represented as values between\n0 and 255\nin order to build good deep learning\nmodels we typically want to scale values\nto be between 0 and 1. so we're actually\ngoing to set up a quick function that\nallows us to scale our images so let's\ngo ahead and do that\ncool so that is our function to scale\nour images so scale\nand return images only now we're\nactually going to transform our data\npipeline so that it only returns the\nimage we don't actually need the label\nat the moment because it's not a super v\nsupervised classification problem if\nwe're going to build a conditional gain\nwhich i might talk about in a future\nvideo we might actually need those\nlabels because we'd actually want to\npass through what type of image we want\nto generate for this particular model\nwe're just going to be generating\ndifferent types we're going to sample\nfrom the latent space be able to\ngenerate different types of fashion so\nlet's take a look at our function first\nup so first up we've defined a new\nfunction and then we've set it equal to\nscale images so that's going to be the\nname of it and then that is going to\ntake in the data from our tensorflow\ndata pipeline we're then going to\nextract the image only because remember\nthe dictionary that we got back included\nthe image and the label we just need the\nimage for this so image equals data and\nthen we're grabbing it using the key\nimage we're then returning the image\ndivided by 255 so this is going to scale\nour image to be between zero and one\nhopefully to build a neural network that\nperforms a little bit better and trains\na little bit faster\ncool all right so that is that now what\nwe need to do is actually apply that to\nour data pipeline so if you've seen uh\nwhat was it i think it was actually the\nimage classification video you'll know\nthat i remember how to do this by the\ninitialism mixture app so remember we\nneed to map our data set we need to\ncache it we need to batch it\nuh what is it b a and then p and then\nprefetch so let's actually oh wait\nthere's an s we also need to shuffle it\nso uh let's actually take a look at how\nto do this so we'd actually go\nmap so and then cache and then s which\nis shuffle and then batch and then\nprefetch so these are the steps that you\ntypically go\nf-e-t-c-h there we go that's how it is\nspelled prefetch so these are the steps\nthat we typically do whenever we're\ngoing and building up a data pipeline\nfor tensorflow so that is exactly what\nwe're going to go on ahead and do so\nlet's go and do it\nokay that is our data set now prepared\nso as i mentioned we are going to run\nthrough the mixture back pipeline so map\ncase shuffle batch and prefetch so the\nfirst thing that we've gotten done is\nreloaded\nthe data set\ncompletely optional you could just as\neasily use the data set that we brought\nin from\nright up here i've just gone and i like\nhaving it in a little condensed area so\nds equals tfds.load and again we're\nloading fashion mnist just so you get\nused to actually how to do that we're\ngrabbing the train partition all right\nthen this is where the magic happens so\nfirst thing that we're doing is we're\nrunning\nthe data set\nthrough the scale\nimages pre-processing step\nand i really like the way that this\nworks because we can actually take like\nalmost apply like data pipelines which\nis effectively what this is doing so\nwe're taking our data and we're running\nit through the scale images function we\ncould go and do other stuff if we wanted\nto do data augmentation i've got a\nbetter idea of how to do that now we\ncould actually go and apply a data\naugmentation step here as well but now\nwe don't actually need it then what\nwe're doing is we're cashing our data\nset so cache\nthe data set\nfor that batch\nwe're then shuffling it so this ensures\nthat we've got a shuffle data set so\nwe're not just looking at a specific set\nof samples so shuffle it up\nand so actually let me just quickly walk\nyou through this so first up what we're\ndoing is we're grabbing the data set\nthat we defined up here and then we're\noverwriting that variable so ds now\nequals ds.map and we're returning it\nonce it's being passed through the scale\nimages function we're then caching it so\nwe're taking ds and then we're caching\nit so by applying the cache function so\nds.cache we're then shuffling it up so\nagain we're grabbing the ds or the data\nset we're then passing through or using\nthe shuffle function and we're\nspecifying it as specifying the value to\nthat shuffle function as what the\nshuffle buffer needs to be which in this\ncase is 60 000\nand then we're overriding the variable\nwe're then taking the data set again and\nwe're batching it into batches of 128\nimages\nbatch into 128 images\nper sample\nso now our data set is now equal to\nds.batch and then we've passed through\n128 today and then we're pre-fetching so\nthis eliminates bottlenecking or slows\ndown\nreduces\nthe likelihood\nof bottlenecking\nso again we're taking our ds variable\nand we're using prefetch passing through\n64 and then that gives us our data set\nso if we run that no issues so again we\ncan type in ds dot as numpy\niterator\nand now what we should get back when we\ntype in dot next is\na set of images which is 128 samples in\nlength which should be 28 by 28 i think\nby one so if we go and do that\ntype in dot shape\nso there you go so 128 images because\nwe've gone and batched it into samples\nof 128\nit's 28 pixels what is that wide so\ni think wide or high 28 by 28 by one\nright so that's going to be the\ndimension so i believe it's width and\nheight then channels so it's just a\ngrayscaled image hence why we've got the\none at the end there so that is our data\nset now built so we've gone and done a\nton of stuff here so first what we've\ngot and done let me make sure we're\nzoomed out so we've gone and brought in\nnumpy for a little bit of data\ntransformation when we're visualizing so\nwe're specifically using np.squeeze\nwe're then setting up the connection to\nthe iterator we're getting the data out\nof the pipeline using the dot next\nfunction we're then visualizing using\nmatplotlib subplots we're then scaling\nit so this is how you define a\npre-processing step whenever you're\nworking with the tensorflow data\npipeline and then i've gone and built up\nan entire data pipeline here so we've\ngone and reloaded our data set and then\ndone the mixture shuffle so we've gone\nand mapped it we've gone encased it\nshuffled batched and pre-fetched it\nwhich gives us our data set that looks\nlike this 128 samples by 28 pixels by 28\npixels by one cool\nthat is step two now done so we've now\ngone and visualized and built our data\nset let's jump back on over to our\nclient\ntime to build the model my guy oh i was\nborn ready to model my guy wait what\nwhat are you talking about anyway\nnow what we're going to do is build two\nmodels the generator model will be built\nto try to generate images of clothing\nand fashion the discriminator will try\nto learn to spot the fakes so the\ngenerator is almost like an artist\ntrying to forge things and the\ndiscriminator is an art critic trying to\nspot them out exactly let's do it radio\nso we're now up to the good bit actually\nstarting to build our deep neural\nnetwork now as i mentioned in the client\nconversation so there's a couple of key\ncomponents that we actually need to\nbuild here so namely the generator and\nthink about this as the part of the gan\nthat is actually doing the generating so\nwe can pass through a bunch of random\nnumbers and it will try to take that set\nof random numbers to generate some\nfashion now there is a type of\ngenerative adversarial neural network\nthat actually gives you a little bit\nmore control and this is called a\nconditional gain so you can actually\npass through the number one which might\nmap to a specific type of image and it\nwill actually try to generate that image\nso we're not going to use a conditional\nagain for this tutorial but if you want\na video on that let me know\nin the comments below and maybe we'll uh\nwe'll try to wrap something up\nthat does that so the first thing that\nwe actually need to do is actually\nimport our modeling component so we need\nto bring in some more dependencies\nnamely we need to bring in the\nsequential\napi and we also need to bring in the\nlayers that we're actually going to use\nso let's go ahead and do that\nthe\nkeras.layers import\nwhat do we need\nokay those are our modeling components\nthat we're gonna need so the first thing\nthat we've actually gone done is brought\nin the sequential api i think i went\nthrough a detailed explanation of how\nthat works in\none of the previous videos it was either\nthe\nimage or the eye detection iris tracking\nvideo or the one\nor the face detection one but just know\nthe sequential api means that we take in\none input and we get to one output it's\nbasically flowing one way\nso that is the api that we're going to\nbe using so bring\nin the sequential\napi for\nthe generator\nand discriminator\nand the cool thing is because the\ngenerator is just going to take in a\nrandom number of random values so\neffectively uh a latent\nset of latent values\nwe can\ndo or use the sequential api for that\nlikewise the discriminator is going to\ntake in an image and try to output a set\nof\nbinary classification values so 0 or 1 1\nor 0 to determine whether or not it is a\nreal or fake image so the generator is\ngoing to take in random values try to\ngenerate an image the discriminator is\ngoing to take in the generated image and\nmaybe some real images and try to\ndetermine whether or not it's fake or\nnot and it's sort of like this balancing\nact between them that's how we actually\ngo about training these bad boys so\nthat's the sequential api then we're\nbringing in a bunch of layers\nthe layers\nfor the neural\nnetwork how cool is this that we can\nactually live in a day and age where we\ncan build ai stuff\nall right so for our neural network\nwe've brought in from\ntensorflow.keras.layers\nimport conf2d\nand then we are also bringing so\nconvolute that's going to allow us\nperform convolutions i believe we're\ngoing to use that in both the\ndiscriminator and the generator we're\nthen also going to bring in our dense\nlayers which are fully connected layers\na flattened which allows us to flatten a\nset of dimensions or vectors or matrices\nwe are then bringing the reshape layer\nwhich allows us to transform what the\noutput from a previous layer looks like\nto a different shape pretty useful\nwhenever you're\ntaking inputs of a certain shape so\nwe're going to use this to take in our\nrandom variables for our generator and\nreshape it into something which gives us\na little bit of spatial quality you'll\nsee that in a sec we are then bringing\nin leaky relu so that is just an\nactivation um so\nleaky relu\nversus relu so\ni find it useful to actually visualize\nwhat the activation functions look like\nso\na relu is just the this value here so\nit'll just be the blue line and then\nfollowing the x-axis the leaky relu has\na little bit of a leak on the negative\nset of values right so that allows us a\nlittle bit more data to transfer\nuh then we've got dropouts a form of\nregularization and we've also got up\nsampling 2d which is going to be used\nfor our generator to\nup sample the images and bring in add in\na little bit more depth to or a little\nbit more space to our image\nokay so these are all the layers that\nwe're going to need so we'll bring in\nthe sequential api and then a bunch of\ndifferent types of layers so that is\nwell those are the modeling components\nthat we're going to need and the next\nthing that we want to do is actually\nbuild up our generator so let's actually\ngo and do this so we are going to create\na new function\nand we'll call it build generator\nand then we're going to instantiate our\nmodel so model equals sequential\nand then we're going to start adding a\nbunch of layers and what we're going to\nreturn at the end is the model right so\nwhat we actually need to do in the\ninterim now is actually work out how to\nbuild this generator so the first thing\nthat we're going to do is define what\nnumber of inputs we're actually going to\ntake in so we might take in let's say\n128 values and use that as our random\nsampling value to actually work out what\nto generate so that's exactly what we're\ngoing to do so let's build our first\nblock and then we'll take a look at what\nthat looks like first up\nokay that is the\nfirst block that we're going to add to\nour deep neural network so we've gone\nand added in a dense layer leaky relu\nlayer and a reshape layer so let's see\nif we can just generate this to begin\nwith so a model\nlet's call it test model for now test\nmodel equals\nbuild generator\nand we've got an issue takes two\npositional arguments but uh this should\nbe wrapped again here\none more set of arrays there okay cool\nall right so that is our test model now\ndefined can we type in dot summary\nbeautiful okay so that is what our\ninitial model looks like and i find it\nbuilding up this way actually get makes\na little bit more sense because you can\nactually see what's happening so the\nfirst thing that we're doing is we're\nspecifying what our input layer is going\nto be and our input layer is going to be\na fully connected layer or dense fully\nconnected layer now the number of units\nis going to be 7 by 7 by 128 now let me\nexplain why we've gone and specified\nthat\nwe are going to be passing through a 128\nrandom values to our generator to help\nit determine what to generate from there\nnow this is why i sort of mentioned that\nyou don't have as much control with this\ntype of gain as you do with a\nconditional gain because the conditional\ngain is going to focus on what type of\nimage you want to generate with this\ntype again it's really just generating\nrandom images from your sample\npopulation so it'll be trying to\ngenerate random fashion images now the\nnice thing is that we can avoid\nsomething called mode collapse which\nstill ensures that it generates a bunch\nof random different types of images so\nit might generate some\njumpers might generate some shoes might\ngenerate some singlets so on and so\nforth but just know that that input\nvalue that we're going to be passing in\nwhich is 128 is what gives our\ngenerator some context to determine what\nto actually generate so we are going to\ngenerate based on 128 random values\nnormally referred to as the latent space\nas well\nso that 128 is then going to be\nconverted into a spatial area so the 128\nvalues think of it just like an array of\n128 values but because we want to\neventually convert this into an image we\nwant to give it some spatial qualities\nright so we're actually going to convert\nit into a shape of 7 by 7 by 128 and\nthat is what our dense layer is going to\ndo so the number of units that we're\ngoing to have is going to be 7 by 7 by\nour random variable so this could just\nbe our random variable here\nso model dot add and then we're passing\nthrough dense and then seven by seven by\n128\nwe could just pass through the actual\nvalue there and then we're specifying\nwhat our input value is going to look\nlike which is going to be 128 values\nthen we are applying a activation so\nwe're specifying that we want to apply a\nleaky relu after the fact this allows us\nto cater for non-linearities in our\nmodel so model.add and then we're\npassing through leaky value and\nspecifying what that output parameter is\nso 0.2 so that is what defines how far\nthat\nbottom bit looks like\nand then we're reshaping it and this is\nwhat now gives us our spatial quality so\nwe're passing through model.add and then\nwe're passing through a reshape layer\nwhich takes our dense output which is\nreally just going to be\n6272 values and we're going to\neffectively be converting it to the\nbeginnings of a image so except it's an\nimage which is seven by seven by 128\nchannels so it's a monster image with a\nton of layers eventually we're going to\nget to the point where we are generating\nor this is outputting an image which is\nthe same shape\nas our data it'll be 28 by 28 by one so\nwe're starting out with seven by seven\nby 128 we're eventually going to get to\n28 by 28 by one by adding in a bunch of\nadditional layers so this is our first\nblock\nso it takes in\nrandom\nvalues and reshapes it\nto 7 by 7 by 128. so and just think of\nthis as the beginnings\nof\na generated let's put this on another\nline\nbeginnings\nof a generated image\njulio all right so that's the first set\nof blocks now done now this is obviously\nnot our model complete because right now\nit's still outputting seven by seven by\n128 which is not in the correct\ndimensions for a generated image it\nneeds to be 28 by 28 by one so\neverything that we do from here on out\nis really to get it towards that shape\nso in order to do that we're going to be\nusing the conf 2d layers and we're also\ngoing to be using the up sampling 2d\nlayers to get us to 28 by 28 by 1. so\nlet's add in a bunch more layers\nokay that is our first up sampling block\nso again\nthe up sampling block is effectively\ngoing to double this spatial quality so\nwe should be going\nto\n14 by 14 by 128 we're then passing it\nthrough a convolutional layer which\nshould effectively condense it down a\nlittle bit i believe oh well it's\npadding same it might not let's actually\ntake a look at that in a second but uh\nso our convolutional neural network\nlayer is going to be have 128 units\nthat's going to preserve the number of\nchannels our kernel size is going to be\nfive by five and our padding is going to\nbe the same so uh it should effectively\nnot crop but we will see actually it\nwill crop because we're using a bigger\nfilter but we will see in a second and\nthen we are passing through our\nactivation which is going to be a leaky\nrelu again so let's run this\nso now okay so it didn't crop so now our\noutput is now 14 by 14 by 128.\nif we really wanted to we could actually\njust apply the exact same up sampling\nblock so we could literally just take\nthis again\nand\nchange the number of filters here let me\nactually show you rather than talking\nabout it right so if i grab this pasted\nthat there and said rather than having\n128 kernels here give me one\nboom boom boom\ntake a look that is our output layer now\noutputting the same shape as an image so\nwe could actually stop there but there's\nnot really enough typically you want to\nadd in a bunch more layers for your\nconvolute for your generator because\nthis gives it a little bit more\nsophistication in terms of actually\nbeing able to generate something which\nis sufficiently complex to look like a\ngenerated image so we are not going to\ndo that just yet we're going to add in a\nbunch more layers in between there to\ngive us a little bit more information\nokay so what we've gone and done is\nwe've gone and added an upsampling 2d\nblock which effectively doubles the size\nof our output from our previous layer so\nit's literally going to go 14 let's\nactually show you let me show you so if\ni actually deactivated those two\nadditional layers boom boom boom so it's\nliterally going to take that layer and\nit's going to double it by two so it's\nperforming up sampling so or not\ndoubling but it's actually\nexpanding the values that it's got and i\nbelieve it's just duplicating it by it's\ntaking the value next to it and sort of\ncopying it over um but then we apply it\npass it through a convolutional 2d\nneural network to then condense that\ninformation and provide a little bit of\nparametric values or parametric\ntransformations to it because you can\nsee that there are no parameters for our\nupsampling layer\nright so let's add our convolutional\nlayer so boom boom boom\nnow we've actually got some parameters\nthat our neural network can learn to be\nable to generate a better set of values\nfor our output\nand then we're passing through a leaky\nrelu layer so this gives us activations\nboom so that's our leaky reality so\nwe're now gone up to 1 million or 1.2\nmillion different parameters for our\ndeep neural network and we're getting\ncloser to what our output image needs to\nbe which is 28 by 28 by 1.\nokay let's add in another upsampling\nblock and we'll go from there we'll see\nwhere up to\nokay so i've gone and added three more\nblocks there so let's just say that's\ndown sampling block one even though\nreally isn't down sampling\nso i've literally gone and copied this\nup sampling block down here so it's\nagain just copying the exact same like\nlayers which will give us the output of\n28 by 28 by what is 128 we're then\napplying another set of convolutions and\na leaky relu and again we're applying\nthe same padding so it really shouldn't\njust change the size of our output again\nwe're going doing the exact same so this\nis really just giving us\nmore\nknowledge or the ability to pass you\nhave more parameters in our deep neural\nnetwork to learn a little bit better so\nreally it should be 28 by 28 by\n128 in terms of our output so if we go\nand run these boom boom boom\nright so we're now at 28 by 28 by 128\nwhich means that all we really need to\ndo is add a final convolutional neural\nnetwork layout\nconvlayer to get to one channel\nso let's add that final layer and that\nshould be a neural network or at least\nour generator built\ncool so that's our final layer so this\nis going to remember we want it to be\none channel so previously we would have\nhad 128 channels based on all the\nkernels that we had in our convolutional\nneural network layers because we now\nhave one filter we're only going to get\none channel out in our image or our\ngenerated image but remember we\npreserved the shape so we had 28 by 28\nand this was really just by passing it\nthrough two different up sampling layers\nwe went from 7 to 14 14 to 28 which now\ngives us so if we go and run this once\nmore\nwe are now at 28 by 28 by 1 with a model\nwhich has 2.15 or 2.16 million\nparameters so we're now looking good so\nthat is our generator now built so we've\ngone and instantiated the sequential api\nwe then went and did a little bit of\nreshaping from our input dimensions of\n128 random pixels went to 7 by 7 by 128\npassed it through an unsampling block\nwhich went to 14 by 14 by 128 passed it\nthrough another upsampling block which\ntook it to 28 by 28 by 128 and then we\nwent and passed it through some\nadditional it's not really down sampling\nlet's just call this a convolutional\nblock\ni don't like telling you it's a down\nsampling block and it really isn't\nbecause down sampling would mean we\nreduce that spatial quality possibly\nbump out the number of channels we're\nnot doing that right so then we're\ntaking it through two up sampling blocks\npassing it through two convolutional\nblocks and we're maintaining the spatial\nshape because we're setting padding\nequal the same\nso two of those which means that we're\nreally just gonna\nhave the exact same values that it's not\nexact same values but the exact same\noutput shape would be 28 by 28 by 128\nand then we are passing it through to\nour final layer which is going to reduce\nthe number of channels so model.add\nwe're passing through a convolutional 2d\nneural network it's best find that we\nonly want one filter or one kernel uh\nthe kernel size is going to be four by\nfour we're specifying padding equals the\nsame because that means we're going to\npreserve that 28 by 28 and we've got an\nactivation of sigmoid because we\nactually want our values to be between 0\nand 1. this is what's going to allow us\nto generate an image so if we go and run\nthrough this now\ntake a look our neural network is 28 by\n28 by 1. so that is a generator now\nbuilt now we can actually go and test\nout our generator so let's minimize this\na little bit so we can actually go and\npass through some random values to our\ngenerator and we should be able to\noutput some stuff so let's try it out\nawesome so that is our generator now\ngenerating values and effectively images\nso\ni went and renamed the model to be equal\nto generator so that's going to be the\nname of that model over here so\ngenerator equals build underscore\ngenerator we can take a look at our\nsummary as we did before and then to\ngenerate an image we can pass through\ngenerator.predict so use the predict\nfunction and then pass through those\nrandom values so here we're actually\ngenerating four different random images\nso i've used numpy so\nnumpy.random.rand so this generates a\nset of numbers using a normal\ndistribution\nso we're passing through how many images\nwe want to generate so four and then\nwe're passing through those 128 random\nvalues which is what our model expects\nto be able to generate some images and\nthen i've just printed out the array\nwhich is over there now rather than\nvisualize or just looking at the numbers\nwe can use matplotlib again to actually\nview what our random images look like at\nthe moment so we can actually take\nthe\nvisualization script that we had from\nover here so we can copy that and we can\nactually just tweak it a little bit to\nbe able to plot this out so we can take\nso this bit is going to be the same we\nare we're actually looping through each\nof our images so let's say um for\nuh idx comma image in\nenumerate\nimage and then let's just double check\nwhat our image shape looks like so we\nare 28 by 28 by one okay so that should\nwork so this will grab one image one\nindex we can get rid of the\nsampling from our data iterator we are\njust going to be passing through our\nimage here\nimage\nindex should be preserved that's fine we\ncan pass through our index here\nlet's see if that works\nthere we go so those are the images that\nwe've just gone and generated so all\ni've done is i've taken the exact same\nvisualization script that we wrote right\nup here\nand i've gone and adjusted it so that we\ncan now visualize the images that we're\noutputting from this\nfrom our generator model really so what\nwe're doing is we're looping through\neach one of our images so for index\ncomma image and enumerate our images so\nthis is going to give us an index and\nthe actual value from that specific\narray we're then using the specific axes\nfrom our plot.subplots function and\nwe're using numpy squeeze to squeeze\ndown our values because remember it's 28\nby 28x1 by using numpy squeeze we're\ngoing to get it by 28 by 28 so it just\nmakes a little bit easier to visualize\nand then we are plotting out each one of\nthose images so we can see 0 1 2 3. so\nthat's what they look like at the moment\npretty crap but once we actually train\nthese this will actually be able to\ngenerate a better set of visualizations\nand better set of fashion components so\nif we go and run this again\nwe should be able we can get rid of this\nimage because that's causing a nightmare\nwe can delete the shape boom so that's\nour next set of images so bang bang\nthat's our next set\nnext set right so i can actually grab\nthis\nput it into here\nyou can say generate\nnew fashion\nso right now it's looking pretty crap\nbut that's because we haven't changed it\nright so new images new images new\nimages and you can see it's generating\ndifferent types of stuff because we are\nrandom sampling or producing a random\nsample to pass through to our initial\nneural network so\npretty cool at least to begin with so we\nare definitely going to make this a\nwhole heap better let's enable scrolling\nfor that\nthis is obviously going to get a whole\nheap better eventually\nonce we've actually gone and trained it\nbut that is our generator now done which\nis one of the hardest parts of actually\nbuilding a gan model so we've gone and\ndone a ton of stuff so first up we've\ngone and imported our different modeling\ncomponents we've gone and built our\ngenerator remember we started off with a\nunique layer which takes in some random\nvalues and reshapes it to give it some\nspatial qualities remember our reshape\nlayer did that we then went and used up\nsampling to expand the size of our image\nso we went we actually doubled the size\nwent and applied some convolutional\nneural network layers to give it a bit\nof\ngive it some trainable parameters and\nthis allows us to give a little bit more\ninformation to what that output actually\nlooks like\nalong with the leaky rallies so what\nwe've then gone and done is pretty much\nused those same\nbuilding blocks so here here to be able\nto increase the size from seven by seven\nby 128 to 28 by 28 by 128\nwe've then gone and added a couple of\nadditional convolutional neural network\nlayer blocks and then last but not least\nthis last layer really really important\nreduces it to one channel which is the\nsame size as our input data we've then\nalso specified an activation of sigmoid\nto give us a value between zero and one\nokay that is our neural network looking\npretty good the next thing that we need\nto do is build our discriminator funnily\nenough this is probably the easier part\nof building and working with gans\nbecause the discriminator is really just\na image classifier that is determining\nwhether or not the image is real or fake\nso if you've watched my image\nclassification tutorial it's really no\ndifferent to that we're just by adding\nin a bunch of dropout layers to increase\nthe regularization because image\nclassification is a pretty easy task for\nneural networks generating new stuff is\na little bit trickier so we want to make\nit a little bit harder for a\ndiscriminator to learn so\nwhat we're going to do is create a\nfunction def build discriminator\nand we are going to effectively do a\nsimilar thing to what we did for our\ngenerators we're going to instantiate\nour model using the sequential api\nand eventually we're going to return our\nmodel and then just to make it a little\nbit easier to see what we're actually\nbuilding let's let's set up a similar\nthing to what we did for our generator\nso we're going to create an instance of\nour discriminator so discrim inator\nequals build\ndiscriminator\nand eventually once we've gone and\nconstructed it we'd actually be able to\ntype in a discriminator\ndot summary right but right now these\ntwo lines aren't going to work because\nif we are going to run that first up\nit's not defined\noh it actually will work so this line\nwon't work because we haven't actually\ngone and passed through any layers to\nthat okay so the first thing that we\nactually want to go ahead and do is\nbuild up our discriminator so let's go\nand do it\nokay that is the first convolutional\nneural network layer block\nso let's take a look so we've gone and\nridden three additional lines there so\nwe've added a convolutional neural\nnetwork layer a leaky relu activation\nand a dropout layer so the convolutional\nneural network layer is going to have 32\nfilters so model.add conf 2d passing\nthrough 32 filters the filters are going\nto have the shape of 5x5\nwe're not specifying padding equals the\nsame so this means it's going to start\ncondensing down the information that\nit's getting\nspecifying our input shape is the same\nas the output shape of our generator so\nour generator is going to generate an\nimage of 28 by 28 by 1 we then pass that\nto our discriminator to determine\nwhether or not it's a real or fake image\nwith the end goal for our generator to\ngenerate images which our discriminator\nthinks are real so it's a bit of a\ntrade-off which we'll sort of tweak in\nour training loop to get to work so the\ninput shape is going to be the size or\nthe output shape of our generator so 28\nby 28 by one\nadding through a activation layer which\nis going to be a leaky reality leaky\nrally is actually recommended practice\nwhen building uh gans or at least\nin a lot of them that i've seen they're\nusing leaky releases activations rather\nthan just a straight relu or sigmoid or\ntan h\nokay so model. add leaky relu specifying\n0.2 as the input parameter and then\nwe're passing through dropout\nbecause we want it to make we want to\nmake it a little bit harder for our\ndiscriminator to learn so dropout\neffectively applies some regularization\nso model dot add dropout and we've\nspecified the positional argument for\nthat as 0.4 so that's the strength of\nthe dropout so that's our first\nconvolutional block and if we take a\nlook once we run through those two new\nlines so to build the discriminator we\ncan use the build discriminator function\nand then discriminator.summary allows us\nto see what it looks like so right now\nour shape is 24 by 24 by 32 and that is\nwhat we're outputting and we've got 832\nprime so a very small model at the\nmoment but we're going to build on this\nso\nlet's add in a bunch of additional\nconvolutional neural network layer\nblocks and then get this down to what is\neffectively an image classifier\nokay so i'm adding a second\nconvolutional neural network layer block\nand it is pretty much identical to the\nfirst one the only difference is i've\ndropped the input shape because we're\nnot we don't need to define the input\nshape anymore because it's not the first\nlayer or first hidden layer\nand i've bumped up the number of filters\nto 64. i'm probably going to copy this\nagain and create another one with 128\nand then let's take a look at what that\nshape looks like\nthis is going to be our third con block\nso we're now down to 16 by 16 by 128 so\nwe're getting closer to just having a\nsingle output value which is a sigma\nwhich has what would it be\nb sigmoid so zero to one\nokay so we're getting there so let's go\nand i'm going to add in a bunch more\nlayers but basically it's going to\nfollow this similar pattern we're then\ngoing to flatten it down and then output\na single value using a dense layer so\nlet's do it\nokay so that is my fourth convolutional\nblock i don't think i'm going to add in\nanother one i think we're good for that\ni'm just trying to mimic the exact same\nmodel that i pre-trained earlier so you\nguys can use the trained weights that\ni've got so that is so effective we got\nfour different convolution blocks so the\nfirst one with 32 filters second one 64.\nthird one with 128 last one with 256 now\nwhat we're going to do is we're going to\nflatten this model actually let's take a\nlook at our apple first so if i go and\nrun this run this run this\nwe have an output of 12 by 12 by 256 so\nby flattening it we'll actually get 12\nby 12 by 256 being passed to a dense\nlayer let's do that so flatten\nthen pass to dense layer\nso model dot add and then we're going to\npass through a flattened layer it should\nflatten it down we are then going to\npass through a dropout layer so\nmodel.add\ndrop out\n0.4\ni wonder if it's best practice to add a\ndrop out after a flat anyway we're gonna\ni've added it in the pre-trained model\nso we kind of need it now to add might\ntake a look at that let me know if you\nget better performance by removing that\nlayer but for now we're going to leave\nit so we are now going to output a\nsingle value and our activation\nis going to be a sigmoid because we want\nthe output value to be between zero or\none so one i can't remember the way i\nconfigured it whether or not one is\nrepresenting a false let me just double\ncheck so\nyeah so one represents a false image or\none is eventually going to represent a\nfalse image 0 is going to represent a\ntrue image so again you can play around\nwith that anyway\nactivation equals sigmoid\nand that looks good\ni think that's our model done\nyeah cool all right so that so you can\nsee that we're originally taking a image\nwhich is of the shape 28 by 28 by one\nand then we're finally going all the way\ndown to the bottom and we're outputting\na 1 which will be a value between 0 and\n1 to represent true or false whether or\nnot the image is fake or real one being\na fake image zero being a real image so\nand again different implementations run\nthese different ways one might be a real\nimage one might be a fake image it very\nmuch depends on\nhow the model is actually set up now we\ncould actually take\nthose images that we actually generated\nfrom our generator right this is what's\ncool about it you can prototype some of\nthis you can take this image over here\nand pass it through to our discriminator\nand it should output a zero one so if i\nwent discriminator\nlet me bring this up this grim in nato\ni can never type this dot predict and\npass through image\nuh we've got issues all right so uh\nnegative dimension size chords by\nsubtracting one what have we gone and\ndone there\ndo we need to pass through dot predict\nno we've got an issue all right let me\ndebug this\nwait we've got\none image inside of image.shape so\nthe model is okay wait i thought we had\nbatched this so for\nimage\ndot shape\nokay that should work so i'm guessing\nthat we're getting an error because\nwe're passing through a single so\nit should be four images now\nokay there we go that might works much\nbetter so don't predict\nokay makes more sense so the reason that\nwe're getting that error is because\neffectively we're trying to run a single\nimage through to\nthat neural network which is going to\nthrow an error so it's effectively like\ndoing that\nso you can see that that's our image if\nwe take a look at the shape it's 28 by\n28 by run if i run that\ntypically when you're passing through an\nimage or a single image to a deep neural\nnetwork you it expects a batch number to\nbe first so because this is a single\nvalue it's a\nsingle image in a batch so we'd actually\nneed to type in np dot expand\ndims\npass through the parameter zero\nfixed so now we can generate a\nprediction on a single image which you\ncan see 0.5 so it's\nnot very confident whether or not this\nis real or not but again we're going to\ntrain a better generator once we get\nthrough that now if you've got more than\none image you don't need to go and wrap\nit in expand dims you can actually just\ngo and do this so if i go and generate a\nnew set of images\nfrom up there and then run it through\nour model\nrun it through the dot predict function\nshould get no errors boom take a look at\nthat pretty cool right so that is our\ngenerator and our discriminator now\nbuilt and you sort of saw how i'm\nbuilding up these layers to ensure that\nour input and output are of the correct\nshape so remember our generator is going\nto take a set of random values which in\nour case are going to be 128 random\nvalues and it's going to output an image\nor a\nvector or a matrix of the shape 28 by 28\nby 1. now our discriminator is going to\nsort of do the opposite of that it's\ngoing to take the output of the\ngenerator so 28 by 28 by one and it's\ngoing to output a single value between\nzero and one to determine whether or not\nit's real or fake\ncool okay that is our set of generators\nnow done let's go jump back on over\ngenerators and discriminators are done\nlet's go and have a chat to our client\nso that's done what about the custom\ntraining loop ah that's one of the most\nimportant parts when building gans we'll\nwrite a custom training loop to train\nboth the generator and the discriminator\nsimultaneously ah got it let's go so\nwe're on the home right now but this bit\nis pretty hardcore i wanna disclose that\nbut we're gonna take it step by step so\nthe training of gans is notoriously\ndifficult and nobody actually really\ntells you why it's difficult but i'm\ngoing to give you my feedback so the\nreason why it's difficult is you need to\nfind a balance between the speed at\nwhich the discriminator trains and the\nspeed at which the generator is able to\nlearn so a big thing that we need to do\nwhenever we're training these side by\nside is ensure that our discriminator\ndoesn't train too fast so that it's able\nto smash out and know what a fake image\nis every single time so balancing that\nis uh really really difficult and is\nreally really important to ensure that\nyou get that right one way that we help\ndo that is we actually inject some\nrandom noise into the outputs from the\ndiscriminator almost to kind of like\ntrick it and slow it down a little bit\nnow\nthe other thing is that it takes quite a\nfair bit of time to train one of these\nthat is actually generating good output\nso um if anybody goes hey you can train\none of these in like 10 minutes then be\nvery very skeptical that's just my\nfeedback and again you might get\ndifferent output from some experts out\nthere in the field um but that that's\njust my experience so just keep those\ntwo things in mind but we're going to\ntake this step by step and go through it\nso\nconsult constructing a training loop now\nwhat i mean by training loop is so\ntypically when you go and train a deep\nneural network using tensorflow or\npytorch you can typically there's a\ncouple of key steps right so you\ninstantiate the model so maybe create uh\nmodeled equal sequential and then model.\nadd a bunch of layers then model.compile\nwhich actually\nassigns a loss function and an optimizer\nand then model.fit which is actually\nwhat goes and trains the model now\nbecause discriminators because scans\nhave two components or a generator and a\ndiscriminator you actually need to train\nthem side by side which means we can't\njust use dot fit we need to do something\na little bit more tricky to ensure that\nwe get this to work so\nwhat we're going to do is define our own\ncustom training loop and by training\nloop we're just changing what the dot\nfit function does by defining our own\ntraining step so we're going to do that\nover here and we're going to do it using\na subclass model you can also do it\nusing\nat tier function decorator there's some\ngood documentation on the tensorflow\nwebsite on that i'll link to it below\nbut for now we're going to do it this\nway and as per usual all this code is\ngoing to be available by github so don't\nstress if you don't get it you can go\nback through and take it at your own\npace\nokay first thing we need to do is set up\nsome losses and optimizers so um in\norder to do that we actually need to\nbring in some losses and optimizers so\nlet's do that we're going to use the\nuh what i think we're going to use\nwhat is it binary cross entropy\nthink we're actually going to use binary\ncross entropy for both\nbecause yeah now we'll come back to that\nwe're going to use binary cross entropy\nand we're going to use the atom\noptimizer so first thing let's uh let's\nimport those\nso i'm going to import some optimizers\nfrom keras so from tensorflow.optimizers\nuh we are going to import adam and then\nfrom tensorflow.keras\ndot losses\nwe are going to import binary cross\nentropy\ncool so our atom optimizers\nis going to be the optimizer\nor both\nand binary cross entropy\nis going to be the loss for both\nnow you're probably thinking nick what\nthe hell how are you going to use binary\ncross entropy when your generator is\noutputting an image well what we\nactually do with that image is we pass\nit through to our discriminator and we\ndetermine whether or not a generator has\nactually produced an image which is able\nto trick the discriminator so we\nactually reward it for\ngetting our discriminator to produce the\nwrong value which is kind of\nnot intuitive but once you actually play\nwith these a bunch of times you'll\nactually be like oh that's actually kind\nof cool so we're actually rewarding our\njust generator for tricking our\ndiscriminator and we're rewarding our\ndiscriminator for getting it the right\none well determining what type of image\nit actually is\ncool all right so those are our losses\nand our optimizers for from tens photo\ncarousel optimizes import atom you can\ntry stochastic gradient descent or\nanother type of optimizer atom is\nprobably what i use most of the time\nand then for our losses so from\ntensorflow.losses import binary cross\nentropy pretty cool right like so if you\nactually go and build some other i know\ni'm veering off into the wayside but\nwhen i was actually building the super\nresolution\nmodel i actually had to use a\ncombination of different types of loss\nfunctions so i actually use mean squared\nerror for the\nperceptual loss and that is how\ndifferent the actual values are from a\nreal super resolution image versus and\nnot a super resolution image and what\nelse did we use um perceptual loss\ni think i used no i didn't use binary\ncross entropy but i used a combination\nof a bunch of different types of losses\nso we actually took the discriminator\nloss versus plus\nthe perceptual loss as the the final\nloss function but again i'll probably go\nthrough that once we actually go through\nthat model okay\nso that is our we've imported our\noptimizers in our losses now we need to\ncreate instances of them so we're just\ngoing to create a sample of examples so\nwe'll go g opt\nequals atom\nand we're going to set the learning rate\nequal to 0.0001\nso our learning rate for our generator\nis going to be faster than the learning\nrate for a discriminator because we\ndon't want our discriminator to go too\nfast and\nabsolutely smash our generator and this\nis part of the learning process right so\nyou might need to tweak this if you're\ndoing different use cases but i found\nthat this has worked\nall right so we're defining our two\noptimizers so i've typed in g opt equals\natom and i've specified a learning rate\nof 0.0001\nand then for our discriminator i've\ncreated a\nagain an optimizer so d opt equals atom\nand then our learning rate is zero dot\nzero z so there's just one extra zero so\nzero zero zero zero one cool and then\ni'm gonna define my losses so g loss\nequals binary cross\nentropy\nand then d loss\nalso equals binary cross\nentropy\nboom\nokay those are our losses and our\noptimize is now set so again the reason\nthat we're specifying the learning rate\nfor the discriminator slower is just so\nit doesn't blow out and learn way too\nfast and cause our generator to lose the\nplot\nand we've also gotten to find our losses\nso that is step 4.1 now done so let's\nset up our losses and optimizes now as a\nbig boy so setting up the subclass model\nso first thing that we need to do is\nimport the model class from keras so\nfrom\ntensorflow\ntensorflow\ncan't type tensorflow.keras dot models\nimport model and this is the base model\nclass so once you get to this level you\nyou've got a ton of control over what\ntensorflow does so you can do a ton of\nstuff if you want to so\nthis is importing the base\nmodel class to subclass\nour training step\ncool all right so i've ridden from\ntensorflow carousel models import model\nnow when you're defining new models\nthere's basically like three\ntwo keys functions so we're gonna create\nagain so class fashion again\nyou can name it whatever you want and we\nare going to pass through our base class\nand then there's a couple of key methods\nso again method is inside of class so\nthere's a couple of key methods that you\nneed to define when you're subclassing a\nmodel so def\ninit\nbecause we need an initialization\nfunction\nwe'll come back to that\ndef train step\nand this is what actually does our\ntraining\nyou can also define and so keep in mind\ntrain step is called when you call the\ndot fit function right so this is really\nreally important to note if you wanted\nto run an evaluation function you'd also\nwrite create a def test\nstep method we're not going to use that\nbut just a key thing to note we also\nneed to have a compile method so def\ncompile\nand again we'll come back to that so\nthose are the key methods that we\nactually need to define in our subclass\nmodel so and the reason that we're going\nto define a subclass model is because\nwe're going to do some fancy crap inside\nof this test step model or test step\nmethod\nthe other way that you'll see people do\nit is they'll actually write um at\noh gosh\nat\ntf function\nand then they'll actually write a custom\ntest step or custom train step\nthey'll do it sort of like that and then\nthey'll do a whole bunch of other stuff\nbut um for now i'd just like a little\nbit more control with the subclass model\nbut just keep in mind that you've got a\nfew different ways to actually go about\ndoing this\nokay so first thing that we need to do\nis go and pass through some\nparameters to our init method so let's\nactually go ahead and do this\nalrighty those are our parameters that\nwe're going to be passing through to our\ninit method so we're passing through our\ngenerator passing through our\ndiscriminator we're passing through any\npositional arguments in any keyboard\nargument so this gives us a little bit\nof flexibility if we want to go and\ninherit from our base model then we're\ngoing to run super\ndot init\nand we're going to pass through any of\nthose arguments\nso again if we want to go and use some\nof the base functionality inside of our\nkeras model we've got the ability to do\nthat now so we've passed through those\narguments and keyword arguments to our\nbase model\nto our base class effectively then the\nnext thing that we're going to go ahead\nand do is set up two attributes for our\ngenerator and our discriminator so that\nmeans that we're going to be able to\nrefer to them inside of the model so\nlet's actually um so create\nattributes\nfor\ngen and discriminator\nself dot\nwe'll call it generator\nequals generator\nself does disgrim in nato\nequals discriminator\nuh we don't we can get rid of the pass i\nthink that's it for our init method yeah\nthat's pretty much it so we've gone and\nridden what have we gone and done so\nwe've gone and passed through\nour generator and discriminator model so\nwe'll instantiate those before we\nactually go or we'll instantiate those\nthen pass them to our subclass model\nand then we're passing through any\nkeyword arguments and any positional\narguments to our base class so this um\nplus through\nargs and kw args to base class\nand then we're creating attributes for\nour generator and discriminator so that\nis our init method now done now the next\nthing that we need to do is actually go\nand compile so to this we're going to\nneed to pass through our optimizers and\nour losses and we're going to need to\nrun super.compile so let's do that\ncool all right so that is our those are\nthe input arguments that we're going to\nbe passing through to our compile method\nso we've got def compile and then we're\npassing through our generator optimizer\na discriminator optimizer a generator\nloss and our discriminator loss we're\nalso going to be passing through any\narguments and keyword arguments we might\nneed because we're going to call\nso compile\nwith base class\nso we're going to run super dot compile\nand we are going to pass through our\narguments and our keyword arguments\nboom okay and then we're going to create\nattributes for each one of these so\nattribute for our optimizers and\nattributes for our losses so let's go\nand do that\nokay that is our compile method now done\nso\nlet's quickly recap so we've gone in to\nfind our knit method and our knit method\ntakes in two key things right takes in\nour generator and our discriminator\nmodels that have already been\ninstantiated\nyou can ignore the super bits for now\njust know that they're necessary evils\nright so we're going to be passing\nthrough any additional arguments that we\nwant to pass through when we create this\nmodel via that\nso then we're creating attributes for\nour generator and our discriminators\nwe're going to be able to refer to them\nas self.generator self.discriminator\nlikewise we're doing something similar\nfor our compile method so we're passing\nthrough our optimizers so g op d op g\nlost d loss and remember we've already\ncreated those over here right so we're\neffectively passing through atom atom\nbinary cross entropy binary cross\nentropy\nand then we're creating attributes for\nthose inside of our\nfashion gain class and again this could\nbe a different you could name it\nwhatever you wanted to it's just the\nnaming convention that i've gone and\ntaken or the fashion game\nokay so those are the\ntwo\neasier bits now done so we've gone down\nthe init method and the compile method i\nknow it's always a little bit painful so\nall right now we're going to get onto\nthe big boy so\nfor our train step the first thing that\nwe need to take in is our batch and this\nis a batch of data keep in mind a batch\nof data in our case is going to be 128\nimages of the size 28 by 28 by 1\nand that has been defined\nright up over here so when we went and\ncreated our data set up there in step or\nsection two\nnow what we need to do is actually do\nsomething with that data and that is\nexactly what we're going to do inside of\nour train step\nso the first thing that we're going to\ndo is we're going to get a batch of data\nso we'll call it real images\nequals batch\nso this first step is get\nthe data\nright so we're getting first things\nfirst we're getting a batch of real\nimages and then we're going to get a\nbackup batch of fake images or generated\nimages right\nand this is going to be self.generator\ntf.random.rand i will just call it dot\nnormal\nand this is effectively what we did\npreviously right so we went and created\nsome random images\nremember we used mp.rand.randn\nlet me show you\nhow did we do it it was\nno that's the data iterator\nwe did it over here so we called\nmp.random.random\npretty much just the equivalent of that\nwith using the tensorflow library is\ntf.rand.normal\ntf.rand is it random random.normal\nand we'd be passing through i don't know\nlike 6 comma 28 by 28 by 1.\ni can do it probably needs to be wrapped\nthere you go so what we're doing is\nwe're effectively generating those\nrandom input values that are going to be\ngoing into our generator right so again\nthese actually wait it's not 120 it\nshould be um six by 128 by 1.\nthose are random values so we're passing\nthrough random values and we're sending\nthose through to our\ngenerator model to be able to generate\nrandom values or generate random images\nto begin with\nand this is because we need real images\nand we need fake images in order to go\non ahead and train our model so let's go\nand do that\nso we are going to generate let's\ngenerate 128 so we've got the same\nnumber of images in our real images as\nour batch so 128 by 128 by one\nand we can close that and then we're\ngoing to specify training equals false\nbecause our deep\ngenerator is not training at the moment\nwe're just making predictions\nokay so that is that now the first thing\nthat we're going to do is train our\ndiscriminator so train the discriminator\nso there's three key steps that we need\nto do in order to go and train our\ndiscriminator so the first one is we are\ngoing to\npass the real\nand fake images\nto the discriminator model\nwe then need to\ncreate labels\nfor real and fake images\nwe are then going to pass through\nor add some noise\nto the outputs\nand this is what helps it from learning\nway too fast we then calculate the loss\nand then apply back prop\npropagation and this is what effectively\nallows our neural network to learn right\nand i'm just going to tab these in okay\nnow in order to actually go and train\nour discriminator we need to start\ncalculating our gradient so we can do\nthis using the tf.gradient tape method\nso let's go ahead and do this so with\nso that is setting up our gradient so\nthis is going to start calculating uh or\nmonitoring each one of our functions the\nnext thing that we want to do is\nactually this next step so pass the real\nand fake images to our discriminator\nmodel so let's do that\nokay so those are the initial\npredictions right so we've gone and\nwritten three lines of code there so\nlet's say break that down\nso the first line is actually taking our\nreal images and we're just passing that\nthrough to our discriminator and we're\nstoring the values inside of a variable\ncalled y hat real so y hat real equals\nself.discriminator we're taking these\nimages from over here and we're passing\nit to the discriminator model that's all\nwe're really doing we're passing through\ntraining equals true because this means\nthat the dropout is going to activate\nright so if you pass through training\nequals false dropout layers aren't going\nto run\nthen we're also doing the same for our\nfake images so y hat fake equals\nself.discriminator we're passing through\nour fake images from over here and again\nwe're specifying training equals true\nand then we're combining it into one set\nof outputs so y hat real fake equals\ntf.concat so this just concatenates them\nboth together and we're going to be\npassing through our y hat real and y hat\nfake so that and that and we're\ncombining on the zero axis so it's going\nto combine all the samples together\ncool so that is the first step now done\nso we've actually gone and produced\npredictions as to whether or not our\ndiscriminator thinks the images are real\nor fake the next thing that we need to\ndo is actually create labels because\nremember this is effectively supervised\nlearning problem at the moment we're\ndetermining whether or not they're real\nor fake zero or one so let's go and do\nthis\nokay so those are now our predictions so\nwe've actually gone and assigned the\nlabels and really you could go and\npre-gen well actually can't really\npre-generate this because we're\ngenerating new predictions each and\nevery time but that's fine so we've now\ngone and created a new variable equal y\nunderscore real fake so this is the\nequivalent of this over here right but\nthese are the predictions from a\ndiscriminator these are the actual\nlabels so why underscore real fake and\ni'm just setting the order so you know\nwhat order they're in so in this\nparticular case again we're creating a\nset of labels for our real images so and\nthe real images are going to be zero\nso really our discriminator is all about\nspotting fakes and we've set that equal\nto tf dot zero's like and we pass\nthrough y hat reel the cool thing about\nthis is it effectively goes and\ngenerates a set of zeros based on the\nshape of your input so in this\nparticular case we're passing through\nour y hat image or y hat real values and\nwe're saying all of these should\nactually be zero so it's going to be in\nthe same shape as our real or our y hat\npredictions but they're going to be\nzeros now we're going to do the same\nthing for our y hat fake so tf dot ones\nlike we're passing through y hat fake so\neffectively we're gonna have 128 zeros\nand 128 ones over here and those are\ngoing to be the labels for our or the\nreal labels for our predictions from our\ndiscriminator so think think of this as\nyour true y values\nand we're going to concatenate both of\nthose together using tf.concat so our\nfinal value is going to be y underscore\nreal fake\nokay so we've now gone and made our\npredictions we've now gone and created\nour labels and let me just quickly show\nyou what xero's like actually does so if\ni actually go and wrap this set of\nactually let's not do it with the that's\nour latent variables so let's just\nenable that um so what would it be it\nwould be\num\nit'd just be like something like that\nright\nso to be predictions like that so if you\ncan see that we've got a bunch of random\nvalues right so if i type in tf.one's\nlike\nand pass that\nyou can see it's generating a bunch of\nones but in this same shape over here\nnow if i did tf 0s like\nit's generating a bunch of zeros now if\ni go and concatenate those so tf.comcat\nthat and if i put this inside of an\narray\nand let's say one's like\nand let's close that array and then type\nin dot access equals zero\nso you can see we've just got a massive\narray and we've got a bunch of zeros and\na bunch of ones so that is effectively\nwhat we've gone and done in this line\nnow what we need to do is add some noise\nto these outputs and then we can\ncalculate loss and apply back prop and\nthat's the discriminator training\ncomponent now done\ncool so that's our noise added to our\nwhite white true value so in this\nparticular case we've gone and added a\nrandom value between and we've\nmultiplied it by a weighting of 0.15 for\nour real images and negative 0.15 to our\nfake images so i played around with this\na bunch when i was actually testing this\nout\nand i found that adding values to our\nzero values and reducing values from our\none values seemed to work a little bit\nbetter otherwise it threw our loss\nmetrics out so um i've you typically see\npeople just adding a random value to\nboth not necessarily adjust based\nwhether or not it's a\nzero value or one value so true or false\nso in this particular case i've gone and\ncustomized this a little bit but let's\ngo through these lines so there's three\nlines here so noise underscore real\nequals 0.15 multiplied by tf dot random\nnot uniform so it's a uniform\ndistribution\nand we are adding the or creating the\nvalues in the shape of y hat real so\nwe're going to match that and then we're\nadding noise for our fake values so\nnoise underscore fake equals minus 0.15\nmultiplied by tf.random.uniform\nand then we're passing through the same\nshape as our tf as our y hat fake values\nfrom over here\nthen we're adding it to this value so\nwe're adding it to our y true values so\nwhy underscore real fake plus equals\ntf.concat so we're effectively just\nconcatenating both of these values back\nso plus equals tf.concat and then\npassing through noise real noise fake so\nthat is injecting those noise into the\ntrue outputs\ncool all right so that is that now done\nnow all that's left to do is actually\ncalculate our loss and apply a back prop\nso let's calculate our loss so total d\nunderscore loss equals self dot d\nplus which is going to be binary cross\nentropy remember binary cross\nand then we want to be passing through\nour real\nthat's why real fake\nnot y hat y underscore real fake\nand then y hat real fake\ncool alrighty that is our loss now\ncalculated so why real fake y hat real\nfake and we are going to calculate that\nthen what we want to do is we want to\napply back prop so we're going to go\noutside of our\ntf.gradient tape\nloop\nand we're going to first up\ncalculate our gradients and then we're\ngoing to apply our gradients using our\noptimizer so let's do this\nand then that's our discriminator trend\ncool that is the training step right\ndiscriminated now done so i know we\nwrote a ton of stuff there but again\nthis is\npretty hardcore in terms of deep\nlearning\nso the first thing that we did is we\nwent and calculated our gradients so we\nspecified a new variable called d grad\nand we set that equal to d dot or d\nunderscore tape dot gradient\nso remember d are using the tf.gradient\ntape\nfunction allows us to calculate all of\nthe operations that are happening by\nusing our deep neural network and that\nmeans that we can use that to calculate\nour gradient with respect to our loss\nthe d underscore tape.gradient and then\nthe first parameter that we pass through\nis our loss so remember this is our\nbinary cross entropy and then we want to\ncalculate the gradient for each one of\nour trainable variables so we're going\nto extract our trainable variables using\nself.discriminator dot trainable\nunderscore variables and then what we're\ngoing to do is we're going to apply our\ngradients using our optimizer so\nself.deopt because remember we passed\nthrough ad optimizer over here which was\nan atom optimizer and then we can use\ndot apply gradients\nto actually go and apply back prop and\nso to that we pass through zip because\nwe want to do our gradient with respect\nto each one of those trainable variables\nso zip and then we're passing through d\ngrad and then again all of our trainable\nvariables so it's effectively going to\nlook at the gradient for each one of\nthose variables and apply backprop using\nour learning rate\nokay that is our discriminator now done\nso i know that that's the hardest bit\njust keep that in mind that is the\nhardest bit of the training step so\nfirst thing that we went and did is\nagain we went and passed our real and\nfake images to our discriminator model\nand we went and concatenated them\ntogether pretty self-explanatory not not\ntoo crazy there we then went and created\nlabels for our real and fake images and\nremember our real images are gonna have\na zero label and our fake images are\ngonna have a one label you can tune this\nyou can do it slightly differently if\nyou wanted to switch them around to keep\nin mind you've got to change the noise\nas well and change um\ni think you can switch them around\nyou'll be you'd be okay you don't need\nto go and do anything else but just keep\nin mind for now that's how we've gone\nand set it up because it's important\nbecause of how we're actually going to\ngo and set up our generator\nthen we inject a bunch of noise so the\nnoise is really just uh being applied to\nour true outputs and this gives us um\nour discriminator a little bit less\ncertainty about whether or not it's\npredicting correctly or not so it\nconfuses it a little bit\nso we then go and pass\njust a random set of values so we're\nusing tf.random.uniform and we're\nscaling that by 0.15\nfor our real outputs because that adds\nsome values to our zero\nto our zero labels and then we're\nsubtracting 0.15 multiplied by\ntf.random.uniform\nto our fake labels we then calculate our\nloss using self.d underscore loss which\nis binary cross entropy we then go and\napply backprop using the gradient tape\nfrom up here\nand the apply gradients method from our\natom optimizer okay that was pretty\nbrutal but again it's quite difficult\nand that's the hardest bit done trust me\nthat is the hardest bit about actually\ngoing and setting up a custom training\nloop against and it's probably the\nhardest one that you'd find in a lot of\ndeep neural networks that you'd actually\nhave to go and build so congratulations\nyou just went and wrote that and you're\ndoing well all right now all that we've\ngot to do now is actually go and set up\na training step or the the training\ncomponent for our generator now so it's\ngoing to be similar but it's not by no\nmeans as complicated as what we just\nwrote so\nlet's go and do a generator now\nso again we're going to use our gradient\ntape so with tf.gradient tape\nas g tape\nnow let's actually write so let's\nactually call it uh to train\nthe generator\nokay so the first thing that we need to\ndo is generate\nsome\nnew images then what we're going to do\nis we're going to\nuh apply or\ncreate\nthe predicted labels\nand this is a little bit\ncounter-intuitive but i'll explain it\nand then again we're going to calculate\nloss\nand then over here we're going to apply\nby backprop\ncool alrighty so let's go ahead and do\nit\nso that is our new set of images now\ngenerated so i've just gone and created\na new variable called gen underscore\nimages and i've set that equal to self\ndot generator and again it's pretty much\nthe same as what we're doing up here but\nthis time we're setting our training\nparameter equal to true so\nself.generator and then we're passing\nthrough tf.random.normal\nor passing through 128 by 128x1\nso this will have 128\nrandom variables which are passed to our\ngenerator and then we'll specify\ntraining equals true now we're going to\ncreate our predicted labels now this is\na little bit counter-intuitive so over\nhere when we actually went and created\nour discriminator\nwhat we actually said was that fake\nvalues had the\nlabel of one now because we're trying to\ntrick a discriminator what we're\nactually going to do is we're going to\nset the variables for our generator here\nto zero so what we want our\ndiscriminator to actually lose out on is\nwe want it to think that our generated\nimages are actually real images so\nthat's how we're going to calculate our\nloss for our generator so you'll see\nthat in a second so let's actually go\nand do this\nokay those are the next two lines of\ncode written so i've gotten written two\nlines of code there so the first thing\nis we're running our generated images\nthrough our discriminator so predicted\nunderscore labels equals self dot\ndiscriminator and we're passing through\nour generated images from here and we're\nsetting training equals false now\nbecause we don't want our discriminator\nto be learning whilst we're actually\ntraining our generator so that's just a\nnuance of how you actually go and set\nthis up\nso that is going to generate the\npredicted outputs whether or not a\ndiscriminator is determining whether or\nnot our generated images are real or\nfalse and remember a discriminator will\noutput a 1 if it thinks the image is\nfalse or not\nso\nover here you'd expect all of these to\nbe ones but this is where we try to\nconfuse our discriminator because what\nwe're actually saying is that our\ngenerator loss\nis rewarded every time it thinks that\nour discriminator is or every time it\nthinks that our generated images are\nactually real so when we go and\ncalculate our loss total underscore g\nunderscore loss is equal to self.g loss\nand we're actually saying that the the\nreal labels are actually zeros\nso we're actually saying that\nthe real image or generated images are\nactually fake images and this is\nsomething that took me a little while to\nget my head around but this is actually\nhow you go and train the generator so\ntf.zeros like and then we're passing\nthrough our predicted labels and then\nwe're passing through what our\ndiscriminator actually predicted so\nwhether or not it actually predicted\ntrue or false and then from here what\nwe'll go and do is back prop and then\nwe'll train our generator so that it's\nable to produce better images every\nsingle time and this is that that's the\ntrick\nright trick to training\nto fake out\nthe discriminator\nbecause over time what our generator\nwill learn to do is produce the images\nwhich are better at faking\noutputs through our discriminator so it\nwill actually start getting zeros by\npassing it through our discriminator\nbecause our discriminator won't know the\ndifference between a real image and a\ngenerated image so all that's really\nleft to do is actually go and calculate\nour gradients apply back prop and then\nreturn our loss metrics then we should\nbe able to train so let's go and wrap\nthis up\nalrighty cool so we're now going and\napplying backprop this is almost\nidentical to what we did for our\ndiscriminator first up we calculate our\ngradients so g grad equals g underscore\ntape dot gradient we pass through our\nlosses the first metric we pass through\nthe variables that we want to calculate\nthe gradient for so\nself.generator.trainablevariables\nnice spelling there be a\ntrainable\nb-a-r-i it should be that this is spelt\nwrong\ndid i spell it wrong over here v-a-r-i-a\nno that's good all right so then we're\npassing through what we want to\ncalculate our gradients for\nand then we're going and applying our\ngradients using our optimizer self dot g\nopt dot apply gradients we're zipping\nthem because we want to do them both at\nthe same time so zip grad and then the\ntrainable variables so\nself.generator.trainablevariables\nall we then need to do is return\nand we haven't gone and tested this out\nwe've gone and written a lot of code and\nnot tested all we need to do is then\nreturn our loss metrics so d underscore\nloss\nis equal to d lot should be total d loss\nyeah\ntotal\nand this is important when actually\nmonitoring the model because you want to\nensure that the loss metrics are\nreducing\nsteadily to together in terms of they're\nnot one is not blowing out versus the\nother they're they're sort of reducing\nor they're actually staying stable you\ndon't really expect them to reduce all\nthat much to be honest\num so g-loss is total g-loss perfect\ni think that is done\nwho knows we're probably gonna need to\ndo a bunch of debugging for that but\nagain all this code's going to be\navailable because we wrote a ton\nthat is our full\nsubclass model so fashion again all of\nthat\nquite a ton\ndon't ask how long it took me to learn\nthis stuff\nbut i'm trying to teach it on to you all\nright cool let's uh we've got our first\nerror so in line 178 it's a predicted\nlabel so over here we've got an error\nand i'm guessing it's because i haven't\nclosed this it is\nand we've got another error so total g\nloss\nwhat's happened there so we can get rid\nof that\nso it looks like we just got misplayed\nall right cool there we go so we've now\ngone and created our subclass model so\nit doesn't look like we had any other\nsyntax errors but training errors is a\ncompletely different beast so let's\nactually go\nand now kick so we actually want to\ncreate an instance of this model and\nthen we're going to set up our callback\ntrain it and review performance so let's\nkick this off\ncool so we've gone and created an\ninstance of our fashion game class\nso create\ninstance of subclassed\nmodel\nand so what i've written is fashgen\nequals fashiongan so it's an instance of\nthis big bad boy class over here and\nthen to that what we're doing is we're\npassing through our generator and\ndiscriminator because that is what our\ninit method expects over here then we're\ngoing to compile it and pass through all\nof our losses and optimizes so let's do\nthat\nand what order are they expected in we i\nthink we can actually just copy these\nbecause we've named the variables the\nsame so uh g of d op g lost d loss g of\nthe object cool we can do that\ncool so that's a compiled successfully\nso fastgan.compile and then we're going\nto pass through our generator optimizer\na discriminator optimizer at g-loss and\nour d-loss is this still zoomed in yeah\nit is\nokay that is that done so that is step\n4.1 now done i know that took a while\nand there was quite a fair bit we can\nget rid of that that is our uh\nour custom training loop and our\nsubclass model now done now with the\ncallback i'm just going to copy this\nover from because i've used this a bunch\nof times\nand this is actually from i think it's\nfrancois chorley's\nsample i actually tweaked it a little\nbit just to get it to work\nbut i'm going to walk you through it so\nwe are going to import a bunch of\ndependencies so import os so this just\nhelps with folder navigation\nand then we're going to import the\neraser image function so from\ntensorflow.carous.preprocessing.image\nimport array to image and then from\ntensorflow.com callbacks import callback\nso that allows us to create our own\ncustom callback on epoc end so if i go\nand import that\nand then i'm not going to write this but\nyou can use this a ton of times for gans\ni'm just going to show you what you need\nto tweak so this model monitor is going\nto allow us to save examples of our\ngenerated images as we're actually\ntraining so\nthere's two key parameters that you want\nto pass through to it the number of\nimages that you want to generate and how\nbig a latent dimension should be so\nremember the latent dimension is just\nthe random values that you're passing to\nyour generator to generate a random\nimage now on epoc end it's going to go\nand do a bunch of stuff so it's actually\ngoing to generate random values it's\ngoing to pass that to our model\ngenerator and then it's going to\ngenerate a bunch of images\ninteresting that we're using uniform\nyeah\nwe could actually go and do that right\nup here but for now i don't want to mess\naround with it but i wonder who knows\nmaybe we'll do that later on um okay so\nwe're going in generating our images we\nare then going and converting it to a\nnumpy array and we're saving it now this\nis the important bit that i wanted to\ntalk about\nin terms of actually saving your images\nyou need to create a folder called\nimages and this is going to this is\nwhere these images are going to save to\nas part of this callback so inside of\nyour root folder so this is the notebook\nthat i'm currently working on just\ncreate another folder called images so\nit's literally a new folder call it\nimages\nand that is where your images were saved\nto if you wanted to save it somewhere\nelse let's say for example you wanted to\ncall it gan images\nall you need to do is change the path or\nthe folder that you want these to go\ninto in this case i'm just going to save\nit into images because that's why i've\ngot it configured but you could change\nthat there\nagain all this code is going to be\navailable and this is optional you don't\nneed to go and use this callback i've\njust found it a little bit easier when\nactually training our model so i'm going\nto create that\nand then we can train so these two are\nreally sort of optional but they\ndefinitely help when monitoring your\nmodel okay training\nso we've now gone and created our\nsubclass model created our generator\ndiscriminator we've gone and created a\ncallback\nthis is how we actually go and train so\nhere's the equals fashscan\ndot train oh actually it's not fit not\ntrained god i'm falling asleep uh and\nthen we pass through our data set\nbecause remember we created a data set\ndoes ds dot as numpy iterator\ndot next\nso that is our data set let's take a\nlook at the shape\nit's 128 by 28 by 28 by one\nso we pass through our data set and then\nwe specify the number of epochs that we\nwant to train for\ni recommend 2 000\nwe're not going to train it for 2000\nbecause it's going to take ages but\nrecommend\n2000\nepochs which is gonna take a long time i\nwon't doubt it i'm not gonna lie it's\ngonna take quite a fair bit of time\nwe'll probably train for 20 and then i'm\ngoing to give you the pre-trained model\nthat you can use as well\nif you want to use this callback you\ndon't have to but if you wanted to use\nit you can pass through callbacks\nand then pass through model monitor\nwhich is the name of this callback over\nhere cool\nalrighty cross your fingers because we\nwanted to wrote that entire training\nloop without testing it so\nthere might be some errors we'll\nactually see so if i go and run this\nwe've got an error already all right\nwhat's happening\nsequential object has no attribute\ntrainable variables so i'm pretty sure\ni've spelt that wrong and you can see i\ndefinitely have\nwhere was this line kicking up it was in\nd opt yeah i thought i wrote it wrong so\ntrainable var\nhope it's there yeah so you can see i've\ngot an error there so that's a typo\ncopy this\ncould be that v-a-r-i-a-b-l-e-s\nokay let's run that again recreate our\nfashion gain instance recompile\ngot another error and got an unexpected\nword training\nself.generator\nokay we've closed that off incorrectly\nthat's fine\num so that is here so the reason that\nwe're getting an error there is because\nwe are effectively passing training\nequals true to this tf.random not normal\nso you can see i'm missing a closing\nbrackets there that's better\nremove that one there\nokay so let's run that\nrecompile\ntry to fit now\nit's looking more promising there you go\nwe're now training\nso what you should notice is that your d\nloss and your g loss sort of balance\nthey're not go no one is going to blow\nout and or ideally you don't want one to\nblow out and you don't want one to\ndecrease really fast and you don't want\none to increase really fast you want it\nto sort of kind of stay steady and this\nis what you're kind of noticing here\nright so it's sort of bubbling around 68\n67 and likewise we're sort of getting\nthat around for our g-loss as well it\nmight increase occasionally but ideally\nyou want it to be stable over the longer\nterm so what we're going to do is we're\ngoing to let that train uh and let's\ngive it a second just to ensure that it\ngenerates some images\nbecause remember it's going to generate\nan image on epoc end\nwhich is dictated by this callback over\nhere so if i go into our images\nyou can see it was generated seconds ago\nthat is our initial image it's literally\njust this random black square with\nlittle white bits on the corners so\nnothing crazy at the moment and this is\ncommon so it'll suck\nfor a ton of epochs and then it'll get\nway better so just keep that in mind so\nyou can see where our generator is\nalready spiking not spiking massively\nbut it's definitely increasing so we'll\nlet that go and then what we'll do is\nwe'll review model performance i'll load\nup the pre-trained model and i'll give\nyou the ability to go and download that\nyourself and then we'll go from there\nwe'll be back five minutes later all\nrighty that is the beginnings of our gan\nnow train so if we actually take a look\nat the loss you can see that the\ndiscriminator started to stabilize a lot\nmore versus our generators starting to\nreduce and loss which is a good sign\nperfectly okay\nif you do run it for the full 2000\nyou'll see that they start to converge\nand they sort of reach this happy medium\nso\num that is perfectly okay as long as\nyou're not getting like one blowing out\none going massively low and so ideally\nyou don't want like what you don't want\nto see is one\nlike let's say for example your\ndiscriminators training and then it goes\nto\n0.00001 loss and the generator spikes\nout of control that is probably an\nindicator of mode collapse or the fact\nthat that just one of the training rates\nis way too high or low um and you've got\nto stabilize and you got to stabilize it\nout but uh but now it looks like it's\nokay and again i'm going to give you the\nthe pre-trained models as well the one\nthat i went and trained for 2000 epoch\nso you can test that out but before we\ndo that let's quickly go and review\nperformance and then we'll jump back on\nover to our client so\nbecause we saved our training inside of\na variable called hist we can access our\nperformance so if i type in here history\nyou can see we've got our discriminator\nloss we've got our generator loss so we\ncan actually go and take a look at those\nso let's actually create a plot\nperfect alrighty so there you can see it\nso what's happening is our discriminator\nhas sort of steadied towards let me zoom\nin on that so our generator has spiked\nup and but you can see it's definitely\ncoming back down and this was pretty\ncommon i saw this a ton of times keep in\nmind we've only trained for 28 bucks\nwhich is by no means the end state you'd\nhave to train for a ton longer to get a\nreally really good model i think i\nactually did the full 2000 epochs but\nyou can see that our discriminator sort\nof bounced around down the bottom at 0.2\nsomething what did it actually have\n0.26 would seem to be about right um\nnow a generator is coming down so it\nmeans that it a generator is learning\nhow to generate real images which is a\ngood sign so that is our training now\ndone you can obviously train for way\nlonger and you will get much better\nperformance um as long as you don't see\none of these spike up and the other\nspike down\nokay that is looking good let's jump\nback on over to our client and then\nwe're going to wrap this up by testing\nit all out\noh actually wait before we do that we've\nalso generated a ton of images right so\nwe can actually take a look and see what\nthis is looking like already so it looks\nlike we've got a bunch of white images\nso again pretty crap at the moment\nwhich is why i'm probably going to show\nyou the final train model all right so\nyou can see we've got some dots so let\nme zoom in\nso you can see\nwe're getting images but they're right\nnow they're white which is a really poor\nindicator of performance\nthese are sucking\nbut again as you train for way longer\nyou're going to start to get\nsignificantly better performance because\nyou can see this is uh epoch 19 which is\neffectively epoch 20 it's starting to\nlearn how to generate images right from\nthe bottom but that is literally how it\nstarts it takes a long time to get one\nof these fully trained up i think i\nactually did spend the full 2000 epochs\nokay let's go back to our client have a\nchat and then we'll go from there\nall that's left to do now is test it out\nsweet let's do it we can use the\ngenerator model to make predictions and\nvisualize those tiny images using\nmatplotlib to save you some time i've\nalso pre-trained a model that you can\nuse which will be available\non github guys nice alrighty so now that\nwe've gone and trained our model let's\nactually go and test it out so before\nloading up the pre-trained model let's\nactually just test out with our baseline\nmodel so we can actually load in weights\nfor pre-trained models and i'm going to\ngive you those weights i'll try to\nupload them on github so you can test\nthis out first thing that we need to do\nis actually go and make predictions so\nwe can create a variable called images\nand use the generator which is now\ntrained through our subclass model so\ngenerator dot\npredict\nwe can use tf.random.normal\nand then pass through the number of\nimages we want to generate so let's\ngenerate like 16\npass through the latent variables pass\nthrough one\nbecause that is the shape it is\nexpecting\nand that should generate images\nimages perfect so those are our images\nnow generated so you can see those there\nthen the next thing that we can do is\nactually visualize them using matplotlib\nso rather than\njust looking at a random set of arrays\nwe can actually go and visualize them a\nlittle bit easier so let's go and do\nthat\nperfect and that's what we're getting\nout of our generators are pretty crap at\nthe moment but again this has been\ntrained for jack all time 20 epochs\nwhich is absolutely nothing so\nlet's actually take a look at what it\nwould look like with a\ntrained model so one that's actually\nbeen trained for quite some time so\nrather than using the base generator\nwhat we'll do is we'll load up some\nweight so i've got some weights sitting\nin a folder called archive so i've got\nthe discriminator weights and the\ngenerator weights we don't need the\ndiscriminatory it's really just a\ntraining thing\nbut let's actually load up\ngeneratormodel.h5 so we can go uh\ngenerator\ndot load weights\nand we are going to go os.path dot join\narchive\nand then it should be generator model.h5\nperfect so that looks like it's loaded\nup so if i go and run this line now\nboom so you can see we are now\ngenerating fashion so again exact same\narchitecture the only difference is i've\ngone and trained it for 2000 ebooks but\nyou can see that by doing or running\nthrough this\ntype of architecture you can actually do\nquite a fair bit obviously it takes a\nlong time to train so what i was\ntypically doing is leaving my computer\ntraining overnight or actually just\nspinning up a collab instance with\nbackground execution which i think you\nmight need collab pro for but honestly\nit's pretty worth it considering that\nthese take quite a fair bit of time to\ntrain\nif you found better results by training\nin different architectures or better\nways of training faster do let me know\nlet's actually make this a little bit\nsmaller so we can see it\nyou can see that we are actually\ngenerating images of fashion so we've\ngot pants we've got bags we've got\njumpers we've got shoes i don't know\nwhat that is i think it might be a bag\nagain we can go and generate more images\nso again we've got like sort of sneakers\nnow i don't know slightly different type\nof bag slightly\nlike a men's boot or something there or\na sandal let's go and generate more\nbut you can see that this is obviously\nsuper powerful so like we've done it on\nfashion but you could do it on just\nabout anything um obviously keep in mind\nthat training is going to be a big\nfactor in the amount of compute that\nyou're going to need it's going to be a\nhuge factor and you saw what our model\nperformed like when we were training\nusing the old baseline model and not a\npre-trained one but i will make these\nweights available to you via github so\nyou can actually test this out but\nyou've seen what the performance looks\nlike once you've actually gone and\ntrained a lot longer so i think when i\nwas training this i actually managed to\nget the generator loss to pretty close\nto what the discriminator was so i think\nit ended up balancing out at around 0.6\nfor each one of them so it was bouncing\naround 0.6 so that's just an indicator\nof what you might need to get to in\norder to get something that actually\nproduces images that look like this\nlast thing that we want to do is just in\ncase we want to use this later on if we\nwanted to you could actually just save\nthe model and we can do that using\ngenerator\ndot save\nand then we can pass through oh\nuh we can pass through whatever we want\nthe name to be so we'll call it\ngenerator.h5\nyou can save the discriminator as well\nthis grim inator dot h5\nboom\nso you can see if we go into our base\nmodel we've now gone and saved the\ndiscriminator and the generator\nthat is this done in a nutshell so again\nwe've gone and done a ton of stuff in\nterms of doing this so first up we\nimported and installed our dependencies\nwe went and visualized our images\nremember these are our real images you\nsaw what our generated images look like\nbelow we took a look at data sets\nbuilding a neural network creating a\ngenerator\nand you saw that effectively your\ngenerator is passing the outputs to your\ndiscriminator\nwe then went and built our discriminator\nand then we built this monster training\nloop which obviously takes quite a fair\nbit of effort uh we took a look at the\nwhat is it the callback and training and\nagain\nthe length of time that we train for is\nby no means long enough to actually get\na great result i'd recommend 2000 epochs\nwe did 20\nbut time permitting i've only got so\nmuch time to generate these tutorials so\ni figured i'd at least try to get this\nout for you guys um and show you what a\npre-chain model does look like because i\nwent and pre-trained this a ton of time\nago and that's what the end output looks\nlike so again we can go and generate\nmore fashion if we wanted to and you can\nsee we're generating generating a bunch\nof random and different types of images\nwhich kind of pass off as as fashion you\ncould all even try to do this on colored\nimages and see what those outputs look\nlike let me do know if you innovate i'm\nalways interested to see what you guys\nbuild thanks again for tuning in guys\npeace thanks so much for tuning in guys\nhopefully you enjoyed this video if you\ndid be sure to give it a big thumbs up\nhit subscribe and tick that bell and all\nthat good stuff hopefully you enjoyed\nthis video where we sort of went back to\nthe old tutorial style where we're\ncoding up from scratch building up our\nmodels and then testing them out in real\ntime thanks again for tuning in guys\npeace\nokay so neural network so the first\nthing that we need to do uh is\n",
  "words": [
    "one",
    "interesting",
    "parts",
    "machine",
    "learning",
    "generative",
    "machine",
    "learning",
    "video",
    "going",
    "using",
    "exact",
    "technique",
    "specifically",
    "generative",
    "adversarial",
    "neural",
    "network",
    "get",
    "started",
    "generative",
    "machine",
    "learning",
    "let",
    "music",
    "music",
    "music",
    "video",
    "going",
    "covering",
    "bunch",
    "stuff",
    "specifically",
    "going",
    "focusing",
    "generative",
    "machine",
    "learning",
    "using",
    "generative",
    "adversarial",
    "neural",
    "network",
    "going",
    "using",
    "build",
    "design",
    "fashion",
    "line",
    "could",
    "anything",
    "could",
    "generate",
    "synthetic",
    "images",
    "whole",
    "range",
    "different",
    "types",
    "fields",
    "spheres",
    "objects",
    "knows",
    "going",
    "take",
    "step",
    "step",
    "code",
    "way",
    "scratch",
    "going",
    "installing",
    "dependencies",
    "getting",
    "data",
    "visualizing",
    "data",
    "going",
    "get",
    "good",
    "bit",
    "actually",
    "build",
    "generator",
    "model",
    "actual",
    "neural",
    "component",
    "able",
    "generate",
    "new",
    "images",
    "new",
    "objects",
    "take",
    "little",
    "bit",
    "design",
    "discriminator",
    "model",
    "think",
    "art",
    "critic",
    "trying",
    "detect",
    "fake",
    "real",
    "image",
    "wrap",
    "custom",
    "training",
    "loop",
    "visualize",
    "results",
    "ready",
    "let",
    "get",
    "hey",
    "nick",
    "use",
    "cases",
    "gans",
    "well",
    "basic",
    "gans",
    "great",
    "generating",
    "synthetic",
    "data",
    "advanced",
    "models",
    "allow",
    "things",
    "like",
    "super",
    "resolution",
    "smoke",
    "removal",
    "style",
    "transfers",
    "wait",
    "think",
    "could",
    "build",
    "us",
    "simple",
    "game",
    "maybe",
    "generate",
    "fashion",
    "sure",
    "let",
    "get",
    "first",
    "thing",
    "need",
    "set",
    "environment",
    "welcome",
    "back",
    "breakdown",
    "board",
    "really",
    "wanted",
    "okay",
    "video",
    "going",
    "building",
    "game",
    "going",
    "building",
    "ghana",
    "generate",
    "generate",
    "fashion",
    "first",
    "thing",
    "going",
    "needing",
    "get",
    "images",
    "fashion",
    "order",
    "going",
    "leverage",
    "tensorflow",
    "data",
    "set",
    "api",
    "aka",
    "tfds",
    "specifically",
    "going",
    "using",
    "fashion",
    "mnist",
    "data",
    "set",
    "going",
    "allow",
    "us",
    "get",
    "images",
    "things",
    "like",
    "shoes",
    "let",
    "get",
    "draw",
    "boot",
    "something",
    "things",
    "like",
    "bags",
    "well",
    "like",
    "got",
    "whole",
    "bunch",
    "random",
    "images",
    "actually",
    "use",
    "particular",
    "data",
    "set",
    "thing",
    "going",
    "actually",
    "going",
    "build",
    "generator",
    "generator",
    "going",
    "take",
    "bunch",
    "random",
    "numbers",
    "commonly",
    "referred",
    "latent",
    "dimension",
    "might",
    "like",
    "1",
    "23",
    "42",
    "71",
    "actually",
    "going",
    "output",
    "going",
    "take",
    "random",
    "numbers",
    "going",
    "output",
    "set",
    "values",
    "represent",
    "image",
    "actually",
    "going",
    "let",
    "say",
    "example",
    "images",
    "getting",
    "back",
    "tfds",
    "going",
    "shape",
    "28",
    "28",
    "might",
    "take",
    "random",
    "values",
    "generator",
    "let",
    "say",
    "particular",
    "case",
    "take",
    "128",
    "random",
    "values",
    "128",
    "values",
    "reshaped",
    "format",
    "allows",
    "us",
    "output",
    "set",
    "values",
    "28",
    "28",
    "actually",
    "take",
    "128",
    "values",
    "build",
    "generator",
    "generator",
    "comprised",
    "number",
    "convolutional",
    "neural",
    "network",
    "layers",
    "well",
    "sampling",
    "layers",
    "eventually",
    "going",
    "get",
    "probably",
    "connect",
    "hey",
    "going",
    "output",
    "image",
    "28",
    "28",
    "one",
    "effectively",
    "generated",
    "image",
    "crux",
    "discriminator",
    "comes",
    "actually",
    "take",
    "generated",
    "image",
    "pass",
    "discriminator",
    "neural",
    "network",
    "going",
    "number",
    "convolutional",
    "neural",
    "network",
    "layers",
    "final",
    "output",
    "neural",
    "network",
    "going",
    "zero",
    "one",
    "represents",
    "whether",
    "true",
    "fake",
    "image",
    "set",
    "zero",
    "represents",
    "true",
    "1",
    "represents",
    "fake",
    "implementations",
    "might",
    "little",
    "bit",
    "differently",
    "definitely",
    "going",
    "work",
    "take",
    "generated",
    "image",
    "bang",
    "discriminator",
    "time",
    "also",
    "take",
    "real",
    "images",
    "way",
    "tfds",
    "going",
    "let",
    "make",
    "uh",
    "entry",
    "one",
    "going",
    "come",
    "tfts",
    "model",
    "able",
    "train",
    "discriminator",
    "real",
    "true",
    "fun",
    "bit",
    "comes",
    "custom",
    "training",
    "loop",
    "drawn",
    "uh",
    "dude",
    "lifting",
    "weights",
    "kind",
    "get",
    "idea",
    "actually",
    "need",
    "balance",
    "training",
    "generator",
    "discriminator",
    "key",
    "step",
    "comes",
    "actually",
    "building",
    "guns",
    "actually",
    "train",
    "discriminator",
    "gets",
    "rewarded",
    "effectively",
    "picks",
    "fakes",
    "whereas",
    "generator",
    "gets",
    "rewarded",
    "able",
    "fool",
    "generator",
    "fool",
    "discriminate",
    "generator",
    "goal",
    "fool",
    "discriminator",
    "cool",
    "thing",
    "keras",
    "tensorflow",
    "going",
    "deep",
    "learning",
    "framework",
    "actually",
    "going",
    "use",
    "actually",
    "create",
    "custom",
    "train",
    "step",
    "actually",
    "build",
    "custom",
    "training",
    "loop",
    "allows",
    "us",
    "build",
    "generator",
    "finally",
    "comes",
    "actually",
    "testing",
    "model",
    "actually",
    "take",
    "generator",
    "model",
    "specifically",
    "discard",
    "discriminator",
    "really",
    "need",
    "anymore",
    "purely",
    "needed",
    "training",
    "pass",
    "128",
    "random",
    "variables",
    "generator",
    "let",
    "say",
    "generator",
    "represented",
    "pink",
    "model",
    "mod",
    "going",
    "going",
    "output",
    "different",
    "images",
    "fashion",
    "might",
    "get",
    "might",
    "get",
    "boot",
    "forth",
    "whole",
    "bunch",
    "applications",
    "use",
    "generating",
    "types",
    "stuff",
    "even",
    "go",
    "slightly",
    "advanced",
    "model",
    "called",
    "conditional",
    "gan",
    "gives",
    "little",
    "bit",
    "control",
    "actually",
    "generate",
    "going",
    "keep",
    "simple",
    "let",
    "get",
    "alrighty",
    "first",
    "thing",
    "told",
    "client",
    "going",
    "order",
    "build",
    "set",
    "environment",
    "order",
    "go",
    "complete",
    "game",
    "bunch",
    "stuff",
    "going",
    "set",
    "environment",
    "first",
    "done",
    "going",
    "visualize",
    "build",
    "data",
    "set",
    "gon",
    "na",
    "take",
    "look",
    "building",
    "neural",
    "network",
    "build",
    "generator",
    "discriminator",
    "talk",
    "lately",
    "later",
    "gon",
    "na",
    "construct",
    "training",
    "loop",
    "pretty",
    "involved",
    "walk",
    "test",
    "generator",
    "first",
    "things",
    "first",
    "let",
    "go",
    "ahead",
    "set",
    "environment",
    "first",
    "thing",
    "need",
    "install",
    "dependencies",
    "main",
    "dependencies",
    "going",
    "around",
    "tensorflow",
    "matplotlib",
    "let",
    "go",
    "ahead",
    "install",
    "able",
    "kick",
    "things",
    "okay",
    "dependencies",
    "going",
    "need",
    "written",
    "one",
    "line",
    "code",
    "full",
    "line",
    "exclamation",
    "mark",
    "pip",
    "install",
    "tensorflow",
    "tensorflow",
    "gpu",
    "map",
    "plot",
    "lib",
    "tensorflow",
    "dash",
    "data",
    "sets",
    "ipi",
    "widgets",
    "tensorflow",
    "tensorflow",
    "gpu",
    "going",
    "core",
    "dependencies",
    "going",
    "need",
    "actually",
    "build",
    "deep",
    "neural",
    "network",
    "matplotlib",
    "largely",
    "going",
    "used",
    "visualization",
    "component",
    "actually",
    "go",
    "visualize",
    "data",
    "sets",
    "next",
    "step",
    "chat",
    "client",
    "going",
    "using",
    "tensorflow",
    "datasets",
    "actual",
    "data",
    "set",
    "actually",
    "going",
    "using",
    "go",
    "tensorflow",
    "data",
    "sets",
    "actually",
    "take",
    "look",
    "bunch",
    "data",
    "sets",
    "available",
    "going",
    "using",
    "fashion",
    "data",
    "set",
    "think",
    "go",
    "catalog",
    "search",
    "fashion",
    "think",
    "yeah",
    "going",
    "using",
    "fashion",
    "mnist",
    "bunch",
    "images",
    "zalando",
    "article",
    "images",
    "28",
    "28",
    "grayscale",
    "going",
    "make",
    "building",
    "deep",
    "neural",
    "network",
    "little",
    "bit",
    "faster",
    "rather",
    "using",
    "monster",
    "images",
    "like",
    "done",
    "super",
    "resolution",
    "tutorial",
    "coming",
    "around",
    "kind",
    "soonish",
    "cool",
    "right",
    "data",
    "sets",
    "ipi",
    "widgets",
    "dependency",
    "whenever",
    "downloading",
    "tensorflow",
    "data",
    "sets",
    "go",
    "run",
    "pretty",
    "sure",
    "got",
    "installed",
    "already",
    "run",
    "relatively",
    "quickly",
    "let",
    "double",
    "check",
    "got",
    "errors",
    "got",
    "warning",
    "saying",
    "need",
    "upgrade",
    "pip",
    "perfectly",
    "fine",
    "ignore",
    "always",
    "get",
    "comments",
    "nick",
    "show",
    "us",
    "versions",
    "using",
    "going",
    "run",
    "quick",
    "pip",
    "list",
    "show",
    "versions",
    "stuff",
    "using",
    "core",
    "things",
    "need",
    "take",
    "look",
    "tense",
    "flow",
    "see",
    "using",
    "tensorflow",
    "tensorflow",
    "gpu",
    "tensorflow",
    "data",
    "sets",
    "using",
    "else",
    "installed",
    "matplotlib",
    "let",
    "take",
    "look",
    "matplotlib",
    "using",
    "hopefully",
    "get",
    "across",
    "board",
    "trying",
    "work",
    "versions",
    "stuff",
    "install",
    "keep",
    "mind",
    "versions",
    "tensorflow",
    "using",
    "namely",
    "matching",
    "version",
    "cuda",
    "cu",
    "dnn",
    "got",
    "installed",
    "probably",
    "shown",
    "billion",
    "times",
    "want",
    "show",
    "actually",
    "type",
    "tensorflow",
    "gpu",
    "google",
    "actually",
    "take",
    "look",
    "use",
    "gpu",
    "uh",
    "sure",
    "page",
    "actually",
    "page",
    "actually",
    "compatibility",
    "guide",
    "go",
    "left",
    "hand",
    "side",
    "build",
    "source",
    "actually",
    "shows",
    "scroll",
    "way",
    "actually",
    "gives",
    "configurations",
    "going",
    "need",
    "using",
    "um",
    "using",
    "means",
    "need",
    "running",
    "windows",
    "means",
    "need",
    "microsoft",
    "visual",
    "studio",
    "code",
    "2019",
    "bazel",
    "two",
    "important",
    "bits",
    "order",
    "enable",
    "gpu",
    "acceleration",
    "need",
    "cu",
    "dnn",
    "ku",
    "kuda",
    "optional",
    "need",
    "keep",
    "key",
    "thing",
    "keep",
    "mind",
    "use",
    "specific",
    "versions",
    "okay",
    "set",
    "dependencies",
    "installed",
    "next",
    "thing",
    "need",
    "first",
    "going",
    "limit",
    "gpu",
    "consumption",
    "actually",
    "show",
    "little",
    "gpu",
    "moment",
    "right",
    "see",
    "kind",
    "spun",
    "stuff",
    "might",
    "need",
    "shut",
    "existing",
    "kernels",
    "want",
    "want",
    "limit",
    "virtual",
    "ram",
    "growth",
    "key",
    "thing",
    "whenever",
    "using",
    "tensorflow",
    "expand",
    "use",
    "ram",
    "want",
    "constrain",
    "little",
    "bit",
    "get",
    "ton",
    "memory",
    "errors",
    "using",
    "uh",
    "gpu",
    "think",
    "set",
    "memory",
    "growth",
    "equal",
    "true",
    "going",
    "show",
    "write",
    "command",
    "take",
    "look",
    "let",
    "okay",
    "memory",
    "growth",
    "set",
    "gone",
    "written",
    "four",
    "lines",
    "code",
    "probably",
    "seen",
    "bring",
    "one",
    "ton",
    "times",
    "first",
    "line",
    "written",
    "import",
    "tense",
    "flow",
    "tf",
    "going",
    "allow",
    "us",
    "leverage",
    "tensorflow",
    "uh",
    "bringing",
    "tense",
    "flow",
    "ever",
    "watched",
    "deep",
    "learning",
    "tutorials",
    "three",
    "lines",
    "going",
    "super",
    "common",
    "first",
    "go",
    "grab",
    "gpus",
    "full",
    "line",
    "gpus",
    "equals",
    "tf",
    "dot",
    "underscore",
    "physical",
    "underscore",
    "devices",
    "going",
    "pick",
    "gpus",
    "actually",
    "type",
    "gpus",
    "see",
    "got",
    "one",
    "got",
    "2070",
    "super",
    "deep",
    "learning",
    "machine",
    "looping",
    "one",
    "actually",
    "multiple",
    "actually",
    "go",
    "loop",
    "every",
    "single",
    "one",
    "sort",
    "sets",
    "scaling",
    "use",
    "multiple",
    "gpus",
    "later",
    "next",
    "line",
    "gpu",
    "gpu",
    "effectively",
    "looping",
    "gpu",
    "gpus",
    "print",
    "gpu",
    "see",
    "going",
    "print",
    "physical",
    "device",
    "actually",
    "setting",
    "memory",
    "growth",
    "remember",
    "sort",
    "mentioned",
    "function",
    "underscore",
    "memory",
    "underscore",
    "growth",
    "passing",
    "gpu",
    "particular",
    "gpu",
    "part",
    "iterator",
    "setting",
    "growth",
    "equal",
    "true",
    "means",
    "going",
    "blow",
    "vram",
    "comes",
    "actually",
    "building",
    "deep",
    "neural",
    "network",
    "last",
    "thing",
    "second",
    "last",
    "thing",
    "need",
    "actually",
    "import",
    "rest",
    "dependencies",
    "far",
    "installed",
    "bunch",
    "taken",
    "look",
    "versions",
    "limited",
    "memory",
    "growth",
    "need",
    "bring",
    "rest",
    "dependencies",
    "bring",
    "data",
    "first",
    "let",
    "bring",
    "rest",
    "dependencies",
    "okay",
    "next",
    "two",
    "dependencies",
    "brought",
    "gone",
    "written",
    "two",
    "lines",
    "first",
    "thing",
    "bringing",
    "tense",
    "flow",
    "data",
    "sets",
    "fashion",
    "nist",
    "line",
    "import",
    "tensorflow",
    "underscore",
    "data",
    "sets",
    "tfds",
    "going",
    "allow",
    "us",
    "use",
    "fashion",
    "mnist",
    "data",
    "set",
    "going",
    "effectively",
    "able",
    "generate",
    "images",
    "kind",
    "look",
    "like",
    "could",
    "scaled",
    "able",
    "generate",
    "color",
    "images",
    "could",
    "scatter",
    "generate",
    "bigger",
    "images",
    "ton",
    "stuff",
    "could",
    "bringing",
    "tensorflow",
    "data",
    "sets",
    "next",
    "line",
    "bringing",
    "bringing",
    "matplotlib",
    "map",
    "plot",
    "lib",
    "vis",
    "stuff",
    "line",
    "matplotlib",
    "import",
    "pi",
    "plot",
    "plt",
    "really",
    "two",
    "core",
    "lines",
    "code",
    "bring",
    "dependencies",
    "next",
    "thing",
    "need",
    "actually",
    "download",
    "import",
    "data",
    "set",
    "let",
    "actually",
    "okay",
    "data",
    "set",
    "loaded",
    "actually",
    "gone",
    "used",
    "tensorflow",
    "data",
    "sets",
    "api",
    "use",
    "tensorflow",
    "data",
    "sets",
    "api",
    "bring",
    "data",
    "source",
    "written",
    "one",
    "line",
    "code",
    "ds",
    "equals",
    "see",
    "specified",
    "dataset",
    "actually",
    "want",
    "bring",
    "case",
    "fashion",
    "mnist",
    "one",
    "cool",
    "thing",
    "tensorflow",
    "data",
    "sets",
    "catalog",
    "kind",
    "reference",
    "bring",
    "ton",
    "different",
    "types",
    "data",
    "sets",
    "think",
    "talked",
    "little",
    "bit",
    "work",
    "tensorflow",
    "data",
    "set",
    "api",
    "previously",
    "getting",
    "tensorflow",
    "data",
    "sets",
    "building",
    "go",
    "download",
    "one",
    "friend",
    "pass",
    "data",
    "set",
    "want",
    "let",
    "say",
    "example",
    "wanted",
    "know",
    "images",
    "satellites",
    "could",
    "actually",
    "pass",
    "eurosat",
    "would",
    "get",
    "satellite",
    "images",
    "actually",
    "show",
    "sort",
    "cool",
    "idea",
    "behind",
    "iterate",
    "really",
    "quick",
    "specify",
    "want",
    "training",
    "partition",
    "specified",
    "split",
    "equals",
    "train",
    "full",
    "line",
    "ds",
    "equals",
    "passing",
    "fashion",
    "mnist",
    "first",
    "uh",
    "positional",
    "argument",
    "string",
    "passing",
    "keyword",
    "argument",
    "split",
    "equals",
    "train",
    "actually",
    "data",
    "set",
    "actually",
    "went",
    "took",
    "look",
    "ds",
    "dot",
    "numpy",
    "iterator",
    "let",
    "take",
    "actually",
    "let",
    "take",
    "look",
    "type",
    "first",
    "okay",
    "take",
    "look",
    "looks",
    "like",
    "prefetch",
    "component",
    "part",
    "tensorflow",
    "datasets",
    "api",
    "went",
    "typed",
    "dot",
    "numpy",
    "iterator",
    "dot",
    "next",
    "able",
    "get",
    "array",
    "go",
    "actually",
    "getting",
    "back",
    "dictionary",
    "see",
    "first",
    "component",
    "image",
    "scroll",
    "let",
    "actually",
    "take",
    "look",
    "keys",
    "got",
    "image",
    "got",
    "label",
    "actually",
    "went",
    "grabbed",
    "image",
    "right",
    "image",
    "actually",
    "going",
    "take",
    "look",
    "visualize",
    "sec",
    "know",
    "got",
    "image",
    "obviously",
    "classification",
    "data",
    "set",
    "take",
    "look",
    "labels",
    "well",
    "particular",
    "case",
    "alrighty",
    "first",
    "component",
    "done",
    "gone",
    "imported",
    "dependencies",
    "data",
    "remember",
    "went",
    "installed",
    "bunch",
    "stuff",
    "specifically",
    "tensorflow",
    "tensorflow",
    "gpu",
    "matplotlib",
    "tensorflow",
    "data",
    "sets",
    "ipi",
    "widgets",
    "went",
    "limited",
    "memory",
    "growth",
    "went",
    "brought",
    "couple",
    "dependencies",
    "tensorflow",
    "data",
    "sets",
    "map",
    "plot",
    "lib",
    "went",
    "downloaded",
    "data",
    "set",
    "going",
    "use",
    "tensorflow",
    "data",
    "sets",
    "catalog",
    "bringing",
    "fashion",
    "mnists",
    "able",
    "generate",
    "fashion",
    "components",
    "also",
    "took",
    "look",
    "ad",
    "briefly",
    "look",
    "data",
    "set",
    "going",
    "dive",
    "bit",
    "second",
    "right",
    "let",
    "jump",
    "back",
    "client",
    "chat",
    "done",
    "done",
    "well",
    "next",
    "always",
    "good",
    "practice",
    "visualize",
    "data",
    "case",
    "got",
    "pretty",
    "small",
    "images",
    "prototype",
    "still",
    "worthwhile",
    "taking",
    "look",
    "matplotlib",
    "nonetheless",
    "mentioned",
    "good",
    "practice",
    "visualize",
    "data",
    "set",
    "try",
    "anything",
    "otherwise",
    "sort",
    "walking",
    "dark",
    "little",
    "bit",
    "problematic",
    "building",
    "machine",
    "learning",
    "things",
    "going",
    "actually",
    "going",
    "take",
    "look",
    "visualize",
    "data",
    "set",
    "order",
    "first",
    "thing",
    "going",
    "bring",
    "numpy",
    "import",
    "numpy",
    "np",
    "specifically",
    "going",
    "use",
    "numpy",
    "data",
    "transformation",
    "see",
    "second",
    "actually",
    "gone",
    "grabbed",
    "brought",
    "numpy",
    "next",
    "thing",
    "actually",
    "want",
    "build",
    "iterator",
    "would",
    "seen",
    "actually",
    "creating",
    "loop",
    "able",
    "continuously",
    "grab",
    "data",
    "tensorflow",
    "datasets",
    "pipeline",
    "actually",
    "got",
    "downloading",
    "tensorflow",
    "fashion",
    "mnist",
    "data",
    "set",
    "using",
    "tensorflow",
    "data",
    "set",
    "source",
    "actually",
    "got",
    "pipeline",
    "think",
    "loaded",
    "images",
    "memory",
    "got",
    "set",
    "repeatable",
    "calls",
    "make",
    "pipeline",
    "bring",
    "data",
    "back",
    "right",
    "order",
    "bring",
    "data",
    "back",
    "first",
    "need",
    "actually",
    "set",
    "iterator",
    "think",
    "like",
    "almost",
    "like",
    "connection",
    "right",
    "call",
    "next",
    "bring",
    "back",
    "next",
    "batch",
    "data",
    "setting",
    "iterator",
    "going",
    "next",
    "bring",
    "back",
    "one",
    "batch",
    "one",
    "image",
    "depending",
    "big",
    "batch",
    "size",
    "go",
    "next",
    "bring",
    "back",
    "another",
    "one",
    "next",
    "continuously",
    "bring",
    "back",
    "data",
    "going",
    "first",
    "set",
    "iterator",
    "effectively",
    "going",
    "look",
    "like",
    "right",
    "going",
    "call",
    "data",
    "iterator",
    "set",
    "let",
    "call",
    "connection",
    "setup",
    "connection",
    "aka",
    "iterator",
    "actually",
    "call",
    "iterator",
    "type",
    "dot",
    "next",
    "continuously",
    "bring",
    "back",
    "batch",
    "traditionally",
    "batch",
    "data",
    "set",
    "actually",
    "got",
    "multiple",
    "images",
    "per",
    "batch",
    "multiple",
    "samples",
    "per",
    "batch",
    "type",
    "data",
    "iterator",
    "derator",
    "dot",
    "next",
    "see",
    "one",
    "image",
    "call",
    "another",
    "one",
    "actually",
    "another",
    "image",
    "watch",
    "numbers",
    "right",
    "actually",
    "going",
    "change",
    "see",
    "changed",
    "changed",
    "change",
    "actually",
    "bringing",
    "back",
    "new",
    "data",
    "sets",
    "every",
    "time",
    "boom",
    "boom",
    "boom",
    "boom",
    "boom",
    "boom",
    "boom",
    "actually",
    "calling",
    "pipeline",
    "bringing",
    "back",
    "new",
    "batch",
    "every",
    "single",
    "time",
    "loading",
    "memory",
    "begin",
    "actually",
    "making",
    "call",
    "bringing",
    "back",
    "data",
    "need",
    "helps",
    "preserve",
    "memory",
    "computer",
    "particularly",
    "hardcore",
    "deep",
    "learning",
    "okay",
    "taken",
    "look",
    "get",
    "data",
    "pipeline",
    "getting",
    "data",
    "pipeline",
    "boom",
    "right",
    "uh",
    "cool",
    "thing",
    "jupiter",
    "lab",
    "know",
    "seen",
    "actually",
    "right",
    "click",
    "output",
    "cell",
    "actually",
    "dis",
    "enable",
    "scrolling",
    "output",
    "rather",
    "go",
    "way",
    "find",
    "next",
    "cell",
    "go",
    "next",
    "bit",
    "actually",
    "allows",
    "condense",
    "output",
    "enable",
    "scrolling",
    "outputs",
    "see",
    "much",
    "cleaner",
    "actually",
    "see",
    "hell",
    "um",
    "hopefully",
    "makes",
    "life",
    "little",
    "bit",
    "easier",
    "well",
    "okay",
    "done",
    "brought",
    "numpy",
    "set",
    "iterator",
    "aka",
    "data",
    "connection",
    "showing",
    "get",
    "data",
    "pipeline",
    "next",
    "thing",
    "actually",
    "want",
    "little",
    "bit",
    "first",
    "thing",
    "going",
    "create",
    "subplots",
    "going",
    "use",
    "actually",
    "create",
    "subplots",
    "using",
    "matplotlib",
    "plot",
    "four",
    "images",
    "let",
    "okay",
    "go",
    "actually",
    "gone",
    "visualized",
    "images",
    "let",
    "take",
    "step",
    "step",
    "let",
    "show",
    "actually",
    "written",
    "written",
    "one",
    "two",
    "three",
    "five",
    "different",
    "lines",
    "code",
    "first",
    "line",
    "actually",
    "establishing",
    "think",
    "way",
    "matplotlibs",
    "almost",
    "like",
    "setting",
    "format",
    "plugging",
    "images",
    "particularly",
    "working",
    "subplots",
    "first",
    "thing",
    "setting",
    "format",
    "set",
    "subplot",
    "formatting",
    "really",
    "saying",
    "want",
    "four",
    "columns",
    "want",
    "total",
    "figure",
    "total",
    "plot",
    "20",
    "20",
    "pixels",
    "terms",
    "size",
    "entire",
    "figure",
    "referenced",
    "figure",
    "fig",
    "actually",
    "whole",
    "thing",
    "ax",
    "one",
    "individual",
    "subplots",
    "right",
    "ax",
    "could",
    "easily",
    "called",
    "subplot",
    "one",
    "subplot",
    "two",
    "subplot",
    "three",
    "actually",
    "take",
    "look",
    "ax",
    "right",
    "four",
    "different",
    "components",
    "first",
    "subplot",
    "second",
    "subplot",
    "third",
    "fourth",
    "right",
    "actually",
    "visualize",
    "something",
    "one",
    "individual",
    "little",
    "subplots",
    "exactly",
    "done",
    "first",
    "thing",
    "setting",
    "subplots",
    "return",
    "values",
    "going",
    "get",
    "back",
    "entire",
    "plot",
    "whole",
    "thing",
    "axes",
    "one",
    "two",
    "three",
    "four",
    "specified",
    "end",
    "calls",
    "equals",
    "four",
    "looping",
    "four",
    "times",
    "going",
    "grabbing",
    "four",
    "individual",
    "images",
    "data",
    "iterator",
    "calling",
    "dot",
    "next",
    "four",
    "times",
    "exactly",
    "idx",
    "range",
    "let",
    "say",
    "going",
    "loop",
    "four",
    "times",
    "get",
    "images",
    "idx",
    "range",
    "four",
    "first",
    "thing",
    "going",
    "going",
    "get",
    "batch",
    "batch",
    "effectively",
    "sample",
    "case",
    "could",
    "easily",
    "call",
    "sample",
    "really",
    "believe",
    "batched",
    "right",
    "getting",
    "one",
    "sample",
    "calling",
    "data",
    "remember",
    "sample",
    "going",
    "comprised",
    "two",
    "parts",
    "image",
    "part",
    "dictionary",
    "label",
    "first",
    "thing",
    "using",
    "axes",
    "using",
    "show",
    "function",
    "image",
    "show",
    "function",
    "available",
    "inside",
    "matplotlib",
    "passing",
    "image",
    "squeezing",
    "actually",
    "inside",
    "set",
    "subarrays",
    "actually",
    "go",
    "grab",
    "image",
    "dot",
    "shape",
    "actually",
    "want",
    "want",
    "condense",
    "got",
    "one",
    "single",
    "value",
    "bottom",
    "actually",
    "type",
    "np",
    "dot",
    "squeeze",
    "get",
    "rid",
    "see",
    "collapsing",
    "type",
    "dot",
    "shape",
    "28",
    "28",
    "allows",
    "us",
    "visualize",
    "whole",
    "heap",
    "easier",
    "okay",
    "first",
    "getting",
    "axes",
    "want",
    "visualize",
    "going",
    "grab",
    "first",
    "index",
    "first",
    "axis",
    "let",
    "actually",
    "take",
    "look",
    "ax",
    "effectively",
    "going",
    "right",
    "grab",
    "index",
    "0",
    "going",
    "going",
    "type",
    "dot",
    "show",
    "pass",
    "image",
    "exactly",
    "done",
    "grabbing",
    "sample",
    "image",
    "passing",
    "actually",
    "visualization",
    "let",
    "actually",
    "write",
    "comments",
    "grab",
    "image",
    "label",
    "plot",
    "image",
    "using",
    "specific",
    "axis",
    "actually",
    "call",
    "subplot",
    "next",
    "thing",
    "setting",
    "title",
    "purely",
    "optional",
    "like",
    "commented",
    "get",
    "rid",
    "numbers",
    "top",
    "numbers",
    "top",
    "descriptive",
    "actually",
    "gone",
    "taken",
    "look",
    "one",
    "labels",
    "presumably",
    "label",
    "5",
    "going",
    "sort",
    "high",
    "heel",
    "label",
    "8",
    "going",
    "bag",
    "label",
    "6",
    "going",
    "jumper",
    "see",
    "two",
    "printing",
    "appending",
    "image",
    "label",
    "plot",
    "title",
    "written",
    "ax",
    "passing",
    "index",
    "grabbing",
    "specific",
    "subplot",
    "point",
    "time",
    "using",
    "dot",
    "title",
    "grabbing",
    "dot",
    "title",
    "attribute",
    "using",
    "dot",
    "set",
    "text",
    "function",
    "able",
    "set",
    "text",
    "see",
    "little",
    "fives",
    "let",
    "zoom",
    "see",
    "five",
    "see",
    "8",
    "forth",
    "right",
    "makes",
    "little",
    "bit",
    "easier",
    "see",
    "actually",
    "plotting",
    "keep",
    "plotting",
    "going",
    "see",
    "different",
    "images",
    "4",
    "might",
    "men",
    "jumper",
    "5",
    "might",
    "shoe",
    "one",
    "might",
    "pants",
    "nine",
    "boot",
    "men",
    "jumper",
    "nine",
    "boot",
    "three",
    "singlet",
    "something",
    "like",
    "uh",
    "sort",
    "get",
    "idea",
    "right",
    "actually",
    "visualize",
    "images",
    "using",
    "dot",
    "next",
    "function",
    "actually",
    "keep",
    "getting",
    "different",
    "sets",
    "images",
    "back",
    "pretty",
    "cool",
    "right",
    "getting",
    "whole",
    "bunch",
    "different",
    "images",
    "performance",
    "jumper",
    "nine",
    "boot",
    "eight",
    "bags",
    "six",
    "singlet",
    "jumper",
    "whatever",
    "sort",
    "get",
    "idea",
    "got",
    "fashion",
    "images",
    "actually",
    "going",
    "use",
    "able",
    "generate",
    "types",
    "images",
    "think",
    "capabilities",
    "could",
    "actually",
    "use",
    "able",
    "generate",
    "synthetic",
    "data",
    "sets",
    "could",
    "use",
    "able",
    "generate",
    "faces",
    "later",
    "really",
    "need",
    "sub",
    "different",
    "types",
    "images",
    "use",
    "similar",
    "pipeline",
    "type",
    "thing",
    "okay",
    "visualization",
    "done",
    "far",
    "gone",
    "done",
    "done",
    "little",
    "bit",
    "data",
    "transformation",
    "using",
    "numpy",
    "remember",
    "used",
    "collapse",
    "array",
    "taken",
    "look",
    "set",
    "connection",
    "data",
    "set",
    "go",
    "get",
    "data",
    "pipeline",
    "taken",
    "look",
    "go",
    "ahead",
    "visualize",
    "guys",
    "per",
    "usual",
    "code",
    "going",
    "available",
    "via",
    "github",
    "include",
    "link",
    "description",
    "going",
    "clean",
    "bit",
    "alrighty",
    "cool",
    "next",
    "thing",
    "actually",
    "need",
    "little",
    "bit",
    "data",
    "processing",
    "right",
    "images",
    "represented",
    "values",
    "0",
    "255",
    "order",
    "build",
    "good",
    "deep",
    "learning",
    "models",
    "typically",
    "want",
    "scale",
    "values",
    "0",
    "actually",
    "going",
    "set",
    "quick",
    "function",
    "allows",
    "us",
    "scale",
    "images",
    "let",
    "go",
    "ahead",
    "cool",
    "function",
    "scale",
    "images",
    "scale",
    "return",
    "images",
    "actually",
    "going",
    "transform",
    "data",
    "pipeline",
    "returns",
    "image",
    "actually",
    "need",
    "label",
    "moment",
    "super",
    "v",
    "supervised",
    "classification",
    "problem",
    "going",
    "build",
    "conditional",
    "gain",
    "might",
    "talk",
    "future",
    "video",
    "might",
    "actually",
    "need",
    "labels",
    "actually",
    "want",
    "pass",
    "type",
    "image",
    "want",
    "generate",
    "particular",
    "model",
    "going",
    "generating",
    "different",
    "types",
    "going",
    "sample",
    "latent",
    "space",
    "able",
    "generate",
    "different",
    "types",
    "fashion",
    "let",
    "take",
    "look",
    "function",
    "first",
    "first",
    "defined",
    "new",
    "function",
    "set",
    "equal",
    "scale",
    "images",
    "going",
    "name",
    "going",
    "take",
    "data",
    "tensorflow",
    "data",
    "pipeline",
    "going",
    "extract",
    "image",
    "remember",
    "dictionary",
    "got",
    "back",
    "included",
    "image",
    "label",
    "need",
    "image",
    "image",
    "equals",
    "data",
    "grabbing",
    "using",
    "key",
    "image",
    "returning",
    "image",
    "divided",
    "255",
    "going",
    "scale",
    "image",
    "zero",
    "one",
    "hopefully",
    "build",
    "neural",
    "network",
    "performs",
    "little",
    "bit",
    "better",
    "trains",
    "little",
    "bit",
    "faster",
    "cool",
    "right",
    "need",
    "actually",
    "apply",
    "data",
    "pipeline",
    "seen",
    "uh",
    "think",
    "actually",
    "image",
    "classification",
    "video",
    "know",
    "remember",
    "initialism",
    "mixture",
    "app",
    "remember",
    "need",
    "map",
    "data",
    "set",
    "need",
    "cache",
    "need",
    "batch",
    "uh",
    "b",
    "p",
    "prefetch",
    "let",
    "actually",
    "oh",
    "wait",
    "also",
    "need",
    "shuffle",
    "uh",
    "let",
    "actually",
    "take",
    "look",
    "actually",
    "go",
    "map",
    "cache",
    "shuffle",
    "batch",
    "prefetch",
    "steps",
    "typically",
    "go",
    "go",
    "spelled",
    "prefetch",
    "steps",
    "typically",
    "whenever",
    "going",
    "building",
    "data",
    "pipeline",
    "tensorflow",
    "exactly",
    "going",
    "go",
    "ahead",
    "let",
    "go",
    "okay",
    "data",
    "set",
    "prepared",
    "mentioned",
    "going",
    "run",
    "mixture",
    "back",
    "pipeline",
    "map",
    "case",
    "shuffle",
    "batch",
    "prefetch",
    "first",
    "thing",
    "gotten",
    "done",
    "reloaded",
    "data",
    "set",
    "completely",
    "optional",
    "could",
    "easily",
    "use",
    "data",
    "set",
    "brought",
    "right",
    "gone",
    "like",
    "little",
    "condensed",
    "area",
    "ds",
    "equals",
    "loading",
    "fashion",
    "mnist",
    "get",
    "used",
    "actually",
    "grabbing",
    "train",
    "partition",
    "right",
    "magic",
    "happens",
    "first",
    "thing",
    "running",
    "data",
    "set",
    "scale",
    "images",
    "step",
    "really",
    "like",
    "way",
    "works",
    "actually",
    "take",
    "like",
    "almost",
    "apply",
    "like",
    "data",
    "pipelines",
    "effectively",
    "taking",
    "data",
    "running",
    "scale",
    "images",
    "function",
    "could",
    "go",
    "stuff",
    "wanted",
    "data",
    "augmentation",
    "got",
    "better",
    "idea",
    "could",
    "actually",
    "go",
    "apply",
    "data",
    "augmentation",
    "step",
    "well",
    "actually",
    "need",
    "cashing",
    "data",
    "set",
    "cache",
    "data",
    "set",
    "batch",
    "shuffling",
    "ensures",
    "got",
    "shuffle",
    "data",
    "set",
    "looking",
    "specific",
    "set",
    "samples",
    "shuffle",
    "actually",
    "let",
    "quickly",
    "walk",
    "first",
    "grabbing",
    "data",
    "set",
    "defined",
    "overwriting",
    "variable",
    "ds",
    "equals",
    "returning",
    "passed",
    "scale",
    "images",
    "function",
    "caching",
    "taking",
    "ds",
    "caching",
    "applying",
    "cache",
    "function",
    "shuffling",
    "grabbing",
    "ds",
    "data",
    "set",
    "passing",
    "using",
    "shuffle",
    "function",
    "specifying",
    "specifying",
    "value",
    "shuffle",
    "function",
    "shuffle",
    "buffer",
    "needs",
    "case",
    "60",
    "000",
    "overriding",
    "variable",
    "taking",
    "data",
    "set",
    "batching",
    "batches",
    "128",
    "images",
    "batch",
    "128",
    "images",
    "per",
    "sample",
    "data",
    "set",
    "equal",
    "passed",
    "128",
    "today",
    "eliminates",
    "bottlenecking",
    "slows",
    "reduces",
    "likelihood",
    "bottlenecking",
    "taking",
    "ds",
    "variable",
    "using",
    "prefetch",
    "passing",
    "64",
    "gives",
    "us",
    "data",
    "set",
    "run",
    "issues",
    "type",
    "ds",
    "dot",
    "numpy",
    "iterator",
    "get",
    "back",
    "type",
    "dot",
    "next",
    "set",
    "images",
    "128",
    "samples",
    "length",
    "28",
    "28",
    "think",
    "one",
    "go",
    "type",
    "dot",
    "shape",
    "go",
    "128",
    "images",
    "gone",
    "batched",
    "samples",
    "128",
    "28",
    "pixels",
    "wide",
    "think",
    "wide",
    "high",
    "28",
    "28",
    "one",
    "right",
    "going",
    "dimension",
    "believe",
    "width",
    "height",
    "channels",
    "grayscaled",
    "image",
    "hence",
    "got",
    "one",
    "end",
    "data",
    "set",
    "built",
    "gone",
    "done",
    "ton",
    "stuff",
    "first",
    "got",
    "done",
    "let",
    "make",
    "sure",
    "zoomed",
    "gone",
    "brought",
    "numpy",
    "little",
    "bit",
    "data",
    "transformation",
    "visualizing",
    "specifically",
    "using",
    "setting",
    "connection",
    "iterator",
    "getting",
    "data",
    "pipeline",
    "using",
    "dot",
    "next",
    "function",
    "visualizing",
    "using",
    "matplotlib",
    "subplots",
    "scaling",
    "define",
    "step",
    "whenever",
    "working",
    "tensorflow",
    "data",
    "pipeline",
    "gone",
    "built",
    "entire",
    "data",
    "pipeline",
    "gone",
    "reloaded",
    "data",
    "set",
    "done",
    "mixture",
    "shuffle",
    "gone",
    "mapped",
    "gone",
    "encased",
    "shuffled",
    "batched",
    "gives",
    "us",
    "data",
    "set",
    "looks",
    "like",
    "128",
    "samples",
    "28",
    "pixels",
    "28",
    "pixels",
    "one",
    "cool",
    "step",
    "two",
    "done",
    "gone",
    "visualized",
    "built",
    "data",
    "set",
    "let",
    "jump",
    "back",
    "client",
    "time",
    "build",
    "model",
    "guy",
    "oh",
    "born",
    "ready",
    "model",
    "guy",
    "wait",
    "talking",
    "anyway",
    "going",
    "build",
    "two",
    "models",
    "generator",
    "model",
    "built",
    "try",
    "generate",
    "images",
    "clothing",
    "fashion",
    "discriminator",
    "try",
    "learn",
    "spot",
    "fakes",
    "generator",
    "almost",
    "like",
    "artist",
    "trying",
    "forge",
    "things",
    "discriminator",
    "art",
    "critic",
    "trying",
    "spot",
    "exactly",
    "let",
    "radio",
    "good",
    "bit",
    "actually",
    "starting",
    "build",
    "deep",
    "neural",
    "network",
    "mentioned",
    "client",
    "conversation",
    "couple",
    "key",
    "components",
    "actually",
    "need",
    "build",
    "namely",
    "generator",
    "think",
    "part",
    "gan",
    "actually",
    "generating",
    "pass",
    "bunch",
    "random",
    "numbers",
    "try",
    "take",
    "set",
    "random",
    "numbers",
    "generate",
    "fashion",
    "type",
    "generative",
    "adversarial",
    "neural",
    "network",
    "actually",
    "gives",
    "little",
    "bit",
    "control",
    "called",
    "conditional",
    "gain",
    "actually",
    "pass",
    "number",
    "one",
    "might",
    "map",
    "specific",
    "type",
    "image",
    "actually",
    "try",
    "generate",
    "image",
    "going",
    "use",
    "conditional",
    "tutorial",
    "want",
    "video",
    "let",
    "know",
    "comments",
    "maybe",
    "uh",
    "try",
    "wrap",
    "something",
    "first",
    "thing",
    "actually",
    "need",
    "actually",
    "import",
    "modeling",
    "component",
    "need",
    "bring",
    "dependencies",
    "namely",
    "need",
    "bring",
    "sequential",
    "api",
    "also",
    "need",
    "bring",
    "layers",
    "actually",
    "going",
    "use",
    "let",
    "go",
    "ahead",
    "import",
    "need",
    "okay",
    "modeling",
    "components",
    "gon",
    "na",
    "need",
    "first",
    "thing",
    "actually",
    "gone",
    "done",
    "brought",
    "sequential",
    "api",
    "think",
    "went",
    "detailed",
    "explanation",
    "works",
    "one",
    "previous",
    "videos",
    "either",
    "image",
    "eye",
    "detection",
    "iris",
    "tracking",
    "video",
    "one",
    "face",
    "detection",
    "one",
    "know",
    "sequential",
    "api",
    "means",
    "take",
    "one",
    "input",
    "get",
    "one",
    "output",
    "basically",
    "flowing",
    "one",
    "way",
    "api",
    "going",
    "using",
    "bring",
    "sequential",
    "api",
    "generator",
    "discriminator",
    "cool",
    "thing",
    "generator",
    "going",
    "take",
    "random",
    "number",
    "random",
    "values",
    "effectively",
    "uh",
    "latent",
    "set",
    "latent",
    "values",
    "use",
    "sequential",
    "api",
    "likewise",
    "discriminator",
    "going",
    "take",
    "image",
    "try",
    "output",
    "set",
    "binary",
    "classification",
    "values",
    "0",
    "1",
    "1",
    "0",
    "determine",
    "whether",
    "real",
    "fake",
    "image",
    "generator",
    "going",
    "take",
    "random",
    "values",
    "try",
    "generate",
    "image",
    "discriminator",
    "going",
    "take",
    "generated",
    "image",
    "maybe",
    "real",
    "images",
    "try",
    "determine",
    "whether",
    "fake",
    "sort",
    "like",
    "balancing",
    "act",
    "actually",
    "go",
    "training",
    "bad",
    "boys",
    "sequential",
    "api",
    "bringing",
    "bunch",
    "layers",
    "layers",
    "neural",
    "network",
    "cool",
    "actually",
    "live",
    "day",
    "age",
    "build",
    "ai",
    "stuff",
    "right",
    "neural",
    "network",
    "brought",
    "import",
    "conf2d",
    "also",
    "bringing",
    "convolute",
    "going",
    "allow",
    "us",
    "perform",
    "convolutions",
    "believe",
    "going",
    "use",
    "discriminator",
    "generator",
    "also",
    "going",
    "bring",
    "dense",
    "layers",
    "fully",
    "connected",
    "layers",
    "flattened",
    "allows",
    "us",
    "flatten",
    "set",
    "dimensions",
    "vectors",
    "matrices",
    "bringing",
    "reshape",
    "layer",
    "allows",
    "us",
    "transform",
    "output",
    "previous",
    "layer",
    "looks",
    "like",
    "different",
    "shape",
    "pretty",
    "useful",
    "whenever",
    "taking",
    "inputs",
    "certain",
    "shape",
    "going",
    "use",
    "take",
    "random",
    "variables",
    "generator",
    "reshape",
    "something",
    "gives",
    "us",
    "little",
    "bit",
    "spatial",
    "quality",
    "see",
    "sec",
    "bringing",
    "leaky",
    "relu",
    "activation",
    "um",
    "leaky",
    "relu",
    "versus",
    "relu",
    "find",
    "useful",
    "actually",
    "visualize",
    "activation",
    "functions",
    "look",
    "like",
    "relu",
    "value",
    "blue",
    "line",
    "following",
    "leaky",
    "relu",
    "little",
    "bit",
    "leak",
    "negative",
    "set",
    "values",
    "right",
    "allows",
    "us",
    "little",
    "bit",
    "data",
    "transfer",
    "uh",
    "got",
    "dropouts",
    "form",
    "regularization",
    "also",
    "got",
    "sampling",
    "2d",
    "going",
    "used",
    "generator",
    "sample",
    "images",
    "bring",
    "add",
    "little",
    "bit",
    "depth",
    "little",
    "bit",
    "space",
    "image",
    "okay",
    "layers",
    "going",
    "need",
    "bring",
    "sequential",
    "api",
    "bunch",
    "different",
    "types",
    "layers",
    "well",
    "modeling",
    "components",
    "going",
    "need",
    "next",
    "thing",
    "want",
    "actually",
    "build",
    "generator",
    "let",
    "actually",
    "go",
    "going",
    "create",
    "new",
    "function",
    "call",
    "build",
    "generator",
    "going",
    "instantiate",
    "model",
    "model",
    "equals",
    "sequential",
    "going",
    "start",
    "adding",
    "bunch",
    "layers",
    "going",
    "return",
    "end",
    "model",
    "right",
    "actually",
    "need",
    "interim",
    "actually",
    "work",
    "build",
    "generator",
    "first",
    "thing",
    "going",
    "define",
    "number",
    "inputs",
    "actually",
    "going",
    "take",
    "might",
    "take",
    "let",
    "say",
    "128",
    "values",
    "use",
    "random",
    "sampling",
    "value",
    "actually",
    "work",
    "generate",
    "exactly",
    "going",
    "let",
    "build",
    "first",
    "block",
    "take",
    "look",
    "looks",
    "like",
    "first",
    "okay",
    "first",
    "block",
    "going",
    "add",
    "deep",
    "neural",
    "network",
    "gone",
    "added",
    "dense",
    "layer",
    "leaky",
    "relu",
    "layer",
    "reshape",
    "layer",
    "let",
    "see",
    "generate",
    "begin",
    "model",
    "let",
    "call",
    "test",
    "model",
    "test",
    "model",
    "equals",
    "build",
    "generator",
    "got",
    "issue",
    "takes",
    "two",
    "positional",
    "arguments",
    "uh",
    "wrapped",
    "one",
    "set",
    "arrays",
    "okay",
    "cool",
    "right",
    "test",
    "model",
    "defined",
    "type",
    "dot",
    "summary",
    "beautiful",
    "okay",
    "initial",
    "model",
    "looks",
    "like",
    "find",
    "building",
    "way",
    "actually",
    "get",
    "makes",
    "little",
    "bit",
    "sense",
    "actually",
    "see",
    "happening",
    "first",
    "thing",
    "specifying",
    "input",
    "layer",
    "going",
    "input",
    "layer",
    "going",
    "fully",
    "connected",
    "layer",
    "dense",
    "fully",
    "connected",
    "layer",
    "number",
    "units",
    "going",
    "7",
    "7",
    "128",
    "let",
    "explain",
    "gone",
    "specified",
    "going",
    "passing",
    "128",
    "random",
    "values",
    "generator",
    "help",
    "determine",
    "generate",
    "sort",
    "mentioned",
    "much",
    "control",
    "type",
    "gain",
    "conditional",
    "gain",
    "conditional",
    "gain",
    "going",
    "focus",
    "type",
    "image",
    "want",
    "generate",
    "type",
    "really",
    "generating",
    "random",
    "images",
    "sample",
    "population",
    "trying",
    "generate",
    "random",
    "fashion",
    "images",
    "nice",
    "thing",
    "avoid",
    "something",
    "called",
    "mode",
    "collapse",
    "still",
    "ensures",
    "generates",
    "bunch",
    "random",
    "different",
    "types",
    "images",
    "might",
    "generate",
    "jumpers",
    "might",
    "generate",
    "shoes",
    "might",
    "generate",
    "singlets",
    "forth",
    "know",
    "input",
    "value",
    "going",
    "passing",
    "128",
    "gives",
    "generator",
    "context",
    "determine",
    "actually",
    "generate",
    "going",
    "generate",
    "based",
    "128",
    "random",
    "values",
    "normally",
    "referred",
    "latent",
    "space",
    "well",
    "128",
    "going",
    "converted",
    "spatial",
    "area",
    "128",
    "values",
    "think",
    "like",
    "array",
    "128",
    "values",
    "want",
    "eventually",
    "convert",
    "image",
    "want",
    "give",
    "spatial",
    "qualities",
    "right",
    "actually",
    "going",
    "convert",
    "shape",
    "7",
    "7",
    "128",
    "dense",
    "layer",
    "going",
    "number",
    "units",
    "going",
    "going",
    "7",
    "7",
    "random",
    "variable",
    "could",
    "random",
    "variable",
    "model",
    "dot",
    "add",
    "passing",
    "dense",
    "seven",
    "seven",
    "128",
    "could",
    "pass",
    "actual",
    "value",
    "specifying",
    "input",
    "value",
    "going",
    "look",
    "like",
    "going",
    "128",
    "values",
    "applying",
    "activation",
    "specifying",
    "want",
    "apply",
    "leaky",
    "relu",
    "fact",
    "allows",
    "us",
    "cater",
    "model",
    "passing",
    "leaky",
    "value",
    "specifying",
    "output",
    "parameter",
    "defines",
    "far",
    "bottom",
    "bit",
    "looks",
    "like",
    "reshaping",
    "gives",
    "us",
    "spatial",
    "quality",
    "passing",
    "passing",
    "reshape",
    "layer",
    "takes",
    "dense",
    "output",
    "really",
    "going",
    "6272",
    "values",
    "going",
    "effectively",
    "converting",
    "beginnings",
    "image",
    "except",
    "image",
    "seven",
    "seven",
    "128",
    "channels",
    "monster",
    "image",
    "ton",
    "layers",
    "eventually",
    "going",
    "get",
    "point",
    "generating",
    "outputting",
    "image",
    "shape",
    "data",
    "28",
    "28",
    "one",
    "starting",
    "seven",
    "seven",
    "128",
    "eventually",
    "going",
    "get",
    "28",
    "28",
    "one",
    "adding",
    "bunch",
    "additional",
    "layers",
    "first",
    "block",
    "takes",
    "random",
    "values",
    "reshapes",
    "7",
    "7",
    "think",
    "beginnings",
    "generated",
    "let",
    "put",
    "another",
    "line",
    "beginnings",
    "generated",
    "image",
    "julio",
    "right",
    "first",
    "set",
    "blocks",
    "done",
    "obviously",
    "model",
    "complete",
    "right",
    "still",
    "outputting",
    "seven",
    "seven",
    "128",
    "correct",
    "dimensions",
    "generated",
    "image",
    "needs",
    "28",
    "28",
    "one",
    "everything",
    "really",
    "get",
    "towards",
    "shape",
    "order",
    "going",
    "using",
    "conf",
    "2d",
    "layers",
    "also",
    "going",
    "using",
    "sampling",
    "2d",
    "layers",
    "get",
    "us",
    "28",
    "28",
    "let",
    "add",
    "bunch",
    "layers",
    "okay",
    "first",
    "sampling",
    "block",
    "sampling",
    "block",
    "effectively",
    "going",
    "double",
    "spatial",
    "quality",
    "going",
    "14",
    "14",
    "128",
    "passing",
    "convolutional",
    "layer",
    "effectively",
    "condense",
    "little",
    "bit",
    "believe",
    "oh",
    "well",
    "padding",
    "might",
    "let",
    "actually",
    "take",
    "look",
    "second",
    "uh",
    "convolutional",
    "neural",
    "network",
    "layer",
    "going",
    "128",
    "units",
    "going",
    "preserve",
    "number",
    "channels",
    "kernel",
    "size",
    "going",
    "five",
    "five",
    "padding",
    "going",
    "uh",
    "effectively",
    "crop",
    "see",
    "actually",
    "crop",
    "using",
    "bigger",
    "filter",
    "see",
    "second",
    "passing",
    "activation",
    "going",
    "leaky",
    "relu",
    "let",
    "run",
    "okay",
    "crop",
    "output",
    "14",
    "14",
    "really",
    "wanted",
    "could",
    "actually",
    "apply",
    "exact",
    "sampling",
    "block",
    "could",
    "literally",
    "take",
    "change",
    "number",
    "filters",
    "let",
    "actually",
    "show",
    "rather",
    "talking",
    "right",
    "grab",
    "pasted",
    "said",
    "rather",
    "128",
    "kernels",
    "give",
    "one",
    "boom",
    "boom",
    "boom",
    "take",
    "look",
    "output",
    "layer",
    "outputting",
    "shape",
    "image",
    "could",
    "actually",
    "stop",
    "really",
    "enough",
    "typically",
    "want",
    "add",
    "bunch",
    "layers",
    "convolute",
    "generator",
    "gives",
    "little",
    "bit",
    "sophistication",
    "terms",
    "actually",
    "able",
    "generate",
    "something",
    "sufficiently",
    "complex",
    "look",
    "like",
    "generated",
    "image",
    "going",
    "yet",
    "going",
    "add",
    "bunch",
    "layers",
    "give",
    "us",
    "little",
    "bit",
    "information",
    "okay",
    "gone",
    "done",
    "gone",
    "added",
    "upsampling",
    "2d",
    "block",
    "effectively",
    "doubles",
    "size",
    "output",
    "previous",
    "layer",
    "literally",
    "going",
    "go",
    "14",
    "let",
    "actually",
    "show",
    "let",
    "show",
    "actually",
    "deactivated",
    "two",
    "additional",
    "layers",
    "boom",
    "boom",
    "boom",
    "literally",
    "going",
    "take",
    "layer",
    "going",
    "double",
    "two",
    "performing",
    "sampling",
    "doubling",
    "actually",
    "expanding",
    "values",
    "got",
    "believe",
    "duplicating",
    "taking",
    "value",
    "next",
    "sort",
    "copying",
    "um",
    "apply",
    "pass",
    "convolutional",
    "2d",
    "neural",
    "network",
    "condense",
    "information",
    "provide",
    "little",
    "bit",
    "parametric",
    "values",
    "parametric",
    "transformations",
    "see",
    "parameters",
    "upsampling",
    "layer",
    "right",
    "let",
    "add",
    "convolutional",
    "layer",
    "boom",
    "boom",
    "boom",
    "actually",
    "got",
    "parameters",
    "neural",
    "network",
    "learn",
    "able",
    "generate",
    "better",
    "set",
    "values",
    "output",
    "passing",
    "leaky",
    "relu",
    "layer",
    "gives",
    "us",
    "activations",
    "boom",
    "leaky",
    "reality",
    "gone",
    "1",
    "million",
    "million",
    "different",
    "parameters",
    "deep",
    "neural",
    "network",
    "getting",
    "closer",
    "output",
    "image",
    "needs",
    "28",
    "28",
    "okay",
    "let",
    "add",
    "another",
    "upsampling",
    "block",
    "go",
    "see",
    "okay",
    "gone",
    "added",
    "three",
    "blocks",
    "let",
    "say",
    "sampling",
    "block",
    "one",
    "even",
    "though",
    "really",
    "sampling",
    "literally",
    "gone",
    "copied",
    "sampling",
    "block",
    "copying",
    "exact",
    "like",
    "layers",
    "give",
    "us",
    "output",
    "28",
    "28",
    "128",
    "applying",
    "another",
    "set",
    "convolutions",
    "leaky",
    "relu",
    "applying",
    "padding",
    "really",
    "change",
    "size",
    "output",
    "going",
    "exact",
    "really",
    "giving",
    "us",
    "knowledge",
    "ability",
    "pass",
    "parameters",
    "deep",
    "neural",
    "network",
    "learn",
    "little",
    "bit",
    "better",
    "really",
    "28",
    "28",
    "128",
    "terms",
    "output",
    "go",
    "run",
    "boom",
    "boom",
    "boom",
    "right",
    "28",
    "28",
    "128",
    "means",
    "really",
    "need",
    "add",
    "final",
    "convolutional",
    "neural",
    "network",
    "layout",
    "convlayer",
    "get",
    "one",
    "channel",
    "let",
    "add",
    "final",
    "layer",
    "neural",
    "network",
    "least",
    "generator",
    "built",
    "cool",
    "final",
    "layer",
    "going",
    "remember",
    "want",
    "one",
    "channel",
    "previously",
    "would",
    "128",
    "channels",
    "based",
    "kernels",
    "convolutional",
    "neural",
    "network",
    "layers",
    "one",
    "filter",
    "going",
    "get",
    "one",
    "channel",
    "image",
    "generated",
    "image",
    "remember",
    "preserved",
    "shape",
    "28",
    "28",
    "really",
    "passing",
    "two",
    "different",
    "sampling",
    "layers",
    "went",
    "7",
    "14",
    "14",
    "28",
    "gives",
    "us",
    "go",
    "run",
    "28",
    "28",
    "1",
    "model",
    "million",
    "parameters",
    "looking",
    "good",
    "generator",
    "built",
    "gone",
    "instantiated",
    "sequential",
    "api",
    "went",
    "little",
    "bit",
    "reshaping",
    "input",
    "dimensions",
    "128",
    "random",
    "pixels",
    "went",
    "7",
    "7",
    "128",
    "passed",
    "unsampling",
    "block",
    "went",
    "14",
    "14",
    "128",
    "passed",
    "another",
    "upsampling",
    "block",
    "took",
    "28",
    "28",
    "128",
    "went",
    "passed",
    "additional",
    "really",
    "sampling",
    "let",
    "call",
    "convolutional",
    "block",
    "like",
    "telling",
    "sampling",
    "block",
    "really",
    "sampling",
    "would",
    "mean",
    "reduce",
    "spatial",
    "quality",
    "possibly",
    "bump",
    "number",
    "channels",
    "right",
    "taking",
    "two",
    "sampling",
    "blocks",
    "passing",
    "two",
    "convolutional",
    "blocks",
    "maintaining",
    "spatial",
    "shape",
    "setting",
    "padding",
    "equal",
    "two",
    "means",
    "really",
    "gon",
    "na",
    "exact",
    "values",
    "exact",
    "values",
    "exact",
    "output",
    "shape",
    "would",
    "28",
    "28",
    "128",
    "passing",
    "final",
    "layer",
    "going",
    "reduce",
    "number",
    "channels",
    "passing",
    "convolutional",
    "2d",
    "neural",
    "network",
    "best",
    "find",
    "want",
    "one",
    "filter",
    "one",
    "kernel",
    "uh",
    "kernel",
    "size",
    "going",
    "four",
    "four",
    "specifying",
    "padding",
    "equals",
    "means",
    "going",
    "preserve",
    "28",
    "28",
    "got",
    "activation",
    "sigmoid",
    "actually",
    "want",
    "values",
    "0",
    "going",
    "allow",
    "us",
    "generate",
    "image",
    "go",
    "run",
    "take",
    "look",
    "neural",
    "network",
    "28",
    "28",
    "generator",
    "built",
    "actually",
    "go",
    "test",
    "generator",
    "let",
    "minimize",
    "little",
    "bit",
    "actually",
    "go",
    "pass",
    "random",
    "values",
    "generator",
    "able",
    "output",
    "stuff",
    "let",
    "try",
    "awesome",
    "generator",
    "generating",
    "values",
    "effectively",
    "images",
    "went",
    "renamed",
    "model",
    "equal",
    "generator",
    "going",
    "name",
    "model",
    "generator",
    "equals",
    "build",
    "underscore",
    "generator",
    "take",
    "look",
    "summary",
    "generate",
    "image",
    "pass",
    "use",
    "predict",
    "function",
    "pass",
    "random",
    "values",
    "actually",
    "generating",
    "four",
    "different",
    "random",
    "images",
    "used",
    "numpy",
    "generates",
    "set",
    "numbers",
    "using",
    "normal",
    "distribution",
    "passing",
    "many",
    "images",
    "want",
    "generate",
    "four",
    "passing",
    "128",
    "random",
    "values",
    "model",
    "expects",
    "able",
    "generate",
    "images",
    "printed",
    "array",
    "rather",
    "visualize",
    "looking",
    "numbers",
    "use",
    "matplotlib",
    "actually",
    "view",
    "random",
    "images",
    "look",
    "like",
    "moment",
    "actually",
    "take",
    "visualization",
    "script",
    "copy",
    "actually",
    "tweak",
    "little",
    "bit",
    "able",
    "plot",
    "take",
    "bit",
    "going",
    "actually",
    "looping",
    "images",
    "let",
    "say",
    "um",
    "uh",
    "idx",
    "comma",
    "image",
    "enumerate",
    "image",
    "let",
    "double",
    "check",
    "image",
    "shape",
    "looks",
    "like",
    "28",
    "28",
    "one",
    "okay",
    "work",
    "grab",
    "one",
    "image",
    "one",
    "index",
    "get",
    "rid",
    "sampling",
    "data",
    "iterator",
    "going",
    "passing",
    "image",
    "image",
    "index",
    "preserved",
    "fine",
    "pass",
    "index",
    "let",
    "see",
    "works",
    "go",
    "images",
    "gone",
    "generated",
    "done",
    "taken",
    "exact",
    "visualization",
    "script",
    "wrote",
    "right",
    "gone",
    "adjusted",
    "visualize",
    "images",
    "outputting",
    "generator",
    "model",
    "really",
    "looping",
    "one",
    "images",
    "index",
    "comma",
    "image",
    "enumerate",
    "images",
    "going",
    "give",
    "us",
    "index",
    "actual",
    "value",
    "specific",
    "array",
    "using",
    "specific",
    "axes",
    "function",
    "using",
    "numpy",
    "squeeze",
    "squeeze",
    "values",
    "remember",
    "28",
    "28x1",
    "using",
    "numpy",
    "squeeze",
    "going",
    "get",
    "28",
    "28",
    "makes",
    "little",
    "bit",
    "easier",
    "visualize",
    "plotting",
    "one",
    "images",
    "see",
    "0",
    "1",
    "2",
    "look",
    "like",
    "moment",
    "pretty",
    "crap",
    "actually",
    "train",
    "actually",
    "able",
    "generate",
    "better",
    "set",
    "visualizations",
    "better",
    "set",
    "fashion",
    "components",
    "go",
    "run",
    "able",
    "get",
    "rid",
    "image",
    "causing",
    "nightmare",
    "delete",
    "shape",
    "boom",
    "next",
    "set",
    "images",
    "bang",
    "bang",
    "next",
    "set",
    "next",
    "set",
    "right",
    "actually",
    "grab",
    "put",
    "say",
    "generate",
    "new",
    "fashion",
    "right",
    "looking",
    "pretty",
    "crap",
    "changed",
    "right",
    "new",
    "images",
    "new",
    "images",
    "new",
    "images",
    "see",
    "generating",
    "different",
    "types",
    "stuff",
    "random",
    "sampling",
    "producing",
    "random",
    "sample",
    "pass",
    "initial",
    "neural",
    "network",
    "pretty",
    "cool",
    "least",
    "begin",
    "definitely",
    "going",
    "make",
    "whole",
    "heap",
    "better",
    "let",
    "enable",
    "scrolling",
    "obviously",
    "going",
    "get",
    "whole",
    "heap",
    "better",
    "eventually",
    "actually",
    "gone",
    "trained",
    "generator",
    "done",
    "one",
    "hardest",
    "parts",
    "actually",
    "building",
    "gan",
    "model",
    "gone",
    "done",
    "ton",
    "stuff",
    "first",
    "gone",
    "imported",
    "different",
    "modeling",
    "components",
    "gone",
    "built",
    "generator",
    "remember",
    "started",
    "unique",
    "layer",
    "takes",
    "random",
    "values",
    "reshapes",
    "give",
    "spatial",
    "qualities",
    "remember",
    "reshape",
    "layer",
    "went",
    "used",
    "sampling",
    "expand",
    "size",
    "image",
    "went",
    "actually",
    "doubled",
    "size",
    "went",
    "applied",
    "convolutional",
    "neural",
    "network",
    "layers",
    "give",
    "bit",
    "give",
    "trainable",
    "parameters",
    "allows",
    "us",
    "give",
    "little",
    "bit",
    "information",
    "output",
    "actually",
    "looks",
    "like",
    "along",
    "leaky",
    "rallies",
    "gone",
    "done",
    "pretty",
    "much",
    "used",
    "building",
    "blocks",
    "able",
    "increase",
    "size",
    "seven",
    "seven",
    "128",
    "28",
    "28",
    "128",
    "gone",
    "added",
    "couple",
    "additional",
    "convolutional",
    "neural",
    "network",
    "layer",
    "blocks",
    "last",
    "least",
    "last",
    "layer",
    "really",
    "really",
    "important",
    "reduces",
    "one",
    "channel",
    "size",
    "input",
    "data",
    "also",
    "specified",
    "activation",
    "sigmoid",
    "give",
    "us",
    "value",
    "zero",
    "one",
    "okay",
    "neural",
    "network",
    "looking",
    "pretty",
    "good",
    "next",
    "thing",
    "need",
    "build",
    "discriminator",
    "funnily",
    "enough",
    "probably",
    "easier",
    "part",
    "building",
    "working",
    "gans",
    "discriminator",
    "really",
    "image",
    "classifier",
    "determining",
    "whether",
    "image",
    "real",
    "fake",
    "watched",
    "image",
    "classification",
    "tutorial",
    "really",
    "different",
    "adding",
    "bunch",
    "dropout",
    "layers",
    "increase",
    "regularization",
    "image",
    "classification",
    "pretty",
    "easy",
    "task",
    "neural",
    "networks",
    "generating",
    "new",
    "stuff",
    "little",
    "bit",
    "trickier",
    "want",
    "make",
    "little",
    "bit",
    "harder",
    "discriminator",
    "learn",
    "going",
    "create",
    "function",
    "def",
    "build",
    "discriminator",
    "going",
    "effectively",
    "similar",
    "thing",
    "generators",
    "going",
    "instantiate",
    "model",
    "using",
    "sequential",
    "api",
    "eventually",
    "going",
    "return",
    "model",
    "make",
    "little",
    "bit",
    "easier",
    "see",
    "actually",
    "building",
    "let",
    "let",
    "set",
    "similar",
    "thing",
    "generator",
    "going",
    "create",
    "instance",
    "discriminator",
    "discrim",
    "inator",
    "equals",
    "build",
    "discriminator",
    "eventually",
    "gone",
    "constructed",
    "actually",
    "able",
    "type",
    "discriminator",
    "dot",
    "summary",
    "right",
    "right",
    "two",
    "lines",
    "going",
    "work",
    "going",
    "run",
    "first",
    "defined",
    "oh",
    "actually",
    "work",
    "line",
    "wo",
    "work",
    "actually",
    "gone",
    "passed",
    "layers",
    "okay",
    "first",
    "thing",
    "actually",
    "want",
    "go",
    "ahead",
    "build",
    "discriminator",
    "let",
    "go",
    "okay",
    "first",
    "convolutional",
    "neural",
    "network",
    "layer",
    "block",
    "let",
    "take",
    "look",
    "gone",
    "ridden",
    "three",
    "additional",
    "lines",
    "added",
    "convolutional",
    "neural",
    "network",
    "layer",
    "leaky",
    "relu",
    "activation",
    "dropout",
    "layer",
    "convolutional",
    "neural",
    "network",
    "layer",
    "going",
    "32",
    "filters",
    "conf",
    "2d",
    "passing",
    "32",
    "filters",
    "filters",
    "going",
    "shape",
    "5x5",
    "specifying",
    "padding",
    "equals",
    "means",
    "going",
    "start",
    "condensing",
    "information",
    "getting",
    "specifying",
    "input",
    "shape",
    "output",
    "shape",
    "generator",
    "generator",
    "going",
    "generate",
    "image",
    "28",
    "28",
    "1",
    "pass",
    "discriminator",
    "determine",
    "whether",
    "real",
    "fake",
    "image",
    "end",
    "goal",
    "generator",
    "generate",
    "images",
    "discriminator",
    "thinks",
    "real",
    "bit",
    "sort",
    "tweak",
    "training",
    "loop",
    "get",
    "work",
    "input",
    "shape",
    "going",
    "size",
    "output",
    "shape",
    "generator",
    "28",
    "28",
    "one",
    "adding",
    "activation",
    "layer",
    "going",
    "leaky",
    "reality",
    "leaky",
    "rally",
    "actually",
    "recommended",
    "practice",
    "building",
    "uh",
    "gans",
    "least",
    "lot",
    "seen",
    "using",
    "leaky",
    "releases",
    "activations",
    "rather",
    "straight",
    "relu",
    "sigmoid",
    "tan",
    "h",
    "okay",
    "model",
    "add",
    "leaky",
    "relu",
    "specifying",
    "input",
    "parameter",
    "passing",
    "dropout",
    "want",
    "make",
    "want",
    "make",
    "little",
    "bit",
    "harder",
    "discriminator",
    "learn",
    "dropout",
    "effectively",
    "applies",
    "regularization",
    "model",
    "dot",
    "add",
    "dropout",
    "specified",
    "positional",
    "argument",
    "strength",
    "dropout",
    "first",
    "convolutional",
    "block",
    "take",
    "look",
    "run",
    "two",
    "new",
    "lines",
    "build",
    "discriminator",
    "use",
    "build",
    "discriminator",
    "function",
    "allows",
    "us",
    "see",
    "looks",
    "like",
    "right",
    "shape",
    "24",
    "24",
    "32",
    "outputting",
    "got",
    "832",
    "prime",
    "small",
    "model",
    "moment",
    "going",
    "build",
    "let",
    "add",
    "bunch",
    "additional",
    "convolutional",
    "neural",
    "network",
    "layer",
    "blocks",
    "get",
    "effectively",
    "image",
    "classifier",
    "okay",
    "adding",
    "second",
    "convolutional",
    "neural",
    "network",
    "layer",
    "block",
    "pretty",
    "much",
    "identical",
    "first",
    "one",
    "difference",
    "dropped",
    "input",
    "shape",
    "need",
    "define",
    "input",
    "shape",
    "anymore",
    "first",
    "layer",
    "first",
    "hidden",
    "layer",
    "bumped",
    "number",
    "filters",
    "probably",
    "going",
    "copy",
    "create",
    "another",
    "one",
    "128",
    "let",
    "take",
    "look",
    "shape",
    "looks",
    "like",
    "going",
    "third",
    "con",
    "block",
    "16",
    "16",
    "128",
    "getting",
    "closer",
    "single",
    "output",
    "value",
    "sigma",
    "would",
    "b",
    "sigmoid",
    "zero",
    "one",
    "okay",
    "getting",
    "let",
    "go",
    "going",
    "add",
    "bunch",
    "layers",
    "basically",
    "going",
    "follow",
    "similar",
    "pattern",
    "going",
    "flatten",
    "output",
    "single",
    "value",
    "using",
    "dense",
    "layer",
    "let",
    "okay",
    "fourth",
    "convolutional",
    "block",
    "think",
    "going",
    "add",
    "another",
    "one",
    "think",
    "good",
    "trying",
    "mimic",
    "exact",
    "model",
    "earlier",
    "guys",
    "use",
    "trained",
    "weights",
    "got",
    "effective",
    "got",
    "four",
    "different",
    "convolution",
    "blocks",
    "first",
    "one",
    "32",
    "filters",
    "second",
    "one",
    "third",
    "one",
    "128",
    "last",
    "one",
    "256",
    "going",
    "going",
    "flatten",
    "model",
    "actually",
    "let",
    "take",
    "look",
    "apple",
    "first",
    "go",
    "run",
    "run",
    "run",
    "output",
    "12",
    "12",
    "256",
    "flattening",
    "actually",
    "get",
    "12",
    "12",
    "256",
    "passed",
    "dense",
    "layer",
    "let",
    "flatten",
    "pass",
    "dense",
    "layer",
    "model",
    "dot",
    "add",
    "going",
    "pass",
    "flattened",
    "layer",
    "flatten",
    "going",
    "pass",
    "dropout",
    "layer",
    "drop",
    "wonder",
    "best",
    "practice",
    "add",
    "drop",
    "flat",
    "anyway",
    "gon",
    "na",
    "added",
    "model",
    "kind",
    "need",
    "add",
    "might",
    "take",
    "look",
    "let",
    "know",
    "get",
    "better",
    "performance",
    "removing",
    "layer",
    "going",
    "leave",
    "going",
    "output",
    "single",
    "value",
    "activation",
    "going",
    "sigmoid",
    "want",
    "output",
    "value",
    "zero",
    "one",
    "one",
    "ca",
    "remember",
    "way",
    "configured",
    "whether",
    "one",
    "representing",
    "false",
    "let",
    "double",
    "check",
    "yeah",
    "one",
    "represents",
    "false",
    "image",
    "one",
    "eventually",
    "going",
    "represent",
    "false",
    "image",
    "0",
    "going",
    "represent",
    "true",
    "image",
    "play",
    "around",
    "anyway",
    "activation",
    "equals",
    "sigmoid",
    "looks",
    "good",
    "think",
    "model",
    "done",
    "yeah",
    "cool",
    "right",
    "see",
    "originally",
    "taking",
    "image",
    "shape",
    "28",
    "28",
    "one",
    "finally",
    "going",
    "way",
    "bottom",
    "outputting",
    "1",
    "value",
    "0",
    "1",
    "represent",
    "true",
    "false",
    "whether",
    "image",
    "fake",
    "real",
    "one",
    "fake",
    "image",
    "zero",
    "real",
    "image",
    "different",
    "implementations",
    "run",
    "different",
    "ways",
    "one",
    "might",
    "real",
    "image",
    "one",
    "might",
    "fake",
    "image",
    "much",
    "depends",
    "model",
    "actually",
    "set",
    "could",
    "actually",
    "take",
    "images",
    "actually",
    "generated",
    "generator",
    "right",
    "cool",
    "prototype",
    "take",
    "image",
    "pass",
    "discriminator",
    "output",
    "zero",
    "one",
    "went",
    "discriminator",
    "let",
    "bring",
    "grim",
    "nato",
    "never",
    "type",
    "dot",
    "predict",
    "pass",
    "image",
    "uh",
    "got",
    "issues",
    "right",
    "uh",
    "negative",
    "dimension",
    "size",
    "chords",
    "subtracting",
    "one",
    "gone",
    "done",
    "need",
    "pass",
    "dot",
    "predict",
    "got",
    "issue",
    "right",
    "let",
    "debug",
    "wait",
    "got",
    "one",
    "image",
    "inside",
    "model",
    "okay",
    "wait",
    "thought",
    "batched",
    "image",
    "dot",
    "shape",
    "okay",
    "work",
    "guessing",
    "getting",
    "error",
    "passing",
    "single",
    "four",
    "images",
    "okay",
    "go",
    "might",
    "works",
    "much",
    "better",
    "predict",
    "okay",
    "makes",
    "sense",
    "reason",
    "getting",
    "error",
    "effectively",
    "trying",
    "run",
    "single",
    "image",
    "neural",
    "network",
    "going",
    "throw",
    "error",
    "effectively",
    "like",
    "see",
    "image",
    "take",
    "look",
    "shape",
    "28",
    "28",
    "run",
    "run",
    "typically",
    "passing",
    "image",
    "single",
    "image",
    "deep",
    "neural",
    "network",
    "expects",
    "batch",
    "number",
    "first",
    "single",
    "value",
    "single",
    "image",
    "batch",
    "actually",
    "need",
    "type",
    "np",
    "dot",
    "expand",
    "dims",
    "pass",
    "parameter",
    "zero",
    "fixed",
    "generate",
    "prediction",
    "single",
    "image",
    "see",
    "confident",
    "whether",
    "real",
    "going",
    "train",
    "better",
    "generator",
    "get",
    "got",
    "one",
    "image",
    "need",
    "go",
    "wrap",
    "expand",
    "dims",
    "actually",
    "go",
    "go",
    "generate",
    "new",
    "set",
    "images",
    "run",
    "model",
    "run",
    "dot",
    "predict",
    "function",
    "get",
    "errors",
    "boom",
    "take",
    "look",
    "pretty",
    "cool",
    "right",
    "generator",
    "discriminator",
    "built",
    "sort",
    "saw",
    "building",
    "layers",
    "ensure",
    "input",
    "output",
    "correct",
    "shape",
    "remember",
    "generator",
    "going",
    "take",
    "set",
    "random",
    "values",
    "case",
    "going",
    "128",
    "random",
    "values",
    "going",
    "output",
    "image",
    "vector",
    "matrix",
    "shape",
    "28",
    "28",
    "discriminator",
    "going",
    "sort",
    "opposite",
    "going",
    "take",
    "output",
    "generator",
    "28",
    "28",
    "one",
    "going",
    "output",
    "single",
    "value",
    "zero",
    "one",
    "determine",
    "whether",
    "real",
    "fake",
    "cool",
    "okay",
    "set",
    "generators",
    "done",
    "let",
    "go",
    "jump",
    "back",
    "generators",
    "discriminators",
    "done",
    "let",
    "go",
    "chat",
    "client",
    "done",
    "custom",
    "training",
    "loop",
    "ah",
    "one",
    "important",
    "parts",
    "building",
    "gans",
    "write",
    "custom",
    "training",
    "loop",
    "train",
    "generator",
    "discriminator",
    "simultaneously",
    "ah",
    "got",
    "let",
    "go",
    "home",
    "right",
    "bit",
    "pretty",
    "hardcore",
    "wan",
    "na",
    "disclose",
    "gon",
    "na",
    "take",
    "step",
    "step",
    "training",
    "gans",
    "notoriously",
    "difficult",
    "nobody",
    "actually",
    "really",
    "tells",
    "difficult",
    "going",
    "give",
    "feedback",
    "reason",
    "difficult",
    "need",
    "find",
    "balance",
    "speed",
    "discriminator",
    "trains",
    "speed",
    "generator",
    "able",
    "learn",
    "big",
    "thing",
    "need",
    "whenever",
    "training",
    "side",
    "side",
    "ensure",
    "discriminator",
    "train",
    "fast",
    "able",
    "smash",
    "know",
    "fake",
    "image",
    "every",
    "single",
    "time",
    "balancing",
    "uh",
    "really",
    "really",
    "difficult",
    "really",
    "really",
    "important",
    "ensure",
    "get",
    "right",
    "one",
    "way",
    "help",
    "actually",
    "inject",
    "random",
    "noise",
    "outputs",
    "discriminator",
    "almost",
    "kind",
    "like",
    "trick",
    "slow",
    "little",
    "bit",
    "thing",
    "takes",
    "quite",
    "fair",
    "bit",
    "time",
    "train",
    "one",
    "actually",
    "generating",
    "good",
    "output",
    "um",
    "anybody",
    "goes",
    "hey",
    "train",
    "one",
    "like",
    "10",
    "minutes",
    "skeptical",
    "feedback",
    "might",
    "get",
    "different",
    "output",
    "experts",
    "field",
    "um",
    "experience",
    "keep",
    "two",
    "things",
    "mind",
    "going",
    "take",
    "step",
    "step",
    "go",
    "consult",
    "constructing",
    "training",
    "loop",
    "mean",
    "training",
    "loop",
    "typically",
    "go",
    "train",
    "deep",
    "neural",
    "network",
    "using",
    "tensorflow",
    "pytorch",
    "typically",
    "couple",
    "key",
    "steps",
    "right",
    "instantiate",
    "model",
    "maybe",
    "create",
    "uh",
    "modeled",
    "equal",
    "sequential",
    "model",
    "add",
    "bunch",
    "layers",
    "actually",
    "assigns",
    "loss",
    "function",
    "optimizer",
    "actually",
    "goes",
    "trains",
    "model",
    "discriminators",
    "scans",
    "two",
    "components",
    "generator",
    "discriminator",
    "actually",
    "need",
    "train",
    "side",
    "side",
    "means",
    "ca",
    "use",
    "dot",
    "fit",
    "need",
    "something",
    "little",
    "bit",
    "tricky",
    "ensure",
    "get",
    "work",
    "going",
    "define",
    "custom",
    "training",
    "loop",
    "training",
    "loop",
    "changing",
    "dot",
    "fit",
    "function",
    "defining",
    "training",
    "step",
    "going",
    "going",
    "using",
    "subclass",
    "model",
    "also",
    "using",
    "tier",
    "function",
    "decorator",
    "good",
    "documentation",
    "tensorflow",
    "website",
    "link",
    "going",
    "way",
    "per",
    "usual",
    "code",
    "going",
    "available",
    "github",
    "stress",
    "get",
    "go",
    "back",
    "take",
    "pace",
    "okay",
    "first",
    "thing",
    "need",
    "set",
    "losses",
    "optimizers",
    "um",
    "order",
    "actually",
    "need",
    "bring",
    "losses",
    "optimizers",
    "let",
    "going",
    "use",
    "uh",
    "think",
    "going",
    "use",
    "binary",
    "cross",
    "entropy",
    "think",
    "actually",
    "going",
    "use",
    "binary",
    "cross",
    "entropy",
    "yeah",
    "come",
    "back",
    "going",
    "use",
    "binary",
    "cross",
    "entropy",
    "going",
    "use",
    "atom",
    "optimizer",
    "first",
    "thing",
    "let",
    "uh",
    "let",
    "import",
    "going",
    "import",
    "optimizers",
    "keras",
    "uh",
    "going",
    "import",
    "adam",
    "dot",
    "losses",
    "going",
    "import",
    "binary",
    "cross",
    "entropy",
    "cool",
    "atom",
    "optimizers",
    "going",
    "optimizer",
    "binary",
    "cross",
    "entropy",
    "going",
    "loss",
    "probably",
    "thinking",
    "nick",
    "hell",
    "going",
    "use",
    "binary",
    "cross",
    "entropy",
    "generator",
    "outputting",
    "image",
    "well",
    "actually",
    "image",
    "pass",
    "discriminator",
    "determine",
    "whether",
    "generator",
    "actually",
    "produced",
    "image",
    "able",
    "trick",
    "discriminator",
    "actually",
    "reward",
    "getting",
    "discriminator",
    "produce",
    "wrong",
    "value",
    "kind",
    "intuitive",
    "actually",
    "play",
    "bunch",
    "times",
    "actually",
    "like",
    "oh",
    "actually",
    "kind",
    "cool",
    "actually",
    "rewarding",
    "generator",
    "tricking",
    "discriminator",
    "rewarding",
    "discriminator",
    "getting",
    "right",
    "one",
    "well",
    "determining",
    "type",
    "image",
    "actually",
    "cool",
    "right",
    "losses",
    "optimizers",
    "tens",
    "photo",
    "carousel",
    "optimizes",
    "import",
    "atom",
    "try",
    "stochastic",
    "gradient",
    "descent",
    "another",
    "type",
    "optimizer",
    "atom",
    "probably",
    "use",
    "time",
    "losses",
    "import",
    "binary",
    "cross",
    "entropy",
    "pretty",
    "cool",
    "right",
    "like",
    "actually",
    "go",
    "build",
    "know",
    "veering",
    "wayside",
    "actually",
    "building",
    "super",
    "resolution",
    "model",
    "actually",
    "use",
    "combination",
    "different",
    "types",
    "loss",
    "functions",
    "actually",
    "use",
    "mean",
    "squared",
    "error",
    "perceptual",
    "loss",
    "different",
    "actual",
    "values",
    "real",
    "super",
    "resolution",
    "image",
    "versus",
    "super",
    "resolution",
    "image",
    "else",
    "use",
    "um",
    "perceptual",
    "loss",
    "think",
    "used",
    "use",
    "binary",
    "cross",
    "entropy",
    "used",
    "combination",
    "bunch",
    "different",
    "types",
    "losses",
    "actually",
    "took",
    "discriminator",
    "loss",
    "versus",
    "plus",
    "perceptual",
    "loss",
    "final",
    "loss",
    "function",
    "probably",
    "go",
    "actually",
    "go",
    "model",
    "okay",
    "imported",
    "optimizers",
    "losses",
    "need",
    "create",
    "instances",
    "going",
    "create",
    "sample",
    "examples",
    "go",
    "g",
    "opt",
    "equals",
    "atom",
    "going",
    "set",
    "learning",
    "rate",
    "equal",
    "learning",
    "rate",
    "generator",
    "going",
    "faster",
    "learning",
    "rate",
    "discriminator",
    "want",
    "discriminator",
    "go",
    "fast",
    "absolutely",
    "smash",
    "generator",
    "part",
    "learning",
    "process",
    "right",
    "might",
    "need",
    "tweak",
    "different",
    "use",
    "cases",
    "found",
    "worked",
    "right",
    "defining",
    "two",
    "optimizers",
    "typed",
    "g",
    "opt",
    "equals",
    "atom",
    "specified",
    "learning",
    "rate",
    "discriminator",
    "created",
    "optimizer",
    "opt",
    "equals",
    "atom",
    "learning",
    "rate",
    "zero",
    "dot",
    "zero",
    "z",
    "one",
    "extra",
    "zero",
    "zero",
    "zero",
    "zero",
    "zero",
    "one",
    "cool",
    "gon",
    "na",
    "define",
    "losses",
    "g",
    "loss",
    "equals",
    "binary",
    "cross",
    "entropy",
    "loss",
    "also",
    "equals",
    "binary",
    "cross",
    "entropy",
    "boom",
    "okay",
    "losses",
    "optimize",
    "set",
    "reason",
    "specifying",
    "learning",
    "rate",
    "discriminator",
    "slower",
    "blow",
    "learn",
    "way",
    "fast",
    "cause",
    "generator",
    "lose",
    "plot",
    "also",
    "gotten",
    "find",
    "losses",
    "step",
    "done",
    "let",
    "set",
    "losses",
    "optimizes",
    "big",
    "boy",
    "setting",
    "subclass",
    "model",
    "first",
    "thing",
    "need",
    "import",
    "model",
    "class",
    "keras",
    "tensorflow",
    "tensorflow",
    "ca",
    "type",
    "dot",
    "models",
    "import",
    "model",
    "base",
    "model",
    "class",
    "get",
    "level",
    "got",
    "ton",
    "control",
    "tensorflow",
    "ton",
    "stuff",
    "want",
    "importing",
    "base",
    "model",
    "class",
    "subclass",
    "training",
    "step",
    "cool",
    "right",
    "ridden",
    "tensorflow",
    "carousel",
    "models",
    "import",
    "model",
    "defining",
    "new",
    "models",
    "basically",
    "like",
    "three",
    "two",
    "keys",
    "functions",
    "gon",
    "na",
    "create",
    "class",
    "fashion",
    "name",
    "whatever",
    "want",
    "going",
    "pass",
    "base",
    "class",
    "couple",
    "key",
    "methods",
    "method",
    "inside",
    "class",
    "couple",
    "key",
    "methods",
    "need",
    "define",
    "subclassing",
    "model",
    "def",
    "init",
    "need",
    "initialization",
    "function",
    "come",
    "back",
    "def",
    "train",
    "step",
    "actually",
    "training",
    "also",
    "define",
    "keep",
    "mind",
    "train",
    "step",
    "called",
    "call",
    "dot",
    "fit",
    "function",
    "right",
    "really",
    "really",
    "important",
    "note",
    "wanted",
    "run",
    "evaluation",
    "function",
    "also",
    "write",
    "create",
    "def",
    "test",
    "step",
    "method",
    "going",
    "use",
    "key",
    "thing",
    "note",
    "also",
    "need",
    "compile",
    "method",
    "def",
    "compile",
    "come",
    "back",
    "key",
    "methods",
    "actually",
    "need",
    "define",
    "subclass",
    "model",
    "reason",
    "going",
    "define",
    "subclass",
    "model",
    "going",
    "fancy",
    "crap",
    "inside",
    "test",
    "step",
    "model",
    "test",
    "step",
    "method",
    "way",
    "see",
    "people",
    "actually",
    "write",
    "um",
    "oh",
    "gosh",
    "tf",
    "function",
    "actually",
    "write",
    "custom",
    "test",
    "step",
    "custom",
    "train",
    "step",
    "sort",
    "like",
    "whole",
    "bunch",
    "stuff",
    "um",
    "like",
    "little",
    "bit",
    "control",
    "subclass",
    "model",
    "keep",
    "mind",
    "got",
    "different",
    "ways",
    "actually",
    "go",
    "okay",
    "first",
    "thing",
    "need",
    "go",
    "pass",
    "parameters",
    "init",
    "method",
    "let",
    "actually",
    "go",
    "ahead",
    "alrighty",
    "parameters",
    "going",
    "passing",
    "init",
    "method",
    "passing",
    "generator",
    "passing",
    "discriminator",
    "passing",
    "positional",
    "arguments",
    "keyboard",
    "argument",
    "gives",
    "us",
    "little",
    "bit",
    "flexibility",
    "want",
    "go",
    "inherit",
    "base",
    "model",
    "going",
    "run",
    "super",
    "dot",
    "init",
    "going",
    "pass",
    "arguments",
    "want",
    "go",
    "use",
    "base",
    "functionality",
    "inside",
    "keras",
    "model",
    "got",
    "ability",
    "passed",
    "arguments",
    "keyword",
    "arguments",
    "base",
    "model",
    "base",
    "class",
    "effectively",
    "next",
    "thing",
    "going",
    "go",
    "ahead",
    "set",
    "two",
    "attributes",
    "generator",
    "discriminator",
    "means",
    "going",
    "able",
    "refer",
    "inside",
    "model",
    "let",
    "actually",
    "um",
    "create",
    "attributes",
    "gen",
    "discriminator",
    "self",
    "dot",
    "call",
    "generator",
    "equals",
    "generator",
    "self",
    "disgrim",
    "nato",
    "equals",
    "discriminator",
    "uh",
    "get",
    "rid",
    "pass",
    "think",
    "init",
    "method",
    "yeah",
    "pretty",
    "much",
    "gone",
    "ridden",
    "gone",
    "done",
    "gone",
    "passed",
    "generator",
    "discriminator",
    "model",
    "instantiate",
    "actually",
    "go",
    "instantiate",
    "pass",
    "subclass",
    "model",
    "passing",
    "keyword",
    "arguments",
    "positional",
    "arguments",
    "base",
    "class",
    "um",
    "plus",
    "args",
    "kw",
    "args",
    "base",
    "class",
    "creating",
    "attributes",
    "generator",
    "discriminator",
    "init",
    "method",
    "done",
    "next",
    "thing",
    "need",
    "actually",
    "go",
    "compile",
    "going",
    "need",
    "pass",
    "optimizers",
    "losses",
    "going",
    "need",
    "run",
    "let",
    "cool",
    "right",
    "input",
    "arguments",
    "going",
    "passing",
    "compile",
    "method",
    "got",
    "def",
    "compile",
    "passing",
    "generator",
    "optimizer",
    "discriminator",
    "optimizer",
    "generator",
    "loss",
    "discriminator",
    "loss",
    "also",
    "going",
    "passing",
    "arguments",
    "keyword",
    "arguments",
    "might",
    "need",
    "going",
    "call",
    "compile",
    "base",
    "class",
    "going",
    "run",
    "super",
    "dot",
    "compile",
    "going",
    "pass",
    "arguments",
    "keyword",
    "arguments",
    "boom",
    "okay",
    "going",
    "create",
    "attributes",
    "one",
    "attribute",
    "optimizers",
    "attributes",
    "losses",
    "let",
    "go",
    "okay",
    "compile",
    "method",
    "done",
    "let",
    "quickly",
    "recap",
    "gone",
    "find",
    "knit",
    "method",
    "knit",
    "method",
    "takes",
    "two",
    "key",
    "things",
    "right",
    "takes",
    "generator",
    "discriminator",
    "models",
    "already",
    "instantiated",
    "ignore",
    "super",
    "bits",
    "know",
    "necessary",
    "evils",
    "right",
    "going",
    "passing",
    "additional",
    "arguments",
    "want",
    "pass",
    "create",
    "model",
    "via",
    "creating",
    "attributes",
    "generator",
    "discriminators",
    "going",
    "able",
    "refer",
    "likewise",
    "something",
    "similar",
    "compile",
    "method",
    "passing",
    "optimizers",
    "g",
    "op",
    "op",
    "g",
    "lost",
    "loss",
    "remember",
    "already",
    "created",
    "right",
    "effectively",
    "passing",
    "atom",
    "atom",
    "binary",
    "cross",
    "entropy",
    "binary",
    "cross",
    "entropy",
    "creating",
    "attributes",
    "inside",
    "fashion",
    "gain",
    "class",
    "could",
    "different",
    "could",
    "name",
    "whatever",
    "wanted",
    "naming",
    "convention",
    "gone",
    "taken",
    "fashion",
    "game",
    "okay",
    "two",
    "easier",
    "bits",
    "done",
    "gone",
    "init",
    "method",
    "compile",
    "method",
    "know",
    "always",
    "little",
    "bit",
    "painful",
    "right",
    "going",
    "get",
    "onto",
    "big",
    "boy",
    "train",
    "step",
    "first",
    "thing",
    "need",
    "take",
    "batch",
    "batch",
    "data",
    "keep",
    "mind",
    "batch",
    "data",
    "case",
    "going",
    "128",
    "images",
    "size",
    "28",
    "28",
    "1",
    "defined",
    "right",
    "went",
    "created",
    "data",
    "set",
    "step",
    "section",
    "two",
    "need",
    "actually",
    "something",
    "data",
    "exactly",
    "going",
    "inside",
    "train",
    "step",
    "first",
    "thing",
    "going",
    "going",
    "get",
    "batch",
    "data",
    "call",
    "real",
    "images",
    "equals",
    "batch",
    "first",
    "step",
    "get",
    "data",
    "right",
    "getting",
    "first",
    "things",
    "first",
    "getting",
    "batch",
    "real",
    "images",
    "going",
    "get",
    "backup",
    "batch",
    "fake",
    "images",
    "generated",
    "images",
    "right",
    "going",
    "call",
    "dot",
    "normal",
    "effectively",
    "previously",
    "right",
    "went",
    "created",
    "random",
    "images",
    "remember",
    "used",
    "let",
    "show",
    "data",
    "iterator",
    "called",
    "pretty",
    "much",
    "equivalent",
    "using",
    "tensorflow",
    "library",
    "random",
    "passing",
    "know",
    "like",
    "6",
    "comma",
    "28",
    "28",
    "probably",
    "needs",
    "wrapped",
    "go",
    "effectively",
    "generating",
    "random",
    "input",
    "values",
    "going",
    "going",
    "generator",
    "right",
    "actually",
    "wait",
    "120",
    "um",
    "six",
    "128",
    "random",
    "values",
    "passing",
    "random",
    "values",
    "sending",
    "generator",
    "model",
    "able",
    "generate",
    "random",
    "values",
    "generate",
    "random",
    "images",
    "begin",
    "need",
    "real",
    "images",
    "need",
    "fake",
    "images",
    "order",
    "go",
    "ahead",
    "train",
    "model",
    "let",
    "go",
    "going",
    "generate",
    "let",
    "generate",
    "128",
    "got",
    "number",
    "images",
    "real",
    "images",
    "batch",
    "128",
    "128",
    "one",
    "close",
    "going",
    "specify",
    "training",
    "equals",
    "false",
    "deep",
    "generator",
    "training",
    "moment",
    "making",
    "predictions",
    "okay",
    "first",
    "thing",
    "going",
    "train",
    "discriminator",
    "train",
    "discriminator",
    "three",
    "key",
    "steps",
    "need",
    "order",
    "go",
    "train",
    "discriminator",
    "first",
    "one",
    "going",
    "pass",
    "real",
    "fake",
    "images",
    "discriminator",
    "model",
    "need",
    "create",
    "labels",
    "real",
    "fake",
    "images",
    "going",
    "pass",
    "add",
    "noise",
    "outputs",
    "helps",
    "learning",
    "way",
    "fast",
    "calculate",
    "loss",
    "apply",
    "back",
    "prop",
    "propagation",
    "effectively",
    "allows",
    "neural",
    "network",
    "learn",
    "right",
    "going",
    "tab",
    "okay",
    "order",
    "actually",
    "go",
    "train",
    "discriminator",
    "need",
    "start",
    "calculating",
    "gradient",
    "using",
    "tape",
    "method",
    "let",
    "go",
    "ahead",
    "setting",
    "gradient",
    "going",
    "start",
    "calculating",
    "uh",
    "monitoring",
    "one",
    "functions",
    "next",
    "thing",
    "want",
    "actually",
    "next",
    "step",
    "pass",
    "real",
    "fake",
    "images",
    "discriminator",
    "model",
    "let",
    "okay",
    "initial",
    "predictions",
    "right",
    "gone",
    "written",
    "three",
    "lines",
    "code",
    "let",
    "say",
    "break",
    "first",
    "line",
    "actually",
    "taking",
    "real",
    "images",
    "passing",
    "discriminator",
    "storing",
    "values",
    "inside",
    "variable",
    "called",
    "hat",
    "real",
    "hat",
    "real",
    "equals",
    "taking",
    "images",
    "passing",
    "discriminator",
    "model",
    "really",
    "passing",
    "training",
    "equals",
    "true",
    "means",
    "dropout",
    "going",
    "activate",
    "right",
    "pass",
    "training",
    "equals",
    "false",
    "dropout",
    "layers",
    "going",
    "run",
    "also",
    "fake",
    "images",
    "hat",
    "fake",
    "equals",
    "passing",
    "fake",
    "images",
    "specifying",
    "training",
    "equals",
    "true",
    "combining",
    "one",
    "set",
    "outputs",
    "hat",
    "real",
    "fake",
    "equals",
    "concatenates",
    "together",
    "going",
    "passing",
    "hat",
    "real",
    "hat",
    "fake",
    "combining",
    "zero",
    "axis",
    "going",
    "combine",
    "samples",
    "together",
    "cool",
    "first",
    "step",
    "done",
    "actually",
    "gone",
    "produced",
    "predictions",
    "whether",
    "discriminator",
    "thinks",
    "images",
    "real",
    "fake",
    "next",
    "thing",
    "need",
    "actually",
    "create",
    "labels",
    "remember",
    "effectively",
    "supervised",
    "learning",
    "problem",
    "moment",
    "determining",
    "whether",
    "real",
    "fake",
    "zero",
    "one",
    "let",
    "go",
    "okay",
    "predictions",
    "actually",
    "gone",
    "assigned",
    "labels",
    "really",
    "could",
    "go",
    "well",
    "actually",
    "ca",
    "really",
    "generating",
    "new",
    "predictions",
    "every",
    "time",
    "fine",
    "gone",
    "created",
    "new",
    "variable",
    "equal",
    "underscore",
    "real",
    "fake",
    "equivalent",
    "right",
    "predictions",
    "discriminator",
    "actual",
    "labels",
    "underscore",
    "real",
    "fake",
    "setting",
    "order",
    "know",
    "order",
    "particular",
    "case",
    "creating",
    "set",
    "labels",
    "real",
    "images",
    "real",
    "images",
    "going",
    "zero",
    "really",
    "discriminator",
    "spotting",
    "fakes",
    "set",
    "equal",
    "tf",
    "dot",
    "zero",
    "like",
    "pass",
    "hat",
    "reel",
    "cool",
    "thing",
    "effectively",
    "goes",
    "generates",
    "set",
    "zeros",
    "based",
    "shape",
    "input",
    "particular",
    "case",
    "passing",
    "hat",
    "image",
    "hat",
    "real",
    "values",
    "saying",
    "actually",
    "zero",
    "going",
    "shape",
    "real",
    "hat",
    "predictions",
    "going",
    "zeros",
    "going",
    "thing",
    "hat",
    "fake",
    "tf",
    "dot",
    "ones",
    "like",
    "passing",
    "hat",
    "fake",
    "effectively",
    "gon",
    "na",
    "128",
    "zeros",
    "128",
    "ones",
    "going",
    "labels",
    "real",
    "labels",
    "predictions",
    "discriminator",
    "think",
    "think",
    "true",
    "values",
    "going",
    "concatenate",
    "together",
    "using",
    "final",
    "value",
    "going",
    "underscore",
    "real",
    "fake",
    "okay",
    "gone",
    "made",
    "predictions",
    "gone",
    "created",
    "labels",
    "let",
    "quickly",
    "show",
    "xero",
    "like",
    "actually",
    "actually",
    "go",
    "wrap",
    "set",
    "actually",
    "let",
    "latent",
    "variables",
    "let",
    "enable",
    "um",
    "would",
    "would",
    "um",
    "like",
    "something",
    "like",
    "right",
    "predictions",
    "like",
    "see",
    "got",
    "bunch",
    "random",
    "values",
    "right",
    "type",
    "like",
    "pass",
    "see",
    "generating",
    "bunch",
    "ones",
    "shape",
    "tf",
    "0s",
    "like",
    "generating",
    "bunch",
    "zeros",
    "go",
    "concatenate",
    "put",
    "inside",
    "array",
    "let",
    "say",
    "one",
    "like",
    "let",
    "close",
    "array",
    "type",
    "dot",
    "access",
    "equals",
    "zero",
    "see",
    "got",
    "massive",
    "array",
    "got",
    "bunch",
    "zeros",
    "bunch",
    "ones",
    "effectively",
    "gone",
    "done",
    "line",
    "need",
    "add",
    "noise",
    "outputs",
    "calculate",
    "loss",
    "apply",
    "back",
    "prop",
    "discriminator",
    "training",
    "component",
    "done",
    "cool",
    "noise",
    "added",
    "white",
    "white",
    "true",
    "value",
    "particular",
    "case",
    "gone",
    "added",
    "random",
    "value",
    "multiplied",
    "weighting",
    "real",
    "images",
    "negative",
    "fake",
    "images",
    "played",
    "around",
    "bunch",
    "actually",
    "testing",
    "found",
    "adding",
    "values",
    "zero",
    "values",
    "reducing",
    "values",
    "one",
    "values",
    "seemed",
    "work",
    "little",
    "bit",
    "better",
    "otherwise",
    "threw",
    "loss",
    "metrics",
    "um",
    "typically",
    "see",
    "people",
    "adding",
    "random",
    "value",
    "necessarily",
    "adjust",
    "based",
    "whether",
    "zero",
    "value",
    "one",
    "value",
    "true",
    "false",
    "particular",
    "case",
    "gone",
    "customized",
    "little",
    "bit",
    "let",
    "go",
    "lines",
    "three",
    "lines",
    "noise",
    "underscore",
    "real",
    "equals",
    "multiplied",
    "tf",
    "dot",
    "random",
    "uniform",
    "uniform",
    "distribution",
    "adding",
    "creating",
    "values",
    "shape",
    "hat",
    "real",
    "going",
    "match",
    "adding",
    "noise",
    "fake",
    "values",
    "noise",
    "underscore",
    "fake",
    "equals",
    "minus",
    "multiplied",
    "passing",
    "shape",
    "tf",
    "hat",
    "fake",
    "values",
    "adding",
    "value",
    "adding",
    "true",
    "values",
    "underscore",
    "real",
    "fake",
    "plus",
    "equals",
    "effectively",
    "concatenating",
    "values",
    "back",
    "plus",
    "equals",
    "passing",
    "noise",
    "real",
    "noise",
    "fake",
    "injecting",
    "noise",
    "true",
    "outputs",
    "cool",
    "right",
    "done",
    "left",
    "actually",
    "calculate",
    "loss",
    "apply",
    "back",
    "prop",
    "let",
    "calculate",
    "loss",
    "total",
    "underscore",
    "loss",
    "equals",
    "self",
    "dot",
    "plus",
    "going",
    "binary",
    "cross",
    "entropy",
    "remember",
    "binary",
    "cross",
    "want",
    "passing",
    "real",
    "real",
    "fake",
    "hat",
    "underscore",
    "real",
    "fake",
    "hat",
    "real",
    "fake",
    "cool",
    "alrighty",
    "loss",
    "calculated",
    "real",
    "fake",
    "hat",
    "real",
    "fake",
    "going",
    "calculate",
    "want",
    "want",
    "apply",
    "back",
    "prop",
    "going",
    "go",
    "outside",
    "tape",
    "loop",
    "going",
    "first",
    "calculate",
    "gradients",
    "going",
    "apply",
    "gradients",
    "using",
    "optimizer",
    "let",
    "discriminator",
    "trend",
    "cool",
    "training",
    "step",
    "right",
    "discriminated",
    "done",
    "know",
    "wrote",
    "ton",
    "stuff",
    "pretty",
    "hardcore",
    "terms",
    "deep",
    "learning",
    "first",
    "thing",
    "went",
    "calculated",
    "gradients",
    "specified",
    "new",
    "variable",
    "called",
    "grad",
    "set",
    "equal",
    "dot",
    "underscore",
    "tape",
    "dot",
    "gradient",
    "remember",
    "using",
    "tape",
    "function",
    "allows",
    "us",
    "calculate",
    "operations",
    "happening",
    "using",
    "deep",
    "neural",
    "network",
    "means",
    "use",
    "calculate",
    "gradient",
    "respect",
    "loss",
    "underscore",
    "first",
    "parameter",
    "pass",
    "loss",
    "remember",
    "binary",
    "cross",
    "entropy",
    "want",
    "calculate",
    "gradient",
    "one",
    "trainable",
    "variables",
    "going",
    "extract",
    "trainable",
    "variables",
    "using",
    "dot",
    "trainable",
    "underscore",
    "variables",
    "going",
    "going",
    "apply",
    "gradients",
    "using",
    "optimizer",
    "remember",
    "passed",
    "ad",
    "optimizer",
    "atom",
    "optimizer",
    "use",
    "dot",
    "apply",
    "gradients",
    "actually",
    "go",
    "apply",
    "back",
    "prop",
    "pass",
    "zip",
    "want",
    "gradient",
    "respect",
    "one",
    "trainable",
    "variables",
    "zip",
    "passing",
    "grad",
    "trainable",
    "variables",
    "effectively",
    "going",
    "look",
    "gradient",
    "one",
    "variables",
    "apply",
    "backprop",
    "using",
    "learning",
    "rate",
    "okay",
    "discriminator",
    "done",
    "know",
    "hardest",
    "bit",
    "keep",
    "mind",
    "hardest",
    "bit",
    "training",
    "step",
    "first",
    "thing",
    "went",
    "went",
    "passed",
    "real",
    "fake",
    "images",
    "discriminator",
    "model",
    "went",
    "concatenated",
    "together",
    "pretty",
    "crazy",
    "went",
    "created",
    "labels",
    "real",
    "fake",
    "images",
    "remember",
    "real",
    "images",
    "gon",
    "na",
    "zero",
    "label",
    "fake",
    "images",
    "gon",
    "na",
    "one",
    "label",
    "tune",
    "slightly",
    "differently",
    "wanted",
    "switch",
    "around",
    "keep",
    "mind",
    "got",
    "change",
    "noise",
    "well",
    "change",
    "um",
    "think",
    "switch",
    "around",
    "okay",
    "need",
    "go",
    "anything",
    "else",
    "keep",
    "mind",
    "gone",
    "set",
    "important",
    "actually",
    "going",
    "go",
    "set",
    "generator",
    "inject",
    "bunch",
    "noise",
    "noise",
    "really",
    "uh",
    "applied",
    "true",
    "outputs",
    "gives",
    "us",
    "um",
    "discriminator",
    "little",
    "bit",
    "less",
    "certainty",
    "whether",
    "predicting",
    "correctly",
    "confuses",
    "little",
    "bit",
    "go",
    "pass",
    "random",
    "set",
    "values",
    "using",
    "scaling",
    "real",
    "outputs",
    "adds",
    "values",
    "zero",
    "zero",
    "labels",
    "subtracting",
    "multiplied",
    "fake",
    "labels",
    "calculate",
    "loss",
    "using",
    "underscore",
    "loss",
    "binary",
    "cross",
    "entropy",
    "go",
    "apply",
    "backprop",
    "using",
    "gradient",
    "tape",
    "apply",
    "gradients",
    "method",
    "atom",
    "optimizer",
    "okay",
    "pretty",
    "brutal",
    "quite",
    "difficult",
    "hardest",
    "bit",
    "done",
    "trust",
    "hardest",
    "bit",
    "actually",
    "going",
    "setting",
    "custom",
    "training",
    "loop",
    "probably",
    "hardest",
    "one",
    "find",
    "lot",
    "deep",
    "neural",
    "networks",
    "actually",
    "go",
    "build",
    "congratulations",
    "went",
    "wrote",
    "well",
    "right",
    "got",
    "actually",
    "go",
    "set",
    "training",
    "step",
    "training",
    "component",
    "generator",
    "going",
    "similar",
    "means",
    "complicated",
    "wrote",
    "let",
    "go",
    "generator",
    "going",
    "use",
    "gradient",
    "tape",
    "tape",
    "g",
    "tape",
    "let",
    "actually",
    "write",
    "let",
    "actually",
    "call",
    "uh",
    "train",
    "generator",
    "okay",
    "first",
    "thing",
    "need",
    "generate",
    "new",
    "images",
    "going",
    "going",
    "uh",
    "apply",
    "create",
    "predicted",
    "labels",
    "little",
    "bit",
    "explain",
    "going",
    "calculate",
    "loss",
    "going",
    "apply",
    "backprop",
    "cool",
    "alrighty",
    "let",
    "go",
    "ahead",
    "new",
    "set",
    "images",
    "generated",
    "gone",
    "created",
    "new",
    "variable",
    "called",
    "gen",
    "underscore",
    "images",
    "set",
    "equal",
    "self",
    "dot",
    "generator",
    "pretty",
    "much",
    "time",
    "setting",
    "training",
    "parameter",
    "equal",
    "true",
    "passing",
    "passing",
    "128",
    "128x1",
    "128",
    "random",
    "variables",
    "passed",
    "generator",
    "specify",
    "training",
    "equals",
    "true",
    "going",
    "create",
    "predicted",
    "labels",
    "little",
    "bit",
    "actually",
    "went",
    "created",
    "discriminator",
    "actually",
    "said",
    "fake",
    "values",
    "label",
    "one",
    "trying",
    "trick",
    "discriminator",
    "actually",
    "going",
    "going",
    "set",
    "variables",
    "generator",
    "zero",
    "want",
    "discriminator",
    "actually",
    "lose",
    "want",
    "think",
    "generated",
    "images",
    "actually",
    "real",
    "images",
    "going",
    "calculate",
    "loss",
    "generator",
    "see",
    "second",
    "let",
    "actually",
    "go",
    "okay",
    "next",
    "two",
    "lines",
    "code",
    "written",
    "gotten",
    "written",
    "two",
    "lines",
    "code",
    "first",
    "thing",
    "running",
    "generated",
    "images",
    "discriminator",
    "predicted",
    "underscore",
    "labels",
    "equals",
    "self",
    "dot",
    "discriminator",
    "passing",
    "generated",
    "images",
    "setting",
    "training",
    "equals",
    "false",
    "want",
    "discriminator",
    "learning",
    "whilst",
    "actually",
    "training",
    "generator",
    "nuance",
    "actually",
    "go",
    "set",
    "going",
    "generate",
    "predicted",
    "outputs",
    "whether",
    "discriminator",
    "determining",
    "whether",
    "generated",
    "images",
    "real",
    "false",
    "remember",
    "discriminator",
    "output",
    "1",
    "thinks",
    "image",
    "false",
    "expect",
    "ones",
    "try",
    "confuse",
    "discriminator",
    "actually",
    "saying",
    "generator",
    "loss",
    "rewarded",
    "every",
    "time",
    "thinks",
    "discriminator",
    "every",
    "time",
    "thinks",
    "generated",
    "images",
    "actually",
    "real",
    "go",
    "calculate",
    "loss",
    "total",
    "underscore",
    "g",
    "underscore",
    "loss",
    "equal",
    "loss",
    "actually",
    "saying",
    "real",
    "labels",
    "actually",
    "zeros",
    "actually",
    "saying",
    "real",
    "image",
    "generated",
    "images",
    "actually",
    "fake",
    "images",
    "something",
    "took",
    "little",
    "get",
    "head",
    "around",
    "actually",
    "go",
    "train",
    "generator",
    "like",
    "passing",
    "predicted",
    "labels",
    "passing",
    "discriminator",
    "actually",
    "predicted",
    "whether",
    "actually",
    "predicted",
    "true",
    "false",
    "go",
    "back",
    "prop",
    "train",
    "generator",
    "able",
    "produce",
    "better",
    "images",
    "every",
    "single",
    "time",
    "trick",
    "right",
    "trick",
    "training",
    "fake",
    "discriminator",
    "time",
    "generator",
    "learn",
    "produce",
    "images",
    "better",
    "faking",
    "outputs",
    "discriminator",
    "actually",
    "start",
    "getting",
    "zeros",
    "passing",
    "discriminator",
    "discriminator",
    "wo",
    "know",
    "difference",
    "real",
    "image",
    "generated",
    "image",
    "really",
    "left",
    "actually",
    "go",
    "calculate",
    "gradients",
    "apply",
    "back",
    "prop",
    "return",
    "loss",
    "metrics",
    "able",
    "train",
    "let",
    "go",
    "wrap",
    "alrighty",
    "cool",
    "going",
    "applying",
    "backprop",
    "almost",
    "identical",
    "discriminator",
    "first",
    "calculate",
    "gradients",
    "g",
    "grad",
    "equals",
    "g",
    "underscore",
    "tape",
    "dot",
    "gradient",
    "pass",
    "losses",
    "first",
    "metric",
    "pass",
    "variables",
    "want",
    "calculate",
    "gradient",
    "nice",
    "spelling",
    "trainable",
    "spelt",
    "wrong",
    "spell",
    "wrong",
    "good",
    "right",
    "passing",
    "want",
    "calculate",
    "gradients",
    "going",
    "applying",
    "gradients",
    "using",
    "optimizer",
    "self",
    "dot",
    "g",
    "opt",
    "dot",
    "apply",
    "gradients",
    "zipping",
    "want",
    "time",
    "zip",
    "grad",
    "trainable",
    "variables",
    "need",
    "return",
    "gone",
    "tested",
    "gone",
    "written",
    "lot",
    "code",
    "tested",
    "need",
    "return",
    "loss",
    "metrics",
    "underscore",
    "loss",
    "equal",
    "lot",
    "total",
    "loss",
    "yeah",
    "total",
    "important",
    "actually",
    "monitoring",
    "model",
    "want",
    "ensure",
    "loss",
    "metrics",
    "reducing",
    "steadily",
    "together",
    "terms",
    "one",
    "blowing",
    "versus",
    "sort",
    "reducing",
    "actually",
    "staying",
    "stable",
    "really",
    "expect",
    "reduce",
    "much",
    "honest",
    "um",
    "total",
    "perfect",
    "think",
    "done",
    "knows",
    "probably",
    "gon",
    "na",
    "need",
    "bunch",
    "debugging",
    "code",
    "going",
    "available",
    "wrote",
    "ton",
    "full",
    "subclass",
    "model",
    "fashion",
    "quite",
    "ton",
    "ask",
    "long",
    "took",
    "learn",
    "stuff",
    "trying",
    "teach",
    "right",
    "cool",
    "let",
    "uh",
    "got",
    "first",
    "error",
    "line",
    "178",
    "predicted",
    "label",
    "got",
    "error",
    "guessing",
    "closed",
    "got",
    "another",
    "error",
    "total",
    "g",
    "loss",
    "happened",
    "get",
    "rid",
    "looks",
    "like",
    "got",
    "misplayed",
    "right",
    "cool",
    "go",
    "gone",
    "created",
    "subclass",
    "model",
    "look",
    "like",
    "syntax",
    "errors",
    "training",
    "errors",
    "completely",
    "different",
    "beast",
    "let",
    "actually",
    "go",
    "kick",
    "actually",
    "want",
    "create",
    "instance",
    "model",
    "going",
    "set",
    "callback",
    "train",
    "review",
    "performance",
    "let",
    "kick",
    "cool",
    "gone",
    "created",
    "instance",
    "fashion",
    "game",
    "class",
    "create",
    "instance",
    "subclassed",
    "model",
    "written",
    "fashgen",
    "equals",
    "fashiongan",
    "instance",
    "big",
    "bad",
    "boy",
    "class",
    "passing",
    "generator",
    "discriminator",
    "init",
    "method",
    "expects",
    "going",
    "compile",
    "pass",
    "losses",
    "optimizes",
    "let",
    "order",
    "expected",
    "think",
    "actually",
    "copy",
    "named",
    "variables",
    "uh",
    "g",
    "op",
    "g",
    "lost",
    "loss",
    "g",
    "object",
    "cool",
    "cool",
    "compiled",
    "successfully",
    "going",
    "pass",
    "generator",
    "optimizer",
    "discriminator",
    "optimizer",
    "still",
    "zoomed",
    "yeah",
    "okay",
    "done",
    "step",
    "done",
    "know",
    "took",
    "quite",
    "fair",
    "bit",
    "get",
    "rid",
    "uh",
    "custom",
    "training",
    "loop",
    "subclass",
    "model",
    "done",
    "callback",
    "going",
    "copy",
    "used",
    "bunch",
    "times",
    "actually",
    "think",
    "francois",
    "chorley",
    "sample",
    "actually",
    "tweaked",
    "little",
    "bit",
    "get",
    "work",
    "going",
    "walk",
    "going",
    "import",
    "bunch",
    "dependencies",
    "import",
    "os",
    "helps",
    "folder",
    "navigation",
    "going",
    "import",
    "eraser",
    "image",
    "function",
    "import",
    "array",
    "image",
    "callbacks",
    "import",
    "callback",
    "allows",
    "us",
    "create",
    "custom",
    "callback",
    "epoc",
    "end",
    "go",
    "import",
    "going",
    "write",
    "use",
    "ton",
    "times",
    "gans",
    "going",
    "show",
    "need",
    "tweak",
    "model",
    "monitor",
    "going",
    "allow",
    "us",
    "save",
    "examples",
    "generated",
    "images",
    "actually",
    "training",
    "two",
    "key",
    "parameters",
    "want",
    "pass",
    "number",
    "images",
    "want",
    "generate",
    "big",
    "latent",
    "dimension",
    "remember",
    "latent",
    "dimension",
    "random",
    "values",
    "passing",
    "generator",
    "generate",
    "random",
    "image",
    "epoc",
    "end",
    "going",
    "go",
    "bunch",
    "stuff",
    "actually",
    "going",
    "generate",
    "random",
    "values",
    "going",
    "pass",
    "model",
    "generator",
    "going",
    "generate",
    "bunch",
    "images",
    "interesting",
    "using",
    "uniform",
    "yeah",
    "could",
    "actually",
    "go",
    "right",
    "want",
    "mess",
    "around",
    "wonder",
    "knows",
    "maybe",
    "later",
    "um",
    "okay",
    "going",
    "generating",
    "images",
    "going",
    "converting",
    "numpy",
    "array",
    "saving",
    "important",
    "bit",
    "wanted",
    "talk",
    "terms",
    "actually",
    "saving",
    "images",
    "need",
    "create",
    "folder",
    "called",
    "images",
    "going",
    "images",
    "going",
    "save",
    "part",
    "callback",
    "inside",
    "root",
    "folder",
    "notebook",
    "currently",
    "working",
    "create",
    "another",
    "folder",
    "called",
    "images",
    "literally",
    "new",
    "folder",
    "call",
    "images",
    "images",
    "saved",
    "wanted",
    "save",
    "somewhere",
    "else",
    "let",
    "say",
    "example",
    "wanted",
    "call",
    "gan",
    "images",
    "need",
    "change",
    "path",
    "folder",
    "want",
    "go",
    "case",
    "going",
    "save",
    "images",
    "got",
    "configured",
    "could",
    "change",
    "code",
    "going",
    "available",
    "optional",
    "need",
    "go",
    "use",
    "callback",
    "found",
    "little",
    "bit",
    "easier",
    "actually",
    "training",
    "model",
    "going",
    "create",
    "train",
    "two",
    "really",
    "sort",
    "optional",
    "definitely",
    "help",
    "monitoring",
    "model",
    "okay",
    "training",
    "gone",
    "created",
    "subclass",
    "model",
    "created",
    "generator",
    "discriminator",
    "gone",
    "created",
    "callback",
    "actually",
    "go",
    "train",
    "equals",
    "fashscan",
    "dot",
    "train",
    "oh",
    "actually",
    "fit",
    "trained",
    "god",
    "falling",
    "asleep",
    "uh",
    "pass",
    "data",
    "set",
    "remember",
    "created",
    "data",
    "set",
    "ds",
    "dot",
    "numpy",
    "iterator",
    "dot",
    "next",
    "data",
    "set",
    "let",
    "take",
    "look",
    "shape",
    "128",
    "28",
    "28",
    "one",
    "pass",
    "data",
    "set",
    "specify",
    "number",
    "epochs",
    "want",
    "train",
    "recommend",
    "2",
    "000",
    "going",
    "train",
    "2000",
    "going",
    "take",
    "ages",
    "recommend",
    "2000",
    "epochs",
    "gon",
    "na",
    "take",
    "long",
    "time",
    "wo",
    "doubt",
    "gon",
    "na",
    "lie",
    "gon",
    "na",
    "take",
    "quite",
    "fair",
    "bit",
    "time",
    "probably",
    "train",
    "20",
    "going",
    "give",
    "model",
    "use",
    "well",
    "want",
    "use",
    "callback",
    "wanted",
    "use",
    "pass",
    "callbacks",
    "pass",
    "model",
    "monitor",
    "name",
    "callback",
    "cool",
    "alrighty",
    "cross",
    "fingers",
    "wanted",
    "wrote",
    "entire",
    "training",
    "loop",
    "without",
    "testing",
    "might",
    "errors",
    "actually",
    "see",
    "go",
    "run",
    "got",
    "error",
    "already",
    "right",
    "happening",
    "sequential",
    "object",
    "attribute",
    "trainable",
    "variables",
    "pretty",
    "sure",
    "spelt",
    "wrong",
    "see",
    "definitely",
    "line",
    "kicking",
    "opt",
    "yeah",
    "thought",
    "wrote",
    "wrong",
    "trainable",
    "var",
    "hope",
    "yeah",
    "see",
    "got",
    "error",
    "typo",
    "copy",
    "could",
    "okay",
    "let",
    "run",
    "recreate",
    "fashion",
    "gain",
    "instance",
    "recompile",
    "got",
    "another",
    "error",
    "got",
    "unexpected",
    "word",
    "training",
    "okay",
    "closed",
    "incorrectly",
    "fine",
    "um",
    "reason",
    "getting",
    "error",
    "effectively",
    "passing",
    "training",
    "equals",
    "true",
    "normal",
    "see",
    "missing",
    "closing",
    "brackets",
    "better",
    "remove",
    "one",
    "okay",
    "let",
    "run",
    "recompile",
    "try",
    "fit",
    "looking",
    "promising",
    "go",
    "training",
    "notice",
    "loss",
    "g",
    "loss",
    "sort",
    "balance",
    "go",
    "one",
    "going",
    "blow",
    "ideally",
    "want",
    "one",
    "blow",
    "want",
    "one",
    "decrease",
    "really",
    "fast",
    "want",
    "one",
    "increase",
    "really",
    "fast",
    "want",
    "sort",
    "kind",
    "stay",
    "steady",
    "kind",
    "noticing",
    "right",
    "sort",
    "bubbling",
    "around",
    "68",
    "67",
    "likewise",
    "sort",
    "getting",
    "around",
    "well",
    "might",
    "increase",
    "occasionally",
    "ideally",
    "want",
    "stable",
    "longer",
    "term",
    "going",
    "going",
    "let",
    "train",
    "uh",
    "let",
    "give",
    "second",
    "ensure",
    "generates",
    "images",
    "remember",
    "going",
    "generate",
    "image",
    "epoc",
    "end",
    "dictated",
    "callback",
    "go",
    "images",
    "see",
    "generated",
    "seconds",
    "ago",
    "initial",
    "image",
    "literally",
    "random",
    "black",
    "square",
    "little",
    "white",
    "bits",
    "corners",
    "nothing",
    "crazy",
    "moment",
    "common",
    "suck",
    "ton",
    "epochs",
    "get",
    "way",
    "better",
    "keep",
    "mind",
    "see",
    "generator",
    "already",
    "spiking",
    "spiking",
    "massively",
    "definitely",
    "increasing",
    "let",
    "go",
    "review",
    "model",
    "performance",
    "load",
    "model",
    "give",
    "ability",
    "go",
    "download",
    "go",
    "back",
    "five",
    "minutes",
    "later",
    "righty",
    "beginnings",
    "gan",
    "train",
    "actually",
    "take",
    "look",
    "loss",
    "see",
    "discriminator",
    "started",
    "stabilize",
    "lot",
    "versus",
    "generators",
    "starting",
    "reduce",
    "loss",
    "good",
    "sign",
    "perfectly",
    "okay",
    "run",
    "full",
    "2000",
    "see",
    "start",
    "converge",
    "sort",
    "reach",
    "happy",
    "medium",
    "um",
    "perfectly",
    "okay",
    "long",
    "getting",
    "like",
    "one",
    "blowing",
    "one",
    "going",
    "massively",
    "low",
    "ideally",
    "want",
    "like",
    "want",
    "see",
    "one",
    "like",
    "let",
    "say",
    "example",
    "discriminators",
    "training",
    "goes",
    "loss",
    "generator",
    "spikes",
    "control",
    "probably",
    "indicator",
    "mode",
    "collapse",
    "fact",
    "one",
    "training",
    "rates",
    "way",
    "high",
    "low",
    "um",
    "got",
    "stabilize",
    "got",
    "stabilize",
    "uh",
    "looks",
    "like",
    "okay",
    "going",
    "give",
    "models",
    "well",
    "one",
    "went",
    "trained",
    "2000",
    "epoch",
    "test",
    "let",
    "quickly",
    "go",
    "review",
    "performance",
    "jump",
    "back",
    "client",
    "saved",
    "training",
    "inside",
    "variable",
    "called",
    "hist",
    "access",
    "performance",
    "type",
    "history",
    "see",
    "got",
    "discriminator",
    "loss",
    "got",
    "generator",
    "loss",
    "actually",
    "go",
    "take",
    "look",
    "let",
    "actually",
    "create",
    "plot",
    "perfect",
    "alrighty",
    "see",
    "happening",
    "discriminator",
    "sort",
    "steadied",
    "towards",
    "let",
    "zoom",
    "generator",
    "spiked",
    "see",
    "definitely",
    "coming",
    "back",
    "pretty",
    "common",
    "saw",
    "ton",
    "times",
    "keep",
    "mind",
    "trained",
    "28",
    "bucks",
    "means",
    "end",
    "state",
    "train",
    "ton",
    "longer",
    "get",
    "really",
    "really",
    "good",
    "model",
    "think",
    "actually",
    "full",
    "2000",
    "epochs",
    "see",
    "discriminator",
    "sort",
    "bounced",
    "around",
    "bottom",
    "something",
    "actually",
    "would",
    "seem",
    "right",
    "um",
    "generator",
    "coming",
    "means",
    "generator",
    "learning",
    "generate",
    "real",
    "images",
    "good",
    "sign",
    "training",
    "done",
    "obviously",
    "train",
    "way",
    "longer",
    "get",
    "much",
    "better",
    "performance",
    "um",
    "long",
    "see",
    "one",
    "spike",
    "spike",
    "okay",
    "looking",
    "good",
    "let",
    "jump",
    "back",
    "client",
    "going",
    "wrap",
    "testing",
    "oh",
    "actually",
    "wait",
    "also",
    "generated",
    "ton",
    "images",
    "right",
    "actually",
    "take",
    "look",
    "see",
    "looking",
    "like",
    "already",
    "looks",
    "like",
    "got",
    "bunch",
    "white",
    "images",
    "pretty",
    "crap",
    "moment",
    "probably",
    "going",
    "show",
    "final",
    "train",
    "model",
    "right",
    "see",
    "got",
    "dots",
    "let",
    "zoom",
    "see",
    "getting",
    "images",
    "right",
    "white",
    "really",
    "poor",
    "indicator",
    "performance",
    "sucking",
    "train",
    "way",
    "longer",
    "going",
    "start",
    "get",
    "significantly",
    "better",
    "performance",
    "see",
    "uh",
    "epoch",
    "19",
    "effectively",
    "epoch",
    "20",
    "starting",
    "learn",
    "generate",
    "images",
    "right",
    "bottom",
    "literally",
    "starts",
    "takes",
    "long",
    "time",
    "get",
    "one",
    "fully",
    "trained",
    "think",
    "actually",
    "spend",
    "full",
    "2000",
    "epochs",
    "okay",
    "let",
    "go",
    "back",
    "client",
    "chat",
    "go",
    "left",
    "test",
    "sweet",
    "let",
    "use",
    "generator",
    "model",
    "make",
    "predictions",
    "visualize",
    "tiny",
    "images",
    "using",
    "matplotlib",
    "save",
    "time",
    "also",
    "model",
    "use",
    "available",
    "github",
    "guys",
    "nice",
    "alrighty",
    "gone",
    "trained",
    "model",
    "let",
    "actually",
    "go",
    "test",
    "loading",
    "model",
    "let",
    "actually",
    "test",
    "baseline",
    "model",
    "actually",
    "load",
    "weights",
    "models",
    "going",
    "give",
    "weights",
    "try",
    "upload",
    "github",
    "test",
    "first",
    "thing",
    "need",
    "actually",
    "go",
    "make",
    "predictions",
    "create",
    "variable",
    "called",
    "images",
    "use",
    "generator",
    "trained",
    "subclass",
    "model",
    "generator",
    "dot",
    "predict",
    "use",
    "pass",
    "number",
    "images",
    "want",
    "generate",
    "let",
    "generate",
    "like",
    "16",
    "pass",
    "latent",
    "variables",
    "pass",
    "one",
    "shape",
    "expecting",
    "generate",
    "images",
    "images",
    "perfect",
    "images",
    "generated",
    "see",
    "next",
    "thing",
    "actually",
    "visualize",
    "using",
    "matplotlib",
    "rather",
    "looking",
    "random",
    "set",
    "arrays",
    "actually",
    "go",
    "visualize",
    "little",
    "bit",
    "easier",
    "let",
    "go",
    "perfect",
    "getting",
    "generators",
    "pretty",
    "crap",
    "moment",
    "trained",
    "jack",
    "time",
    "20",
    "epochs",
    "absolutely",
    "nothing",
    "let",
    "actually",
    "take",
    "look",
    "would",
    "look",
    "like",
    "trained",
    "model",
    "one",
    "actually",
    "trained",
    "quite",
    "time",
    "rather",
    "using",
    "base",
    "generator",
    "load",
    "weight",
    "got",
    "weights",
    "sitting",
    "folder",
    "called",
    "archive",
    "got",
    "discriminator",
    "weights",
    "generator",
    "weights",
    "need",
    "discriminatory",
    "really",
    "training",
    "thing",
    "let",
    "actually",
    "load",
    "go",
    "uh",
    "generator",
    "dot",
    "load",
    "weights",
    "going",
    "go",
    "dot",
    "join",
    "archive",
    "generator",
    "perfect",
    "looks",
    "like",
    "loaded",
    "go",
    "run",
    "line",
    "boom",
    "see",
    "generating",
    "fashion",
    "exact",
    "architecture",
    "difference",
    "gone",
    "trained",
    "2000",
    "ebooks",
    "see",
    "running",
    "type",
    "architecture",
    "actually",
    "quite",
    "fair",
    "bit",
    "obviously",
    "takes",
    "long",
    "time",
    "train",
    "typically",
    "leaving",
    "computer",
    "training",
    "overnight",
    "actually",
    "spinning",
    "collab",
    "instance",
    "background",
    "execution",
    "think",
    "might",
    "need",
    "collab",
    "pro",
    "honestly",
    "pretty",
    "worth",
    "considering",
    "take",
    "quite",
    "fair",
    "bit",
    "time",
    "train",
    "found",
    "better",
    "results",
    "training",
    "different",
    "architectures",
    "better",
    "ways",
    "training",
    "faster",
    "let",
    "know",
    "let",
    "actually",
    "make",
    "little",
    "bit",
    "smaller",
    "see",
    "see",
    "actually",
    "generating",
    "images",
    "fashion",
    "got",
    "pants",
    "got",
    "bags",
    "got",
    "jumpers",
    "got",
    "shoes",
    "know",
    "think",
    "might",
    "bag",
    "go",
    "generate",
    "images",
    "got",
    "like",
    "sort",
    "sneakers",
    "know",
    "slightly",
    "different",
    "type",
    "bag",
    "slightly",
    "like",
    "men",
    "boot",
    "something",
    "sandal",
    "let",
    "go",
    "generate",
    "see",
    "obviously",
    "super",
    "powerful",
    "like",
    "done",
    "fashion",
    "could",
    "anything",
    "um",
    "obviously",
    "keep",
    "mind",
    "training",
    "going",
    "big",
    "factor",
    "amount",
    "compute",
    "going",
    "need",
    "going",
    "huge",
    "factor",
    "saw",
    "model",
    "performed",
    "like",
    "training",
    "using",
    "old",
    "baseline",
    "model",
    "one",
    "make",
    "weights",
    "available",
    "via",
    "github",
    "actually",
    "test",
    "seen",
    "performance",
    "looks",
    "like",
    "actually",
    "gone",
    "trained",
    "lot",
    "longer",
    "think",
    "training",
    "actually",
    "managed",
    "get",
    "generator",
    "loss",
    "pretty",
    "close",
    "discriminator",
    "think",
    "ended",
    "balancing",
    "around",
    "one",
    "bouncing",
    "around",
    "indicator",
    "might",
    "need",
    "get",
    "order",
    "get",
    "something",
    "actually",
    "produces",
    "images",
    "look",
    "like",
    "last",
    "thing",
    "want",
    "case",
    "want",
    "use",
    "later",
    "wanted",
    "could",
    "actually",
    "save",
    "model",
    "using",
    "generator",
    "dot",
    "save",
    "pass",
    "oh",
    "uh",
    "pass",
    "whatever",
    "want",
    "name",
    "call",
    "save",
    "discriminator",
    "well",
    "grim",
    "inator",
    "dot",
    "h5",
    "boom",
    "see",
    "go",
    "base",
    "model",
    "gone",
    "saved",
    "discriminator",
    "generator",
    "done",
    "nutshell",
    "gone",
    "done",
    "ton",
    "stuff",
    "terms",
    "first",
    "imported",
    "installed",
    "dependencies",
    "went",
    "visualized",
    "images",
    "remember",
    "real",
    "images",
    "saw",
    "generated",
    "images",
    "look",
    "like",
    "took",
    "look",
    "data",
    "sets",
    "building",
    "neural",
    "network",
    "creating",
    "generator",
    "saw",
    "effectively",
    "generator",
    "passing",
    "outputs",
    "discriminator",
    "went",
    "built",
    "discriminator",
    "built",
    "monster",
    "training",
    "loop",
    "obviously",
    "takes",
    "quite",
    "fair",
    "bit",
    "effort",
    "uh",
    "took",
    "look",
    "callback",
    "training",
    "length",
    "time",
    "train",
    "means",
    "long",
    "enough",
    "actually",
    "get",
    "great",
    "result",
    "recommend",
    "2000",
    "epochs",
    "20",
    "time",
    "permitting",
    "got",
    "much",
    "time",
    "generate",
    "tutorials",
    "figured",
    "least",
    "try",
    "get",
    "guys",
    "um",
    "show",
    "model",
    "look",
    "like",
    "went",
    "ton",
    "time",
    "ago",
    "end",
    "output",
    "looks",
    "like",
    "go",
    "generate",
    "fashion",
    "wanted",
    "see",
    "generating",
    "generating",
    "bunch",
    "random",
    "different",
    "types",
    "images",
    "kind",
    "pass",
    "fashion",
    "could",
    "even",
    "try",
    "colored",
    "images",
    "see",
    "outputs",
    "look",
    "like",
    "let",
    "know",
    "innovate",
    "always",
    "interested",
    "see",
    "guys",
    "build",
    "thanks",
    "tuning",
    "guys",
    "peace",
    "thanks",
    "much",
    "tuning",
    "guys",
    "hopefully",
    "enjoyed",
    "video",
    "sure",
    "give",
    "big",
    "thumbs",
    "hit",
    "subscribe",
    "tick",
    "bell",
    "good",
    "stuff",
    "hopefully",
    "enjoyed",
    "video",
    "sort",
    "went",
    "back",
    "old",
    "tutorial",
    "style",
    "coding",
    "scratch",
    "building",
    "models",
    "testing",
    "real",
    "time",
    "thanks",
    "tuning",
    "guys",
    "peace",
    "okay",
    "neural",
    "network",
    "first",
    "thing",
    "need",
    "uh"
  ],
  "keywords": [
    "one",
    "learning",
    "video",
    "going",
    "using",
    "exact",
    "neural",
    "network",
    "get",
    "let",
    "bunch",
    "stuff",
    "build",
    "fashion",
    "line",
    "could",
    "generate",
    "images",
    "whole",
    "different",
    "types",
    "take",
    "step",
    "code",
    "way",
    "dependencies",
    "getting",
    "data",
    "good",
    "bit",
    "actually",
    "generator",
    "model",
    "component",
    "able",
    "new",
    "little",
    "discriminator",
    "think",
    "trying",
    "fake",
    "real",
    "image",
    "custom",
    "training",
    "loop",
    "visualize",
    "use",
    "well",
    "generating",
    "models",
    "things",
    "like",
    "super",
    "us",
    "first",
    "thing",
    "need",
    "set",
    "back",
    "really",
    "wanted",
    "okay",
    "building",
    "order",
    "tensorflow",
    "api",
    "something",
    "got",
    "random",
    "particular",
    "numbers",
    "latent",
    "might",
    "1",
    "output",
    "values",
    "say",
    "shape",
    "28",
    "case",
    "128",
    "allows",
    "number",
    "convolutional",
    "layers",
    "sampling",
    "eventually",
    "probably",
    "effectively",
    "generated",
    "pass",
    "final",
    "zero",
    "whether",
    "true",
    "work",
    "time",
    "also",
    "make",
    "uh",
    "train",
    "weights",
    "kind",
    "key",
    "cool",
    "deep",
    "create",
    "variables",
    "go",
    "called",
    "gives",
    "keep",
    "alrighty",
    "client",
    "done",
    "gon",
    "na",
    "look",
    "pretty",
    "test",
    "ahead",
    "around",
    "matplotlib",
    "written",
    "gpu",
    "plot",
    "sets",
    "used",
    "next",
    "available",
    "yeah",
    "rather",
    "right",
    "run",
    "show",
    "see",
    "mind",
    "times",
    "want",
    "type",
    "um",
    "means",
    "two",
    "important",
    "moment",
    "growth",
    "ton",
    "memory",
    "equal",
    "write",
    "gone",
    "four",
    "lines",
    "bring",
    "import",
    "tf",
    "bringing",
    "three",
    "grab",
    "equals",
    "dot",
    "underscore",
    "every",
    "single",
    "sort",
    "setting",
    "remember",
    "function",
    "passing",
    "iterator",
    "second",
    "brought",
    "ds",
    "specified",
    "know",
    "would",
    "went",
    "took",
    "numpy",
    "looks",
    "array",
    "label",
    "obviously",
    "labels",
    "components",
    "taking",
    "try",
    "pipeline",
    "call",
    "batch",
    "big",
    "size",
    "another",
    "change",
    "boom",
    "find",
    "outputs",
    "much",
    "easier",
    "subplot",
    "total",
    "end",
    "grabbing",
    "sample",
    "inside",
    "value",
    "index",
    "0",
    "performance",
    "guys",
    "typically",
    "scale",
    "better",
    "apply",
    "oh",
    "shuffle",
    "looking",
    "variable",
    "passed",
    "specifying",
    "built",
    "define",
    "learn",
    "sequential",
    "input",
    "binary",
    "dense",
    "layer",
    "spatial",
    "leaky",
    "relu",
    "activation",
    "add",
    "adding",
    "block",
    "added",
    "takes",
    "arguments",
    "7",
    "give",
    "seven",
    "blocks",
    "14",
    "parameters",
    "trained",
    "trainable",
    "dropout",
    "false",
    "error",
    "noise",
    "quite",
    "loss",
    "optimizer",
    "subclass",
    "losses",
    "optimizers",
    "cross",
    "entropy",
    "atom",
    "gradient",
    "g",
    "created",
    "class",
    "base",
    "method",
    "init",
    "compile",
    "predictions",
    "calculate",
    "tape",
    "hat",
    "gradients",
    "predicted",
    "callback",
    "save",
    "2000"
  ]
}