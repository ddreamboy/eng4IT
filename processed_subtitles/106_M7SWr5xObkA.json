{
  "text": "hey what's up everyone and welcome back\nto another video so in this video we're\ngonna walk through a comprehensive\ntutorial on natural language processing\nin python i actually originally created\nthis tutorial for the pycon conference\nand i've actually had it on my channel\nin the data science playlist but i've\nrealized through a bunch of messages and\nyoutube comments that not a lot of you\nhave seen it so i figured i'd repost it\non my channel i did film this video over\na year ago but all the major concepts\nremain the same if you do have any\ntroubles like installing a library or a\npackage so let me know in the comments\nand i'll try my best to help as many of\ny'all out as possible a couple quick\nannouncements before i get started i did\njust recently create a patreon and i\nalso am starting to offer channel\nmemberships here on youtube a bunch of\ncool perks and a great way to support\nthe channel i definitely recommend\nchecking those out i left a\nlinks to join in the description as well\nas a video i'll pop it up\nright here i think\nthat you can watch to see all the\ndetails on the the memberships and what\nperks you get a lot of cool stuff uh\nexciting way to kind of engage with\ny'all a little bit more that's all i\nhave hopefully enjoyed this tutorial\ntime to travel back in time\npeace\nhey how's it going everyone and welcome\nto my pycon 2020 tutorial on natural\nlanguage processing in python\ni'm bummed that i couldn't be there in\nperson to share this with all of you but\nthis online presentation is the next\nbest thing so i'm excited to get started\nin this tutorial we'll kind of walk\nthrough so we'll start with the basics\nand kind of some of the more traditional\nnlp models and we'll walk our way up to\nsome miscellaneous nlp techniques all\nthe way up to these state-of-the-art\nlanguage models\nlike open ai gpt2\nso this tutorial is going to start with\nthe basics it's going to start with kind\nof the nlp fundamentals we're going to\nlook at\nbasically how you can take text and\nconvert it into a numerical vector and\nprobably the easiest way to do that is\nwhat's called bag of words so i'll be\nthe first model we look at we'll spend a\nlittle bit of time high-level overview\nof it and then we'll jump into python\ncode related to it then the next thing\nwe'll look at is a slightly different\nvariation of how we can convert text to\na numerical vector and that will be a\nword vector approach\nand we'll also see how we can use python\ncode to implement models that use word\nvectors\nafter we go through kind of the nlp\nfundamentals we're going to go through\nall sorts of different miscellaneous nlp\ntechniques so we'll go through regex's\nbasically pattern matching in python\nwe'll go through stemming and\nlimitization\nwe'll go through some\nbasic spell correction some basic part\nof speech tagging and we'll finish this\ntutorial off by kind of giving it a high\nlevel introduction to the\nstate-of-the-art models that i i\nmentioned so this open ai gpt-2 model\nand more generally transformer what is\ncalled in transformer architectures so\nanother big name one is bert\nso we'll kind of introduce and see how\nthose came about and i'll show you how\nyou can use python code to play around\nwith those and build even more powerful\nmodels the first model we're going to\ntalk about is called the bag of words\nmodel or also sometimes called bags of\nwords model\nand to give you a little bit of\nintuition behind this approach i mean\nwhenever we're doing any sort of data\nscience a related task we like working\nwith numerical vectors\nand obviously our issue with text is\nit's not a numerical vector so this bag\nbag of words approach is kind of the\neasiest way to take\nuh sentences and you know pieces of text\nand convert it into a numerical\nrepresentation so imagine we have the\nfour utterances i love the book\nthis is a great book\nthe fit is great and i love the shoes\nthese come from two different\ntypes of categories at our retail store\nthe books department and maybe the\nclothing department and we're trying to\nbuild a model to tell them apart\nwell what we can do with the bag of\nwords approach is it basically says take\nall the\nwords that we see in all of our\nutterances and just extract out each\nindividual word each unique word so that\nwould give us\ni\nlove\nthe\nbook this\nis a great fit and shoes\ntake all those words\nand then\nbased on what words what should which of\nthese sentences have which of these\ntotal unique words\nwe create a vector where one a one\nrepresents\nthis sentence has the word and a zero\nrepresents it doesn't\nand so that's a basic approach of a\nbinary bag of words model and so what\ndoes that look like in code well i think\nwhenever we're building a\nbag of words model\nthe easiest library to use is the\nscikit-learn library so we're going to\nextract a couple things from the\nscikit-learn library\ni'll make my text a little bit bigger\nfrom\nsklearn\ndot feature\nextraction\ndot text\nimport\ncount\nvectorizer\nand tf or sorry we'll just start with a\ncount vectorizer so when it says count\nvectorizer this can be either a binary\nbag of words that we just mentioned ones\nand zeros it also could be if you\nimagine\nuh one of these phrases i mentioned i\nlove the book\nthe book was great if that was all like\none sentence maybe a count vectorizer\ncould also say that book appeared twice\nso it would be a rough a straight count\nof the\nuh word in the overall\nsentence how many times the word\nappeared in a sentence but i usually\njust tend to use a binary approach\nif you ever don't know how to find\nsomething like this if you know of the\nbag of words model but you don't know\nwhere to look to find it i just\nliterally would do like a google search\nsk learning bag of words and you'll get\nto the scikit-learn documentation\nand\nthrough that\nyou should be able to find some examples\nuh using\na bag of words basically so here we have\nthe count vectorizer that i just showed\nyou and it shows us how to\nto utilize it so we might reference this\na couple times as we do this\nso we have our count vectorizer\num and\nabove this i'm going to just real quick\nto find some training that rinses so i'm\ngoing to call this train axe\ni'm going to say\nlet's use our examples i love the book\nas the first one i love the book\nthis is a great book was the second one\nuh and then the last two were\nthe fit\nis great\nand\ni\nlove the shoes\nall right so that's our four training\nutterances we can imagine\nuh\nso the first thing we might want to do\nis utilize our cap vectorizer to\ntransform this into a vector\nrepresentation so once again i'm going\nto reference the\ndocumentation here\nand you know i know what i'm trying to\ndo but i sometimes forget the syntax so\nthis is why i always reference\ndocumentation like this\nuh so it looks like\nwe want to use vectorizer.fit transform\nso vectorizer equals count vectorizer\nand then fit transform on our sentences\nso we can do that so we can say\nvectorizer\nequals count\nvectorizer\nvectorizer and then we can say our\nvectors are\nare equal to\nvectorizer\ndot fit transform\ntrain x\nand when this says fit transform\nit fits a dictionary around our training\nutterances so basically that's the first\nstep in it's it's finding all the unique\nwords so it knows how to make these\nvectors and then it's going ahead and\ntransforming the utterances we pass in\nbased on that vector that we just fitted\nso now we get our vectors\nso i could do like print\nvectors zero and it'll be the vector for\num i love the book\nif this runs\ntrain x is not defined i need to run\nthis cell too i'm using google collab\nright now to run this\nokay it looks good i might just do dot\nvector i think it is\nnot found vectors\nlet's just see how we print this out\nreal quick\nuh\noh i guess we could do\ntwo array\nto see how it actually looks\nwe could also do\nvectorizer.getfeaturenames that looks\nhelpful too\nso i'm going to print some things out\ni'm going to print out\nvectorizer.getfeaturenames\nand i'm also going to print out let's\nsay\nvectors\nprint vectors dot 2 array\nlet's see what happens okay cool\nso\nour\ndifferent words in this dictionary are\nbook fit great is love choose the this\nand one thing to note is it looks like\nit actually uh stripped away the a so\nthat might be just a feature of\nyou know one word utterances it it\nstrips away uh part of the\nway that this count vectorizer is\nimplemented but if we look at the vector\nhere\nlet's look at the first utterance let's\ni love the book so\nwe have a one here\nfor\nbook right we have a one here for\nthe\nlove right\nthus the second to last so we have a one\nthere\nand\ni guess it's stripped away i as well so\nwe don't see that but we get a vector\nrepresentation other than the one word\nutterances where this vectorizer doesn't\ncount and one thing to note is that i\nthink by default if i made this i love\nthe book book\nuh count vectorizer is non-binary so it\nactually matters how many\ntimes you type something in so if you\nwanted to you could do\ni think binary equals true\nhere and now it switches back to ones\nand zeros\nokay so that's the basics of\nthe bag of words just\nseeing it and making a very toy example\nof doing it let's build a quick\nmodel to actually classify\nthese as being clothing related and\nthese as being\nthese first two being book related so\ni'm going to call something train y so\nthat's going to be first one's books\nsecond one's books and whenever i have a\nrepeated string like this\ni like to make it its own\nvariable just so i make sure i don't\nactually like misspell it so we're going\nto make a class category real quick and\njust\nlabel books as one of the categories and\nthat's going to be the string books\nand\nclothing as the other category\nand you'll see why i'm doing this in a\nsec so now when i want to\ni'm doing this in order so i have four\nthings i'm going to give them their four\nuh labels in order so we have\ncategory.books we have category\ndot books and then the last two are\nabout clothing so those are category dot\nclothing\nall right\nand i'm going to run that again\nrun this again i guess and now we're\ngoing to build a simple classifier\nto\njust hide this print so you can see\neverything real quick\num\ni built a simple classifier to\nclassify whether these are book a book\ncategory\nuh and this is clothing and we can use\nit on new utterances\nso how do we do that\nwell once again the scikit-learn library\nis very helpful in this\nand what a good classic uh\nclassification model for text is often a\nlinear svm so that's what we're going to\nuse\nso we can do from sklearn import svm\nand then we can define our svm by the\nfollowing\ni'm going to say classifier svm equals\nsvm.svc\nwith a linear\nkernel because i want to use a linear\nsvm because it's i know\nuh just with my null like background\nknowledge that this is a good\nclassification model for text often\ntimes\nand then with that\nsvm we're going to do a fit\non our\ntraining\nvectors which is right here these\nvectors because we couldn't just pass in\nthe text so i'm going to call this train\nx vectors\nvectors\nand then our y labels which is just\ntrain y\nso this should fit a model to the four\nutterances we have here and ah we got a\nerror i didn't rerun this so\ncool and then finally\nlet's do the fun part and predict\nnew\nutterances with this so i'm going to\ncall this text x\nequals vectorizer\ndot transform\nso before we had to fit a dictionary and\ntransform based on our training\nutterances now that we already have a\nvectorizer we can just do transform\ninstead of fit transform and so i'm\ngoing to just say something like\nhow about\ni like the book\nso\nbased on what we have here because we\nsaid the word book you'd expect that\nthis would classify as\nthe books category so we can do\nclassifier\nclf svm.predict\non our testx and see what happens\nnice books\nthat's cool let's try like\ni love the shoes maybe we say something\nlike\nshoes are all right\nand wow that says clothing\nand obviously we only trained this with\nfour utterances so it's going to be so\npowerful\nbut\num so yeah it's only gonna be so\npowerful but as we added more and more\nutterances to this our dictionary would\ngrow more and more within this\nvectorizer fit transform and we'd get a\nmore powerful model so the more training\nunder instances we feed into a bag of\nwords model oftentimes the better it\ndoes\nand i guess\none of the caveats to that is\nyou might build such a big dictionary\nthat it might get hard to process like\nthe model so another technique you can\nadd in here and\ndefinitely sklearn offers you some\nability to do this just maybe you only\ntake the top 1000 words that occur\nso that's like another option\nanother thing to know about bag of words\nis right now we are doing a unigram\napproach\nwhere we're just taking each individual\nword by itself\nbut you could also do a bi-gram approach\nit's called which would\ncategorize i love\nlove the\nthe book\nall as their own unique utterances so\nlet's try to do that real quick let's\nsee\nuh\nlet's see what we have engram range\nthat's probably going to help us\nso i'm going to type in endgram range\nbecause it probably will give me some\nmore information about that\nequals\noh so i'm gonna say one\none and two to get i think both\none words and two words so we can print\nthis stuff out again and see if that is\nthe case\nyeah so now you see we get\ntwo words\nas well so that we just now captured you\nbigrams as well\nand one thing to note about these\ndiagrams one reason you might want to\nuse two words in a row is let's say we\nwere talking about if things were\npositive or negative\nwell if i say something is great that's\nvery positive but if i use a biogram and\nthe word before great\nis not\nyou know not great\nthen that's a completely different\nsentiment so that's one way you know\nengrams are important and as you you\nknow\nwords depend on their context so\nuh\nyou know adding additional words in this\nsometimes helps it could also be\ndetrimental if you had too many words\nhere in the endogram range\nyou might have so many like random\nmiscellaneous like three word\nphrases that only occur once in the\nentire set and it might actually skew\nthe classifier off a bit so i'm going to\njust stick for now with\njust a single\nuh\none unigram model\nand here's the final the final thing\ni'll say about bagger words before we\nget into the next model is here's the\nlimitation is\nif we have a word\nthat is not in our training utterances\nwe're not going to know how to handle it\nso if i said something like\ni love the story\nwell to us\nthat's very similar\nyou know to book and books\nuh\nbut to our model we've never seen the\nword story so we might not know how to\nhandle this so let's see what happens\nwhen we run this\nand here's the issue yeah it classified\nit as clothing\neven though it's obvious to us that\nstory and book\nare very related and honestly i think\neven if we typed in something like books\nit doesn't know that book and books are\nthe same because it has seen the word\nbook\nright here but it's never seen books so\nit's like pretty dumb if it hasn't seen\na word\nand yeah again it says clothing even\nthough it's very obvious to us\nthat book and books\nare the same so that's one downfall of\nbag of words is it's great on the stuff\nit's trained on\nbut if it hasn't seen a word then it\njust fails miserably and you know that's\nnot\ngreat when\nin the human language you know we can\nsay different things in so many\ndifferent ways\nso this leads us to the topic of word\nvectors and word vectors are another\napproach to turning text into a\nnumerical vector and the big goal with\nword vectors is to convert text into a\nnumerical vector that captures some of\nthat semantic meaning in the vector\nspace that you're mapping this piece of\ntext to\nand what i mean by that is that imagine\nyou have the words red blue and yellow\nthese are all three different types of\ncolors and what we're trying to do with\nword vectors is have in our we imagine\nyou have this big like vector space we\nwant the similar words to be mapped in a\nsimilar spot of that vector space so red\nblue and yellow would\nall be colors and should be mapped\nsomewhere similar\nand there's many different approaches to\ntraining a\nvector to ultimately do this and one of\nthe popular methods is called word to\nvec\nand word effect works in one of two ways\nusually there's two popular like most\npopular i guess ways to train in this\nword of act one's called continuous bag\nof words and one's called uh skip gram\nthe different details of these two\napproaches isn't super super important\nbut what is important is to kind of\nunderstand at a high level how they work\nso\nimagine we're going back to our kind of\nexample of classifying between books and\nclothing related tweets\nso imagine we have the three phrases we\nsay\nbest book i've read in years we have\ngreat story and characters and we have\nno development of characters during book\nso these are three phrases related to\nbooks we can tell this pretty easily by\nreading these but how do we train a\nmodel to see that\nand what the word divac approaches do\nthe continuous bag of words and the skip\ngram approaches is they look at a\nwindow of text\nso oftentimes that window might be\nyou know five tokens long so best book\ni've read in so that would be kind of\nour context window\nand basically what we'll do is we'll\nselectively look at different tokens in\nthat context window\nand based on that token\nwe'll\nutilize the surrounding tokens\nto kind of figure out the context of\neach token and kind of start developing\na meaning of each token and so\ntranslating that to this example\nwe can start if we read enough text we\ncan start to see relationships between\nwords\nso for example here one relationship we\nmight develop over time is\nbook often appears close to read so now\nwe can associate book and read into a\nsimilar spot of the vector space and\nthis is ultimately going to be trained\nthrough some sort of neural network\narchitecture\nfor the most part so given our neural\nnetwork architecture where you know\nour model is learning that book and read\nare close together uh in this next\nexample you know great story and\ncharacters we might start relating story\nand characters\ntogether\num\nand you know we'll see these together so\nwe'll know that those are related should\nbe similar uh\nvector space\nuh and then this last example no\ndevelopment of characters during the\nbook well maybe we relate development in\ncharacters that's what our model is\nlearning uh\nyou know also it's probably learning\nthat characters and book are related and\nand we can kind of start\nbuilding out far like bigger\nrelationships like okay we've seen\ncharacters in book here we know that\nthose are probably related\nokay we go up here story and characters\nare often together so i'm guessing that\nstory and book\nare\nprobably related as well and\nthis is a very toy example of what's\nhappening with hundreds of thousands of\nlike sentences being fed into these\ntypes of models and we're ultimately\nbuilding up these\nword vectors out of that\nso it's one thing to talk about it let's\nstart jumping into some python code to\nactually show you what that looks like\nand i think the best spot to\neasily utilize word vectors in python\nwould probably be to use the spacey\nlibrary\nso\ni whenever i'm like\ntrying to remember how to do something\nlike this you know i'm going to be doing\na google search\nfinding some sort of information on how\ni use\na spacey and use the word vectors they\noffer as you can see we're probably\ngonna have to download some\nuh\nword vector model because we're not\ngonna train this from scratch we'll use\nwhat's something that's already been\ntrained\nuh\nand then we can kind of follow along\nlike this it looks like\nokay\num\nyeah you can look through here maybe you\nlook through\nthe actual documentation but space is a\ngood place to start and let's start\nimplementing this\nokay so\nwe're gonna need to use the spacey\nlibrary before we can use the spacey\nlibrary we're going to need to\ndownload that language these these train\nvectors so i already did that previously\nso i'm going to just go ahead to the top\nof my\ngoogle collab file and this also\nprobably should work with a jupiter\nnotebook or whatever you're using or\nyour your browser if you might need to\ndo like a pip install here but i'm going\nto insert a code cell and i want to make\nthis above i'm just do a couple\ninstallations of things that i need here\nbecause by default the google collab\nthat i'm using won't have the spacey\nlanguage model that or the spacey word\nvector model\nembeddings that we need so we're gonna\nhave to install those so\nin here i can do pip install spacey so i\nthink space is already installed but\nsometimes you need the most up-to-date\nversion and i think this will help us do\nthat\nand\nwe're going to do python-m\nspacey download\nwe're gonna download the medium sized uh\nembeddings\nbut as we can see in this dock that i\nwalk through\nthere's also a large model but that\nmight take a bit long to download but if\nyou want to try some more powerful word\nvectors maybe try that large model okay\nso i'm going to run this\nit's downloading\nokay so we've downloaded it and one\nquick note is sometimes if you're using\ngoogle collab\nand you do this you might have to\nrestart the runtime before\nuh changes take effect like imagine you\nalready imported spacey uh i don't think\nthat spacey would recognize that this\nmodel is here unless you restart that\nruntime so\ni just did that to be\ndoubly sure so import spacey that's our\nfirst step\nokay it looks like\nit uh imported properly\nand now what we want to do is load in\nthat\nwe're going to call nlp\nthat uh word embeddings model that i\njust downloaded so n core\nweb\nmedium size so let's see if this works\nhopefully it does we only need to load\nthis in once and then we'll stay in\nmemory\nand like if you're not remembering how\nto do this i always like to remind\npeople literally just do a google search\nword vectors spacey documentation\nthe article i was looking at was in the\nactual docs you could probably actually\nfind\nthe docs here and that will give us yeah\nlook at this this gives us some nice\nexamples right there and that makes it a\nlot easier than trying to\nfigure it out from memory\nso let's see\n[Music]\ni want to see a\nactual value looks like\nvectors or values\ni think i can do dot value and get my\nvector\nall right\nokay\nso what we're going to do here is\nbasically we need to take text so\nwe can if we want to use the same text\nfrom above so i'm going to just rerun\nthis cell\nso now we have all these examples that\nwe were using previously with our bag of\nwords model so that's called train x\nand now i'm going to say our docs\nequal basically we're going to need to\nconvert\nif i can see the initialization here\ni look up one more time word vectors\nspacey example\ni just want an example to see how i can\ndo things\nspacey 101\nlooks like i already had clicked this\nnext one no\nthis one right here so let's try this\nokay look at this\nyep there we go this is nice it's\nshowing us how we can\nmake this a little bit bigger so you can\nsee\nthis is showing us how we can get that\nvector if we pass in dock and all p\ndo vector then we get\na vector representation and did that\nlet's see\noh maybe the doc container\nhad the information here in the docs\nvector vector vector vector i want to\nfind it here\nokay here we go\nyeah a real valued meaning\nrepresentation defaults to an average of\nthe token vectors so basically if we do\nnlp of our phrase and then do dot vector\nit's going to take all the individual\nword word embeddings\nand\naverage them together\nso that's i think what we ultimately\nwant if we want to build a model around\nthis\num and also to note that these word\nvectors are usually hundreds of\nuh they have you know a dimension one by\nlike several hundred so they they're\npretty big but uh they kind of have to\nbe to capture the information we need\nokay so docs are going to be\nnlp\nof text\nfor text in our train x that we just\ndefined\nand now\neach of these\nitems in this docs list is a\nword vector representation of our\nsentences that we defined\nabove\nhere\nso i love this book this is a great book\nthe fit is great i love the shoes\nand just so you have that\nhere it might be helpful for me to just\nprint out\ntrain x so i can remember that\num okay so that's what we're ultimately\nconverting so now if i print out\ndocs\nyou're gonna see that it's\num\noh i guess it keeps the\nif i print out docs dot vector\nor oops\nif i print out let's say\nthe first\ndocs vector\ngonna see it this is the word embedding\nrepresentation the average word\nembedding for each of the words in i\nlove the book\nso that's cool we did that pretty easily\nusing spacey and a couple lines of code\nuh and now that we have this\nokay so now that we have this let's\nbuild the same model that we built for\nthe s uh the back of words model for the\nspacey model\nso we can define a classifier again\nand i'll define it like this\nso we're going to define a classifier\ni'm going to give it a slightly\ndifferent name i'm going to say\nsvm and i'll give it\nwv for word vectors at the end just so\nwe can tell these apart\nand we want it to not fit to the train x\nvectors anymore but we want to fit to\nwe'll define a word vector\nword vector\nvectors we'll say\ni'll just call them\ntrain\nx word vectors just to separate a little\nbit from the\nother word vectors and that's going to\nbe\na dot vector\nx dot vector for x in\ndocs\nso we're just getting all of those as a\nlist\nso now we're going to pass that into\nour fit\nand the the y\nlabels stay the same as above\nbooks books category clothing clothing\nall right\nso i let's see what happens here we're\nfitting this\nrun this again run this again\nsvm is not defined okay i need to\nre-import svm from scikit-learn\ncool i think there's one other thing i\nneeded to reimport\nuh\n[Music]\nnope i think we're good\nokay so we have our model and now what\nhappens if we\ntry to predict\nsome\nnovel utterances\nso to do this we can pass in\nnlp of a phrase so how about i love the\nbook we'll just do that again here\nand then we'll have to grab the vector\nfor that\nwhat's going to happen here\noh\ntrying to do too much in one spot\nso instead of doing that i'm going to\nsay\ntest x\nword\nvectors\nequals\nwell\ni'll also define a\ntest\ndocs\nthat's going to be equal to\ntrying to do a lot here\nsorry i feel like this is simple but as\ni do this live it's uh\nsometimes tough so test is going to be\nequal to\nis equal to\na list of words so let's just say i love\nthe book test docs now is the nlp\nrepresentation of test\nx or\ntext\nfor\ntext in\nfor text in\ntest x and then finally the word vectors\nare going to be\num\nx dot vector for\nx in test docs\nuh cool so now we can predict\ntest\nx word vectors\ncool\nsorry i i was trying to simplify it too\nmuch and as a result\nno\noops\nso close uh how did i keep getting\ndouble things here\nokay\ni love the book\nworks as expected i mean that was an\nexact take from here so that recognizes\nbooks but now let's try to find the\npower of\nof word vectors\nand let's type in something like\ni love\nthe story\nand we're hoping that story and book\nhave a similar word vector\nrepresentation so that when we do this\naveraging\nwhen we do this nlp text which basically\naverages all those word embeddings\ntogether and actually get the vector\nvalue of that average embedding\nthat\nwe get a books classification again\nthere we go look at that i love the\nstory gets books as well and now let's\ntry testing out some things for clothing\nrelated so i love the shoes works so if\nthis is properly capturing the semantics\nyou should be able to say something like\ni love the the hat and hopefully that\nsays clothing\nlook at that cool\ni love\nthe hats that should also be clothing\ni love the books even though we haven't\nseen books exactly and now knows that\nbook and books are more related because\nit's seen those in similar context\nwindows\nand yeah there's all sorts of cool stuff\nthat we can do with this you know these\nearrings\nhurt or something\nand already with just four training\nexamples because there's so much power\nbaked into\nthe\nspacey word embeddings the word\nembeddings in general even just with\nthis medium-sized model that\nwe can already predict a lot of things\ncorrectly\njust by knowing that you know semantic\nspace and i think this like word vectors\nare so cool and this concept is so cool\nthat we can\ndo so much with language like this so\nthis gets me really excited um\ni guess before i end this little seg\nsection uh there are some drawbacks to\nword vectors they're not a cure for\neverything\ni think one thing you'll see is that it\nworked out pretty well for us because we\nonly had\ntwo categories we only had books and\nclothing but if we were trying to use\nword vectors by themselves and i think\nat the exercise at the end of this\ntutorial we might see this\num\nif we were to say use 10 different\ncategories and instead of these phrases\nbeing like four words they were like\n50 words then when we're trying to\ncapture a a\nembedding\nfor the entire sentence we're averaging\ntogether like 50 word individual word\nvectors\nand the actual meanings of all these\nindividual words might get kind of lost\nin that averaging process so sometimes\nthey're not quite as precise as maybe\nusing bag of words in that case because\nthings get kind of mixed together\nanother drawback of the standard word to\nback word embeddings is\nuh this is a little bit different than\nthe context that we're worrying about\nbut imagine we\nwere trying to\nget some sort of meaning for the word\ncheck\nso\ni went to the bank\nand wrote a check\nso that's gonna the check in this case\nis gonna have a specific meaning but if\ni also had another\nword embedded another sentence that was\nlet me check that out\ncheck and let me check that out is very\ndifferent than writing a check\nbut the word vector is going to be the\nsame for both of those so you do get for\nwords that have multiple meanings you do\nget a little bit of a messiness because\nboth their meanings are kind of try to\nare captured in the training process and\nultimately\npart of those meetings are probably lost\nso\nword vectors are very\ncool uh pretty powerful but they don't\nsolve everything\nand\nultimately left a lot of room for\nimprovement that it was ultimately\na lot has been developed recently as i\nkind of mentioned at the start of this\ntutorial\nall right in this next section of the\ntutorial we're going to kind of do a\nrapid fire overview of a bunch of\ndifferent nlp techniques that are good\nto add into your nlp toolkit and so the\nfirst technique we're going to look at\nis using regexes\nso regexay regexes are pattern matching\nof strings and they're not a python\nspecific concept but we can definitely\neffectively utilize them in python so\nreal quick i'm going to start with a\nquick overview of pi of regex's\nuh basically the way you can think about\nthem is\nyou could have all sorts of different\ntypes of phone numbers so like one two\nthree one two three one two three four\nwould be a valid number you know maybe\na different set of numbers that's the\nsame format\num\nfive five five five five five five five\nfive five would also be another valid\nphone number\nmaybe you want to\nwrite it a little bit differently and\nmaybe do like plus one\ndash\nparentheses one two three dash\ntwo\nthese are three different ways to write\na phone number but they're all like\ntechnically valid ways to go about it so\none way we could use regex's is\npattern matching on like figuring out if\nsomething's a telephone number or not\nand so like in this case one thing we\ncould see\nis that it\nhas three digits followed by some sort\nof punctuation or maybe no punctuation\nfollowed by three more digits followed\nby uh four digits\nand basically we could define a regex\nfor that and similarly for this like\nlast one\nyou could define in your reg x that it\ncould have a plus one or plus some\nnumber optionally and so regix's allow\nus to\neasily capture\nthese different types of patterns and\neffectively add this into kind of like\ncode that we're writing so you know\nphone numbers writing reg x's for phone\nnumbers is one thing it also might have\nlike a password checker where if you're\nlike logging onto a site they say oh you\nneed one symbol\none character one uppercase character\nand a number in your password and it has\nto be plus 10 10 digits or more a regex\ncan help the people implementing that\nsite on the back end make sure that your\npassword meets those specs\nemails formats another thing so i think\nthe easiest way to get into regex is to\njust start right away with an example so\ni'm going to say as the example is that\nsay we want to match a string that\nstarts with the letters a and b\nand they can have in between in the\nmiddle of the string it can have any\nnumber of characters it doesn't matter\nhow many characters it has as long as\nthere's no white space so we don't want\nany white space in what we're trying to\nmatch and then it needs to end with the\nletters cd\nso whenever i'm going about working on a\nregex i usually will start looking at a\nlike regex cheat sheet to just remind\nmyself of what we can do with regex so\ni'll link this in the github page\nfor this tutorial but a page like this\nhas all sorts of useful tidbits\nso\nultimately what we're going to want to\ndo is\nhave like exactly\na or b so\nor exactly a b which is going to be a\ngroup\nfollowed by any number of characters so\nif you see it there's this special dot\nuh or period that's any character except\nnew line so maybe we will utilize that\nand then there's these quantifiers so if\nwe\ncould have zero or more one or more\nzero or one we can use these quantifiers\nand there's some other stuff too like\nword boundaries is useful but let's try\nto write a regex for the case i just\nsaid so\ni'm going to the site regex101.com\nand\nit actually has the flavor you can test\nso i clicked on python so what i said is\nit needs to be a b to start\nfollowed by any number of characters\nwhich is period\nand then it doesn't matter how many\nthere are\nso it can be the star that means\nuh\nit can be zero or more followed by c d\nso that's like the basic\nimplementation as we can see if i do a b\ne e c d\nit matches if i do ba\nall of this followed by x or something\nit's not going to match that one because\nit doesn't start with a b and it doesn't\nend with cd\ncontinuing on i said that there couldn't\nbe any white space so let's see if a b\nspace cd works and that does work so now\nwe need to fix our regex to disallow\nwhite spaces going back to our cheat\nsheet we see that there's this\nnot character\nnot a b or c so we can utilize that\nalong with\num\nwhite space this character class so we\ncan do\nforward slash\nforward slash s\nto do the white space we're going to do\nin this\nbrackets we'll do not white space\nso let's\ngo back to here so instead of dot star\nwe're gonna do\nbrackets not\nslash s\nclose bracket and then one or more of\nthat\nand so now we see that this works but\nthis one now doesn't because it has\nwhite space in the middle\nif i remove that white space it's now a\nmatch\nthat's pretty cool\none thing to note here is i could do\nsomething like\nuh\nx x a b c d x x\nuh\nand that's still saying\nit's\na\nmatch or at least like it's saying the\nline is a match because that's included\nif we wanted to make sure that it's\nexclusively\nuh\nthis to start the line in this end line\nwe can use a couple more special\ncharacters so this is the end of a\nstring\nand this is the start of a string\nthis is used without the brackets and it\nmeans start of string but if it's used\nin here it means not\nso we can add that so start of string\nfollowed by end of string so now this\none doesn't match\nat all\nand that's the solution to that little\nexercise so what does this look like in\ncode well we can quickly\ngo to our google cloud file we can\nimport the regular expression library\nwhich is just\nre\nin python\nand we can start out with our\nregular expression which we can define\nto be whatever we want so if we were\ndefining the one i just described it\nwould be\ncarrot a b\nfollowed by\nnot\nwhite space\nstar\nand then followed by cd followed by the\nend of line so now we compile that to\nallow python to know that that's a\nactual regular expression\nand oftentimes when we're defining\nregular expressions in python we use a r\nin front of it just to\nhelp us highlight and help us know\nand then we want to do if we want to see\nif something has that we could say here\nare like\ntests\nor our phrases\nabcd\nxxx\na b\nx x x c d\na b space c d so only the first and\nthird\nshould match so if we wanted to check if\nsomething matches we can do\nfor phrase\nin phrases there's gonna be two main\nfunctions that we're gonna use when\nwe're checking for matches i'm gonna do\na regular expression.match we're gonna\npass in first our regular expression and\nthen we're going to pass in the phrase\nthat we wanted to see if it matches\nand if it does match i'm going to make\nthis a conditional thing so if\nthe regular expression matches then i'm\ngoing to have it append to a list called\nmatches\nthe phrase\nso we'll have to find the matches list\nokay\nand then finally\nprint matches\nrun\nokay so as you see as we expected the\nfirst and third match\nnow here's something interesting\nlet's say\nwe took away the\nrequirement of this happening to be the\nfirst and last thing in the line\nand now we added something like aaa\nin front and then ccc\nif we wanted to just check if a regex is\nin an entire string so here we do have\nit in the string but it's not the start\nit's not the entire thing there's places\nin this that don't match\nif we rerun this code\nyou'll see that this regular\nexpression.match function no longer\nuh\nsays true on this one right here\nso the other main thing we're probably\ngoing to use when we're searching text\nfor regular expressions is re.search\nand as you can see now it still matches\nthose those two\nand quickly just to kind of apply this\nto our toy example that we've been doing\nimagine we wanted to create a regular\nexpression that matches read\nstory\nand book maybe we were making some sort\nof hard-coded rule\nto\nwe were making some sort of hard-coded\nrule to\nfind if something's in the books\ncategory\nso\ni liked\nthat story\num\ni like\nthat book\num\nthis hat is nice\nas you can see this regular expression\nthis is the or sign it counts those\none quick nuance if we wanted to make\nour\nour regex more complicated is that i\ncould try to trick this and say like\nhistory\nlike\nuh\nin instead of read i can maybe say\nuh\ni\nthe car\ntreaded\nup the hill i don't know if that makes\nsense but you can see that read is\ninside of that but it\nuh\nis not actually referencing the word\nread\nwe'll notice that still\nmatching those things so one thing\nthat's useful to know about is this word\nboundaries\ncharacter within the reg x so\nforward slash b\nmeans it needs to be\nbetween word boundaries\nand now watch what happens when i run\nthis again no matches\nso\nit now knows that story has to be by\nitself and one thing that's nice too\nwith that uh\nslash b format is that\nyou can have a period or something at\nthe end it knows that that's a word\nboundary uh so that's like one way we\nmaybe would apply it to our example that\nwe've been working on but there's so\nmany uh\nuse cases for regexes\nall right next we're going to quickly\nlook at stemming and limitization in\npython and these are two techniques to\nnormalize text and what i mean by that\nis that in our first example with bag of\nwords we saw an example or a problem\nwhere\nwhen we trained the model with the word\nbook\nit didn't know the words books even\nthough to us that's very straightforward\nso one example of what stemming and\nlimitization will can do is take books\nand kind of reduce it down to a more\ncanonical form of book\nand it can do like several different\nthings so imagine\nthese techniques could help you turn\nreading to read\nbooks to book stories and now here's\nwhere a little bit there's a little bit\nof a difference stemming follows an\nalgorithm and is not guaranteed to give\nyou a\nactual physical\ntrue english word so it might reduce it\ndown to story whereas for levitizing it\nwould take stories and it actually is\nusing a dictionary making sure that\neverything in outputs is an actual word\nso that would output story there so how\ndo we use this in python well i think\nthe library that's\nuh easiest to you ah what did i do\num\nthe library that's easiest to\naccess stemming and alignmentization is\nprobably the nltk library\nwe'll use the ltk library\nokay so first off we're going to need to\nimport ltk\nand that nlt\nnltk stands for a natural language\ntoolkit\ni don't know i'm trying to do too much\nimport nltk and we're also going to need\nto do\nis\nimport a couple or download a couple of\nthings for nltk and this should work out\nof the box i believe\nif you just do nlt you get download\nso we're getting stop words word net and\ni think the stop words is going to be\nfor my next example i don't know if\nwe'll need that for the stemming\nlimitation\nsection all right downloading all that\nand now let's go ahead and start with a\nstemmer\nso\nto\nuh\nget a stemmer i recommend importing two\ndifferent things i recommend importing\ntokenize library which basically can\ntake a sentence and break it into its\nindividual words\nas well as the actual stemmer and we're\ngoing to use the porter stemmer for this\nexample so i'm going to alt key\nnltk.stab we'll import the porter\nstemmer and now let's uh go ahead and\ninitialize that stemmer\nand now we can just type in like a test\nphrase so\nreading the books we'll say\nand\nto\nfirst we'll need to tokenize that\nbecause if we just try to stem\nbut just try to\ni do stemmer.stem this is all you have\nto do to stem with the phrase\nit doesn't\nknow how to process this because this\nalgorithm expects a single word\nso we're going to have to tokenize it so\nwe can just say words equal\nword tokenize of our phrase\nand now we can just do\nstemmed words we're gonna do this in a\nfor loop so that for word in words\nwe're going to stem the word\nstemmer.stem of the word\nand we're gonna append that to our\nstemmed words\nokay\nand now we can do a join by spaces\nof the stemmed words\nas the final thing so we started with\nreading the books and the stemmer now\ngot us down to reading the book and this\nimmediately can be seen to be very\nhelpful for like that bag of words model\nthat we built earlier where if you\ndidn't have many training examples if\nyou did stem the training examples you\ndid have and stemmed any incoming phrase\nthat you didn't hadn't been trained on\nit would probably help improve accuracy\nthere\na couple examples to see so if i did i\nsaid stories was one that was a little\nbit weird see how it\nstems it to story\nso it's not guaranteed to have a word\nand sometimes you'll get\ncollisions on two words that aren't\nnecessarily similar so there are some\ndrawbacks but it's a nice little quick\ntrick to have\ni think i just want to check something\nwith that whole tokenization\none thing to note is that you also might\nwant to like strip out your punctuation\nor handle that a little bit separately\nif you're trying to return a phrase that\nmakes sense because as you can see right\nthere it uh\nat least the way i define this you have\nto be a little bit tricky because when\nyou tokenize this i print out the words\nit treats the punctuation as its own uh\nown word\nall right moving on to lemmatization\num\nlet's now\nfrom nltk.stem\nimport\nthe\nword net limitizer so this is using this\ncorpus called word net to help\num\nreduce these words to a more simple form\nall right\num\nso pretty similar to the last one we\ncould just do lemmatizer equals wordnet\nlemmatizer\nand\nsame thing for the example as before\nwe need to tokenize whatever phrase we\nuse\noh my god i didn't mean to copy the cell\nno\ndelete this\ni just wanted to copy this part\nall right so\nreading the books let's see what happens\nif we um\ni'll copy this too\nno not the cell just the\npiece\nand this will be\nlemmatized\nties words\nuh dot\nand it's pretty straightforward instead\nof stem it's lemma\nties that's 2ms\nthe word\nand\nlet's see what happens here\nprint\nlemmatized or i guess we'll do\nwe'll join again dot join of limitized\nwords\nall right run that\nwhat's gonna happen reading the book\nokay hmm it's a little bit different\nthan our above example it turned it into\nread the book and here's one\nthing that's a little bit trickier about\nusing the lemmatizer with nltk\nis it expects\nfor each of these words it expects the\npart of speech and by default it\nsays that each token is a noun by\ndefault so that's why i converted books\nto\na book but not reading to read if i\ninstead said these were all\nverbs we'll see that it is read the book\ni guess that's interesting uh it still\nmade books book\num\nis it books he book yeah i guess like if\nyou think of books as a verb\nhe like\nhe books it\nthat would be book that makes sense uh\nso i guess there is a verb books but\nyeah the one caveat is you for trying to\neffectively utilize this sometimes you\nhave to do some sort of part of speech\ntagging which i believe i'll get to in\nthe tutorial\num\nand then maybe utilize that part of\nspeech tag here to like truly reduce\ndown all the words but it might be also\nhelpful to just reduce down all your\nyour noun phrases or all your verbs\num so that is lemmatizing and we'll\nwe'll move on from there\nuh next while we're still looking at\nnltk we're going to quickly go over stop\nwords and basically stop words are the\nset of english words that are kind of\nmost common and sometimes we might want\nto strip those out\nof our phrases because they don't add\nmuch\nmeaning to our sentences\nso a couple examples of stop words might\nbe this that he it\nthe these them\nthose types of things so we can easily\ndo this in\nnltk\nso here we go\nokay so we're going to continue using\nnltk we've already imported it at this\npoint and we're going to do\nactually maybe just for help will be\nhelpful is if i copy in this just in\ncase people run these independently\nuh\npaste in that and then we're also gonna\nhave to\ncopy in the\nactual stop words so we're gonna import\nstop words\nand here we go so our stop words are\ngonna be\num\ni don't want to stop words because\notherwise i'll overwrite this import of\nstop words so\ni'll just say stop words\nequals\nstop words\ndot\nwords and then we have the passion that\nwe want english i believe\nactually maybe it would\nwork without doing english so let's uh\nprint out stop words\ni think it's oh i guess\ni guess you do have to specify english\nbecause i don't know what that means\nuh cool i me my myself we\nand i'm curious how many of these are\nthere so length of stop words\nuh 179 so there's 179 words we might\nwant to\nextract out and this just gives us a\nnice easy interface to do it so what we\ncould do is we could have a phrase just\nlike before\nso here is an example\nsentence\nremoving\nstop words\ni'll just say demonstrating\nthe removal\nof stop\nwords\nso that's the phrase to begin with and\nnow\nwe will\ntokenize that just like that we did\nbefore so our\nwords will equal word tokenize of the\nphrase\nand then forward in words\nwell\ni guess our\nstripped phrase i'll call it that\nequals\num\nso if word\nnot\nin\nstop words\nthen we can append it to the stripped\nphrase you probably do this in a list\ncomprehension too so whatever you prefer\nuh word\nand then we can do\ndot join of the\nwords or we could just keep this as a\nlist because i guess it might not fully\nmake sense as a strip phrase\nuh\nwe'll we'll join them anyway uh join\nstripped\nphrase\nokay here example sentence demonstrating\nremoval stock words so is and\nthe\nof we're all removed as stop words and\nhow this could help us is going back to\nthe\nword vector model from above\num\nif we maybe\nsaid something like i went to the bank\nand wrote a check\nyou know we might get bogged down by\nsome of these stop words in the actual\nmeaning of this as we\naverage together those word vectors so\nremoving the stop words might give us a\nlittle bit more precise uh capturing of\nmeaning in something like this that's\njust one example there's other examples\nwhere uh\nremoving stop words is helpful\nnext we're going to do kind of a rapid\nfire round of the rapid fire round and\ni'm going to just quickly go through\nanother library called text blob that\nallows you to access all sorts of\ndifferent uh nice little things with uh\nlanguage quickly and it builds off the\nnltk library\nand it gives provides like a really nice\ninterface for doing a lot of different\nthings\nso i'm going to just say\nvarious other techniques\nand we'll look at\nspell correction\nuh\nsentiment\nand\nlet's say\npart of speech tagging\nall right so when you're wanting to use\nthis text blob library i recommend\nactually just\nlooking at the\nreference\nand i will make sure i link this and\nthat resources.txt i mentioned would be\non my github\nbut\nit's very straightforward one pager that\nhas all sorts of nice things that you\ncan do so from text blob import text\nblog that'll be our first line as you\ncan see you literally just taken a\nphrase\nand right here b dot tags we've already\ndone part of speech tagging you can do\nlike dot noun phrases dot words\ndot sentiment uh\ndo all this stuff\nwith like one line like just a single\ndot something\nso the first one i wanted to look at was\nthe spell correct and where is that\ncorrect here we go\nso let's go to the code from\ntext blob\noh shoot\nfrom text blob\nimport text\nblob cool\nokay so whenever we do anything with\ntextblob uh what we're going to want to\nstart with is just\nbasically making our phrase a text blob\nobject\nso we're going to go\nphrase\nand say whatever it is\nequals this is an example\nand then what we're going to want to do\nis\num\njust do\nwe could even just surround this\nor i'll just say\ntb phrase so text blob phrase or\nconverting it\nas text blob\nand then phrase\nokay and now with tv phrase we already\ncan do all sorts of nice things so the\nfirst thing i mentioned was spell\ncorrect so tb phrase dot correct is all\nyou have to do to spell correct that\nthis is an example\nwhat if i said this is an\nexample and use two e's\nsee it's still this is an example if i\nuse like two eyes here probably will\nstill think look at that it's correcting\nthis\nvery quickly and when i mentioned that\noriginal goal of this tutorial was you\nknow taking tweets and processing it you\ncan imagine that a lot of tweets are\nmisspelled and have this so being able\nto in\nuh you know we could easily make this\ntwo lines of code\nuh actually even a single line of code\nif we really wanted to get fancy\ndo spell correcting on a phrase that's\npretty pretty helpful so what else can\nwe do with text blob well i mentioned\nyou could do part of speech tagging\nso\ni'm going to go back to our reading\nexamples so\ni read the book\nso\nobviously we don't have any spelling\nerrors there\nbut one thing we could do is get the\nparts of speech and i just have to do\ndot tags to do that\nah okay what do we say\nuh looks like you're missing some\nrequired data for this feature\nokay so i'm going to real quick add this\nto my\ngoogle cloud file and just like i\ndownloaded some stuff earlier\ni will run this\ncool it's done\nand now we should be able to do part of\nspeech tagging list object is not\ncallable\nit might have been just dot tags it\nmight already be baked in look at that\nuh so we have i which is\na noun we have read which is the verb\nwe have the\nwhich is\nwhatever dt stands for and then a book\nwhich is another noun\nand\num\nif you just look up like part of speech\ntagging\nyou probably will\nfind information on the different\nvariants\nand so this\ni'll link this in my github i found this\npage as a helpful resource\nto\nlearn about the different things that\nyou see so you we did see okay like dt\nis the\nwhat do these things actually mean\num\nadver like we can just kind of look\nthrough this and help us\nfind so this is a determiner the dt uh\nnow known as a singular now known s is\nplural\nso yeah this has all sorts of uh\nuseful information right here so i'll\nlink this if you want to know more about\nthe tags here another cool thing is\nwe can do dot sentiment\num\nyou might have to do a sentiment like\nthis\nlet's see go back to the reference\nyeah it's just not sentiment okay\num\nso\nthe book was great\npolarity 0.8 so high numbers here\nare going to mean positive but if i say\nthe book\nsucked\nit was so bad\nright there\nwe see that has a negative sentiment\nbecause this is a you know negative\nvalue here in the polarity\num\nthe book is another example the book was\nhorrible\nanother negative example here\num so you can look into the details of\nthe sentiment to see that but yeah\nin this api reference for text blog\nthere's all sorts of really accessible\neasy things you can do so it's another\ngreat resource\nto utilize and i just want to make the\npoint of\none of the main goals of this tutorial\nis to show you all these different\nthings you can use so you can kind of\nbuild upon this knowledge and apply it\nas you see fit and your\nown tasks that you are working on\nall right so moving towards the kind of\nstate of the art\nmodels in natural language processing i\nwanted to start with recurrent neural\nnetworks\nuh so\nif you remember back to word vectors one\nof the issues that we had with them was\nthat they were\nkind of set in stone and once you've\ntrained uh your word vectors uh no\nmatter what way you utilize a\nword inside of a like bigger phrase that\nword vector that you've trained is the\nsame\nso we don't have kind of context\ndependent word vectors when we're just\nusing a preset trained of preset a\npre-trained set of word vectors out of\nthe box so immediately\nrecurrent neural networks can help us\nkind of solve that problem so here i\nhave a diagram of a recurrent neural\nnetwork and so basically how a recurrent\nneural network would work with language\nis we would feed in words into the\nnetwork one at a time so we would have\n[Music]\nsomething like you know the word check\ncome into our network and so that would\ncome in\nuh\nthe network would process it and produce\na hidden state based off of it and also\nfeed that hidden state\nto the next\nyou know\nlayer of the\nrecurrent neural network so maybe the\nnext word is\nout\nand so that now we have check and out\nall feeding into the same\nnetwork\nbasically\nwe can do this as much as we want\nwe're always feeding the output from the\nlast input back into the network\nso we have check out the\nbook\nso\nbasically\nyou know we're feeding this in and\ncreating some sort of at the end of book\nwe'll get some sort of final hidden\nstate and we can use this hidden state\nin\nnlp applications\nand so what's nice about this is if we\nhad another phrase that was write me a\ncheck\nwell the\nway that check was used here is at the\nstart of the sentence there's no other\nwords around it\nand you're going to get a different kind\nof embedding for this check than you\nwill when it says write me a check\nbased on the context of write me a you\nknow the embedding this final hidden\nstatement be more related to banking\nversus check out the book would be kind\nof a more uh related to\na hidden state that more represents\ncheck as in check it out\nuh so that's like a little toy example\nuh but that's ultimately like one of our\ngoals with something like a recurrent\nneural network uh but the current neural\nnetwork you know for a while this was\nkind of the go-to to build all sorts of\nstate-of-the-art\nlanguage models\nbut it did have some drawbacks which\nultimately got replaced by like the\nopenai gpt2 model i introduced at the\nstart of the video and some other\nother types of models so what are those\ndrawbacks well\nthe first one i would say is that\nthe long dependencies don't always work\nwell with a recurrent node network so\nimagine you have the sentence like i\nneed to go to the bank today so that i\ncan make a deposit i hope it's not\nclosed\nuh while we are feeding in we would be\nfeeding in each of these words to the\nnetwork and like always that hidden\nstate would have had some influence from\neach of these words\nby the time we get to closed\na word like bank is pretty far away\nso in the final output embedding that\nhidden state it might not be clear to\nthe network that the thing that we're\ntalking about that is closed is the bank\nbecause it's so far away in this\nsentence so that's one drawback with\nrecurrent neural networks another\ndrawback is more on my performance side\nof things is that because we're feeding\nthese one token at a time\nthe sequential nature of that makes it\ntough to\nparalyze the training and utilization of\nrnn's and in language models sometimes\nand you know we can't as effectively\nutilize modern gpus\nright so that leads us into\nattention and a big paper that came out\nwas called attention is all you need and\nthis kind of set off a\nwhole new wave in natural language\nprocessing\nso what is the difference with attention\nwell basically you can feed in a phrase\nand\nas you iterate through the tokens in\nthat phrase and\nyou can basically\nfigure out what needs to be attended to\nso if you like walk through this\nsentence i need to go to the bank and\nwrite a check\nwell\nyou know right when we get to the word\nwhen we get to the word check let's say\nwe're feeding these all in and this is\nall being fed in at the same time uh\nusing these positional encoding so it\ndoesn't have to be one after the other\nbut basically\nif you get to a word like check\nwell we can it will probably spike off\nright and bank as like relevant items to\ncheck\nuh i need to go\ngo might like set off if you're looking\nahead uh bank might be triggered as like\nan important indicator to go\nuh\netc so basically we're based on what\nword we're\nlooking at\nour network kind of\nlearns to\nattend to other words and figure out\nwhat's most important\ngiven a certain token that we're looking\nat\nand to kind of bring that to a high\nlevel the network we can kind of think\nof\nlearns to ask questions\nuh\nabout phrases it sees and these are not\nyou know\nthey're not actually asking you know\nhuman questions but baked into these\nattention networks they're basically\nlooking at each token asking all sorts\nof\nquestions on that token\nand learning\nbased on that token what else is\nimportant in the phrase and this is a\nsuper powerful technique and also this\nattention can work at a longer range of\ndependencies\nthan in our traditional rnn so this set\noff all sorts of really impressive stuff\nin the nlp world\nso\nthe sort of video i mentioned the openai\ngpt2 architecture\num one thing to note about that is its\nuh forward nature so basically the task\nthat this is trained on is language\nmodeling so give in some words what are\nthe words that are coming after it\nand that's why it was really good at\nbeing able to auto complete like a story\nand when i typed in a phrase it could\nutilize the fact that it's been trained\non figuring out and predicting what the\nnext word should be based on what it's\nalready seen\num so let's open aigp\ngpt\nthen there's this elmo network that's\nalso listed here i just wanted to note\nthis one i don't know as much about elmo\nnetwork but one thing to note is that it\nuses lstms and it's\nkind of the core\nof it\nand an lstm is a type of recurrent\nneural network it's supposed to handle\nlong longer dependencies a bit better\nbut still compared to\nattention\nand actually\nand actually uh i\nwrote the header as this as transformer\narchitectures but elmo's not\ntechnically a transformer architecture\nit's just another powerful language\nmodel\num\nand then finally what we're going to\nfocus on right now is burt which is a\nbi-directional\ntransformer architecture and as a result\nof it being bi-directional it uh\nreally captures a lot of\nvery impressive things about language\nthat even like the open a gbt model\ncan't so we're going to just look at how\nwe can use python to interact with the\nbert model real quick\nokay so to do this we're going to use\nspacey again\nso\ni'll link this article on my github page\nbut basically spacey provides us with a\nreally easy way to interact with\ntransformers\nand if you just follow this link\nbasically it gives you all the lines you\nneed very quickly to get up and running\nso let's utilize spacey real quick\num so the first two things we're going\nto need to do is pip install trend\nspacey transformers and python-m spacey\ndownload this model\nokay\nso\nah i want to make a a title not an\nitalicized title um\ntransformer architectures i'll say\nall right so\nwe wanted to download this large model\nand we also wanted to do a pip install\nof spacey transformers\nand what these uh exclamation points\nmean i'm pretty sure is that this should\nonly\nthis cell should only be run once within\nuh\ngoogle collab so we don't accidentally\ndownload\nthe model again\nokay downloaded and\ntook a little while but\nran successfully cool\nall right and now how do we actually now\nthat we have it downloaded we can go\nback to that article and see how we can\nactually use it\nuh it looks pretty straightforward uh\nand you can if you're using a gpu you\ncould\nutilize these lines of code\nand you'll note some details about that\ni'm going to just i'll copy this in real\nquick\ni think all i need\nis going to be\ni'm not going to use this right now i'm\nnot going to use my gpu\ndon't think we need an ump right now so\ni think you need to import spacey and\ntorch and it should be good and then\nwe're loading this large bert model and\none thing to note about\nwhen we're using transformers we're not\ngoing to train it when we're using like\nbert model we're not going to probably\ntrain it from scratch\nthe bert model itself is super super\nmassive it takes multiple days on like\nsome of the most powerful tensor\nprocessing unit machines to\ntrain this model so it's not something\nyou're going to want to do you know at\nhome\nbut luckily they open source these\nmodels and we can utilize them\neffectively\nand one thing to note too is that even\nif you you are loading in this model\nthat's already been pre-trained one of\nthe nice features about one of the nice\nfeatures about models like this is that\nthey can be fine-tuned so you can\nfine-tune them on your specific task and\nget some really\nimpressive results by doing that and\nthat's a lot easier of a step than\nretraining it from scratch\nokay so\nrun this see if it loads properly\nif you're using google collab you\nprobably have to restart the runtime\nsometimes it doesn't realize\nwhere this model is unless you do that\nokay cool we have it\num\nand now it's just like it is like if\nwe're using spacey\nnow that we've loaded in this bert model\nas our nlp engine\nbasically we can do the exact same stuff\nas we did\nfor\nthe word vectors when we were in that\nsection of the tutorial\nso here we go\nso basically what i think i'm going to\ndo is\njust to find another set of training\nutterances and\ntest utterances\njust like we did here i'm actually going\nto copy this in real quick because\ni don't think it's necessary to write it\nbecause i want to change the example up\na little bit\num\nall right\nso we're about to see the power of\nbert and transformers in general okay so\nthis time i\ni'm defining another category class but\nthis time instead of the difference\nbetween\nbooks and clothing our two categories\nare\nbook related items and banking related\nitems\nso here we've defined\nseven\ndifferent\nuh utterances each with their associated\ncategory\nand we can read through these real quick\nokay cool\nso\nlet's build our model around this\nand this is the exact same code\nbasically that we used before we just\nneed to now\nall it changes is that we have\nthis model as our nlp engine as opposed\nto\nthe word vectors that we were using\nearlier\nokay so i'm just copying in some code\nhere\nthis is yeah as i said before this is\nthe exact same code as we were using\nbefore so\nwe can train it\non the train x here so i'm going to run\nthis cell real quick\nso we want to train our model on that\nand we can predict new utterances here\nso i'm going to just\nstart as a toy example say book\nokay and as a result it says books here\nthat's what we expect\none thing i want to note here though is\none of our training utterances is check\nout the book\nand so with that example i was kind of\nwalking through is you could say check\nout the book but if maybe you're writing\nabout more banking related terms you\nmight say i need to write a check\nso\ni'm going to just type in\nyou know i need\nto\nwrite a check\nand you know in our trading audiences\nhere check the word check only appears\nin\nthe books category of our trading\nentrances and these three back here that\nare about\nbanking\nneed to make a deposit to the bank\nbalance inquiry savings save money\nthere's no mention of the word check\nso\na true test of the power of this bert\nmodel is that oh does it know that when\nwe say i need to write a check we're\ntalking about banking and not talking\nabout this word we've already seen in\nthe other category\nand let's mode of truth\nand look at that i need to write a check\nutilizing the power of the bert model\nand just like some small number of\ntraining examples we see that\nuh\nit's it's banking related uh which is\ncrazy and you know maybe you said\ncheck this story out you know once again\nit knows that\ncheck in this context is\nuh\nis about books so it's really impressive\nthe capabilities because we've\ntrained such a big model\nleveraging it is is really really\npowerful stuff\nand you can continue playing with this\nuh one thing i recommend looking into as\ni kind of alluded to is\none of the nice things that you can do\nwith\na pre-trained model like bert here is\nyou can fine-tune it to a specific task\nso here's some code that they use in\nspacey to show you how you would\nfine-tune this nlp\nyou know model\nto a specific you know maybe\nclassification task\nso that's really useful\nthe next thing i want to mention is\nthis this\ninterface that spacey provides to bert\nis really nice but there definitely is\nroom that if you wanted to build off of\na model like bert you can't really do\nthat so if you wanted to dive deeper\ninto\nthese types of models i recommend\nchecking out hugging faces transformers\ngithub repo\nand in that\nthey have pretty much all the most\npopular\ndifferent uh\nmodels that have come out recently\nwithin natural language processing they\nhave\nthose models written in pie torch that\nyou can look around with so like bert\nhere\nthis modeling bert file\nyou'll see that\npretty much\na entire pie torch model is defined here\nso if you wanted to build off of bert\nand you know fine tune it specifically\nor\nuh something you could use this hugging\nface library\nall right so that's uh what we're going\nto talk about for\num\ntransformers and how the code would look\nto easily interface with that\nuh and one thing i want to note is that\ni can only teach you so much about the\nactual inner workings of bert in this\ntutorial i was kind of more focusing on\nquickly getting up to speed and using\nthis in python\num\nbut i'll list out on the github repo\nresources that you can go to learn more\nabout these types of things that i've\nfound useful myself\nall right that concludes most of what\nwe're actually going to do in the\ntutorial but i will leave this tutorial\nwith an exercise that you guys can kind\nof tie everything together\nand really try to build your like\nkeep building on your nlp skills\nso\nif you go to my github page github.com\nkeith galley slash pycon2020\nbasically\ni've curated some data\nfor a nlp exercise you can work on and\nthe data it comes from amazon reviews in\nseveral different categories\nand there's also a test set of amazon\ndata in several different categories\neach the same categories and the goal of\nthis task is to basically use this\ntraining data and build the best\nclassifier you can to properly\ncategorize the reviews here in the\ntest set\nthe kind of bigger picture context you\ncould think about this problem then is\nimagine you are\na in charge of\nthe\nsocial media like analytics for a large\nretail store\nand you're getting people tweeting at\nyou all the time about all your\ndifferent products and you wanted to\nbuild a model to automatically like\nroute those tweets to the appropriate\nparties that could handle\nquestions about them then a model like\nthis would be super useful\nso\nthere's the training and test data for\nthat task and i also provided this nlp\nexercise notebook\nand in this\ni kind of quickly walked through\nuh getting up and using this data and\nyou know loading it in as training data\nloading in as test data building a model\non the training data\nwhich\nby default the simplest model that i\nincluded here is a bag of words model\nbut i'm going to be kind of adding\nby the time you probably see this video\ni'll have several uh models included in\nhere but this bag of word model can help\nyou get started on using this data and\nthen basically based on that\nmodel\nuh evaluate the performance on how you\ndo on the\ntest set so as you can see\nout of the box bag of words categorized\n65 of the\ntest\nreviews correctly\ninto their category\nand if you wanted to break that down a\nlittle bit more\nclosely i calculated the f1 scores here\nfor each category and as you can see\nlike it did really bad job on the\nautomotive\nquestions\nbut\ndid well on beauty and books\nand you can kind of utilize this data as\nyou see fit and play around with\nbuilding different types of\ncategorizers classification models\naround this a couple of recommendations\ni have\ni think it'd be really interesting to\nsee how well a fine-tuned\nbert model would do on this task\ni also think it would be very\ninteresting to\ntake the bag of words model but also\nleverage some of those other\ntechniques we mentioned so like\nlemmatizing words\nuh\nstripping away stop words\nuh\nyou know maybe doing some part of speech\ntagging see if you can leverage that and\nto make it more important all really\ninteresting techniques to try to build\nmore powerful models\nuh\nwhat else do i want to say here\ni guess quickly\ni didn't actually show you what the data\nlooked like but if i actually clicked on\none of these\nfiles\nmaybe something like\nbeauty would be easier to see\nuh\ni'll also link my source where i scraped\nall this data someone already did a lot\nof the grunt work for me and i want to\ngive them credit so i'll have that\nincluded\nin the read me of my github page\nbut yeah basically you have this review\ntext\nalways talking about\nthe category here so this is beauty so\nlike\noutstanding top organic shampoo what our\nmodel we would expect to learn is off of\nshampoo one thing to note here is this\nis not perfect data you'll get some\nreviews that are just like really good\num\nand thanks and they don't give us much\ninformation so we're never going to be\nable to get like 100\non this task but maybe another exercise\nyou could work on is how could we strip\nout these kind of\nnon-specific\ntypes of words and not use them as\ntraining information and\ntest information\nall right we're going to end this\ntutorial here hopefully you had some fun\nuh learnings about some different nlp\ntechniques in python\nuh there's a lot we covered in this\nvideo and hopefully my my goal out of\nthis tutorial was for you to all see\nsome of these techniques\nand have them in your mind so that you\ncould take them and kind of build on\nyour knowledge and apply them in\ndifferent areas\nso really it's like seeing all of what\nyou can do and then taking it from there\nand going also just a reminder before i\nleave uh if you have any questions or\nanything feel free to connect with me on\nyou know linkedin instagram twitter and\nyou can search up keith galley one word\nyou'll probably find me and also if you\nwant to find more tutorials that i've\nposted you can check out my youtube\nchannel youtube.com kgmit thank you very\nmuch pycon for having me this was fun\neven if it wasn't in person it wasn't\nwhat i expected but i i it was exciting\nto go through this process and uh\nhopefully i'm\nwill uh be there in person\nnext year all right take care everyone\nthanks for watching\npeace out\n[Music]\nyou\n",
  "words": [
    "hey",
    "everyone",
    "welcome",
    "back",
    "another",
    "video",
    "video",
    "gon",
    "na",
    "walk",
    "comprehensive",
    "tutorial",
    "natural",
    "language",
    "processing",
    "python",
    "actually",
    "originally",
    "created",
    "tutorial",
    "pycon",
    "conference",
    "actually",
    "channel",
    "data",
    "science",
    "playlist",
    "realized",
    "bunch",
    "messages",
    "youtube",
    "comments",
    "lot",
    "seen",
    "figured",
    "repost",
    "channel",
    "film",
    "video",
    "year",
    "ago",
    "major",
    "concepts",
    "remain",
    "troubles",
    "like",
    "installing",
    "library",
    "package",
    "let",
    "know",
    "comments",
    "try",
    "best",
    "help",
    "many",
    "possible",
    "couple",
    "quick",
    "announcements",
    "get",
    "started",
    "recently",
    "create",
    "patreon",
    "also",
    "starting",
    "offer",
    "channel",
    "memberships",
    "youtube",
    "bunch",
    "cool",
    "perks",
    "great",
    "way",
    "support",
    "channel",
    "definitely",
    "recommend",
    "checking",
    "left",
    "links",
    "join",
    "description",
    "well",
    "video",
    "pop",
    "right",
    "think",
    "watch",
    "see",
    "details",
    "memberships",
    "perks",
    "get",
    "lot",
    "cool",
    "stuff",
    "uh",
    "exciting",
    "way",
    "kind",
    "engage",
    "little",
    "bit",
    "hopefully",
    "enjoyed",
    "tutorial",
    "time",
    "travel",
    "back",
    "time",
    "peace",
    "hey",
    "going",
    "everyone",
    "welcome",
    "pycon",
    "2020",
    "tutorial",
    "natural",
    "language",
    "processing",
    "python",
    "bummed",
    "could",
    "person",
    "share",
    "online",
    "presentation",
    "next",
    "best",
    "thing",
    "excited",
    "get",
    "started",
    "tutorial",
    "kind",
    "walk",
    "start",
    "basics",
    "kind",
    "traditional",
    "nlp",
    "models",
    "walk",
    "way",
    "miscellaneous",
    "nlp",
    "techniques",
    "way",
    "language",
    "models",
    "like",
    "open",
    "ai",
    "gpt2",
    "tutorial",
    "going",
    "start",
    "basics",
    "going",
    "start",
    "kind",
    "nlp",
    "fundamentals",
    "going",
    "look",
    "basically",
    "take",
    "text",
    "convert",
    "numerical",
    "vector",
    "probably",
    "easiest",
    "way",
    "called",
    "bag",
    "words",
    "first",
    "model",
    "look",
    "spend",
    "little",
    "bit",
    "time",
    "overview",
    "jump",
    "python",
    "code",
    "related",
    "next",
    "thing",
    "look",
    "slightly",
    "different",
    "variation",
    "convert",
    "text",
    "numerical",
    "vector",
    "word",
    "vector",
    "approach",
    "also",
    "see",
    "use",
    "python",
    "code",
    "implement",
    "models",
    "use",
    "word",
    "vectors",
    "go",
    "kind",
    "nlp",
    "fundamentals",
    "going",
    "go",
    "sorts",
    "different",
    "miscellaneous",
    "nlp",
    "techniques",
    "go",
    "regex",
    "basically",
    "pattern",
    "matching",
    "python",
    "go",
    "stemming",
    "limitization",
    "go",
    "basic",
    "spell",
    "correction",
    "basic",
    "part",
    "speech",
    "tagging",
    "finish",
    "tutorial",
    "kind",
    "giving",
    "high",
    "level",
    "introduction",
    "models",
    "mentioned",
    "open",
    "ai",
    "model",
    "generally",
    "transformer",
    "called",
    "transformer",
    "architectures",
    "another",
    "big",
    "name",
    "one",
    "bert",
    "kind",
    "introduce",
    "see",
    "came",
    "show",
    "use",
    "python",
    "code",
    "play",
    "around",
    "build",
    "even",
    "powerful",
    "models",
    "first",
    "model",
    "going",
    "talk",
    "called",
    "bag",
    "words",
    "model",
    "also",
    "sometimes",
    "called",
    "bags",
    "words",
    "model",
    "give",
    "little",
    "bit",
    "intuition",
    "behind",
    "approach",
    "mean",
    "whenever",
    "sort",
    "data",
    "science",
    "related",
    "task",
    "like",
    "working",
    "numerical",
    "vectors",
    "obviously",
    "issue",
    "text",
    "numerical",
    "vector",
    "bag",
    "bag",
    "words",
    "approach",
    "kind",
    "easiest",
    "way",
    "take",
    "uh",
    "sentences",
    "know",
    "pieces",
    "text",
    "convert",
    "numerical",
    "representation",
    "imagine",
    "four",
    "utterances",
    "love",
    "book",
    "great",
    "book",
    "fit",
    "great",
    "love",
    "shoes",
    "come",
    "two",
    "different",
    "types",
    "categories",
    "retail",
    "store",
    "books",
    "department",
    "maybe",
    "clothing",
    "department",
    "trying",
    "build",
    "model",
    "tell",
    "apart",
    "well",
    "bag",
    "words",
    "approach",
    "basically",
    "says",
    "take",
    "words",
    "see",
    "utterances",
    "extract",
    "individual",
    "word",
    "unique",
    "word",
    "would",
    "give",
    "us",
    "love",
    "book",
    "great",
    "fit",
    "shoes",
    "take",
    "words",
    "based",
    "words",
    "sentences",
    "total",
    "unique",
    "words",
    "create",
    "vector",
    "one",
    "one",
    "represents",
    "sentence",
    "word",
    "zero",
    "represents",
    "basic",
    "approach",
    "binary",
    "bag",
    "words",
    "model",
    "look",
    "like",
    "code",
    "well",
    "think",
    "whenever",
    "building",
    "bag",
    "words",
    "model",
    "easiest",
    "library",
    "use",
    "library",
    "going",
    "extract",
    "couple",
    "things",
    "library",
    "make",
    "text",
    "little",
    "bit",
    "bigger",
    "sklearn",
    "dot",
    "feature",
    "extraction",
    "dot",
    "text",
    "import",
    "count",
    "vectorizer",
    "tf",
    "sorry",
    "start",
    "count",
    "vectorizer",
    "says",
    "count",
    "vectorizer",
    "either",
    "binary",
    "bag",
    "words",
    "mentioned",
    "ones",
    "zeros",
    "also",
    "could",
    "imagine",
    "uh",
    "one",
    "phrases",
    "mentioned",
    "love",
    "book",
    "book",
    "great",
    "like",
    "one",
    "sentence",
    "maybe",
    "count",
    "vectorizer",
    "could",
    "also",
    "say",
    "book",
    "appeared",
    "twice",
    "would",
    "rough",
    "straight",
    "count",
    "uh",
    "word",
    "overall",
    "sentence",
    "many",
    "times",
    "word",
    "appeared",
    "sentence",
    "usually",
    "tend",
    "use",
    "binary",
    "approach",
    "ever",
    "know",
    "find",
    "something",
    "like",
    "know",
    "bag",
    "words",
    "model",
    "know",
    "look",
    "find",
    "literally",
    "would",
    "like",
    "google",
    "search",
    "sk",
    "learning",
    "bag",
    "words",
    "get",
    "documentation",
    "able",
    "find",
    "examples",
    "uh",
    "using",
    "bag",
    "words",
    "basically",
    "count",
    "vectorizer",
    "showed",
    "shows",
    "us",
    "utilize",
    "might",
    "reference",
    "couple",
    "times",
    "count",
    "vectorizer",
    "um",
    "going",
    "real",
    "quick",
    "find",
    "training",
    "rinses",
    "going",
    "call",
    "train",
    "axe",
    "going",
    "say",
    "let",
    "use",
    "examples",
    "love",
    "book",
    "first",
    "one",
    "love",
    "book",
    "great",
    "book",
    "second",
    "one",
    "uh",
    "last",
    "two",
    "fit",
    "great",
    "love",
    "shoes",
    "right",
    "four",
    "training",
    "utterances",
    "imagine",
    "uh",
    "first",
    "thing",
    "might",
    "want",
    "utilize",
    "cap",
    "vectorizer",
    "transform",
    "vector",
    "representation",
    "going",
    "reference",
    "documentation",
    "know",
    "know",
    "trying",
    "sometimes",
    "forget",
    "syntax",
    "always",
    "reference",
    "documentation",
    "like",
    "uh",
    "looks",
    "like",
    "want",
    "use",
    "transform",
    "vectorizer",
    "equals",
    "count",
    "vectorizer",
    "fit",
    "transform",
    "sentences",
    "say",
    "vectorizer",
    "equals",
    "count",
    "vectorizer",
    "vectorizer",
    "say",
    "vectors",
    "equal",
    "vectorizer",
    "dot",
    "fit",
    "transform",
    "train",
    "x",
    "says",
    "fit",
    "transform",
    "fits",
    "dictionary",
    "around",
    "training",
    "utterances",
    "basically",
    "first",
    "step",
    "finding",
    "unique",
    "words",
    "knows",
    "make",
    "vectors",
    "going",
    "ahead",
    "transforming",
    "utterances",
    "pass",
    "based",
    "vector",
    "fitted",
    "get",
    "vectors",
    "could",
    "like",
    "print",
    "vectors",
    "zero",
    "vector",
    "um",
    "love",
    "book",
    "runs",
    "train",
    "x",
    "defined",
    "need",
    "run",
    "cell",
    "using",
    "google",
    "collab",
    "right",
    "run",
    "okay",
    "looks",
    "good",
    "might",
    "dot",
    "vector",
    "think",
    "found",
    "vectors",
    "let",
    "see",
    "print",
    "real",
    "quick",
    "uh",
    "oh",
    "guess",
    "could",
    "two",
    "array",
    "see",
    "actually",
    "looks",
    "could",
    "also",
    "looks",
    "helpful",
    "going",
    "print",
    "things",
    "going",
    "print",
    "also",
    "going",
    "print",
    "let",
    "say",
    "vectors",
    "print",
    "vectors",
    "dot",
    "2",
    "array",
    "let",
    "see",
    "happens",
    "okay",
    "cool",
    "different",
    "words",
    "dictionary",
    "book",
    "fit",
    "great",
    "love",
    "choose",
    "one",
    "thing",
    "note",
    "looks",
    "like",
    "actually",
    "uh",
    "stripped",
    "away",
    "might",
    "feature",
    "know",
    "one",
    "word",
    "utterances",
    "strips",
    "away",
    "uh",
    "part",
    "way",
    "count",
    "vectorizer",
    "implemented",
    "look",
    "vector",
    "let",
    "look",
    "first",
    "utterance",
    "let",
    "love",
    "book",
    "one",
    "book",
    "right",
    "one",
    "love",
    "right",
    "thus",
    "second",
    "last",
    "one",
    "guess",
    "stripped",
    "away",
    "well",
    "see",
    "get",
    "vector",
    "representation",
    "one",
    "word",
    "utterances",
    "vectorizer",
    "count",
    "one",
    "thing",
    "note",
    "think",
    "default",
    "made",
    "love",
    "book",
    "book",
    "uh",
    "count",
    "vectorizer",
    "actually",
    "matters",
    "many",
    "times",
    "type",
    "something",
    "wanted",
    "could",
    "think",
    "binary",
    "equals",
    "true",
    "switches",
    "back",
    "ones",
    "zeros",
    "okay",
    "basics",
    "bag",
    "words",
    "seeing",
    "making",
    "toy",
    "example",
    "let",
    "build",
    "quick",
    "model",
    "actually",
    "classify",
    "clothing",
    "related",
    "first",
    "two",
    "book",
    "related",
    "going",
    "call",
    "something",
    "train",
    "going",
    "first",
    "one",
    "books",
    "second",
    "one",
    "books",
    "whenever",
    "repeated",
    "string",
    "like",
    "like",
    "make",
    "variable",
    "make",
    "sure",
    "actually",
    "like",
    "misspell",
    "going",
    "make",
    "class",
    "category",
    "real",
    "quick",
    "label",
    "books",
    "one",
    "categories",
    "going",
    "string",
    "books",
    "clothing",
    "category",
    "see",
    "sec",
    "want",
    "order",
    "four",
    "things",
    "going",
    "give",
    "four",
    "uh",
    "labels",
    "order",
    "category",
    "dot",
    "books",
    "last",
    "two",
    "clothing",
    "category",
    "dot",
    "clothing",
    "right",
    "going",
    "run",
    "run",
    "guess",
    "going",
    "build",
    "simple",
    "classifier",
    "hide",
    "print",
    "see",
    "everything",
    "real",
    "quick",
    "um",
    "built",
    "simple",
    "classifier",
    "classify",
    "whether",
    "book",
    "book",
    "category",
    "uh",
    "clothing",
    "use",
    "new",
    "utterances",
    "well",
    "library",
    "helpful",
    "good",
    "classic",
    "uh",
    "classification",
    "model",
    "text",
    "often",
    "linear",
    "svm",
    "going",
    "use",
    "sklearn",
    "import",
    "svm",
    "define",
    "svm",
    "following",
    "going",
    "say",
    "classifier",
    "svm",
    "equals",
    "linear",
    "kernel",
    "want",
    "use",
    "linear",
    "svm",
    "know",
    "uh",
    "null",
    "like",
    "background",
    "knowledge",
    "good",
    "classification",
    "model",
    "text",
    "often",
    "times",
    "svm",
    "going",
    "fit",
    "training",
    "vectors",
    "right",
    "vectors",
    "could",
    "pass",
    "text",
    "going",
    "call",
    "train",
    "x",
    "vectors",
    "vectors",
    "labels",
    "train",
    "fit",
    "model",
    "four",
    "utterances",
    "ah",
    "got",
    "error",
    "rerun",
    "cool",
    "finally",
    "let",
    "fun",
    "part",
    "predict",
    "new",
    "utterances",
    "going",
    "call",
    "text",
    "x",
    "equals",
    "vectorizer",
    "dot",
    "transform",
    "fit",
    "dictionary",
    "transform",
    "based",
    "training",
    "utterances",
    "already",
    "vectorizer",
    "transform",
    "instead",
    "fit",
    "transform",
    "going",
    "say",
    "something",
    "like",
    "like",
    "book",
    "based",
    "said",
    "word",
    "book",
    "expect",
    "would",
    "classify",
    "books",
    "category",
    "classifier",
    "clf",
    "testx",
    "see",
    "happens",
    "nice",
    "books",
    "cool",
    "let",
    "try",
    "like",
    "love",
    "shoes",
    "maybe",
    "say",
    "something",
    "like",
    "shoes",
    "right",
    "wow",
    "says",
    "clothing",
    "obviously",
    "trained",
    "four",
    "utterances",
    "going",
    "powerful",
    "um",
    "yeah",
    "gon",
    "na",
    "powerful",
    "added",
    "utterances",
    "dictionary",
    "would",
    "grow",
    "within",
    "vectorizer",
    "fit",
    "transform",
    "get",
    "powerful",
    "model",
    "training",
    "instances",
    "feed",
    "bag",
    "words",
    "model",
    "oftentimes",
    "better",
    "guess",
    "one",
    "caveats",
    "might",
    "build",
    "big",
    "dictionary",
    "might",
    "get",
    "hard",
    "process",
    "like",
    "model",
    "another",
    "technique",
    "add",
    "definitely",
    "sklearn",
    "offers",
    "ability",
    "maybe",
    "take",
    "top",
    "1000",
    "words",
    "occur",
    "like",
    "another",
    "option",
    "another",
    "thing",
    "know",
    "bag",
    "words",
    "right",
    "unigram",
    "approach",
    "taking",
    "individual",
    "word",
    "could",
    "also",
    "approach",
    "called",
    "would",
    "categorize",
    "love",
    "love",
    "book",
    "unique",
    "utterances",
    "let",
    "try",
    "real",
    "quick",
    "let",
    "see",
    "uh",
    "let",
    "see",
    "engram",
    "range",
    "probably",
    "going",
    "help",
    "us",
    "going",
    "type",
    "endgram",
    "range",
    "probably",
    "give",
    "information",
    "equals",
    "oh",
    "gon",
    "na",
    "say",
    "one",
    "one",
    "two",
    "get",
    "think",
    "one",
    "words",
    "two",
    "words",
    "print",
    "stuff",
    "see",
    "case",
    "yeah",
    "see",
    "get",
    "two",
    "words",
    "well",
    "captured",
    "bigrams",
    "well",
    "one",
    "thing",
    "note",
    "diagrams",
    "one",
    "reason",
    "might",
    "want",
    "use",
    "two",
    "words",
    "row",
    "let",
    "say",
    "talking",
    "things",
    "positive",
    "negative",
    "well",
    "say",
    "something",
    "great",
    "positive",
    "use",
    "biogram",
    "word",
    "great",
    "know",
    "great",
    "completely",
    "different",
    "sentiment",
    "one",
    "way",
    "know",
    "engrams",
    "important",
    "know",
    "words",
    "depend",
    "context",
    "uh",
    "know",
    "adding",
    "additional",
    "words",
    "sometimes",
    "helps",
    "could",
    "also",
    "detrimental",
    "many",
    "words",
    "endogram",
    "range",
    "might",
    "many",
    "like",
    "random",
    "miscellaneous",
    "like",
    "three",
    "word",
    "phrases",
    "occur",
    "entire",
    "set",
    "might",
    "actually",
    "skew",
    "classifier",
    "bit",
    "going",
    "stick",
    "single",
    "uh",
    "one",
    "unigram",
    "model",
    "final",
    "final",
    "thing",
    "say",
    "bagger",
    "words",
    "get",
    "next",
    "model",
    "limitation",
    "word",
    "training",
    "utterances",
    "going",
    "know",
    "handle",
    "said",
    "something",
    "like",
    "love",
    "story",
    "well",
    "us",
    "similar",
    "know",
    "book",
    "books",
    "uh",
    "model",
    "never",
    "seen",
    "word",
    "story",
    "might",
    "know",
    "handle",
    "let",
    "see",
    "happens",
    "run",
    "issue",
    "yeah",
    "classified",
    "clothing",
    "even",
    "though",
    "obvious",
    "us",
    "story",
    "book",
    "related",
    "honestly",
    "think",
    "even",
    "typed",
    "something",
    "like",
    "books",
    "know",
    "book",
    "books",
    "seen",
    "word",
    "book",
    "right",
    "never",
    "seen",
    "books",
    "like",
    "pretty",
    "dumb",
    "seen",
    "word",
    "yeah",
    "says",
    "clothing",
    "even",
    "though",
    "obvious",
    "us",
    "book",
    "books",
    "one",
    "downfall",
    "bag",
    "words",
    "great",
    "stuff",
    "trained",
    "seen",
    "word",
    "fails",
    "miserably",
    "know",
    "great",
    "human",
    "language",
    "know",
    "say",
    "different",
    "things",
    "many",
    "different",
    "ways",
    "leads",
    "us",
    "topic",
    "word",
    "vectors",
    "word",
    "vectors",
    "another",
    "approach",
    "turning",
    "text",
    "numerical",
    "vector",
    "big",
    "goal",
    "word",
    "vectors",
    "convert",
    "text",
    "numerical",
    "vector",
    "captures",
    "semantic",
    "meaning",
    "vector",
    "space",
    "mapping",
    "piece",
    "text",
    "mean",
    "imagine",
    "words",
    "red",
    "blue",
    "yellow",
    "three",
    "different",
    "types",
    "colors",
    "trying",
    "word",
    "vectors",
    "imagine",
    "big",
    "like",
    "vector",
    "space",
    "want",
    "similar",
    "words",
    "mapped",
    "similar",
    "spot",
    "vector",
    "space",
    "red",
    "blue",
    "yellow",
    "would",
    "colors",
    "mapped",
    "somewhere",
    "similar",
    "many",
    "different",
    "approaches",
    "training",
    "vector",
    "ultimately",
    "one",
    "popular",
    "methods",
    "called",
    "word",
    "vec",
    "word",
    "effect",
    "works",
    "one",
    "two",
    "ways",
    "usually",
    "two",
    "popular",
    "like",
    "popular",
    "guess",
    "ways",
    "train",
    "word",
    "act",
    "one",
    "called",
    "continuous",
    "bag",
    "words",
    "one",
    "called",
    "uh",
    "skip",
    "gram",
    "different",
    "details",
    "two",
    "approaches",
    "super",
    "super",
    "important",
    "important",
    "kind",
    "understand",
    "high",
    "level",
    "work",
    "imagine",
    "going",
    "back",
    "kind",
    "example",
    "classifying",
    "books",
    "clothing",
    "related",
    "tweets",
    "imagine",
    "three",
    "phrases",
    "say",
    "best",
    "book",
    "read",
    "years",
    "great",
    "story",
    "characters",
    "development",
    "characters",
    "book",
    "three",
    "phrases",
    "related",
    "books",
    "tell",
    "pretty",
    "easily",
    "reading",
    "train",
    "model",
    "see",
    "word",
    "divac",
    "approaches",
    "continuous",
    "bag",
    "words",
    "skip",
    "gram",
    "approaches",
    "look",
    "window",
    "text",
    "oftentimes",
    "window",
    "might",
    "know",
    "five",
    "tokens",
    "long",
    "best",
    "book",
    "read",
    "would",
    "kind",
    "context",
    "window",
    "basically",
    "selectively",
    "look",
    "different",
    "tokens",
    "context",
    "window",
    "based",
    "token",
    "utilize",
    "surrounding",
    "tokens",
    "kind",
    "figure",
    "context",
    "token",
    "kind",
    "start",
    "developing",
    "meaning",
    "token",
    "translating",
    "example",
    "start",
    "read",
    "enough",
    "text",
    "start",
    "see",
    "relationships",
    "words",
    "example",
    "one",
    "relationship",
    "might",
    "develop",
    "time",
    "book",
    "often",
    "appears",
    "close",
    "read",
    "associate",
    "book",
    "read",
    "similar",
    "spot",
    "vector",
    "space",
    "ultimately",
    "going",
    "trained",
    "sort",
    "neural",
    "network",
    "architecture",
    "part",
    "given",
    "neural",
    "network",
    "architecture",
    "know",
    "model",
    "learning",
    "book",
    "read",
    "close",
    "together",
    "uh",
    "next",
    "example",
    "know",
    "great",
    "story",
    "characters",
    "might",
    "start",
    "relating",
    "story",
    "characters",
    "together",
    "um",
    "know",
    "see",
    "together",
    "know",
    "related",
    "similar",
    "uh",
    "vector",
    "space",
    "uh",
    "last",
    "example",
    "development",
    "characters",
    "book",
    "well",
    "maybe",
    "relate",
    "development",
    "characters",
    "model",
    "learning",
    "uh",
    "know",
    "also",
    "probably",
    "learning",
    "characters",
    "book",
    "related",
    "kind",
    "start",
    "building",
    "far",
    "like",
    "bigger",
    "relationships",
    "like",
    "okay",
    "seen",
    "characters",
    "book",
    "know",
    "probably",
    "related",
    "okay",
    "go",
    "story",
    "characters",
    "often",
    "together",
    "guessing",
    "story",
    "book",
    "probably",
    "related",
    "well",
    "toy",
    "example",
    "happening",
    "hundreds",
    "thousands",
    "like",
    "sentences",
    "fed",
    "types",
    "models",
    "ultimately",
    "building",
    "word",
    "vectors",
    "one",
    "thing",
    "talk",
    "let",
    "start",
    "jumping",
    "python",
    "code",
    "actually",
    "show",
    "looks",
    "like",
    "think",
    "best",
    "spot",
    "easily",
    "utilize",
    "word",
    "vectors",
    "python",
    "would",
    "probably",
    "use",
    "spacey",
    "library",
    "whenever",
    "like",
    "trying",
    "remember",
    "something",
    "like",
    "know",
    "going",
    "google",
    "search",
    "finding",
    "sort",
    "information",
    "use",
    "spacey",
    "use",
    "word",
    "vectors",
    "offer",
    "see",
    "probably",
    "gon",
    "na",
    "download",
    "uh",
    "word",
    "vector",
    "model",
    "gon",
    "na",
    "train",
    "scratch",
    "use",
    "something",
    "already",
    "trained",
    "uh",
    "kind",
    "follow",
    "along",
    "like",
    "looks",
    "like",
    "okay",
    "um",
    "yeah",
    "look",
    "maybe",
    "look",
    "actual",
    "documentation",
    "space",
    "good",
    "place",
    "start",
    "let",
    "start",
    "implementing",
    "okay",
    "gon",
    "na",
    "need",
    "use",
    "spacey",
    "library",
    "use",
    "spacey",
    "library",
    "going",
    "need",
    "download",
    "language",
    "train",
    "vectors",
    "already",
    "previously",
    "going",
    "go",
    "ahead",
    "top",
    "google",
    "collab",
    "file",
    "also",
    "probably",
    "work",
    "jupiter",
    "notebook",
    "whatever",
    "using",
    "browser",
    "might",
    "need",
    "like",
    "pip",
    "install",
    "going",
    "insert",
    "code",
    "cell",
    "want",
    "make",
    "couple",
    "installations",
    "things",
    "need",
    "default",
    "google",
    "collab",
    "using",
    "wo",
    "spacey",
    "language",
    "model",
    "spacey",
    "word",
    "vector",
    "model",
    "embeddings",
    "need",
    "gon",
    "na",
    "install",
    "pip",
    "install",
    "spacey",
    "think",
    "space",
    "already",
    "installed",
    "sometimes",
    "need",
    "version",
    "think",
    "help",
    "us",
    "going",
    "spacey",
    "download",
    "gon",
    "na",
    "download",
    "medium",
    "sized",
    "uh",
    "embeddings",
    "see",
    "dock",
    "walk",
    "also",
    "large",
    "model",
    "might",
    "take",
    "bit",
    "long",
    "download",
    "want",
    "try",
    "powerful",
    "word",
    "vectors",
    "maybe",
    "try",
    "large",
    "model",
    "okay",
    "going",
    "run",
    "downloading",
    "okay",
    "downloaded",
    "one",
    "quick",
    "note",
    "sometimes",
    "using",
    "google",
    "collab",
    "might",
    "restart",
    "runtime",
    "uh",
    "changes",
    "take",
    "effect",
    "like",
    "imagine",
    "already",
    "imported",
    "spacey",
    "uh",
    "think",
    "spacey",
    "would",
    "recognize",
    "model",
    "unless",
    "restart",
    "runtime",
    "doubly",
    "sure",
    "import",
    "spacey",
    "first",
    "step",
    "okay",
    "looks",
    "like",
    "uh",
    "imported",
    "properly",
    "want",
    "load",
    "going",
    "call",
    "nlp",
    "uh",
    "word",
    "embeddings",
    "model",
    "downloaded",
    "n",
    "core",
    "web",
    "medium",
    "size",
    "let",
    "see",
    "works",
    "hopefully",
    "need",
    "load",
    "stay",
    "memory",
    "like",
    "remembering",
    "always",
    "like",
    "remind",
    "people",
    "literally",
    "google",
    "search",
    "word",
    "vectors",
    "spacey",
    "documentation",
    "article",
    "looking",
    "actual",
    "docs",
    "could",
    "probably",
    "actually",
    "find",
    "docs",
    "give",
    "us",
    "yeah",
    "look",
    "gives",
    "us",
    "nice",
    "examples",
    "right",
    "makes",
    "lot",
    "easier",
    "trying",
    "figure",
    "memory",
    "let",
    "see",
    "music",
    "want",
    "see",
    "actual",
    "value",
    "looks",
    "like",
    "vectors",
    "values",
    "think",
    "dot",
    "value",
    "get",
    "vector",
    "right",
    "okay",
    "going",
    "basically",
    "need",
    "take",
    "text",
    "want",
    "use",
    "text",
    "going",
    "rerun",
    "cell",
    "examples",
    "using",
    "previously",
    "bag",
    "words",
    "model",
    "called",
    "train",
    "x",
    "going",
    "say",
    "docs",
    "equal",
    "basically",
    "going",
    "need",
    "convert",
    "see",
    "initialization",
    "look",
    "one",
    "time",
    "word",
    "vectors",
    "spacey",
    "example",
    "want",
    "example",
    "see",
    "things",
    "spacey",
    "101",
    "looks",
    "like",
    "already",
    "clicked",
    "next",
    "one",
    "one",
    "right",
    "let",
    "try",
    "okay",
    "look",
    "yep",
    "go",
    "nice",
    "showing",
    "us",
    "make",
    "little",
    "bit",
    "bigger",
    "see",
    "showing",
    "us",
    "get",
    "vector",
    "pass",
    "dock",
    "p",
    "vector",
    "get",
    "vector",
    "representation",
    "let",
    "see",
    "oh",
    "maybe",
    "doc",
    "container",
    "information",
    "docs",
    "vector",
    "vector",
    "vector",
    "vector",
    "want",
    "find",
    "okay",
    "go",
    "yeah",
    "real",
    "valued",
    "meaning",
    "representation",
    "defaults",
    "average",
    "token",
    "vectors",
    "basically",
    "nlp",
    "phrase",
    "dot",
    "vector",
    "going",
    "take",
    "individual",
    "word",
    "word",
    "embeddings",
    "average",
    "together",
    "think",
    "ultimately",
    "want",
    "want",
    "build",
    "model",
    "around",
    "um",
    "also",
    "note",
    "word",
    "vectors",
    "usually",
    "hundreds",
    "uh",
    "know",
    "dimension",
    "one",
    "like",
    "several",
    "hundred",
    "pretty",
    "big",
    "uh",
    "kind",
    "capture",
    "information",
    "need",
    "okay",
    "docs",
    "going",
    "nlp",
    "text",
    "text",
    "train",
    "x",
    "defined",
    "items",
    "docs",
    "list",
    "word",
    "vector",
    "representation",
    "sentences",
    "defined",
    "love",
    "book",
    "great",
    "book",
    "fit",
    "great",
    "love",
    "shoes",
    "might",
    "helpful",
    "print",
    "train",
    "x",
    "remember",
    "um",
    "okay",
    "ultimately",
    "converting",
    "print",
    "docs",
    "gon",
    "na",
    "see",
    "um",
    "oh",
    "guess",
    "keeps",
    "print",
    "docs",
    "dot",
    "vector",
    "oops",
    "print",
    "let",
    "say",
    "first",
    "docs",
    "vector",
    "gon",
    "na",
    "see",
    "word",
    "embedding",
    "representation",
    "average",
    "word",
    "embedding",
    "words",
    "love",
    "book",
    "cool",
    "pretty",
    "easily",
    "using",
    "spacey",
    "couple",
    "lines",
    "code",
    "uh",
    "okay",
    "let",
    "build",
    "model",
    "built",
    "uh",
    "back",
    "words",
    "model",
    "spacey",
    "model",
    "define",
    "classifier",
    "define",
    "like",
    "going",
    "define",
    "classifier",
    "going",
    "give",
    "slightly",
    "different",
    "name",
    "going",
    "say",
    "svm",
    "give",
    "wv",
    "word",
    "vectors",
    "end",
    "tell",
    "apart",
    "want",
    "fit",
    "train",
    "x",
    "vectors",
    "anymore",
    "want",
    "fit",
    "define",
    "word",
    "vector",
    "word",
    "vector",
    "vectors",
    "say",
    "call",
    "train",
    "x",
    "word",
    "vectors",
    "separate",
    "little",
    "bit",
    "word",
    "vectors",
    "going",
    "dot",
    "vector",
    "x",
    "dot",
    "vector",
    "x",
    "docs",
    "getting",
    "list",
    "going",
    "pass",
    "fit",
    "labels",
    "stay",
    "books",
    "books",
    "category",
    "clothing",
    "clothing",
    "right",
    "let",
    "see",
    "happens",
    "fitting",
    "run",
    "run",
    "svm",
    "defined",
    "okay",
    "need",
    "svm",
    "cool",
    "think",
    "one",
    "thing",
    "needed",
    "reimport",
    "uh",
    "music",
    "nope",
    "think",
    "good",
    "okay",
    "model",
    "happens",
    "try",
    "predict",
    "novel",
    "utterances",
    "pass",
    "nlp",
    "phrase",
    "love",
    "book",
    "grab",
    "vector",
    "going",
    "happen",
    "oh",
    "trying",
    "much",
    "one",
    "spot",
    "instead",
    "going",
    "say",
    "test",
    "x",
    "word",
    "vectors",
    "equals",
    "well",
    "also",
    "define",
    "test",
    "docs",
    "going",
    "equal",
    "trying",
    "lot",
    "sorry",
    "feel",
    "like",
    "simple",
    "live",
    "uh",
    "sometimes",
    "tough",
    "test",
    "going",
    "equal",
    "equal",
    "list",
    "words",
    "let",
    "say",
    "love",
    "book",
    "test",
    "docs",
    "nlp",
    "representation",
    "test",
    "x",
    "text",
    "text",
    "text",
    "test",
    "x",
    "finally",
    "word",
    "vectors",
    "going",
    "um",
    "x",
    "dot",
    "vector",
    "x",
    "test",
    "docs",
    "uh",
    "cool",
    "predict",
    "test",
    "x",
    "word",
    "vectors",
    "cool",
    "sorry",
    "trying",
    "simplify",
    "much",
    "result",
    "oops",
    "close",
    "uh",
    "keep",
    "getting",
    "double",
    "things",
    "okay",
    "love",
    "book",
    "works",
    "expected",
    "mean",
    "exact",
    "take",
    "recognizes",
    "books",
    "let",
    "try",
    "find",
    "power",
    "word",
    "vectors",
    "let",
    "type",
    "something",
    "like",
    "love",
    "story",
    "hoping",
    "story",
    "book",
    "similar",
    "word",
    "vector",
    "representation",
    "averaging",
    "nlp",
    "text",
    "basically",
    "averages",
    "word",
    "embeddings",
    "together",
    "actually",
    "get",
    "vector",
    "value",
    "average",
    "embedding",
    "get",
    "books",
    "classification",
    "go",
    "look",
    "love",
    "story",
    "gets",
    "books",
    "well",
    "let",
    "try",
    "testing",
    "things",
    "clothing",
    "related",
    "love",
    "shoes",
    "works",
    "properly",
    "capturing",
    "semantics",
    "able",
    "say",
    "something",
    "like",
    "love",
    "hat",
    "hopefully",
    "says",
    "clothing",
    "look",
    "cool",
    "love",
    "hats",
    "also",
    "clothing",
    "love",
    "books",
    "even",
    "though",
    "seen",
    "books",
    "exactly",
    "knows",
    "book",
    "books",
    "related",
    "seen",
    "similar",
    "context",
    "windows",
    "yeah",
    "sorts",
    "cool",
    "stuff",
    "know",
    "earrings",
    "hurt",
    "something",
    "already",
    "four",
    "training",
    "examples",
    "much",
    "power",
    "baked",
    "spacey",
    "word",
    "embeddings",
    "word",
    "embeddings",
    "general",
    "even",
    "model",
    "already",
    "predict",
    "lot",
    "things",
    "correctly",
    "knowing",
    "know",
    "semantic",
    "space",
    "think",
    "like",
    "word",
    "vectors",
    "cool",
    "concept",
    "cool",
    "much",
    "language",
    "like",
    "gets",
    "really",
    "excited",
    "um",
    "guess",
    "end",
    "little",
    "seg",
    "section",
    "uh",
    "drawbacks",
    "word",
    "vectors",
    "cure",
    "everything",
    "think",
    "one",
    "thing",
    "see",
    "worked",
    "pretty",
    "well",
    "us",
    "two",
    "categories",
    "books",
    "clothing",
    "trying",
    "use",
    "word",
    "vectors",
    "think",
    "exercise",
    "end",
    "tutorial",
    "might",
    "see",
    "um",
    "say",
    "use",
    "10",
    "different",
    "categories",
    "instead",
    "phrases",
    "like",
    "four",
    "words",
    "like",
    "50",
    "words",
    "trying",
    "capture",
    "embedding",
    "entire",
    "sentence",
    "averaging",
    "together",
    "like",
    "50",
    "word",
    "individual",
    "word",
    "vectors",
    "actual",
    "meanings",
    "individual",
    "words",
    "might",
    "get",
    "kind",
    "lost",
    "averaging",
    "process",
    "sometimes",
    "quite",
    "precise",
    "maybe",
    "using",
    "bag",
    "words",
    "case",
    "things",
    "get",
    "kind",
    "mixed",
    "together",
    "another",
    "drawback",
    "standard",
    "word",
    "back",
    "word",
    "embeddings",
    "uh",
    "little",
    "bit",
    "different",
    "context",
    "worrying",
    "imagine",
    "trying",
    "get",
    "sort",
    "meaning",
    "word",
    "check",
    "went",
    "bank",
    "wrote",
    "check",
    "gon",
    "na",
    "check",
    "case",
    "gon",
    "na",
    "specific",
    "meaning",
    "also",
    "another",
    "word",
    "embedded",
    "another",
    "sentence",
    "let",
    "check",
    "check",
    "let",
    "check",
    "different",
    "writing",
    "check",
    "word",
    "vector",
    "going",
    "get",
    "words",
    "multiple",
    "meanings",
    "get",
    "little",
    "bit",
    "messiness",
    "meanings",
    "kind",
    "try",
    "captured",
    "training",
    "process",
    "ultimately",
    "part",
    "meetings",
    "probably",
    "lost",
    "word",
    "vectors",
    "cool",
    "uh",
    "pretty",
    "powerful",
    "solve",
    "everything",
    "ultimately",
    "left",
    "lot",
    "room",
    "improvement",
    "ultimately",
    "lot",
    "developed",
    "recently",
    "kind",
    "mentioned",
    "start",
    "tutorial",
    "right",
    "next",
    "section",
    "tutorial",
    "going",
    "kind",
    "rapid",
    "fire",
    "overview",
    "bunch",
    "different",
    "nlp",
    "techniques",
    "good",
    "add",
    "nlp",
    "toolkit",
    "first",
    "technique",
    "going",
    "look",
    "using",
    "regexes",
    "regexay",
    "regexes",
    "pattern",
    "matching",
    "strings",
    "python",
    "specific",
    "concept",
    "definitely",
    "effectively",
    "utilize",
    "python",
    "real",
    "quick",
    "going",
    "start",
    "quick",
    "overview",
    "pi",
    "regex",
    "uh",
    "basically",
    "way",
    "think",
    "could",
    "sorts",
    "different",
    "types",
    "phone",
    "numbers",
    "like",
    "one",
    "two",
    "three",
    "one",
    "two",
    "three",
    "one",
    "two",
    "three",
    "four",
    "would",
    "valid",
    "number",
    "know",
    "maybe",
    "different",
    "set",
    "numbers",
    "format",
    "um",
    "five",
    "five",
    "five",
    "five",
    "five",
    "five",
    "five",
    "five",
    "five",
    "five",
    "would",
    "also",
    "another",
    "valid",
    "phone",
    "number",
    "maybe",
    "want",
    "write",
    "little",
    "bit",
    "differently",
    "maybe",
    "like",
    "plus",
    "one",
    "dash",
    "parentheses",
    "one",
    "two",
    "three",
    "dash",
    "two",
    "three",
    "different",
    "ways",
    "write",
    "phone",
    "number",
    "like",
    "technically",
    "valid",
    "ways",
    "go",
    "one",
    "way",
    "could",
    "use",
    "regex",
    "pattern",
    "matching",
    "like",
    "figuring",
    "something",
    "telephone",
    "number",
    "like",
    "case",
    "one",
    "thing",
    "could",
    "see",
    "three",
    "digits",
    "followed",
    "sort",
    "punctuation",
    "maybe",
    "punctuation",
    "followed",
    "three",
    "digits",
    "followed",
    "uh",
    "four",
    "digits",
    "basically",
    "could",
    "define",
    "regex",
    "similarly",
    "like",
    "last",
    "one",
    "could",
    "define",
    "reg",
    "x",
    "could",
    "plus",
    "one",
    "plus",
    "number",
    "optionally",
    "regix",
    "allow",
    "us",
    "easily",
    "capture",
    "different",
    "types",
    "patterns",
    "effectively",
    "add",
    "kind",
    "like",
    "code",
    "writing",
    "know",
    "phone",
    "numbers",
    "writing",
    "reg",
    "x",
    "phone",
    "numbers",
    "one",
    "thing",
    "also",
    "might",
    "like",
    "password",
    "checker",
    "like",
    "logging",
    "onto",
    "site",
    "say",
    "oh",
    "need",
    "one",
    "symbol",
    "one",
    "character",
    "one",
    "uppercase",
    "character",
    "number",
    "password",
    "plus",
    "10",
    "10",
    "digits",
    "regex",
    "help",
    "people",
    "implementing",
    "site",
    "back",
    "end",
    "make",
    "sure",
    "password",
    "meets",
    "specs",
    "emails",
    "formats",
    "another",
    "thing",
    "think",
    "easiest",
    "way",
    "get",
    "regex",
    "start",
    "right",
    "away",
    "example",
    "going",
    "say",
    "example",
    "say",
    "want",
    "match",
    "string",
    "starts",
    "letters",
    "b",
    "middle",
    "string",
    "number",
    "characters",
    "matter",
    "many",
    "characters",
    "long",
    "white",
    "space",
    "want",
    "white",
    "space",
    "trying",
    "match",
    "needs",
    "end",
    "letters",
    "cd",
    "whenever",
    "going",
    "working",
    "regex",
    "usually",
    "start",
    "looking",
    "like",
    "regex",
    "cheat",
    "sheet",
    "remind",
    "regex",
    "link",
    "github",
    "page",
    "tutorial",
    "page",
    "like",
    "sorts",
    "useful",
    "tidbits",
    "ultimately",
    "going",
    "want",
    "like",
    "exactly",
    "b",
    "exactly",
    "b",
    "going",
    "group",
    "followed",
    "number",
    "characters",
    "see",
    "special",
    "dot",
    "uh",
    "period",
    "character",
    "except",
    "new",
    "line",
    "maybe",
    "utilize",
    "quantifiers",
    "could",
    "zero",
    "one",
    "zero",
    "one",
    "use",
    "quantifiers",
    "stuff",
    "like",
    "word",
    "boundaries",
    "useful",
    "let",
    "try",
    "write",
    "regex",
    "case",
    "said",
    "going",
    "site",
    "actually",
    "flavor",
    "test",
    "clicked",
    "python",
    "said",
    "needs",
    "b",
    "start",
    "followed",
    "number",
    "characters",
    "period",
    "matter",
    "many",
    "star",
    "means",
    "uh",
    "zero",
    "followed",
    "c",
    "like",
    "basic",
    "implementation",
    "see",
    "b",
    "e",
    "e",
    "c",
    "matches",
    "ba",
    "followed",
    "x",
    "something",
    "going",
    "match",
    "one",
    "start",
    "b",
    "end",
    "cd",
    "continuing",
    "said",
    "could",
    "white",
    "space",
    "let",
    "see",
    "b",
    "space",
    "cd",
    "works",
    "work",
    "need",
    "fix",
    "regex",
    "disallow",
    "white",
    "spaces",
    "going",
    "back",
    "cheat",
    "sheet",
    "see",
    "character",
    "b",
    "c",
    "utilize",
    "along",
    "um",
    "white",
    "space",
    "character",
    "class",
    "forward",
    "slash",
    "forward",
    "slash",
    "white",
    "space",
    "going",
    "brackets",
    "white",
    "space",
    "let",
    "go",
    "back",
    "instead",
    "dot",
    "star",
    "gon",
    "na",
    "brackets",
    "slash",
    "close",
    "bracket",
    "one",
    "see",
    "works",
    "one",
    "white",
    "space",
    "middle",
    "remove",
    "white",
    "space",
    "match",
    "pretty",
    "cool",
    "one",
    "thing",
    "note",
    "could",
    "something",
    "like",
    "uh",
    "x",
    "x",
    "b",
    "c",
    "x",
    "x",
    "uh",
    "still",
    "saying",
    "match",
    "least",
    "like",
    "saying",
    "line",
    "match",
    "included",
    "wanted",
    "make",
    "sure",
    "exclusively",
    "uh",
    "start",
    "line",
    "end",
    "line",
    "use",
    "couple",
    "special",
    "characters",
    "end",
    "string",
    "start",
    "string",
    "used",
    "without",
    "brackets",
    "means",
    "start",
    "string",
    "used",
    "means",
    "add",
    "start",
    "string",
    "followed",
    "end",
    "string",
    "one",
    "match",
    "solution",
    "little",
    "exercise",
    "look",
    "like",
    "code",
    "well",
    "quickly",
    "go",
    "google",
    "cloud",
    "file",
    "import",
    "regular",
    "expression",
    "library",
    "python",
    "start",
    "regular",
    "expression",
    "define",
    "whatever",
    "want",
    "defining",
    "one",
    "described",
    "would",
    "carrot",
    "b",
    "followed",
    "white",
    "space",
    "star",
    "followed",
    "cd",
    "followed",
    "end",
    "line",
    "compile",
    "allow",
    "python",
    "know",
    "actual",
    "regular",
    "expression",
    "oftentimes",
    "defining",
    "regular",
    "expressions",
    "python",
    "use",
    "r",
    "front",
    "help",
    "us",
    "highlight",
    "help",
    "us",
    "know",
    "want",
    "want",
    "see",
    "something",
    "could",
    "say",
    "like",
    "tests",
    "phrases",
    "abcd",
    "xxx",
    "b",
    "x",
    "x",
    "x",
    "c",
    "b",
    "space",
    "c",
    "first",
    "third",
    "match",
    "wanted",
    "check",
    "something",
    "matches",
    "phrase",
    "phrases",
    "gon",
    "na",
    "two",
    "main",
    "functions",
    "gon",
    "na",
    "use",
    "checking",
    "matches",
    "gon",
    "na",
    "regular",
    "gon",
    "na",
    "pass",
    "first",
    "regular",
    "expression",
    "going",
    "pass",
    "phrase",
    "wanted",
    "see",
    "matches",
    "match",
    "going",
    "make",
    "conditional",
    "thing",
    "regular",
    "expression",
    "matches",
    "going",
    "append",
    "list",
    "called",
    "matches",
    "phrase",
    "find",
    "matches",
    "list",
    "okay",
    "finally",
    "print",
    "matches",
    "run",
    "okay",
    "see",
    "expected",
    "first",
    "third",
    "match",
    "something",
    "interesting",
    "let",
    "say",
    "took",
    "away",
    "requirement",
    "happening",
    "first",
    "last",
    "thing",
    "line",
    "added",
    "something",
    "like",
    "aaa",
    "front",
    "ccc",
    "wanted",
    "check",
    "regex",
    "entire",
    "string",
    "string",
    "start",
    "entire",
    "thing",
    "places",
    "match",
    "rerun",
    "code",
    "see",
    "regular",
    "function",
    "longer",
    "uh",
    "says",
    "true",
    "one",
    "right",
    "main",
    "thing",
    "probably",
    "going",
    "use",
    "searching",
    "text",
    "regular",
    "expressions",
    "see",
    "still",
    "matches",
    "two",
    "quickly",
    "kind",
    "apply",
    "toy",
    "example",
    "imagine",
    "wanted",
    "create",
    "regular",
    "expression",
    "matches",
    "read",
    "story",
    "book",
    "maybe",
    "making",
    "sort",
    "rule",
    "making",
    "sort",
    "rule",
    "find",
    "something",
    "books",
    "category",
    "liked",
    "story",
    "um",
    "like",
    "book",
    "um",
    "hat",
    "nice",
    "see",
    "regular",
    "expression",
    "sign",
    "counts",
    "one",
    "quick",
    "nuance",
    "wanted",
    "make",
    "regex",
    "complicated",
    "could",
    "try",
    "trick",
    "say",
    "like",
    "history",
    "like",
    "uh",
    "instead",
    "read",
    "maybe",
    "say",
    "uh",
    "car",
    "treaded",
    "hill",
    "know",
    "makes",
    "sense",
    "see",
    "read",
    "inside",
    "uh",
    "actually",
    "referencing",
    "word",
    "read",
    "notice",
    "still",
    "matching",
    "things",
    "one",
    "thing",
    "useful",
    "know",
    "word",
    "boundaries",
    "character",
    "within",
    "reg",
    "x",
    "forward",
    "slash",
    "b",
    "means",
    "needs",
    "word",
    "boundaries",
    "watch",
    "happens",
    "run",
    "matches",
    "knows",
    "story",
    "one",
    "thing",
    "nice",
    "uh",
    "slash",
    "b",
    "format",
    "period",
    "something",
    "end",
    "knows",
    "word",
    "boundary",
    "uh",
    "like",
    "one",
    "way",
    "maybe",
    "would",
    "apply",
    "example",
    "working",
    "many",
    "uh",
    "use",
    "cases",
    "regexes",
    "right",
    "next",
    "going",
    "quickly",
    "look",
    "stemming",
    "limitization",
    "python",
    "two",
    "techniques",
    "normalize",
    "text",
    "mean",
    "first",
    "example",
    "bag",
    "words",
    "saw",
    "example",
    "problem",
    "trained",
    "model",
    "word",
    "book",
    "know",
    "words",
    "books",
    "even",
    "though",
    "us",
    "straightforward",
    "one",
    "example",
    "stemming",
    "limitization",
    "take",
    "books",
    "kind",
    "reduce",
    "canonical",
    "form",
    "book",
    "like",
    "several",
    "different",
    "things",
    "imagine",
    "techniques",
    "could",
    "help",
    "turn",
    "reading",
    "read",
    "books",
    "book",
    "stories",
    "little",
    "bit",
    "little",
    "bit",
    "difference",
    "stemming",
    "follows",
    "algorithm",
    "guaranteed",
    "give",
    "actual",
    "physical",
    "true",
    "english",
    "word",
    "might",
    "reduce",
    "story",
    "whereas",
    "levitizing",
    "would",
    "take",
    "stories",
    "actually",
    "using",
    "dictionary",
    "making",
    "sure",
    "everything",
    "outputs",
    "actual",
    "word",
    "would",
    "output",
    "story",
    "use",
    "python",
    "well",
    "think",
    "library",
    "uh",
    "easiest",
    "ah",
    "um",
    "library",
    "easiest",
    "access",
    "stemming",
    "alignmentization",
    "probably",
    "nltk",
    "library",
    "use",
    "ltk",
    "library",
    "okay",
    "first",
    "going",
    "need",
    "import",
    "ltk",
    "nlt",
    "nltk",
    "stands",
    "natural",
    "language",
    "toolkit",
    "know",
    "trying",
    "much",
    "import",
    "nltk",
    "also",
    "going",
    "need",
    "import",
    "couple",
    "download",
    "couple",
    "things",
    "nltk",
    "work",
    "box",
    "believe",
    "nlt",
    "get",
    "download",
    "getting",
    "stop",
    "words",
    "word",
    "net",
    "think",
    "stop",
    "words",
    "going",
    "next",
    "example",
    "know",
    "need",
    "stemming",
    "limitation",
    "section",
    "right",
    "downloading",
    "let",
    "go",
    "ahead",
    "start",
    "stemmer",
    "uh",
    "get",
    "stemmer",
    "recommend",
    "importing",
    "two",
    "different",
    "things",
    "recommend",
    "importing",
    "tokenize",
    "library",
    "basically",
    "take",
    "sentence",
    "break",
    "individual",
    "words",
    "well",
    "actual",
    "stemmer",
    "going",
    "use",
    "porter",
    "stemmer",
    "example",
    "going",
    "alt",
    "key",
    "import",
    "porter",
    "stemmer",
    "let",
    "uh",
    "go",
    "ahead",
    "initialize",
    "stemmer",
    "type",
    "like",
    "test",
    "phrase",
    "reading",
    "books",
    "say",
    "first",
    "need",
    "tokenize",
    "try",
    "stem",
    "try",
    "stem",
    "phrase",
    "know",
    "process",
    "algorithm",
    "expects",
    "single",
    "word",
    "going",
    "tokenize",
    "say",
    "words",
    "equal",
    "word",
    "tokenize",
    "phrase",
    "stemmed",
    "words",
    "gon",
    "na",
    "loop",
    "word",
    "words",
    "going",
    "stem",
    "word",
    "word",
    "gon",
    "na",
    "append",
    "stemmed",
    "words",
    "okay",
    "join",
    "spaces",
    "stemmed",
    "words",
    "final",
    "thing",
    "started",
    "reading",
    "books",
    "stemmer",
    "got",
    "us",
    "reading",
    "book",
    "immediately",
    "seen",
    "helpful",
    "like",
    "bag",
    "words",
    "model",
    "built",
    "earlier",
    "many",
    "training",
    "examples",
    "stem",
    "training",
    "examples",
    "stemmed",
    "incoming",
    "phrase",
    "trained",
    "would",
    "probably",
    "help",
    "improve",
    "accuracy",
    "couple",
    "examples",
    "see",
    "said",
    "stories",
    "one",
    "little",
    "bit",
    "weird",
    "see",
    "stems",
    "story",
    "guaranteed",
    "word",
    "sometimes",
    "get",
    "collisions",
    "two",
    "words",
    "necessarily",
    "similar",
    "drawbacks",
    "nice",
    "little",
    "quick",
    "trick",
    "think",
    "want",
    "check",
    "something",
    "whole",
    "tokenization",
    "one",
    "thing",
    "note",
    "also",
    "might",
    "want",
    "like",
    "strip",
    "punctuation",
    "handle",
    "little",
    "bit",
    "separately",
    "trying",
    "return",
    "phrase",
    "makes",
    "sense",
    "see",
    "right",
    "uh",
    "least",
    "way",
    "define",
    "little",
    "bit",
    "tricky",
    "tokenize",
    "print",
    "words",
    "treats",
    "punctuation",
    "uh",
    "word",
    "right",
    "moving",
    "lemmatization",
    "um",
    "let",
    "import",
    "word",
    "net",
    "limitizer",
    "using",
    "corpus",
    "called",
    "word",
    "net",
    "help",
    "um",
    "reduce",
    "words",
    "simple",
    "form",
    "right",
    "um",
    "pretty",
    "similar",
    "last",
    "one",
    "could",
    "lemmatizer",
    "equals",
    "wordnet",
    "lemmatizer",
    "thing",
    "example",
    "need",
    "tokenize",
    "whatever",
    "phrase",
    "use",
    "oh",
    "god",
    "mean",
    "copy",
    "cell",
    "delete",
    "wanted",
    "copy",
    "part",
    "right",
    "reading",
    "books",
    "let",
    "see",
    "happens",
    "um",
    "copy",
    "cell",
    "piece",
    "lemmatized",
    "ties",
    "words",
    "uh",
    "dot",
    "pretty",
    "straightforward",
    "instead",
    "stem",
    "lemma",
    "ties",
    "2ms",
    "word",
    "let",
    "see",
    "happens",
    "print",
    "lemmatized",
    "guess",
    "join",
    "dot",
    "join",
    "limitized",
    "words",
    "right",
    "run",
    "gon",
    "na",
    "happen",
    "reading",
    "book",
    "okay",
    "hmm",
    "little",
    "bit",
    "different",
    "example",
    "turned",
    "read",
    "book",
    "one",
    "thing",
    "little",
    "bit",
    "trickier",
    "using",
    "lemmatizer",
    "nltk",
    "expects",
    "words",
    "expects",
    "part",
    "speech",
    "default",
    "says",
    "token",
    "noun",
    "default",
    "converted",
    "books",
    "book",
    "reading",
    "read",
    "instead",
    "said",
    "verbs",
    "see",
    "read",
    "book",
    "guess",
    "interesting",
    "uh",
    "still",
    "made",
    "books",
    "book",
    "um",
    "books",
    "book",
    "yeah",
    "guess",
    "like",
    "think",
    "books",
    "verb",
    "like",
    "books",
    "would",
    "book",
    "makes",
    "sense",
    "uh",
    "guess",
    "verb",
    "books",
    "yeah",
    "one",
    "caveat",
    "trying",
    "effectively",
    "utilize",
    "sometimes",
    "sort",
    "part",
    "speech",
    "tagging",
    "believe",
    "get",
    "tutorial",
    "um",
    "maybe",
    "utilize",
    "part",
    "speech",
    "tag",
    "like",
    "truly",
    "reduce",
    "words",
    "might",
    "also",
    "helpful",
    "reduce",
    "noun",
    "phrases",
    "verbs",
    "um",
    "lemmatizing",
    "move",
    "uh",
    "next",
    "still",
    "looking",
    "nltk",
    "going",
    "quickly",
    "go",
    "stop",
    "words",
    "basically",
    "stop",
    "words",
    "set",
    "english",
    "words",
    "kind",
    "common",
    "sometimes",
    "might",
    "want",
    "strip",
    "phrases",
    "add",
    "much",
    "meaning",
    "sentences",
    "couple",
    "examples",
    "stop",
    "words",
    "might",
    "types",
    "things",
    "easily",
    "nltk",
    "go",
    "okay",
    "going",
    "continue",
    "using",
    "nltk",
    "already",
    "imported",
    "point",
    "going",
    "actually",
    "maybe",
    "help",
    "helpful",
    "copy",
    "case",
    "people",
    "run",
    "independently",
    "uh",
    "paste",
    "also",
    "gon",
    "na",
    "copy",
    "actual",
    "stop",
    "words",
    "gon",
    "na",
    "import",
    "stop",
    "words",
    "go",
    "stop",
    "words",
    "gon",
    "na",
    "um",
    "want",
    "stop",
    "words",
    "otherwise",
    "overwrite",
    "import",
    "stop",
    "words",
    "say",
    "stop",
    "words",
    "equals",
    "stop",
    "words",
    "dot",
    "words",
    "passion",
    "want",
    "english",
    "believe",
    "actually",
    "maybe",
    "would",
    "work",
    "without",
    "english",
    "let",
    "uh",
    "print",
    "stop",
    "words",
    "think",
    "oh",
    "guess",
    "guess",
    "specify",
    "english",
    "know",
    "means",
    "uh",
    "cool",
    "curious",
    "many",
    "length",
    "stop",
    "words",
    "uh",
    "179",
    "179",
    "words",
    "might",
    "want",
    "extract",
    "gives",
    "us",
    "nice",
    "easy",
    "interface",
    "could",
    "could",
    "phrase",
    "like",
    "example",
    "sentence",
    "removing",
    "stop",
    "words",
    "say",
    "demonstrating",
    "removal",
    "stop",
    "words",
    "phrase",
    "begin",
    "tokenize",
    "like",
    "words",
    "equal",
    "word",
    "tokenize",
    "phrase",
    "forward",
    "words",
    "well",
    "guess",
    "stripped",
    "phrase",
    "call",
    "equals",
    "um",
    "word",
    "stop",
    "words",
    "append",
    "stripped",
    "phrase",
    "probably",
    "list",
    "comprehension",
    "whatever",
    "prefer",
    "uh",
    "word",
    "dot",
    "join",
    "words",
    "could",
    "keep",
    "list",
    "guess",
    "might",
    "fully",
    "make",
    "sense",
    "strip",
    "phrase",
    "uh",
    "join",
    "anyway",
    "uh",
    "join",
    "stripped",
    "phrase",
    "okay",
    "example",
    "sentence",
    "demonstrating",
    "removal",
    "stock",
    "words",
    "removed",
    "stop",
    "words",
    "could",
    "help",
    "us",
    "going",
    "back",
    "word",
    "vector",
    "model",
    "um",
    "maybe",
    "said",
    "something",
    "like",
    "went",
    "bank",
    "wrote",
    "check",
    "know",
    "might",
    "get",
    "bogged",
    "stop",
    "words",
    "actual",
    "meaning",
    "average",
    "together",
    "word",
    "vectors",
    "removing",
    "stop",
    "words",
    "might",
    "give",
    "us",
    "little",
    "bit",
    "precise",
    "uh",
    "capturing",
    "meaning",
    "something",
    "like",
    "one",
    "example",
    "examples",
    "uh",
    "removing",
    "stop",
    "words",
    "helpful",
    "next",
    "going",
    "kind",
    "rapid",
    "fire",
    "round",
    "rapid",
    "fire",
    "round",
    "going",
    "quickly",
    "go",
    "another",
    "library",
    "called",
    "text",
    "blob",
    "allows",
    "access",
    "sorts",
    "different",
    "uh",
    "nice",
    "little",
    "things",
    "uh",
    "language",
    "quickly",
    "builds",
    "nltk",
    "library",
    "gives",
    "provides",
    "like",
    "really",
    "nice",
    "interface",
    "lot",
    "different",
    "things",
    "going",
    "say",
    "various",
    "techniques",
    "look",
    "spell",
    "correction",
    "uh",
    "sentiment",
    "let",
    "say",
    "part",
    "speech",
    "tagging",
    "right",
    "wanting",
    "use",
    "text",
    "blob",
    "library",
    "recommend",
    "actually",
    "looking",
    "reference",
    "make",
    "sure",
    "link",
    "mentioned",
    "would",
    "github",
    "straightforward",
    "one",
    "pager",
    "sorts",
    "nice",
    "things",
    "text",
    "blob",
    "import",
    "text",
    "blog",
    "first",
    "line",
    "see",
    "literally",
    "taken",
    "phrase",
    "right",
    "b",
    "dot",
    "tags",
    "already",
    "done",
    "part",
    "speech",
    "tagging",
    "like",
    "dot",
    "noun",
    "phrases",
    "dot",
    "words",
    "dot",
    "sentiment",
    "uh",
    "stuff",
    "like",
    "one",
    "line",
    "like",
    "single",
    "dot",
    "something",
    "first",
    "one",
    "wanted",
    "look",
    "spell",
    "correct",
    "correct",
    "go",
    "let",
    "go",
    "code",
    "text",
    "blob",
    "oh",
    "shoot",
    "text",
    "blob",
    "import",
    "text",
    "blob",
    "cool",
    "okay",
    "whenever",
    "anything",
    "textblob",
    "uh",
    "going",
    "want",
    "start",
    "basically",
    "making",
    "phrase",
    "text",
    "blob",
    "object",
    "going",
    "go",
    "phrase",
    "say",
    "whatever",
    "equals",
    "example",
    "going",
    "want",
    "um",
    "could",
    "even",
    "surround",
    "say",
    "tb",
    "phrase",
    "text",
    "blob",
    "phrase",
    "converting",
    "text",
    "blob",
    "phrase",
    "okay",
    "tv",
    "phrase",
    "already",
    "sorts",
    "nice",
    "things",
    "first",
    "thing",
    "mentioned",
    "spell",
    "correct",
    "tb",
    "phrase",
    "dot",
    "correct",
    "spell",
    "correct",
    "example",
    "said",
    "example",
    "use",
    "two",
    "e",
    "see",
    "still",
    "example",
    "use",
    "like",
    "two",
    "eyes",
    "probably",
    "still",
    "think",
    "look",
    "correcting",
    "quickly",
    "mentioned",
    "original",
    "goal",
    "tutorial",
    "know",
    "taking",
    "tweets",
    "processing",
    "imagine",
    "lot",
    "tweets",
    "misspelled",
    "able",
    "uh",
    "know",
    "could",
    "easily",
    "make",
    "two",
    "lines",
    "code",
    "uh",
    "actually",
    "even",
    "single",
    "line",
    "code",
    "really",
    "wanted",
    "get",
    "fancy",
    "spell",
    "correcting",
    "phrase",
    "pretty",
    "pretty",
    "helpful",
    "else",
    "text",
    "blob",
    "well",
    "mentioned",
    "could",
    "part",
    "speech",
    "tagging",
    "going",
    "go",
    "back",
    "reading",
    "examples",
    "read",
    "book",
    "obviously",
    "spelling",
    "errors",
    "one",
    "thing",
    "could",
    "get",
    "parts",
    "speech",
    "dot",
    "tags",
    "ah",
    "okay",
    "say",
    "uh",
    "looks",
    "like",
    "missing",
    "required",
    "data",
    "feature",
    "okay",
    "going",
    "real",
    "quick",
    "add",
    "google",
    "cloud",
    "file",
    "like",
    "downloaded",
    "stuff",
    "earlier",
    "run",
    "cool",
    "done",
    "able",
    "part",
    "speech",
    "tagging",
    "list",
    "object",
    "callable",
    "might",
    "dot",
    "tags",
    "might",
    "already",
    "baked",
    "look",
    "uh",
    "noun",
    "read",
    "verb",
    "whatever",
    "dt",
    "stands",
    "book",
    "another",
    "noun",
    "um",
    "look",
    "like",
    "part",
    "speech",
    "tagging",
    "probably",
    "find",
    "information",
    "different",
    "variants",
    "link",
    "github",
    "found",
    "page",
    "helpful",
    "resource",
    "learn",
    "different",
    "things",
    "see",
    "see",
    "okay",
    "like",
    "dt",
    "things",
    "actually",
    "mean",
    "um",
    "adver",
    "like",
    "kind",
    "look",
    "help",
    "us",
    "find",
    "determiner",
    "dt",
    "uh",
    "known",
    "singular",
    "known",
    "plural",
    "yeah",
    "sorts",
    "uh",
    "useful",
    "information",
    "right",
    "link",
    "want",
    "know",
    "tags",
    "another",
    "cool",
    "thing",
    "dot",
    "sentiment",
    "um",
    "might",
    "sentiment",
    "like",
    "let",
    "see",
    "go",
    "back",
    "reference",
    "yeah",
    "sentiment",
    "okay",
    "um",
    "book",
    "great",
    "polarity",
    "high",
    "numbers",
    "going",
    "mean",
    "positive",
    "say",
    "book",
    "sucked",
    "bad",
    "right",
    "see",
    "negative",
    "sentiment",
    "know",
    "negative",
    "value",
    "polarity",
    "um",
    "book",
    "another",
    "example",
    "book",
    "horrible",
    "another",
    "negative",
    "example",
    "um",
    "look",
    "details",
    "sentiment",
    "see",
    "yeah",
    "api",
    "reference",
    "text",
    "blog",
    "sorts",
    "really",
    "accessible",
    "easy",
    "things",
    "another",
    "great",
    "resource",
    "utilize",
    "want",
    "make",
    "point",
    "one",
    "main",
    "goals",
    "tutorial",
    "show",
    "different",
    "things",
    "use",
    "kind",
    "build",
    "upon",
    "knowledge",
    "apply",
    "see",
    "fit",
    "tasks",
    "working",
    "right",
    "moving",
    "towards",
    "kind",
    "state",
    "art",
    "models",
    "natural",
    "language",
    "processing",
    "wanted",
    "start",
    "recurrent",
    "neural",
    "networks",
    "uh",
    "remember",
    "back",
    "word",
    "vectors",
    "one",
    "issues",
    "kind",
    "set",
    "stone",
    "trained",
    "uh",
    "word",
    "vectors",
    "uh",
    "matter",
    "way",
    "utilize",
    "word",
    "inside",
    "like",
    "bigger",
    "phrase",
    "word",
    "vector",
    "trained",
    "kind",
    "context",
    "dependent",
    "word",
    "vectors",
    "using",
    "preset",
    "trained",
    "preset",
    "set",
    "word",
    "vectors",
    "box",
    "immediately",
    "recurrent",
    "neural",
    "networks",
    "help",
    "us",
    "kind",
    "solve",
    "problem",
    "diagram",
    "recurrent",
    "neural",
    "network",
    "basically",
    "recurrent",
    "neural",
    "network",
    "would",
    "work",
    "language",
    "would",
    "feed",
    "words",
    "network",
    "one",
    "time",
    "would",
    "music",
    "something",
    "like",
    "know",
    "word",
    "check",
    "come",
    "network",
    "would",
    "come",
    "uh",
    "network",
    "would",
    "process",
    "produce",
    "hidden",
    "state",
    "based",
    "also",
    "feed",
    "hidden",
    "state",
    "next",
    "know",
    "layer",
    "recurrent",
    "neural",
    "network",
    "maybe",
    "next",
    "word",
    "check",
    "feeding",
    "network",
    "basically",
    "much",
    "want",
    "always",
    "feeding",
    "output",
    "last",
    "input",
    "back",
    "network",
    "check",
    "book",
    "basically",
    "know",
    "feeding",
    "creating",
    "sort",
    "end",
    "book",
    "get",
    "sort",
    "final",
    "hidden",
    "state",
    "use",
    "hidden",
    "state",
    "nlp",
    "applications",
    "nice",
    "another",
    "phrase",
    "write",
    "check",
    "well",
    "way",
    "check",
    "used",
    "start",
    "sentence",
    "words",
    "around",
    "going",
    "get",
    "different",
    "kind",
    "embedding",
    "check",
    "says",
    "write",
    "check",
    "based",
    "context",
    "write",
    "know",
    "embedding",
    "final",
    "hidden",
    "statement",
    "related",
    "banking",
    "versus",
    "check",
    "book",
    "would",
    "kind",
    "uh",
    "related",
    "hidden",
    "state",
    "represents",
    "check",
    "check",
    "uh",
    "like",
    "little",
    "toy",
    "example",
    "uh",
    "ultimately",
    "like",
    "one",
    "goals",
    "something",
    "like",
    "recurrent",
    "neural",
    "network",
    "uh",
    "current",
    "neural",
    "network",
    "know",
    "kind",
    "build",
    "sorts",
    "language",
    "models",
    "drawbacks",
    "ultimately",
    "got",
    "replaced",
    "like",
    "openai",
    "gpt2",
    "model",
    "introduced",
    "start",
    "video",
    "types",
    "models",
    "drawbacks",
    "well",
    "first",
    "one",
    "would",
    "say",
    "long",
    "dependencies",
    "always",
    "work",
    "well",
    "recurrent",
    "node",
    "network",
    "imagine",
    "sentence",
    "like",
    "need",
    "go",
    "bank",
    "today",
    "make",
    "deposit",
    "hope",
    "closed",
    "uh",
    "feeding",
    "would",
    "feeding",
    "words",
    "network",
    "like",
    "always",
    "hidden",
    "state",
    "would",
    "influence",
    "words",
    "time",
    "get",
    "closed",
    "word",
    "like",
    "bank",
    "pretty",
    "far",
    "away",
    "final",
    "output",
    "embedding",
    "hidden",
    "state",
    "might",
    "clear",
    "network",
    "thing",
    "talking",
    "closed",
    "bank",
    "far",
    "away",
    "sentence",
    "one",
    "drawback",
    "recurrent",
    "neural",
    "networks",
    "another",
    "drawback",
    "performance",
    "side",
    "things",
    "feeding",
    "one",
    "token",
    "time",
    "sequential",
    "nature",
    "makes",
    "tough",
    "paralyze",
    "training",
    "utilization",
    "rnn",
    "language",
    "models",
    "sometimes",
    "know",
    "ca",
    "effectively",
    "utilize",
    "modern",
    "gpus",
    "right",
    "leads",
    "us",
    "attention",
    "big",
    "paper",
    "came",
    "called",
    "attention",
    "need",
    "kind",
    "set",
    "whole",
    "new",
    "wave",
    "natural",
    "language",
    "processing",
    "difference",
    "attention",
    "well",
    "basically",
    "feed",
    "phrase",
    "iterate",
    "tokens",
    "phrase",
    "basically",
    "figure",
    "needs",
    "attended",
    "like",
    "walk",
    "sentence",
    "need",
    "go",
    "bank",
    "write",
    "check",
    "well",
    "know",
    "right",
    "get",
    "word",
    "get",
    "word",
    "check",
    "let",
    "say",
    "feeding",
    "fed",
    "time",
    "uh",
    "using",
    "positional",
    "encoding",
    "one",
    "basically",
    "get",
    "word",
    "like",
    "check",
    "well",
    "probably",
    "spike",
    "right",
    "bank",
    "like",
    "relevant",
    "items",
    "check",
    "uh",
    "need",
    "go",
    "go",
    "might",
    "like",
    "set",
    "looking",
    "ahead",
    "uh",
    "bank",
    "might",
    "triggered",
    "like",
    "important",
    "indicator",
    "go",
    "uh",
    "etc",
    "basically",
    "based",
    "word",
    "looking",
    "network",
    "kind",
    "learns",
    "attend",
    "words",
    "figure",
    "important",
    "given",
    "certain",
    "token",
    "looking",
    "kind",
    "bring",
    "high",
    "level",
    "network",
    "kind",
    "think",
    "learns",
    "ask",
    "questions",
    "uh",
    "phrases",
    "sees",
    "know",
    "actually",
    "asking",
    "know",
    "human",
    "questions",
    "baked",
    "attention",
    "networks",
    "basically",
    "looking",
    "token",
    "asking",
    "sorts",
    "questions",
    "token",
    "learning",
    "based",
    "token",
    "else",
    "important",
    "phrase",
    "super",
    "powerful",
    "technique",
    "also",
    "attention",
    "work",
    "longer",
    "range",
    "dependencies",
    "traditional",
    "rnn",
    "set",
    "sorts",
    "really",
    "impressive",
    "stuff",
    "nlp",
    "world",
    "sort",
    "video",
    "mentioned",
    "openai",
    "gpt2",
    "architecture",
    "um",
    "one",
    "thing",
    "note",
    "uh",
    "forward",
    "nature",
    "basically",
    "task",
    "trained",
    "language",
    "modeling",
    "give",
    "words",
    "words",
    "coming",
    "really",
    "good",
    "able",
    "auto",
    "complete",
    "like",
    "story",
    "typed",
    "phrase",
    "could",
    "utilize",
    "fact",
    "trained",
    "figuring",
    "predicting",
    "next",
    "word",
    "based",
    "already",
    "seen",
    "um",
    "let",
    "open",
    "aigp",
    "gpt",
    "elmo",
    "network",
    "also",
    "listed",
    "wanted",
    "note",
    "one",
    "know",
    "much",
    "elmo",
    "network",
    "one",
    "thing",
    "note",
    "uses",
    "lstms",
    "kind",
    "core",
    "lstm",
    "type",
    "recurrent",
    "neural",
    "network",
    "supposed",
    "handle",
    "long",
    "longer",
    "dependencies",
    "bit",
    "better",
    "still",
    "compared",
    "attention",
    "actually",
    "actually",
    "uh",
    "wrote",
    "header",
    "transformer",
    "architectures",
    "elmo",
    "technically",
    "transformer",
    "architecture",
    "another",
    "powerful",
    "language",
    "model",
    "um",
    "finally",
    "going",
    "focus",
    "right",
    "burt",
    "transformer",
    "architecture",
    "result",
    "uh",
    "really",
    "captures",
    "lot",
    "impressive",
    "things",
    "language",
    "even",
    "like",
    "open",
    "gbt",
    "model",
    "ca",
    "going",
    "look",
    "use",
    "python",
    "interact",
    "bert",
    "model",
    "real",
    "quick",
    "okay",
    "going",
    "use",
    "spacey",
    "link",
    "article",
    "github",
    "page",
    "basically",
    "spacey",
    "provides",
    "us",
    "really",
    "easy",
    "way",
    "interact",
    "transformers",
    "follow",
    "link",
    "basically",
    "gives",
    "lines",
    "need",
    "quickly",
    "get",
    "running",
    "let",
    "utilize",
    "spacey",
    "real",
    "quick",
    "um",
    "first",
    "two",
    "things",
    "going",
    "need",
    "pip",
    "install",
    "trend",
    "spacey",
    "transformers",
    "spacey",
    "download",
    "model",
    "okay",
    "ah",
    "want",
    "make",
    "title",
    "italicized",
    "title",
    "um",
    "transformer",
    "architectures",
    "say",
    "right",
    "wanted",
    "download",
    "large",
    "model",
    "also",
    "wanted",
    "pip",
    "install",
    "spacey",
    "transformers",
    "uh",
    "exclamation",
    "points",
    "mean",
    "pretty",
    "sure",
    "cell",
    "run",
    "within",
    "uh",
    "google",
    "collab",
    "accidentally",
    "download",
    "model",
    "okay",
    "downloaded",
    "took",
    "little",
    "ran",
    "successfully",
    "cool",
    "right",
    "actually",
    "downloaded",
    "go",
    "back",
    "article",
    "see",
    "actually",
    "use",
    "uh",
    "looks",
    "pretty",
    "straightforward",
    "uh",
    "using",
    "gpu",
    "could",
    "utilize",
    "lines",
    "code",
    "note",
    "details",
    "going",
    "copy",
    "real",
    "quick",
    "think",
    "need",
    "going",
    "going",
    "use",
    "right",
    "going",
    "use",
    "gpu",
    "think",
    "need",
    "ump",
    "right",
    "think",
    "need",
    "import",
    "spacey",
    "torch",
    "good",
    "loading",
    "large",
    "bert",
    "model",
    "one",
    "thing",
    "note",
    "using",
    "transformers",
    "going",
    "train",
    "using",
    "like",
    "bert",
    "model",
    "going",
    "probably",
    "train",
    "scratch",
    "bert",
    "model",
    "super",
    "super",
    "massive",
    "takes",
    "multiple",
    "days",
    "like",
    "powerful",
    "tensor",
    "processing",
    "unit",
    "machines",
    "train",
    "model",
    "something",
    "going",
    "want",
    "know",
    "home",
    "luckily",
    "open",
    "source",
    "models",
    "utilize",
    "effectively",
    "one",
    "thing",
    "note",
    "even",
    "loading",
    "model",
    "already",
    "one",
    "nice",
    "features",
    "one",
    "nice",
    "features",
    "models",
    "like",
    "specific",
    "task",
    "get",
    "really",
    "impressive",
    "results",
    "lot",
    "easier",
    "step",
    "retraining",
    "scratch",
    "okay",
    "run",
    "see",
    "loads",
    "properly",
    "using",
    "google",
    "collab",
    "probably",
    "restart",
    "runtime",
    "sometimes",
    "realize",
    "model",
    "unless",
    "okay",
    "cool",
    "um",
    "like",
    "like",
    "using",
    "spacey",
    "loaded",
    "bert",
    "model",
    "nlp",
    "engine",
    "basically",
    "exact",
    "stuff",
    "word",
    "vectors",
    "section",
    "tutorial",
    "go",
    "basically",
    "think",
    "going",
    "find",
    "another",
    "set",
    "training",
    "utterances",
    "test",
    "utterances",
    "like",
    "actually",
    "going",
    "copy",
    "real",
    "quick",
    "think",
    "necessary",
    "write",
    "want",
    "change",
    "example",
    "little",
    "bit",
    "um",
    "right",
    "see",
    "power",
    "bert",
    "transformers",
    "general",
    "okay",
    "time",
    "defining",
    "another",
    "category",
    "class",
    "time",
    "instead",
    "difference",
    "books",
    "clothing",
    "two",
    "categories",
    "book",
    "related",
    "items",
    "banking",
    "related",
    "items",
    "defined",
    "seven",
    "different",
    "uh",
    "utterances",
    "associated",
    "category",
    "read",
    "real",
    "quick",
    "okay",
    "cool",
    "let",
    "build",
    "model",
    "around",
    "exact",
    "code",
    "basically",
    "used",
    "need",
    "changes",
    "model",
    "nlp",
    "engine",
    "opposed",
    "word",
    "vectors",
    "using",
    "earlier",
    "okay",
    "copying",
    "code",
    "yeah",
    "said",
    "exact",
    "code",
    "using",
    "train",
    "train",
    "x",
    "going",
    "run",
    "cell",
    "real",
    "quick",
    "want",
    "train",
    "model",
    "predict",
    "new",
    "utterances",
    "going",
    "start",
    "toy",
    "example",
    "say",
    "book",
    "okay",
    "result",
    "says",
    "books",
    "expect",
    "one",
    "thing",
    "want",
    "note",
    "though",
    "one",
    "training",
    "utterances",
    "check",
    "book",
    "example",
    "kind",
    "walking",
    "could",
    "say",
    "check",
    "book",
    "maybe",
    "writing",
    "banking",
    "related",
    "terms",
    "might",
    "say",
    "need",
    "write",
    "check",
    "going",
    "type",
    "know",
    "need",
    "write",
    "check",
    "know",
    "trading",
    "audiences",
    "check",
    "word",
    "check",
    "appears",
    "books",
    "category",
    "trading",
    "entrances",
    "three",
    "back",
    "banking",
    "need",
    "make",
    "deposit",
    "bank",
    "balance",
    "inquiry",
    "savings",
    "save",
    "money",
    "mention",
    "word",
    "check",
    "true",
    "test",
    "power",
    "bert",
    "model",
    "oh",
    "know",
    "say",
    "need",
    "write",
    "check",
    "talking",
    "banking",
    "talking",
    "word",
    "already",
    "seen",
    "category",
    "let",
    "mode",
    "truth",
    "look",
    "need",
    "write",
    "check",
    "utilizing",
    "power",
    "bert",
    "model",
    "like",
    "small",
    "number",
    "training",
    "examples",
    "see",
    "uh",
    "banking",
    "related",
    "uh",
    "crazy",
    "know",
    "maybe",
    "said",
    "check",
    "story",
    "know",
    "knows",
    "check",
    "context",
    "uh",
    "books",
    "really",
    "impressive",
    "capabilities",
    "trained",
    "big",
    "model",
    "leveraging",
    "really",
    "really",
    "powerful",
    "stuff",
    "continue",
    "playing",
    "uh",
    "one",
    "thing",
    "recommend",
    "looking",
    "kind",
    "alluded",
    "one",
    "nice",
    "things",
    "model",
    "like",
    "bert",
    "specific",
    "task",
    "code",
    "use",
    "spacey",
    "show",
    "would",
    "nlp",
    "know",
    "model",
    "specific",
    "know",
    "maybe",
    "classification",
    "task",
    "really",
    "useful",
    "next",
    "thing",
    "want",
    "mention",
    "interface",
    "spacey",
    "provides",
    "bert",
    "really",
    "nice",
    "definitely",
    "room",
    "wanted",
    "build",
    "model",
    "like",
    "bert",
    "ca",
    "really",
    "wanted",
    "dive",
    "deeper",
    "types",
    "models",
    "recommend",
    "checking",
    "hugging",
    "faces",
    "transformers",
    "github",
    "repo",
    "pretty",
    "much",
    "popular",
    "different",
    "uh",
    "models",
    "come",
    "recently",
    "within",
    "natural",
    "language",
    "processing",
    "models",
    "written",
    "pie",
    "torch",
    "look",
    "around",
    "like",
    "bert",
    "modeling",
    "bert",
    "file",
    "see",
    "pretty",
    "much",
    "entire",
    "pie",
    "torch",
    "model",
    "defined",
    "wanted",
    "build",
    "bert",
    "know",
    "fine",
    "tune",
    "specifically",
    "uh",
    "something",
    "could",
    "use",
    "hugging",
    "face",
    "library",
    "right",
    "uh",
    "going",
    "talk",
    "um",
    "transformers",
    "code",
    "would",
    "look",
    "easily",
    "interface",
    "uh",
    "one",
    "thing",
    "want",
    "note",
    "teach",
    "much",
    "actual",
    "inner",
    "workings",
    "bert",
    "tutorial",
    "kind",
    "focusing",
    "quickly",
    "getting",
    "speed",
    "using",
    "python",
    "um",
    "list",
    "github",
    "repo",
    "resources",
    "go",
    "learn",
    "types",
    "things",
    "found",
    "useful",
    "right",
    "concludes",
    "actually",
    "going",
    "tutorial",
    "leave",
    "tutorial",
    "exercise",
    "guys",
    "kind",
    "tie",
    "everything",
    "together",
    "really",
    "try",
    "build",
    "like",
    "keep",
    "building",
    "nlp",
    "skills",
    "go",
    "github",
    "page",
    "keith",
    "galley",
    "slash",
    "pycon2020",
    "basically",
    "curated",
    "data",
    "nlp",
    "exercise",
    "work",
    "data",
    "comes",
    "amazon",
    "reviews",
    "several",
    "different",
    "categories",
    "also",
    "test",
    "set",
    "amazon",
    "data",
    "several",
    "different",
    "categories",
    "categories",
    "goal",
    "task",
    "basically",
    "use",
    "training",
    "data",
    "build",
    "best",
    "classifier",
    "properly",
    "categorize",
    "reviews",
    "test",
    "set",
    "kind",
    "bigger",
    "picture",
    "context",
    "could",
    "think",
    "problem",
    "imagine",
    "charge",
    "social",
    "media",
    "like",
    "analytics",
    "large",
    "retail",
    "store",
    "getting",
    "people",
    "tweeting",
    "time",
    "different",
    "products",
    "wanted",
    "build",
    "model",
    "automatically",
    "like",
    "route",
    "tweets",
    "appropriate",
    "parties",
    "could",
    "handle",
    "questions",
    "model",
    "like",
    "would",
    "super",
    "useful",
    "training",
    "test",
    "data",
    "task",
    "also",
    "provided",
    "nlp",
    "exercise",
    "notebook",
    "kind",
    "quickly",
    "walked",
    "uh",
    "getting",
    "using",
    "data",
    "know",
    "loading",
    "training",
    "data",
    "loading",
    "test",
    "data",
    "building",
    "model",
    "training",
    "data",
    "default",
    "simplest",
    "model",
    "included",
    "bag",
    "words",
    "model",
    "going",
    "kind",
    "adding",
    "time",
    "probably",
    "see",
    "video",
    "several",
    "uh",
    "models",
    "included",
    "bag",
    "word",
    "model",
    "help",
    "get",
    "started",
    "using",
    "data",
    "basically",
    "based",
    "model",
    "uh",
    "evaluate",
    "performance",
    "test",
    "set",
    "see",
    "box",
    "bag",
    "words",
    "categorized",
    "65",
    "test",
    "reviews",
    "correctly",
    "category",
    "wanted",
    "break",
    "little",
    "bit",
    "closely",
    "calculated",
    "f1",
    "scores",
    "category",
    "see",
    "like",
    "really",
    "bad",
    "job",
    "automotive",
    "questions",
    "well",
    "beauty",
    "books",
    "kind",
    "utilize",
    "data",
    "see",
    "fit",
    "play",
    "around",
    "building",
    "different",
    "types",
    "categorizers",
    "classification",
    "models",
    "around",
    "couple",
    "recommendations",
    "think",
    "really",
    "interesting",
    "see",
    "well",
    "bert",
    "model",
    "would",
    "task",
    "also",
    "think",
    "would",
    "interesting",
    "take",
    "bag",
    "words",
    "model",
    "also",
    "leverage",
    "techniques",
    "mentioned",
    "like",
    "lemmatizing",
    "words",
    "uh",
    "stripping",
    "away",
    "stop",
    "words",
    "uh",
    "know",
    "maybe",
    "part",
    "speech",
    "tagging",
    "see",
    "leverage",
    "make",
    "important",
    "really",
    "interesting",
    "techniques",
    "try",
    "build",
    "powerful",
    "models",
    "uh",
    "else",
    "want",
    "say",
    "guess",
    "quickly",
    "actually",
    "show",
    "data",
    "looked",
    "like",
    "actually",
    "clicked",
    "one",
    "files",
    "maybe",
    "something",
    "like",
    "beauty",
    "would",
    "easier",
    "see",
    "uh",
    "also",
    "link",
    "source",
    "scraped",
    "data",
    "someone",
    "already",
    "lot",
    "grunt",
    "work",
    "want",
    "give",
    "credit",
    "included",
    "read",
    "github",
    "page",
    "yeah",
    "basically",
    "review",
    "text",
    "always",
    "talking",
    "category",
    "beauty",
    "like",
    "outstanding",
    "top",
    "organic",
    "shampoo",
    "model",
    "would",
    "expect",
    "learn",
    "shampoo",
    "one",
    "thing",
    "note",
    "perfect",
    "data",
    "get",
    "reviews",
    "like",
    "really",
    "good",
    "um",
    "thanks",
    "give",
    "us",
    "much",
    "information",
    "never",
    "going",
    "able",
    "get",
    "like",
    "100",
    "task",
    "maybe",
    "another",
    "exercise",
    "could",
    "work",
    "could",
    "strip",
    "kind",
    "types",
    "words",
    "use",
    "training",
    "information",
    "test",
    "information",
    "right",
    "going",
    "end",
    "tutorial",
    "hopefully",
    "fun",
    "uh",
    "learnings",
    "different",
    "nlp",
    "techniques",
    "python",
    "uh",
    "lot",
    "covered",
    "video",
    "hopefully",
    "goal",
    "tutorial",
    "see",
    "techniques",
    "mind",
    "could",
    "take",
    "kind",
    "build",
    "knowledge",
    "apply",
    "different",
    "areas",
    "really",
    "like",
    "seeing",
    "taking",
    "going",
    "also",
    "reminder",
    "leave",
    "uh",
    "questions",
    "anything",
    "feel",
    "free",
    "connect",
    "know",
    "linkedin",
    "instagram",
    "twitter",
    "search",
    "keith",
    "galley",
    "one",
    "word",
    "probably",
    "find",
    "also",
    "want",
    "find",
    "tutorials",
    "posted",
    "check",
    "youtube",
    "channel",
    "kgmit",
    "thank",
    "much",
    "pycon",
    "fun",
    "even",
    "person",
    "expected",
    "exciting",
    "go",
    "process",
    "uh",
    "hopefully",
    "uh",
    "person",
    "next",
    "year",
    "right",
    "take",
    "care",
    "everyone",
    "thanks",
    "watching",
    "peace",
    "music"
  ],
  "keywords": [
    "back",
    "another",
    "video",
    "gon",
    "na",
    "tutorial",
    "natural",
    "language",
    "processing",
    "python",
    "actually",
    "data",
    "lot",
    "seen",
    "like",
    "library",
    "let",
    "know",
    "try",
    "best",
    "help",
    "many",
    "couple",
    "quick",
    "get",
    "also",
    "cool",
    "great",
    "way",
    "recommend",
    "join",
    "well",
    "right",
    "think",
    "see",
    "stuff",
    "uh",
    "kind",
    "little",
    "bit",
    "hopefully",
    "time",
    "going",
    "could",
    "next",
    "thing",
    "start",
    "nlp",
    "models",
    "techniques",
    "look",
    "basically",
    "take",
    "text",
    "numerical",
    "vector",
    "probably",
    "easiest",
    "called",
    "bag",
    "words",
    "first",
    "model",
    "code",
    "related",
    "different",
    "word",
    "approach",
    "use",
    "vectors",
    "go",
    "sorts",
    "regex",
    "stemming",
    "spell",
    "part",
    "speech",
    "tagging",
    "mentioned",
    "transformer",
    "big",
    "one",
    "bert",
    "around",
    "build",
    "even",
    "powerful",
    "sometimes",
    "give",
    "mean",
    "whenever",
    "sort",
    "task",
    "sentences",
    "representation",
    "imagine",
    "four",
    "utterances",
    "love",
    "book",
    "fit",
    "shoes",
    "two",
    "types",
    "categories",
    "books",
    "maybe",
    "clothing",
    "trying",
    "says",
    "individual",
    "would",
    "us",
    "based",
    "sentence",
    "building",
    "things",
    "make",
    "dot",
    "import",
    "count",
    "vectorizer",
    "phrases",
    "say",
    "find",
    "something",
    "google",
    "able",
    "examples",
    "using",
    "utilize",
    "might",
    "reference",
    "um",
    "real",
    "training",
    "call",
    "train",
    "last",
    "want",
    "transform",
    "always",
    "looks",
    "equals",
    "equal",
    "x",
    "dictionary",
    "pass",
    "print",
    "defined",
    "need",
    "run",
    "cell",
    "collab",
    "okay",
    "good",
    "oh",
    "guess",
    "helpful",
    "happens",
    "note",
    "away",
    "type",
    "wanted",
    "example",
    "string",
    "sure",
    "category",
    "classifier",
    "svm",
    "define",
    "already",
    "instead",
    "said",
    "nice",
    "trained",
    "yeah",
    "process",
    "add",
    "information",
    "case",
    "sentiment",
    "important",
    "context",
    "three",
    "set",
    "final",
    "story",
    "similar",
    "pretty",
    "meaning",
    "space",
    "ultimately",
    "works",
    "super",
    "work",
    "read",
    "characters",
    "easily",
    "reading",
    "five",
    "token",
    "neural",
    "network",
    "together",
    "spacey",
    "download",
    "actual",
    "whatever",
    "embeddings",
    "looking",
    "docs",
    "phrase",
    "list",
    "embedding",
    "end",
    "getting",
    "much",
    "test",
    "really",
    "exercise",
    "check",
    "bank",
    "number",
    "write",
    "followed",
    "character",
    "match",
    "b",
    "white",
    "link",
    "github",
    "page",
    "useful",
    "line",
    "c",
    "matches",
    "slash",
    "still",
    "quickly",
    "regular",
    "expression",
    "nltk",
    "stop",
    "stemmer",
    "tokenize",
    "copy",
    "blob",
    "state",
    "recurrent",
    "hidden",
    "feeding",
    "banking",
    "attention",
    "questions",
    "transformers"
  ]
}