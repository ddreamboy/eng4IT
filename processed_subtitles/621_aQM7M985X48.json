{
  "text": "let's talk about transfer learning in\nmachine learning\ntransfer learning is when you take what\nanother model\nhas learned about a similar domain and\nused that to train your new model from a\npredefined starting point\n so let's take an example that's not in\nthe context of machine learning okay\nlet's say you have two friends and\nyou're trying to teach those friends how\nto skateboard\nwe'll say one friend has never\nskateboarded before never snowboarded\nnever done any sort of similar activity\nwhereas another friend already knows how\nto surf\nokay now which of these friends do you\nthink is going to pick up skateboarding\nfaster\nprobably the one that has some\nexperience in a related domain\nthe one that surfed before in a sense\nthis is what transfer learning is in\nmachine learning\nit says let's take what another model\nhas already learned that's similar to\nwhat i'm trying to teach my model that's\nnew\nand start from there when we do our\ntraining i'm joseph from roboflow and\ntoday i'm going to take you through\nhow you can use transfer learning in\ncomputer vision problems\nso that your object detection models can\nimprove\nfaster so the example that i'm\ngoing to be working with today\nis a chess piece data set so this\ndata set is available in its full\nform on public.roboflow.com i'll put the\nlink in the description\nwhat the dataset is is it's actually a\nseries of about\nuh well this one has 189 images and\n1762 annotations across 12 classes\neach of the chess pieces and the goal of\nwhat we're trying to do\nis teach our model to do object\ndetection of each of these individual\nchess pieces\n okay seems simple enough so what\nwe may want to do is we can start\ntraining from totally from scratch\nright that means that we take a model\nwhere\nthe model knows nothing the weights are\nrandomly initiated\nand then those weights learn over time\nand try to get to the closest point\npossible\nwe could also start training from a\ndifferent\ncheckpoint now that other checkpoint we\nwould hope\nwould be something that's somewhat\nsimilar remember my example with our\nfriends\nif we have one friend that we're trying\nto teach them both skateboarding\nand we have a friend that has learned\nhow to snowboard maybe having that\nfriend\nstart would do much better in the\nlessons whereas the friend that's never\ndone\nanything in the related domain will take\nlonger to learn\nthat's kind of what we want to do here\n so i've actually gone ahead already\nand with my data set of 189 images\ni've generated two versions now these\nversions don't have any augmentation\nthe only thing that's been done is their\nimages have been resized to 416 by 416.\n so that's a 416x416 image and same\nwith this one\n okay now when we kick off training\nwe're going to use roboflow specifically\nroboflow train\nto kick off a training job now we're\ngoing to do two different training jobs\nokay to demonstrate the power\nof transfer learning in roboflow when\nyou click\nuse roboflowtrain you're basically\nsaying please\nroboflow train a model for me without me\nneeding to write any code\nand give me the results of that model in\nthe form of\nan api or a model i can use for future\nlabeling\nor just to benchmark my existing\nperformance now you'll notice that i can\ndo two things\ni can train from scratch or i can start\nfrom a checkpoint\ntrain from scratch means don't use any\npre-existing weights that is the rates\nwill be rent the weights will be\nrandomly initiated and the model will\njust learn the best that it can\nalternatively i can start from a\ncheckpoint now\nlogically this checkpoint that i start\nfrom should be\nsomething that's a similar domain now\nin roboflow those things that i start\nfrom could be\ndata sets that i already have on my\naccount so i actually already have\na different trained model on my account\nof chess pieces\nfrom this example here or i could start\nfrom\na public model and so the public models\nare models that are\navailable on roboflow so for example\nthe common objects in context data set\nthe\ncoco data set is where i could start my\ntraining from\nnow i'm going to do two tests here i'm\ngoing to do one model\nwhere i kick it off from the coco data\nset and then i'm gonna do another model\nwhere i kick it off\ncompletely from scratch okay now if\nyou're unfamiliar with the coco data set\nit's also available on roboflow public\nand basically what it is is a data set\nof hundreds of thousands of images\nof objects in common contexts things\nlike\nforks and spoons and persons and cars\nand dogs and microwaves\nand things like that and so what i'm\nthinking here is\nif i start my training on the chess\npiece data set i'll try\none from the coco data set to see if the\ncommon objects in context data set\napproximates pretty well\n and then i'll do another one completely\nfrom scratch so i'm going to start\ntraining from the coco data set\nand that goes ahead and that training\njob starts and then on the other one on\nv2 i'm going to start training\ncompletely from scratch\n so those two have kicked off now\nwhile those start to train let's talk\nthrough the advantages\nof transfer learning there's a few\nthings that transfer learning does for\nus\none it may and likely will\nimprove the accuracy of our model\nbecause think about it if we're saying\nthat our model needs to learn\nbut it can start from what another model\nhas learned in a similar space\nthen that model may optimize to learn\nthe new domain\neven better so in other words the first\nadvantage is\nit may make for more accurate models\nthe second advantage is it actually will\nmaybe\nlikely will make our training time be\nfaster if we have a model that's\nstarting from a predefined\nsp checkpoint of a similar domain then\nwe would think that\nthe new model can learn you know the\nrest of the way\nwhereas the model that starts completely\nfrom scratch might take a little bit\nlonger to optimize its loss function\nand understand our new domain more\nsuccessfully\nagain think back to our example of\nteaching your friends a new skill\nif a friend has experience in a similar\ndomain\nyou would expect they can pick up the\nnew skill faster than if they didn't\nnow transfer learning isn't totally a\nfree lunch\nmind you um and we'll talk more about\nthat\nafter we let these models train so i'm\ngoing to let these models kick off and\ntrain\nnow uh while those models train there is\none other thing that i do want to walk\nthrough\n so we described transfer learning as\nbeing advantageous\nif we were starting from you know maybe\na related domain\nbut we could even do transfer learning\non the same domain\nso what do i mean by that let's say that\nwith my chess pieces data set here\nyou know right now i only have 189\nimages\nnow let's say that in theory i had gone\nout and i'd collected and i\nlabeled additional images i actually\nhave gone ahead and done that so i'm\ngoing to go here and i'm going to click\nadd more images\n and then i have actually gone and i've\ncreated\ni'm going to select my folder and on my\ndesktop here i have an additional 100\nimages and annotations\n so i'm going to select that folder and\ni'm going to click upload\nyep and all of those images and their\nannotations\nare going to be processed here for my\ndata set\nand as i said i have an additional 100\nimages\nnow why is this advantageous now when i\ntrain again in the future i don't want\nto start completely from scratch\nright if my model has learned what 189\nimages\nand 1700 annotations look like\nthen i want to start from that point in\ntime to add an additional 100 images\nso these additional 100 images i'm going\nto add them to my data set\nand i'm going to add them all to my\ntraining set in fact\nso they'll all be added to my training\nset and those are all going to be\nuploaded now in theory\nright if i did this out in the wild and\ni've collected\nnew images of the same domain when i\nstart training again\n i don't want to throw out everything\nthat my model learned\nbefore i collected new images i want to\nstart\nfrom the same point where i last left\noff so with all these new images added\nto our data set\nwe can go ahead and create a new version\nand then\nwe can kick off training again but\nbut we'll kick off training from a\ncheckpoint and that checkpoint will be\nthe ones that we just started\nso i'm going to wait for these models to\nfinish training\nand for this upload to finish and then\ni'll come right back and i'll report the\nresults\nof accuracy and the time that each of\nthem took to train\nso hold that thought i'll be right back\nand we'll do a little movie magic\n so our 100 new images finished\nuploading\nand both of our models finished training\nwhich means we can compare the model\nresults v1 versus v2\n and then we can also look and see if we\nstart\nwith our training checkpoint for the\nnext 100 images\nhow that's going to change our model\nperformance so first and foremost\nremember\nmodel v1 the dataset v1 was the\none that we did the cocoa weights\npre-training\ncheckpoint on this is the one where we\ndid transfer learning\nv2 was the model where we started from\nscratch\nwhere the weight started in a random\nposition and learned only on this data\nset\n so if we look at the v1 model results we\nsee that\nit does quite well the map here is 98.5\nthe precision is 73.3 and the recall is\n99.3\nnow i'll put a link in the description\nfor explaining what each of these\nmetrics are in greater depth if that's\nof interest\ndon't forget to like and subscribe to\nour channel as we post additional\nupdates like that\nbut in general here we're going to be\nfocusing on map so 98.5\nfor the transfer learning model now\nbefore i\ntune over to the v2 let me also tell you\nsomething about the transfer learning\nmodel\nwe kicked off both of these training\njobs\nat the same time but the transfer\nlearning model\nfinished 49 minutes before the model\nthat started\ntraining from scratch so the v1 model\ntook about an hour to train\nand the v2 model took about two hours to\ntrain from scratch\n so we already know the transfer\nlearning model was a little bit faster\nnow its accuracy is quite good how does\nthat compare the v2 model we see the map\nis\n97.9 precision 53.5 and recall 99.6\nso the mean average precision of the\ntransfer learning model is ever so\nslightly higher at 98.5\nthe precision is much higher at 73.3 and\nthe recall is actually slightly lower\n0.03 lower um at 99.3\nthat could be attributed to noise or it\ncould be actually that it learned\nsomething very specific about our chess\npieces\num but in general here we see that\ntransfer learning has done a good job\nfor us our model trained faster\n and it's ever so slightly more accurate\nnow i told you that i\nadded a hundred new images to this data\nset you saw me upload them so now we\nhave 289 images\nand that translates to having 289 images\nin 2870 annotations so over a thousand\nmore annotations\nnow i'm going to actually go back to our\ndata set here\nand i'm going to generate a version i'm\njust going to call this version v3\nand this dataset version is going to\nhave the same pre-processing step so\nit's going to be a 416x416 image that we\npassed to our model\nand no augmentation so it's just going\nto be the raw images\nthat we're going to pass to our model\nhere\n and so this v3 one we're also going to\nkick off a training job\nso we'll click use roboflow train and\nremember we can train from scratch or we\ncan start from a checkpoint except\nthis time the checkpoint we can start\nfrom is we can grab\na checkpoint from the chess pieces data\nset\n and we can actually grab a model from\none of\nour existing ones on our account\n so i already have the as i mentioned\nthat chess pieces data set on my account\n so i'm going to start training from that\nexact model\n and i'm going to kick off training and\nwhile that kicks off\nwe're going to go ahead here and use a\nlittle bit of movie magic and wait for\nthat to finish\nwhile training is in progress and we're\ngoing to come back and update you with\nhow long that took in comparison and\nwe'll see the metrics and results to see\nif there's any\nnotable difference okay so see you in a\nbit\nthe model finished training so\nlet's recap what we did we first\nuploaded 189 images\nwe did training from a checkpoint on the\ncoco dataset\nwe also did training from scratch then\nwe added 100 more images to our data set\nand we started training from a\ncheckpoint on that same model\nso our model enough to start from\nscratch it could remember what it\nlearned from those first 189 images\nand then apply that forward on our next\n100 images\nso that was v3 i need a little drumroll\nthe results\n98.9 percent map\nso remember v1 saw 98.5\nv2 so 97.9 that was training from\nscratch\nand v3 saw a 98.9 percent map\nnow notice the precision is slightly\nlower\nand the recall is a little bit higher i\nmean those numbers kind of depend on the\nthresholds that you use i'll link in the\ndescription\nmore about those metrics but all in all\nwe've done it\nwe've successfully used transfer\nlearning in computer vision\nplease like and subscribe to our youtube\nchannel\nto get up-to-date updates about videos\ntutorials blogs and new features in\nroboflow\nand thanks so much for tuning in happy\ntransfer learning\n\n",
  "sentences": [
    "let's talk about transfer learning in\nmachine learning\ntransfer learning is when you take what\nanother model\nhas learned about a similar domain and\nused that to train your new model from a\npredefined starting point\n so let's take an example that's not in\nthe context of machine learning okay\nlet's say you have two friends and\nyou're trying to teach those friends how\nto skateboard\nwe'll say one friend has never\nskateboarded before never snowboarded\nnever done any sort of similar activity\nwhereas another friend already knows how\nto surf\nokay now which of these friends do you\nthink is going to pick up skateboarding\nfaster\nprobably the one that has some\nexperience in a related domain\nthe one that surfed before in a sense\nthis is what transfer learning is in\nmachine learning\nit says let's take what another model\nhas already learned that's similar to\nwhat i'm trying to teach my model that's\nnew\nand start from there when we do our\ntraining i'm joseph from roboflow and\ntoday i'm going to take you through\nhow you can use transfer learning in\ncomputer vision problems\nso that your object detection models can\nimprove\nfaster so the example that i'm\ngoing to be working with today\nis a chess piece data set so this\ndata set is available in its full\nform on public.roboflow.com i'll put the\nlink in the description\nwhat the dataset is is it's actually a\nseries of about\nuh well this one has 189 images and\n1762 annotations across 12 classes\neach of the chess pieces and the goal of\nwhat we're trying to do\nis teach our model to do object\ndetection of each of these individual\nchess pieces\n okay seems simple enough so what\nwe may want to do is we can start\ntraining from totally from scratch\nright that means that we take a model\nwhere\nthe model knows nothing the weights are\nrandomly initiated\nand then those weights learn over time\nand try to get to the closest point\npossible\nwe could also start training from a\ndifferent\ncheckpoint now that other checkpoint we\nwould hope\nwould be something that's somewhat\nsimilar remember my example with our\nfriends\nif we have one friend that we're trying\nto teach them both skateboarding\nand we have a friend that has learned\nhow to snowboard maybe having that\nfriend\nstart would do much better in the\nlessons whereas the friend that's never\ndone\nanything in the related domain will take\nlonger to learn\nthat's kind of what we want to do here\n so i've actually gone ahead already\nand with my data set of 189 images\ni've generated two versions now these\nversions don't have any augmentation\nthe only thing that's been done is their\nimages have been resized to 416 by 416.\n so that's a 416x416 image and same\nwith this one\n okay now when we kick off training\nwe're going to use roboflow specifically\nroboflow train\nto kick off a training job now we're\ngoing to do two different training jobs\nokay to demonstrate the power\nof transfer learning in roboflow when\nyou click\nuse roboflowtrain you're basically\nsaying please\nroboflow train a model for me without me\nneeding to write any code\nand give me the results of that model in\nthe form of\nan api or a model i can use for future\nlabeling\nor just to benchmark my existing\nperformance now you'll notice that i can\ndo two things\ni can train from scratch or i can start\nfrom a checkpoint\ntrain from scratch means don't use any\npre-existing weights that is the rates\nwill be rent the weights will be\nrandomly initiated and the model will\njust learn the best that it can\nalternatively i can start from a\ncheckpoint now\nlogically this checkpoint that i start\nfrom should be\nsomething that's a similar domain now\nin roboflow those things that i start\nfrom could be\ndata sets that i already have on my\naccount so i actually already have\na different trained model on my account\nof chess pieces\nfrom this example here or i could start\nfrom\na public model and so the public models\nare models that are\navailable on roboflow so for example\nthe common objects in context data set\nthe\ncoco data set is where i could start my\ntraining from\nnow i'm going to do two tests here i'm\ngoing to do one model\nwhere i kick it off from the coco data\nset and then i'm gonna do another model\nwhere i kick it off\ncompletely from scratch okay now if\nyou're unfamiliar with the coco data set\nit's also available on roboflow public\nand basically what it is is a data set\nof hundreds of thousands of images\nof objects in common contexts things\nlike\nforks and spoons and persons and cars\nand dogs and microwaves\nand things like that and so what i'm\nthinking here is\nif i start my training on the chess\npiece data set i'll try\none from the coco data set to see if the\ncommon objects in context data set\napproximates pretty well\n and then i'll do another one completely\nfrom scratch so i'm going to start\ntraining from the coco data set\nand that goes ahead and that training\njob starts and then on the other one on\nv2 i'm going to start training\ncompletely from scratch\n so those two have kicked off now\nwhile those start to train let's talk\nthrough the advantages\nof transfer learning there's a few\nthings that transfer learning does for\nus\none it may and likely will\nimprove the accuracy of our model\nbecause think about it if we're saying\nthat our model needs to learn\nbut it can start from what another model\nhas learned in a similar space\nthen that model may optimize to learn\nthe new domain\neven better so in other words the first\nadvantage is\nit may make for more accurate models\nthe second advantage is it actually will\nmaybe\nlikely will make our training time be\nfaster if we have a model that's\nstarting from a predefined\nsp checkpoint of a similar domain then\nwe would think that\nthe new model can learn you know the\nrest of the way\nwhereas the model that starts completely\nfrom scratch might take a little bit\nlonger to optimize its loss function\nand understand our new domain more\nsuccessfully\nagain think back to our example of\nteaching your friends a new skill\nif a friend has experience in a similar\ndomain\nyou would expect they can pick up the\nnew skill faster than if they didn't\nnow transfer learning isn't totally a\nfree lunch\nmind you um and we'll talk more about\nthat\nafter we let these models train so i'm\ngoing to let these models kick off and\ntrain\nnow uh while those models train there is\none other thing that i do want to walk\nthrough\n so we described transfer learning as\nbeing advantageous\nif we were starting from you know maybe\na related domain\nbut we could even do transfer learning\non the same domain\nso what do i mean by that let's say that\nwith my chess pieces data set here\nyou know right now i only have 189\nimages\nnow let's say that in theory i had gone\nout and i'd collected and i\nlabeled additional images i actually\nhave gone ahead and done that so i'm\ngoing to go here and i'm going to click\nadd more images\n and then i have actually gone and i've\ncreated\ni'm going to select my folder and on my\ndesktop here i have an additional 100\nimages and annotations\n so i'm going to select that folder and\ni'm going to click upload\nyep and all of those images and their\nannotations\nare going to be processed here for my\ndata set\nand as i said i have an additional 100\nimages\nnow why is this advantageous now when i\ntrain again in the future i don't want\nto start completely from scratch\nright if my model has learned what 189\nimages\nand 1700 annotations look like\nthen i want to start from that point in\ntime to add an additional 100 images\nso these additional 100 images i'm going\nto add them to my data set\nand i'm going to add them all to my\ntraining set in fact\nso they'll all be added to my training\nset and those are all going to be\nuploaded now in theory\nright if i did this out in the wild and\ni've collected\nnew images of the same domain when i\nstart training again\n i don't want to throw out everything\nthat my model learned\nbefore i collected new images i want to\nstart\nfrom the same point where i last left\noff so with all these new images added\nto our data set\nwe can go ahead and create a new version\nand then\nwe can kick off training again but\nbut we'll kick off training from a\ncheckpoint and that checkpoint will be\nthe ones that we just started\nso i'm going to wait for these models to\nfinish training\nand for this upload to finish and then\ni'll come right back and i'll report the\nresults\nof accuracy and the time that each of\nthem took to train\nso hold that thought i'll be right back\nand we'll do a little movie magic\n so our 100 new images finished\nuploading\nand both of our models finished training\nwhich means we can compare the model\nresults v1 versus v2\n and then we can also look and see if we\nstart\nwith our training checkpoint for the\nnext 100 images\nhow that's going to change our model\nperformance so first and foremost\nremember\nmodel v1 the dataset v1 was the\none that we did the cocoa weights\npre-training\ncheckpoint on this is the one where we\ndid transfer learning\nv2 was the model where we started from\nscratch\nwhere the weight started in a random\nposition and learned only on this data\nset\n so if we look at the v1 model results we\nsee that\nit does quite well the map here is 98.5\nthe precision is 73.3 and the recall is\n99.3\nnow i'll put a link in the description\nfor explaining what each of these\nmetrics are in greater depth if that's\nof interest\ndon't forget to like and subscribe to\nour channel as we post additional\nupdates like that\nbut in general here we're going to be\nfocusing on map so 98.5\nfor the transfer learning model now\nbefore i\ntune over to the v2 let me also tell you\nsomething about the transfer learning\nmodel\nwe kicked off both of these training\njobs\nat the same time but the transfer\nlearning model\nfinished 49 minutes before the model\nthat started\ntraining from scratch so the v1 model\ntook about an hour to train\nand the v2 model took about two hours to\ntrain from scratch\n so we already know the transfer\nlearning model was a little bit faster\nnow its accuracy is quite good how does\nthat compare the v2 model we see the map\nis\n97.9 precision 53.5 and recall 99.6\nso the mean average precision of the\ntransfer learning model is ever so\nslightly higher at 98.5\nthe precision is much higher at 73.3 and\nthe recall is actually slightly lower\n0.03 lower um at 99.3\nthat could be attributed to noise or it\ncould be actually that it learned\nsomething very specific about our chess\npieces\num but in general here we see that\ntransfer learning has done a good job\nfor us our model trained faster\n and it's ever so slightly more accurate\nnow i told you that i\nadded a hundred new images to this data\nset you saw me upload them so now we\nhave 289 images\nand that translates to having 289 images\nin 2870 annotations so over a thousand\nmore annotations\nnow i'm going to actually go back to our\ndata set here\nand i'm going to generate a version i'm\njust going to call this version v3\nand this dataset version is going to\nhave the same pre-processing step so\nit's going to be a 416x416 image that we\npassed to our model\nand no augmentation so it's just going\nto be the raw images\nthat we're going to pass to our model\nhere\n and so this v3 one we're also going to\nkick off a training job\nso we'll click use roboflow train and\nremember we can train from scratch or we\ncan start from a checkpoint except\nthis time the checkpoint we can start\nfrom is we can grab\na checkpoint from the chess pieces data\nset\n and we can actually grab a model from\none of\nour existing ones on our account\n so i already have the as i mentioned\nthat chess pieces data set on my account\n so i'm going to start training from that\nexact model\n and i'm going to kick off training and\nwhile that kicks off\nwe're going to go ahead here and use a\nlittle bit of movie magic and wait for\nthat to finish\nwhile training is in progress and we're\ngoing to come back and update you with\nhow long that took in comparison and\nwe'll see the metrics and results to see\nif there's any\nnotable difference okay so see you in a\nbit\nthe model finished training so\nlet's recap what we did we first\nuploaded 189 images\nwe did training from a checkpoint on the\ncoco dataset\nwe also did training from scratch then\nwe added 100 more images to our data set\nand we started training from a\ncheckpoint on that same model\nso our model enough to start from\nscratch it could remember what it\nlearned from those first 189 images\nand then apply that forward on our next\n100 images\nso that was v3 i need a little drumroll\nthe results\n98.9 percent map\nso remember v1 saw 98.5\nv2 so 97.9 that was training from\nscratch\nand v3 saw a 98.9 percent map\nnow notice the precision is slightly\nlower\nand the recall is a little bit higher i\nmean those numbers kind of depend on the\nthresholds that you use i'll link in the\ndescription\nmore about those metrics but all in all\nwe've done it\nwe've successfully used transfer\nlearning in computer vision\nplease like and subscribe to our youtube\nchannel\nto get up-to-date updates about videos\ntutorials blogs and new features in\nroboflow\nand thanks so much for tuning in happy\ntransfer learning"
  ],
  "paragraphs": [
    "let's talk about transfer learning in",
    "machine learning",
    "transfer learning is when you take what",
    "another model",
    "has learned about a similar domain and",
    "used that to train your new model from a",
    "predefined starting point",
    "so let's take an example that's not in",
    "the context of machine learning okay",
    "let's say you have two friends and",
    "you're trying to teach those friends how",
    "to skateboard",
    "we'll say one friend has never",
    "skateboarded before never snowboarded",
    "never done any sort of similar activity",
    "whereas another friend already knows how",
    "to surf",
    "okay now which of these friends do you",
    "think is going to pick up skateboarding",
    "faster",
    "probably the one that has some",
    "experience in a related domain",
    "the one that surfed before in a sense",
    "this is what transfer learning is in",
    "machine learning",
    "it says let's take what another model",
    "has already learned that's similar to",
    "what i'm trying to teach my model that's",
    "new",
    "and start from there when we do our",
    "training i'm joseph from roboflow and",
    "today i'm going to take you through",
    "how you can use transfer learning in",
    "computer vision problems",
    "so that your object detection models can",
    "improve",
    "faster so the example that i'm",
    "going to be working with today",
    "is a chess piece data set so this",
    "data set is available in its full",
    "form on public.roboflow.com i'll put the",
    "link in the description",
    "what the dataset is is it's actually a",
    "series of about",
    "uh well this one has 189 images and",
    "1762 annotations across 12 classes",
    "each of the chess pieces and the goal of",
    "what we're trying to do",
    "is teach our model to do object",
    "detection of each of these individual",
    "chess pieces",
    "okay seems simple enough so what",
    "we may want to do is we can start",
    "training from totally from scratch",
    "right that means that we take a model",
    "where",
    "the model knows nothing the weights are",
    "randomly initiated",
    "and then those weights learn over time",
    "and try to get to the closest point",
    "possible",
    "we could also start training from a",
    "different",
    "checkpoint now that other checkpoint we",
    "would hope",
    "would be something that's somewhat",
    "similar remember my example with our",
    "friends",
    "if we have one friend that we're trying",
    "to teach them both skateboarding",
    "and we have a friend that has learned",
    "how to snowboard maybe having that",
    "friend",
    "start would do much better in the",
    "lessons whereas the friend that's never",
    "done",
    "anything in the related domain will take",
    "longer to learn",
    "that's kind of what we want to do here",
    "so i've actually gone ahead already",
    "and with my data set of 189 images",
    "i've generated two versions now these",
    "versions don't have any augmentation",
    "the only thing that's been done is their",
    "images have been resized to 416 by 416.",
    "so that's a 416x416 image and same",
    "with this one",
    "okay now when we kick off training",
    "we're going to use roboflow specifically",
    "roboflow train",
    "to kick off a training job now we're",
    "going to do two different training jobs",
    "okay to demonstrate the power",
    "of transfer learning in roboflow when",
    "you click",
    "use roboflowtrain you're basically",
    "saying please",
    "roboflow train a model for me without me",
    "needing to write any code",
    "and give me the results of that model in",
    "the form of",
    "an api or a model i can use for future",
    "labeling",
    "or just to benchmark my existing",
    "performance now you'll notice that i can",
    "do two things",
    "i can train from scratch or i can start",
    "from a checkpoint",
    "train from scratch means don't use any",
    "pre-existing weights that is the rates",
    "will be rent the weights will be",
    "randomly initiated and the model will",
    "just learn the best that it can",
    "alternatively i can start from a",
    "checkpoint now",
    "logically this checkpoint that i start",
    "from should be",
    "something that's a similar domain now",
    "in roboflow those things that i start",
    "from could be",
    "data sets that i already have on my",
    "account so i actually already have",
    "a different trained model on my account",
    "of chess pieces",
    "from this example here or i could start",
    "from",
    "a public model and so the public models",
    "are models that are",
    "available on roboflow so for example",
    "the common objects in context data set",
    "the",
    "coco data set is where i could start my",
    "training from",
    "now i'm going to do two tests here i'm",
    "going to do one model",
    "where i kick it off from the coco data",
    "set and then i'm gonna do another model",
    "where i kick it off",
    "completely from scratch okay now if",
    "you're unfamiliar with the coco data set",
    "it's also available on roboflow public",
    "and basically what it is is a data set",
    "of hundreds of thousands of images",
    "of objects in common contexts things",
    "like",
    "forks and spoons and persons and cars",
    "and dogs and microwaves",
    "and things like that and so what i'm",
    "thinking here is",
    "if i start my training on the chess",
    "piece data set i'll try",
    "one from the coco data set to see if the",
    "common objects in context data set",
    "approximates pretty well",
    "and then i'll do another one completely",
    "from scratch so i'm going to start",
    "training from the coco data set",
    "and that goes ahead and that training",
    "job starts and then on the other one on",
    "v2 i'm going to start training",
    "completely from scratch",
    "so those two have kicked off now",
    "while those start to train let's talk",
    "through the advantages",
    "of transfer learning there's a few",
    "things that transfer learning does for",
    "us",
    "one it may and likely will",
    "improve the accuracy of our model",
    "because think about it if we're saying",
    "that our model needs to learn",
    "but it can start from what another model",
    "has learned in a similar space",
    "then that model may optimize to learn",
    "the new domain",
    "even better so in other words the first",
    "advantage is",
    "it may make for more accurate models",
    "the second advantage is it actually will",
    "maybe",
    "likely will make our training time be",
    "faster if we have a model that's",
    "starting from a predefined",
    "sp checkpoint of a similar domain then",
    "we would think that",
    "the new model can learn you know the",
    "rest of the way",
    "whereas the model that starts completely",
    "from scratch might take a little bit",
    "longer to optimize its loss function",
    "and understand our new domain more",
    "successfully",
    "again think back to our example of",
    "teaching your friends a new skill",
    "if a friend has experience in a similar",
    "domain",
    "you would expect they can pick up the",
    "new skill faster than if they didn't",
    "now transfer learning isn't totally a",
    "free lunch",
    "mind you um and we'll talk more about",
    "that",
    "after we let these models train so i'm",
    "going to let these models kick off and",
    "train",
    "now uh while those models train there is",
    "one other thing that i do want to walk",
    "through",
    "so we described transfer learning as",
    "being advantageous",
    "if we were starting from you know maybe",
    "a related domain",
    "but we could even do transfer learning",
    "on the same domain",
    "so what do i mean by that let's say that",
    "with my chess pieces data set here",
    "you know right now i only have 189",
    "images",
    "now let's say that in theory i had gone",
    "out and i'd collected and i",
    "labeled additional images i actually",
    "have gone ahead and done that so i'm",
    "going to go here and i'm going to click",
    "add more images",
    "and then i have actually gone and i've",
    "created",
    "i'm going to select my folder and on my",
    "desktop here i have an additional 100",
    "images and annotations",
    "so i'm going to select that folder and",
    "i'm going to click upload",
    "yep and all of those images and their",
    "annotations",
    "are going to be processed here for my",
    "data set",
    "and as i said i have an additional 100",
    "images",
    "now why is this advantageous now when i",
    "train again in the future i don't want",
    "to start completely from scratch",
    "right if my model has learned what 189",
    "images",
    "and 1700 annotations look like",
    "then i want to start from that point in",
    "time to add an additional 100 images",
    "so these additional 100 images i'm going",
    "to add them to my data set",
    "and i'm going to add them all to my",
    "training set in fact",
    "so they'll all be added to my training",
    "set and those are all going to be",
    "uploaded now in theory",
    "right if i did this out in the wild and",
    "i've collected",
    "new images of the same domain when i",
    "start training again",
    "i don't want to throw out everything",
    "that my model learned",
    "before i collected new images i want to",
    "start",
    "from the same point where i last left",
    "off so with all these new images added",
    "to our data set",
    "we can go ahead and create a new version",
    "and then",
    "we can kick off training again but",
    "but we'll kick off training from a",
    "checkpoint and that checkpoint will be",
    "the ones that we just started",
    "so i'm going to wait for these models to",
    "finish training",
    "and for this upload to finish and then",
    "i'll come right back and i'll report the",
    "results",
    "of accuracy and the time that each of",
    "them took to train",
    "so hold that thought i'll be right back",
    "and we'll do a little movie magic",
    "so our 100 new images finished",
    "uploading",
    "and both of our models finished training",
    "which means we can compare the model",
    "results v1 versus v2",
    "and then we can also look and see if we",
    "start",
    "with our training checkpoint for the",
    "next 100 images",
    "how that's going to change our model",
    "performance so first and foremost",
    "remember",
    "model v1 the dataset v1 was the",
    "one that we did the cocoa weights",
    "pre-training",
    "checkpoint on this is the one where we",
    "did transfer learning",
    "v2 was the model where we started from",
    "scratch",
    "where the weight started in a random",
    "position and learned only on this data",
    "set",
    "so if we look at the v1 model results we",
    "see that",
    "it does quite well the map here is 98.5",
    "the precision is 73.3 and the recall is",
    "99.3",
    "now i'll put a link in the description",
    "for explaining what each of these",
    "metrics are in greater depth if that's",
    "of interest",
    "don't forget to like and subscribe to",
    "our channel as we post additional",
    "updates like that",
    "but in general here we're going to be",
    "focusing on map so 98.5",
    "for the transfer learning model now",
    "before i",
    "tune over to the v2 let me also tell you",
    "something about the transfer learning",
    "model",
    "we kicked off both of these training",
    "jobs",
    "at the same time but the transfer",
    "learning model",
    "finished 49 minutes before the model",
    "that started",
    "training from scratch so the v1 model",
    "took about an hour to train",
    "and the v2 model took about two hours to",
    "train from scratch",
    "so we already know the transfer",
    "learning model was a little bit faster",
    "now its accuracy is quite good how does",
    "that compare the v2 model we see the map",
    "is",
    "97.9 precision 53.5 and recall 99.6",
    "so the mean average precision of the",
    "transfer learning model is ever so",
    "slightly higher at 98.5",
    "the precision is much higher at 73.3 and",
    "the recall is actually slightly lower",
    "0.03 lower um at 99.3",
    "that could be attributed to noise or it",
    "could be actually that it learned",
    "something very specific about our chess",
    "pieces",
    "um but in general here we see that",
    "transfer learning has done a good job",
    "for us our model trained faster",
    "and it's ever so slightly more accurate",
    "now i told you that i",
    "added a hundred new images to this data",
    "set you saw me upload them so now we",
    "have 289 images",
    "and that translates to having 289 images",
    "in 2870 annotations so over a thousand",
    "more annotations",
    "now i'm going to actually go back to our",
    "data set here",
    "and i'm going to generate a version i'm",
    "just going to call this version v3",
    "and this dataset version is going to",
    "have the same pre-processing step so",
    "it's going to be a 416x416 image that we",
    "passed to our model",
    "and no augmentation so it's just going",
    "to be the raw images",
    "that we're going to pass to our model",
    "here",
    "and so this v3 one we're also going to",
    "kick off a training job",
    "so we'll click use roboflow train and",
    "remember we can train from scratch or we",
    "can start from a checkpoint except",
    "this time the checkpoint we can start",
    "from is we can grab",
    "a checkpoint from the chess pieces data",
    "set",
    "and we can actually grab a model from",
    "one of",
    "our existing ones on our account",
    "so i already have the as i mentioned",
    "that chess pieces data set on my account",
    "so i'm going to start training from that",
    "exact model",
    "and i'm going to kick off training and",
    "while that kicks off",
    "we're going to go ahead here and use a",
    "little bit of movie magic and wait for",
    "that to finish",
    "while training is in progress and we're",
    "going to come back and update you with",
    "how long that took in comparison and",
    "we'll see the metrics and results to see",
    "if there's any",
    "notable difference okay so see you in a",
    "bit",
    "the model finished training so",
    "let's recap what we did we first",
    "uploaded 189 images",
    "we did training from a checkpoint on the",
    "coco dataset",
    "we also did training from scratch then",
    "we added 100 more images to our data set",
    "and we started training from a",
    "checkpoint on that same model",
    "so our model enough to start from",
    "scratch it could remember what it",
    "learned from those first 189 images",
    "and then apply that forward on our next",
    "100 images",
    "so that was v3 i need a little drumroll",
    "the results",
    "98.9 percent map",
    "so remember v1 saw 98.5",
    "v2 so 97.9 that was training from",
    "scratch",
    "and v3 saw a 98.9 percent map",
    "now notice the precision is slightly",
    "lower",
    "and the recall is a little bit higher i",
    "mean those numbers kind of depend on the",
    "thresholds that you use i'll link in the",
    "description",
    "more about those metrics but all in all",
    "we've done it",
    "we've successfully used transfer",
    "learning in computer vision",
    "please like and subscribe to our youtube",
    "channel",
    "to get up-to-date updates about videos",
    "tutorials blogs and new features in",
    "roboflow",
    "and thanks so much for tuning in happy",
    "transfer learning"
  ],
  "keywords": [
    "tutorials",
    "difference",
    "processed",
    "finish",
    "anything",
    "specifically",
    "even",
    "nothing",
    "activity",
    "collected",
    "surfed",
    "best",
    "subscribe",
    "snowboard",
    "generate",
    "friend",
    "weight",
    "model",
    "wait",
    "machine",
    "thinking",
    "quite",
    "hundred",
    "goal",
    "kicked",
    "passed",
    "forward",
    "somewhat",
    "needs",
    "uploading",
    "exact",
    "already",
    "na",
    "computer",
    "416",
    "learning",
    "features",
    "give",
    "persons",
    "videos",
    "training",
    "simple",
    "random",
    "labeling",
    "need",
    "youtube",
    "without",
    "rent",
    "minutes",
    "report",
    "throw",
    "kick",
    "generated",
    "tuning",
    "two",
    "thing",
    "hundreds",
    "v1",
    "versions",
    "tests",
    "update",
    "explaining",
    "likely",
    "data",
    "started",
    "trained",
    "think",
    "grab",
    "one",
    "optimize",
    "api",
    "friends",
    "results",
    "example",
    "uh",
    "point",
    "well",
    "skateboarding",
    "free",
    "thresholds",
    "blogs",
    "coco",
    "walk",
    "except",
    "never",
    "comparison",
    "starts",
    "greater",
    "recall",
    "second",
    "resized",
    "whereas",
    "mentioned",
    "space",
    "cars",
    "slightly",
    "today",
    "tune",
    "come",
    "gon",
    "something",
    "unfamiliar",
    "related",
    "1762",
    "told",
    "ever",
    "create",
    "jobs",
    "take",
    "public",
    "let",
    "time",
    "add",
    "code",
    "saw",
    "talk",
    "full",
    "object",
    "seems",
    "right",
    "everything",
    "learned",
    "future",
    "logically",
    "teaching",
    "skill",
    "attributed",
    "closest",
    "performance",
    "annotations",
    "depth",
    "focusing",
    "may",
    "look",
    "goes",
    "things",
    "models",
    "mind",
    "snowboarded",
    "please",
    "general",
    "skateboard",
    "needing",
    "version",
    "also",
    "foremost",
    "progress",
    "problems",
    "pretty",
    "theory",
    "classes",
    "thousand",
    "series",
    "labeled",
    "go",
    "click",
    "recap",
    "domain",
    "next",
    "see",
    "want",
    "available",
    "movie",
    "spoons",
    "checkpoint",
    "good",
    "hope",
    "advantages",
    "took",
    "weights",
    "advantageous",
    "put",
    "enough",
    "thought",
    "understand",
    "approximates",
    "channel",
    "faster",
    "says",
    "416x416",
    "teach",
    "dataset",
    "way",
    "get",
    "hours",
    "back",
    "image",
    "hour",
    "mean",
    "described",
    "much",
    "folder",
    "power",
    "possible",
    "expect",
    "experience",
    "randomly",
    "predefined",
    "compare",
    "existing",
    "100",
    "v3",
    "dogs",
    "created",
    "say",
    "try",
    "happy",
    "another",
    "could",
    "49",
    "ones",
    "learn",
    "rates",
    "train",
    "desktop",
    "189",
    "thousands",
    "wild",
    "hold",
    "average",
    "microwaves",
    "v2",
    "depend",
    "actually",
    "across",
    "percent",
    "better",
    "augmentation",
    "new",
    "improve",
    "thanks",
    "link",
    "additional",
    "ahead",
    "fact",
    "surf",
    "gone",
    "vision",
    "use",
    "know",
    "function",
    "2870",
    "advantage",
    "starting",
    "transfer",
    "us",
    "description",
    "context",
    "initiated",
    "okay",
    "added",
    "tell",
    "make",
    "interest",
    "contexts",
    "notable",
    "forks",
    "numbers",
    "done",
    "rest",
    "forget",
    "saying",
    "step",
    "roboflow",
    "used",
    "joseph",
    "probably",
    "last",
    "like",
    "write",
    "common",
    "roboflowtrain",
    "sense",
    "sp",
    "would",
    "sort",
    "objects",
    "images",
    "skateboarded",
    "specific",
    "versus",
    "accuracy",
    "call",
    "loss",
    "long",
    "means",
    "totally",
    "updates",
    "select",
    "translates",
    "metrics",
    "set",
    "magic",
    "first",
    "kicks",
    "successfully",
    "12",
    "finished",
    "account",
    "lower",
    "289",
    "change",
    "1700",
    "lunch",
    "higher",
    "benchmark",
    "um",
    "going",
    "piece",
    "basically",
    "remember",
    "apply",
    "said",
    "post",
    "cocoa",
    "working",
    "precision",
    "pick",
    "might",
    "maybe",
    "chess",
    "trying",
    "scratch",
    "similar",
    "start",
    "pieces",
    "uploaded",
    "upload",
    "map",
    "lessons",
    "position",
    "words",
    "individual",
    "noise",
    "detection",
    "left",
    "notice",
    "job",
    "sets",
    "kind",
    "completely",
    "different",
    "form",
    "yep",
    "longer",
    "little",
    "raw",
    "bit",
    "pass",
    "demonstrate",
    "drumroll",
    "accurate",
    "knows",
    "alternatively"
  ]
}