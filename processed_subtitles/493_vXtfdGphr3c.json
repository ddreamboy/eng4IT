{
  "text": "hello everyone in this video you'll\nlearn all about reinforcement learning\nhow to train chat GPT and build the\nworld's best go playing program\nbut let's start with something simpler\nremember pong\nit was one of the very first video games\nthe player on the left has a simple\npolicy it just follows the ball up and\ndown\nlet's train an AI pong playing Agent to\ncompete with it\nto do this we'll need to define a policy\nthat takes the state of the world as\ninput and outputs an action up in this\ncase\nas input we'll take the ball X and Y\npositions and the two paddle positions\nwe'll also include the velocities so our\npolicy will get eight inputs\nto keep the figure simple I won't show\nthe velocities\nwe'll represent our policy as a neural\nnetwork our network will multiply each\nof these values with a weight\nand sum them up\nwe'll add a sigmoid function to convert\nthe result to a value between 0 and 1.\nif the result is larger than 0.5 we'll\nmove up otherwise down that's it\ngreat let's play some pong\nwe'll start with random weights\na random Pond player isn't very good it\njust hides in the corner\nthis is like if you've never played\ntennis and you're matched against Serena\nWilliams\nyou'd probably hide too\nor maybe you'd ask your opponent how to\nplay\nsuppose your partner told you exactly\nwhat to do for every step\nwe can use this kind of supervision to\ntrain our policy\nlet's say our coach says to follow the\nball\nhear the balls above our paddle so the\nright answer is up\nbut suppose our Network outputs 0.2\nwhich means down\nwe'd like to change that 0.2 to a 1.0\nhere's a different case where the right\nanswer is down\nand we'd like to drive the output to\nzero a more confident down answer\nwe can accomplish this by defining an\nerror function\nand optimizing the weights to reduce\nthis error\ninstead of numbers let's draw these\nweights as colored lines\npositive voids are blue and negative\nweights are red\nstronger weights of thicker lines\nand the weights change a tiny bit after\neach optimization step\nif we train on thousands of actions the\nweights on the right start to converge\nnow that we're trained up let's play\nsome games\nour pompling agent is pretty decent now\nalthough it doesn't win very often\nhow can we train our agent to win more\nwell you can think of the game as a\nsequence of actions\nwhich may lead to a loss or a win\nsuppose we lose\nwe'd like our coach to tell us which\nactions were at fault maybe we messed up\nat the end\nthen we could retrain our policy based\non this feedback as we've been doing\nbefore\ntraining a network like this is called\nsupervised learning and you need access\nto a really good coach\nbut suppose you don't have a coach all\nyou know is that this sequence of\nactions resulted in a loss and this one\nproduced a win\nthis kind of problem is called\nreinforcement learning\nwe don't know which actions were\nresponsible\nso we'll just penalize all actions we\nmade in the loss and reward or reinforce\nall actions we made in the win\nthis converts it into a supervised\nlearning problem just like we solved in\nthe first part of this video\nactually we can do a little bit better\nby penalizing later actions more as the\ngame losing the stake typically happens\ntowards the end and similarly for\nRewards\nafter training on 3000 games our agent\nhas improved to play about as well as\nthe opponent\nthe score is pretty even\nand after about 12 000 games our agent\nfigured out that hitting the ball with\nthe corner of the paddle makes it go\nfaster\ncool\nand now it wins most of the time\nreinforcement learning is cool you learn\nby trying stuff out and sometimes you\ndiscover things that even the coaches\ndon't know\nbut there's some challenges with this\napproach our neural network tries to\nminimize an error function\nyou can think of this function as a\nlandscape and we minimize it by Rolling\ndownhill using an approach known as\ngradient descent\ngradient descent can get stuck in local\nMinima so we need to explore a wider\nrange of policies to find the best one\nso let's make the policy probabilistic a\n0.7 now means 70 chance of moving up\nso the agent will actually move down 30\nof the time\nthis bit of Randomness allows our agent\nto get out of some of those local Minima\nto sum it up here's the approach we've\ndescribed so far\nwe'll start with randomly initialized\nweights\nwe then train with supervised learning\nand refine with reinforcement learning\namazingly we can skip the middle step\nand run reinforcement learning from\nrandom initial weights\nit takes longer to train but can still\nwork for many problems\nand we can do something even more\namazing\nso far we've assumed the ball and paddle\npositions are provided as input to the\npolicy\nsuppose instead all we had was a photo\nof the screen\nthe computer will see this as a bunch of\nnumbers it has no idea that this number\nis the ball and these are the paddles\nin other words the policy has to learn\nnot just how to act but also how to see\nit's helpful to encode velocity\ninformation as well\none way to do this is to subtract the\nprevious from the current frame\nnow positive values are from the current\nframe and negatives are from the\nprevious one\ninstead of a 2D image we'll stack all\nthe rows into one long Vector of pixels\nand we'll connect all these pixels to a\nneuron\nnow we'll add more neurons 10 in total\nwe'll use rectified linear activation\nfunctions\nthese are pretty standard\nthey connect to a sigmoid neuron at the\nend\nand it will output the probability of\nmoving up\nthis may look complicated but it's\nactually really small for a deep net\nthere are only 11 neurons in this\nnetwork\ncompare this to the human brain which\nhas billions of neurons in the visual\ncortex\nhow can you see with only 11 neurons\nlet's train the network and find out\nwe'll start with completely random\nweights\nit took about 6 million games to learn a\npolicy that beats the computer on\naverage\nthat's almost a week running on my\nMacBook Air\nbut what's this network actually\nlearning does it really learn how to see\nlet's examine the first neuron\nit has a weight for every pixel in the\nimage\nremember these pixels form a 2D image\nand each one has a corresponding weight\nso we can visualize the weights as an\nimage in the shape of the pong screen\nit looks like noise because I've\nrandomly initialized the weights the\nwhite ones here have the largest\nabsolute value\nnow we'll start training the policy by\nplaying games\nand you can see that it converges to a\nspecific pattern after 1 million and 6\nmillion games\nthese streaks correspond to ball\ntrajectories that this neuron is\nattending to\nand it's particularly interested in\nthese paddle positions\nit's also looking at the opponent's\npaddle which is a good indicator of ball\nposition\nthis is just one neuron\nthe others look for different patterns\nso our simple network has kind of\nlearned how to see\nthis approach to reinforcement learning\nis called policy gradient and you can\nmake it even better by limiting the size\nof each update in a variant known as PPO\ntogether they power many exciting\napplications\nwe'll talk about two of them alphago and\nchat GPT in the next two videos\nstay tuned\n",
  "words": [
    "hello",
    "everyone",
    "video",
    "learn",
    "reinforcement",
    "learning",
    "train",
    "chat",
    "gpt",
    "build",
    "world",
    "best",
    "go",
    "playing",
    "program",
    "let",
    "start",
    "something",
    "simpler",
    "remember",
    "pong",
    "one",
    "first",
    "video",
    "games",
    "player",
    "left",
    "simple",
    "policy",
    "follows",
    "ball",
    "let",
    "train",
    "ai",
    "pong",
    "playing",
    "agent",
    "compete",
    "need",
    "define",
    "policy",
    "takes",
    "state",
    "world",
    "input",
    "outputs",
    "action",
    "case",
    "input",
    "take",
    "ball",
    "x",
    "positions",
    "two",
    "paddle",
    "positions",
    "also",
    "include",
    "velocities",
    "policy",
    "get",
    "eight",
    "inputs",
    "keep",
    "figure",
    "simple",
    "wo",
    "show",
    "velocities",
    "represent",
    "policy",
    "neural",
    "network",
    "network",
    "multiply",
    "values",
    "weight",
    "sum",
    "add",
    "sigmoid",
    "function",
    "convert",
    "result",
    "value",
    "0",
    "result",
    "larger",
    "move",
    "otherwise",
    "great",
    "let",
    "play",
    "pong",
    "start",
    "random",
    "weights",
    "random",
    "pond",
    "player",
    "good",
    "hides",
    "corner",
    "like",
    "never",
    "played",
    "tennis",
    "matched",
    "serena",
    "williams",
    "probably",
    "hide",
    "maybe",
    "ask",
    "opponent",
    "play",
    "suppose",
    "partner",
    "told",
    "exactly",
    "every",
    "step",
    "use",
    "kind",
    "supervision",
    "train",
    "policy",
    "let",
    "say",
    "coach",
    "says",
    "follow",
    "ball",
    "hear",
    "balls",
    "paddle",
    "right",
    "answer",
    "suppose",
    "network",
    "outputs",
    "means",
    "like",
    "change",
    "different",
    "case",
    "right",
    "answer",
    "like",
    "drive",
    "output",
    "zero",
    "confident",
    "answer",
    "accomplish",
    "defining",
    "error",
    "function",
    "optimizing",
    "weights",
    "reduce",
    "error",
    "instead",
    "numbers",
    "let",
    "draw",
    "weights",
    "colored",
    "lines",
    "positive",
    "voids",
    "blue",
    "negative",
    "weights",
    "red",
    "stronger",
    "weights",
    "thicker",
    "lines",
    "weights",
    "change",
    "tiny",
    "bit",
    "optimization",
    "step",
    "train",
    "thousands",
    "actions",
    "weights",
    "right",
    "start",
    "converge",
    "trained",
    "let",
    "play",
    "games",
    "pompling",
    "agent",
    "pretty",
    "decent",
    "although",
    "win",
    "often",
    "train",
    "agent",
    "win",
    "well",
    "think",
    "game",
    "sequence",
    "actions",
    "may",
    "lead",
    "loss",
    "win",
    "suppose",
    "lose",
    "like",
    "coach",
    "tell",
    "us",
    "actions",
    "fault",
    "maybe",
    "messed",
    "end",
    "could",
    "retrain",
    "policy",
    "based",
    "feedback",
    "training",
    "network",
    "like",
    "called",
    "supervised",
    "learning",
    "need",
    "access",
    "really",
    "good",
    "coach",
    "suppose",
    "coach",
    "know",
    "sequence",
    "actions",
    "resulted",
    "loss",
    "one",
    "produced",
    "win",
    "kind",
    "problem",
    "called",
    "reinforcement",
    "learning",
    "know",
    "actions",
    "responsible",
    "penalize",
    "actions",
    "made",
    "loss",
    "reward",
    "reinforce",
    "actions",
    "made",
    "win",
    "converts",
    "supervised",
    "learning",
    "problem",
    "like",
    "solved",
    "first",
    "part",
    "video",
    "actually",
    "little",
    "bit",
    "better",
    "penalizing",
    "later",
    "actions",
    "game",
    "losing",
    "stake",
    "typically",
    "happens",
    "towards",
    "end",
    "similarly",
    "rewards",
    "training",
    "3000",
    "games",
    "agent",
    "improved",
    "play",
    "well",
    "opponent",
    "score",
    "pretty",
    "even",
    "12",
    "000",
    "games",
    "agent",
    "figured",
    "hitting",
    "ball",
    "corner",
    "paddle",
    "makes",
    "go",
    "faster",
    "cool",
    "wins",
    "time",
    "reinforcement",
    "learning",
    "cool",
    "learn",
    "trying",
    "stuff",
    "sometimes",
    "discover",
    "things",
    "even",
    "coaches",
    "know",
    "challenges",
    "approach",
    "neural",
    "network",
    "tries",
    "minimize",
    "error",
    "function",
    "think",
    "function",
    "landscape",
    "minimize",
    "rolling",
    "downhill",
    "using",
    "approach",
    "known",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "get",
    "stuck",
    "local",
    "minima",
    "need",
    "explore",
    "wider",
    "range",
    "policies",
    "find",
    "best",
    "one",
    "let",
    "make",
    "policy",
    "probabilistic",
    "means",
    "70",
    "chance",
    "moving",
    "agent",
    "actually",
    "move",
    "30",
    "time",
    "bit",
    "randomness",
    "allows",
    "agent",
    "get",
    "local",
    "minima",
    "sum",
    "approach",
    "described",
    "far",
    "start",
    "randomly",
    "initialized",
    "weights",
    "train",
    "supervised",
    "learning",
    "refine",
    "reinforcement",
    "learning",
    "amazingly",
    "skip",
    "middle",
    "step",
    "run",
    "reinforcement",
    "learning",
    "random",
    "initial",
    "weights",
    "takes",
    "longer",
    "train",
    "still",
    "work",
    "many",
    "problems",
    "something",
    "even",
    "amazing",
    "far",
    "assumed",
    "ball",
    "paddle",
    "positions",
    "provided",
    "input",
    "policy",
    "suppose",
    "instead",
    "photo",
    "screen",
    "computer",
    "see",
    "bunch",
    "numbers",
    "idea",
    "number",
    "ball",
    "paddles",
    "words",
    "policy",
    "learn",
    "act",
    "also",
    "see",
    "helpful",
    "encode",
    "velocity",
    "information",
    "well",
    "one",
    "way",
    "subtract",
    "previous",
    "current",
    "frame",
    "positive",
    "values",
    "current",
    "frame",
    "negatives",
    "previous",
    "one",
    "instead",
    "2d",
    "image",
    "stack",
    "rows",
    "one",
    "long",
    "vector",
    "pixels",
    "connect",
    "pixels",
    "neuron",
    "add",
    "neurons",
    "10",
    "total",
    "use",
    "rectified",
    "linear",
    "activation",
    "functions",
    "pretty",
    "standard",
    "connect",
    "sigmoid",
    "neuron",
    "end",
    "output",
    "probability",
    "moving",
    "may",
    "look",
    "complicated",
    "actually",
    "really",
    "small",
    "deep",
    "net",
    "11",
    "neurons",
    "network",
    "compare",
    "human",
    "brain",
    "billions",
    "neurons",
    "visual",
    "cortex",
    "see",
    "11",
    "neurons",
    "let",
    "train",
    "network",
    "find",
    "start",
    "completely",
    "random",
    "weights",
    "took",
    "6",
    "million",
    "games",
    "learn",
    "policy",
    "beats",
    "computer",
    "average",
    "almost",
    "week",
    "running",
    "macbook",
    "air",
    "network",
    "actually",
    "learning",
    "really",
    "learn",
    "see",
    "let",
    "examine",
    "first",
    "neuron",
    "weight",
    "every",
    "pixel",
    "image",
    "remember",
    "pixels",
    "form",
    "2d",
    "image",
    "one",
    "corresponding",
    "weight",
    "visualize",
    "weights",
    "image",
    "shape",
    "pong",
    "screen",
    "looks",
    "like",
    "noise",
    "randomly",
    "initialized",
    "weights",
    "white",
    "ones",
    "largest",
    "absolute",
    "value",
    "start",
    "training",
    "policy",
    "playing",
    "games",
    "see",
    "converges",
    "specific",
    "pattern",
    "1",
    "million",
    "6",
    "million",
    "games",
    "streaks",
    "correspond",
    "ball",
    "trajectories",
    "neuron",
    "attending",
    "particularly",
    "interested",
    "paddle",
    "positions",
    "also",
    "looking",
    "opponent",
    "paddle",
    "good",
    "indicator",
    "ball",
    "position",
    "one",
    "neuron",
    "others",
    "look",
    "different",
    "patterns",
    "simple",
    "network",
    "kind",
    "learned",
    "see",
    "approach",
    "reinforcement",
    "learning",
    "called",
    "policy",
    "gradient",
    "make",
    "even",
    "better",
    "limiting",
    "size",
    "update",
    "variant",
    "known",
    "ppo",
    "together",
    "power",
    "many",
    "exciting",
    "applications",
    "talk",
    "two",
    "alphago",
    "chat",
    "gpt",
    "next",
    "two",
    "videos",
    "stay",
    "tuned"
  ],
  "keywords": [
    "video",
    "learn",
    "reinforcement",
    "learning",
    "train",
    "chat",
    "gpt",
    "world",
    "best",
    "go",
    "playing",
    "let",
    "start",
    "something",
    "remember",
    "pong",
    "one",
    "first",
    "games",
    "player",
    "simple",
    "policy",
    "ball",
    "agent",
    "need",
    "takes",
    "input",
    "outputs",
    "case",
    "positions",
    "two",
    "paddle",
    "also",
    "velocities",
    "get",
    "neural",
    "network",
    "values",
    "weight",
    "sum",
    "add",
    "sigmoid",
    "function",
    "result",
    "value",
    "move",
    "play",
    "random",
    "weights",
    "good",
    "corner",
    "like",
    "maybe",
    "opponent",
    "suppose",
    "every",
    "step",
    "use",
    "kind",
    "coach",
    "right",
    "answer",
    "means",
    "change",
    "different",
    "output",
    "error",
    "instead",
    "numbers",
    "lines",
    "positive",
    "bit",
    "actions",
    "pretty",
    "win",
    "well",
    "think",
    "game",
    "sequence",
    "may",
    "loss",
    "end",
    "training",
    "called",
    "supervised",
    "really",
    "know",
    "problem",
    "made",
    "actually",
    "better",
    "even",
    "cool",
    "time",
    "approach",
    "minimize",
    "known",
    "gradient",
    "descent",
    "local",
    "minima",
    "find",
    "make",
    "moving",
    "far",
    "randomly",
    "initialized",
    "many",
    "screen",
    "computer",
    "see",
    "previous",
    "current",
    "frame",
    "2d",
    "image",
    "pixels",
    "connect",
    "neuron",
    "neurons",
    "look",
    "11",
    "6",
    "million"
  ]
}