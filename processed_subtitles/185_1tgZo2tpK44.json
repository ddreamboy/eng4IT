{
  "text": "hello everyone and welcome to another\nepisode of Code Emporium as we continue\nour journey through the wonderful world\nof building a translator using a\nTransformer neural network now in the\nlast dozen videos we have built the\nTransformer neural network from scratch\nand here is the code to prove so so in\nthis video though I wanted to actually\ngo through some of the inference\ntranslators that we get after actually\ntraining this model and I also want to\noffer some insights of what you can do\nto build your own translator if you want\nto see this entire code and with some of\nthe data sets that I've used I've\nuploaded everything to GitHub and the\nlink is in the description below before\nI get started with this video I wanted\nto give a quick shout out to this user\naccount\ntiger07 where they helped point out a\nspecific issue where in my last training\nvideos for building out the Transformer\nI kind of made some errors they pointed\nout like two lines of code that I kind\nof needed to change throughout the\nnetwork and this is the line itself it\nwas just reshaping of these tensors\nbecause I think it was Pi torch when I\ntried to do I was just doing a values\nlike some reshape which would have\ncompletely discombobulated all my\ntensors I needed to do a permutation of\nsome layers it's a very minor technical\nissue but it was a major one in the\nsense that it stalled me for a very long\ntime and I'm really grateful that you\nknow when I reached out to the community\nyou all responded very well I also\nwanted to give a shout out to this\naccount ping NG sorry for mispronouncing\nthat if anything who recommended the\nsame exact solution but also detailed\nwhy that was the case so thank you so\nmuch as well and that just shows how\nwell this community does respond because\neven slack nor chat GPT could really\nhelp me out but you all did so thank you\nso much for now let's just go to the\npart where we're actually instantiating\nthe Transformer and the only thing I\nreally want to point out here is that we\nare going to use only one encoder and\none decoder layer so this is going to be\nthe simplest of the simplest Transformer\nneural networks so that it could just\ntrain this faster to see some reasonable\nresults that's the only reason I did\nthis and I did this training for about\none and a half hours where I trained it\nfor 10 epochs where I'll scroll here you\nsee that's 10 epochs with the data set\nof around 200 000 English sentences to\ntranslate into a language called kannada\nand all of this training Epoch I printed\nout 100 epochs at a time for now let's\nget to the good part of Transformer\ninference\nso this here is a character language\nmodel where we're generating one\ncharacter at a time and so you'll see\neverything is going to be within a for\nLoop where I am generating one character\nat a time you generate the token and\nappend it to the sentence and just keep\ndoing this until you generate an end\ntoken which will signify the end of a\nsentence\non doing so you'll get a few examples\nso let's just look at a few of them when\nI say I try to translate what should we\ndo when the day starts the the\ntranslation it gives is this sentence\nover here which says\nwhich is what do we do about this\num although it doesn't translate exactly\nto you know the sentence at least we can\nkind of see that there are some\ncommonalities that are retained like\nthis lasts two words over here and these\nlast two words over here it is retained\nin some way so\nthat's these two words that just means\nwhat should we do and that just\ncorresponds to this part of the sentence\nso it kind of gets a part of the\nsentence right but very clearly it\ndistorts the entire meaning so it's not\nquite getting everything correctly you\ncan attribute this to a few cases so the\nfirst thing is like the model is just\ntoo simple it only has like one encoder\nand one decoder and if you increase the\nnumber of encoder and decoder components\nyou probably might be able to pick up on\nmore idiosyncrasies in language that's\nprobably the biggest reason but you can\nalso try increasing your training set or\nincreasing the number of epochs for your\ntraining time before continuing with the\nvideo I want to tell you about our\nsponsor Taro this is a social platform\nto help software Engineers grow in their\ncareer so say you land us off for a job\nbut then what it could be really hard to\nnavigate your career and it's tough to\nget good career advice Taro facilitates\nthese discussions whether you are an\nentry level or a senior you can be a\npart of discussions to get advice from\nsoftware Engineers across many companies\nthere are many non-technical questions\nthat I wish I could have asked someone\nin the past to advance my career but\nreally never found a good form to do so\nbut I think Taro is that good place I'm\na machine learning engineer which does\noverlap with software engineers and\nwhile the platform does not have too\nmany machine learning engineering\nquestions at the moment I'm doing my\nbest to answer any questions that are\nthere still whenever I can and I think\nthis community is really nice to be a\npart of still so if you're looking for a\npremium community of software Engineers\nto be a part of consider signing up for\nTaro using my link in the description to\nget 20 off your annual purchase\nthanks for listening and let's get back\nto the video now the second sentence is\nhow is this the truth the actual\ntranslation is\nwhereas this here is the generated\ntranslation which is\nso this is not really a meaningful\nsentence but you can see that there are\nsome commonalities between them again so\nyou can see this\nume is generated as well\nmeans like this hege is how Satya is\ntruth\nso in it translates to how is this the\ntruth\nso it does generate some part of it and\nyou can see just by looking at these two\nexamples you can kind of see that it is\ndefinitely learning something although\nit might not be complex enough to pick\nup everything about these sentences now\nthis example is my name is a j which\nshould translate to\nbut it translates to\nso you can see again there are some\ncommonalities with hesuru which means\nname Nana is my but nanu is it's very\nclose to it but it means me it did not\npick up at all on my name at all Ajay\nalso although the overall translation is\noff once again we see some words that\nare actually common and correct now this\none's interesting why do we care about\nthis the actual translation that it gave\nwas\nwith punctuation it'll be like why\nwhat's the reason\nthat would be the actual translation of\nthis which is actually very close to the\nto the translation of this initial\nsentence over here so not bad it did\npretty good there\nthe next is this is the best thing ever\nwhereas you know here it generates this\nsentence but it actual translation is\nthis sentence which translates to this\nis very unusual so though the the\nmeanings are kind of off you can see\nthat there are just some some\ncommonalities between them again\nnow this is probably the most\ninteresting example throughout the lot\nwhere I wanted to translate I am here\nthe actual translation is\nthe projected translation by the\ntranslator was\nso there are different meanings this me\nthis means I am here whereas this means\nI've heard something and although there\nare different meanings though you can\nsee that from a character by character\ngeneration that this translator is\nperforming it's actually doing extremely\nwell the only thing in the translator's\neyes that it got wrong was this eat and\nthis K like these two alphabets are like\nthe only thing that are different in\nthis entire translation barring some\nsmall like La this they're both Luz but\neither way just one or two characters\nare the only things that are wrong so in\nthe translator's eyes this translation\nis actually a pretty good one\nbut this kind of made me think more\nabout the fact that okay this is a\ncharacter translation but in general\nword translations might actually perform\nbetter but the the caveat of using word\ntranslations is that your Transformer\nwill need to have a much larger\nvocabulary as opposed to what it has now\nso you can see if I scroll up to see\nlike what is the length of any possible\ncharacters that are possibly generated\nor tokens that are generated you can see\nit's only a small set of values here\nmaybe like a you know maybe a hot fifty\nto 100 tokens or something like that but\nif it was words then it all possible\nwords were in this list this entire list\nwould explode to the tens of thousands\nbecause there can be so many words that\nneed to be generated so there's always\nthis trade-off between larger vocabulary\nsize but interpretable values so you\nneed like much more complex systems like\nyou need probably a more complex\ntranslator and also way more parameters\nin order order to account for you know\nwords themselves\nbut with words though the sentence\nlength will technically decrease while\nhere I have provided like the maximum\nsentence length to be 200 characters the\nnumber of words in a sentence doesn't\nhave to be like 200 characters 200 words\nit could be just you know a dozen or\nsomething like that that we can cap it\nout now here's an interesting one too it\nsays click this and this would be the\ntranslation which is\nwhich is like this but the actual\ntranslation is click click click click\nclick click click click Marty so\nalthough it does get this last part\nright click Marty and it does get it\nhere it just loves click click click\nclick click so it's just funny but it is\nonce again understanding what the task\nis at least to an extent now the same\nthing is here where is the mall the\ntranslation is well the translation that\nit generated was which is where where so\nat least you got the where part is but\nit didn't generate anything else\nnow what should we do the translation is\nnow it generated this correct but it\nabsolutely fumbles on this one here\ntoday what should we do\num this I have no idea why it generates\nthis it says\nso it just was\nthe is to do and I guess that's like a\nvery common phrase that you see\neverywhere in in both English and also\nin this language Canada and that's why\nyou're seeing like all kinds of forms\nwhen you see like oh do it probably\ntries to do Mahdi like every single\neverywhere it just tries to to create\nthis scenario which again very\ninteresting but it completely fumbles\ndespite you know in English when we see\nthis it's kind of like the very same\nsentence as what we did before just an\nextra word so that's just uh it's an\ninteresting note but if I phrase it as\nwhy did they do this this sentence\nactually generated almost perfectly well\nbut again this is something to do with\ndoing something so it's a very common\ncommon sentence in general or a common\nphrase so that's probably why it did so\nwell this last part here is also a very\ninteresting one it's what's the word on\nthe street and the generated translation\nis\nso what is the topic of this or what is\nthis about is the translation which kind\nof does semantically relate to to what\nthis actually means this little idiom\nhere let's now go through some insights\nwhere I'm probably going to give you\nsome information and some tips when\nbuilding out a Transformer on your own\nwith any language so first of all I\nwanted to say create a translator with\nthe language that you understand ideally\nbecause it's just so much easier to see\nwhere the Transformer is doing things\ncorrectly and where it's also doing\nthings incorrectly so generating that\ninsight for your yourself I think is\nvery important and you can better do so\nif you understand the language itself so\nin this case people were saying why did\nyou use Canada it's because I can\nunderstand and I can properly evaluate\nit otherwise I wouldn't be able to come\nup with the insights that I did\npiggybacking off of that here's a I\nthink a pretty important Insight that I\nhaven't really seen anywhere but I'll\ndescribe here so when training typically\nthe English character set is known as\nwhat we call an alphabet where every\ncharacter kind of has a phonetic\nrepresentation to it\nin a language like Canada it's more of\nan alpha syllabary so you have\nindividual units to actually be complete\nlike syllables themselves\nand in doing so that means that even\nthough like for example this word I\nthink I typed it out here so this is a\ncharacter this is ma\nma if you write it out in English it\nwould be M A with like uh you know like\nan accent on top of it\nthat would be so it's like multiple\ncharacters in English but it's a single\ncharacter in The kannada Language\nhowever when I was dealing with tokens\nhere the way that I'm tokenizing the\ndata is I'm also treating it as like\nmultiple tokens so it'll be\nplus ah even though in the current\nlanguage it's actually supposed to be\none character I am treating it as two\ncharacters and so what semantically just\nmakes more sense is to create a\ntokenizer that will not just divvy up\nthe entire you know Canada like word\ninto very sub characters but rather\ndivvy the kannada word into actual\nkannada characters themselves which may\nor may not be a combination of two or\nmore of these characters also Alpha\nsilveries are a type of\nscript that are not confined to just\nthis language Canada there are many\nAlpha syllabares out there and so just\nunderstanding the writing style may\nactually create a translator that is\nmore meaningful and so I highly\nrecommend you try this out now another\nthe another Insight that I mentioned is\nkind of similar to what I described\nbefore this is a generating one\ncharacter at a time so it's a smaller\nvocabulary but longer sentence length\nbut you can play with generating like\nword at a time where it'll have a much\nlarger vocabulary but smaller sentence\nlength and a good mix of Both Worlds is\nto use something called byte pair\ntokenizations or bite pairing Coatings\nwhich are like sub words now the issue\nwith this is that it's very hard to\ncreate a bike pair encoding for certain\nlanguages if they don't you know if\nthey're not really don't have a great\nresearch or online presence so it's hard\nfor me to find one for the language\nCanada and hence I went with character\ntokenizations for now to illustrate\nConcepts and ideas but if you're able to\ncreate like bike pairing Coatings for\nyour languages input and outputs then I\nthink that might be like a good starting\npoint in fact I think this is exactly\nwhat's happening in the main paper and a\nlot of other research associated with\ngenerative models these days another one\nis to make sure that your training set\nhas a large variety of words in general\nyou could see that above when I\nIllustrated here there's a lot of\nsentences that are like to do right\nMarty is there's like a lot of these\nsentences that that kind of go ma\ntobacco and Modi over here in fact if\nyou look at this data set there's\nactually 10 000 cases at least of like\nwell there's like millions of Records\nhere but there's like 10 000 cases at\nthe very least where we just have this\nentire word called matabeko which is I\nhave to do and that's a very common\nphrase so I would suggest you try to\nplot out every single word and their\nfrequency counts just to get an idea of\nwhat kind of data set you're dealing\nwith whether it's very catered to like\nnews government articles politics or if\nit's catered to just like General and\nrandom sentences which ideally would be\nthe case for General translators and the\nother one is just more technical where\nyou're increasing the number of encoder\nand decoder units as I've only used one\nkeeping it very simple but you can\nideally try with more encoder decoder\nunits to pick up more complexities and\nintricacies in your languages overall\nyeah this model has definitely learned\nsomething and you can use it for you\nknow other languages instead of Canada\nas well so I still hope that this all of\nthis which is going to be available on\nGitHub and all of the videos before this\nillustrate in general the concept of how\nTransformers work\nbut if you want actually I'm going to be\nthinking about making a full video just\nend to end from start to finish of\nexplaining Transformer neural networks\nif you want to see that Mega video\nplease do comment below to see oh my\ngosh I want to see that Mega video which\nwill be very similar to like these 12\nvideos but probably you know in a much\nmore Continuous Flow if I can make it to\ndo so thank you all so much for watching\nand we're going to be continuing our\nwonderful Journey Through the worlds and\nLandscape of artificial intelligence\nthank you all so much and I'll see you\nnext time\n",
  "words": [
    "hello",
    "everyone",
    "welcome",
    "another",
    "episode",
    "code",
    "emporium",
    "continue",
    "journey",
    "wonderful",
    "world",
    "building",
    "translator",
    "using",
    "transformer",
    "neural",
    "network",
    "last",
    "dozen",
    "videos",
    "built",
    "transformer",
    "neural",
    "network",
    "scratch",
    "code",
    "prove",
    "video",
    "though",
    "wanted",
    "actually",
    "go",
    "inference",
    "translators",
    "get",
    "actually",
    "training",
    "model",
    "also",
    "want",
    "offer",
    "insights",
    "build",
    "translator",
    "want",
    "see",
    "entire",
    "code",
    "data",
    "sets",
    "used",
    "uploaded",
    "everything",
    "github",
    "link",
    "description",
    "get",
    "started",
    "video",
    "wanted",
    "give",
    "quick",
    "shout",
    "user",
    "account",
    "tiger07",
    "helped",
    "point",
    "specific",
    "issue",
    "last",
    "training",
    "videos",
    "building",
    "transformer",
    "kind",
    "made",
    "errors",
    "pointed",
    "like",
    "two",
    "lines",
    "code",
    "kind",
    "needed",
    "change",
    "throughout",
    "network",
    "line",
    "reshaping",
    "tensors",
    "think",
    "pi",
    "torch",
    "tried",
    "values",
    "like",
    "reshape",
    "would",
    "completely",
    "discombobulated",
    "tensors",
    "needed",
    "permutation",
    "layers",
    "minor",
    "technical",
    "issue",
    "major",
    "one",
    "sense",
    "stalled",
    "long",
    "time",
    "really",
    "grateful",
    "know",
    "reached",
    "community",
    "responded",
    "well",
    "also",
    "wanted",
    "give",
    "shout",
    "account",
    "ping",
    "ng",
    "sorry",
    "mispronouncing",
    "anything",
    "recommended",
    "exact",
    "solution",
    "also",
    "detailed",
    "case",
    "thank",
    "much",
    "well",
    "shows",
    "well",
    "community",
    "respond",
    "even",
    "slack",
    "chat",
    "gpt",
    "could",
    "really",
    "help",
    "thank",
    "much",
    "let",
    "go",
    "part",
    "actually",
    "instantiating",
    "transformer",
    "thing",
    "really",
    "want",
    "point",
    "going",
    "use",
    "one",
    "encoder",
    "one",
    "decoder",
    "layer",
    "going",
    "simplest",
    "simplest",
    "transformer",
    "neural",
    "networks",
    "could",
    "train",
    "faster",
    "see",
    "reasonable",
    "results",
    "reason",
    "training",
    "one",
    "half",
    "hours",
    "trained",
    "10",
    "epochs",
    "scroll",
    "see",
    "10",
    "epochs",
    "data",
    "set",
    "around",
    "200",
    "000",
    "english",
    "sentences",
    "translate",
    "language",
    "called",
    "kannada",
    "training",
    "epoch",
    "printed",
    "100",
    "epochs",
    "time",
    "let",
    "get",
    "good",
    "part",
    "transformer",
    "inference",
    "character",
    "language",
    "model",
    "generating",
    "one",
    "character",
    "time",
    "see",
    "everything",
    "going",
    "within",
    "loop",
    "generating",
    "one",
    "character",
    "time",
    "generate",
    "token",
    "append",
    "sentence",
    "keep",
    "generate",
    "end",
    "token",
    "signify",
    "end",
    "sentence",
    "get",
    "examples",
    "let",
    "look",
    "say",
    "try",
    "translate",
    "day",
    "starts",
    "translation",
    "gives",
    "sentence",
    "says",
    "um",
    "although",
    "translate",
    "exactly",
    "know",
    "sentence",
    "least",
    "kind",
    "see",
    "commonalities",
    "retained",
    "like",
    "lasts",
    "two",
    "words",
    "last",
    "two",
    "words",
    "retained",
    "way",
    "two",
    "words",
    "means",
    "corresponds",
    "part",
    "sentence",
    "kind",
    "gets",
    "part",
    "sentence",
    "right",
    "clearly",
    "distorts",
    "entire",
    "meaning",
    "quite",
    "getting",
    "everything",
    "correctly",
    "attribute",
    "cases",
    "first",
    "thing",
    "like",
    "model",
    "simple",
    "like",
    "one",
    "encoder",
    "one",
    "decoder",
    "increase",
    "number",
    "encoder",
    "decoder",
    "components",
    "probably",
    "might",
    "able",
    "pick",
    "idiosyncrasies",
    "language",
    "probably",
    "biggest",
    "reason",
    "also",
    "try",
    "increasing",
    "training",
    "set",
    "increasing",
    "number",
    "epochs",
    "training",
    "time",
    "continuing",
    "video",
    "want",
    "tell",
    "sponsor",
    "taro",
    "social",
    "platform",
    "help",
    "software",
    "engineers",
    "grow",
    "career",
    "say",
    "land",
    "us",
    "job",
    "could",
    "really",
    "hard",
    "navigate",
    "career",
    "tough",
    "get",
    "good",
    "career",
    "advice",
    "taro",
    "facilitates",
    "discussions",
    "whether",
    "entry",
    "level",
    "senior",
    "part",
    "discussions",
    "get",
    "advice",
    "software",
    "engineers",
    "across",
    "many",
    "companies",
    "many",
    "questions",
    "wish",
    "could",
    "asked",
    "someone",
    "past",
    "advance",
    "career",
    "really",
    "never",
    "found",
    "good",
    "form",
    "think",
    "taro",
    "good",
    "place",
    "machine",
    "learning",
    "engineer",
    "overlap",
    "software",
    "engineers",
    "platform",
    "many",
    "machine",
    "learning",
    "engineering",
    "questions",
    "moment",
    "best",
    "answer",
    "questions",
    "still",
    "whenever",
    "think",
    "community",
    "really",
    "nice",
    "part",
    "still",
    "looking",
    "premium",
    "community",
    "software",
    "engineers",
    "part",
    "consider",
    "signing",
    "taro",
    "using",
    "link",
    "description",
    "get",
    "20",
    "annual",
    "purchase",
    "thanks",
    "listening",
    "let",
    "get",
    "back",
    "video",
    "second",
    "sentence",
    "truth",
    "actual",
    "translation",
    "whereas",
    "generated",
    "translation",
    "really",
    "meaningful",
    "sentence",
    "see",
    "commonalities",
    "see",
    "ume",
    "generated",
    "well",
    "means",
    "like",
    "hege",
    "satya",
    "truth",
    "translates",
    "truth",
    "generate",
    "part",
    "see",
    "looking",
    "two",
    "examples",
    "kind",
    "see",
    "definitely",
    "learning",
    "something",
    "although",
    "might",
    "complex",
    "enough",
    "pick",
    "everything",
    "sentences",
    "example",
    "name",
    "j",
    "translate",
    "translates",
    "see",
    "commonalities",
    "hesuru",
    "means",
    "name",
    "nana",
    "nanu",
    "close",
    "means",
    "pick",
    "name",
    "ajay",
    "also",
    "although",
    "overall",
    "translation",
    "see",
    "words",
    "actually",
    "common",
    "correct",
    "one",
    "interesting",
    "care",
    "actual",
    "translation",
    "gave",
    "punctuation",
    "like",
    "reason",
    "would",
    "actual",
    "translation",
    "actually",
    "close",
    "translation",
    "initial",
    "sentence",
    "bad",
    "pretty",
    "good",
    "next",
    "best",
    "thing",
    "ever",
    "whereas",
    "know",
    "generates",
    "sentence",
    "actual",
    "translation",
    "sentence",
    "translates",
    "unusual",
    "though",
    "meanings",
    "kind",
    "see",
    "commonalities",
    "probably",
    "interesting",
    "example",
    "throughout",
    "lot",
    "wanted",
    "translate",
    "actual",
    "translation",
    "projected",
    "translation",
    "translator",
    "different",
    "meanings",
    "means",
    "whereas",
    "means",
    "heard",
    "something",
    "although",
    "different",
    "meanings",
    "though",
    "see",
    "character",
    "character",
    "generation",
    "translator",
    "performing",
    "actually",
    "extremely",
    "well",
    "thing",
    "translator",
    "eyes",
    "got",
    "wrong",
    "eat",
    "k",
    "like",
    "two",
    "alphabets",
    "like",
    "thing",
    "different",
    "entire",
    "translation",
    "barring",
    "small",
    "like",
    "la",
    "luz",
    "either",
    "way",
    "one",
    "two",
    "characters",
    "things",
    "wrong",
    "translator",
    "eyes",
    "translation",
    "actually",
    "pretty",
    "good",
    "one",
    "kind",
    "made",
    "think",
    "fact",
    "okay",
    "character",
    "translation",
    "general",
    "word",
    "translations",
    "might",
    "actually",
    "perform",
    "better",
    "caveat",
    "using",
    "word",
    "translations",
    "transformer",
    "need",
    "much",
    "larger",
    "vocabulary",
    "opposed",
    "see",
    "scroll",
    "see",
    "like",
    "length",
    "possible",
    "characters",
    "possibly",
    "generated",
    "tokens",
    "generated",
    "see",
    "small",
    "set",
    "values",
    "maybe",
    "like",
    "know",
    "maybe",
    "hot",
    "fifty",
    "100",
    "tokens",
    "something",
    "like",
    "words",
    "possible",
    "words",
    "list",
    "entire",
    "list",
    "would",
    "explode",
    "tens",
    "thousands",
    "many",
    "words",
    "need",
    "generated",
    "always",
    "larger",
    "vocabulary",
    "size",
    "interpretable",
    "values",
    "need",
    "like",
    "much",
    "complex",
    "systems",
    "like",
    "need",
    "probably",
    "complex",
    "translator",
    "also",
    "way",
    "parameters",
    "order",
    "order",
    "account",
    "know",
    "words",
    "words",
    "though",
    "sentence",
    "length",
    "technically",
    "decrease",
    "provided",
    "like",
    "maximum",
    "sentence",
    "length",
    "200",
    "characters",
    "number",
    "words",
    "sentence",
    "like",
    "200",
    "characters",
    "200",
    "words",
    "could",
    "know",
    "dozen",
    "something",
    "like",
    "cap",
    "interesting",
    "one",
    "says",
    "click",
    "would",
    "translation",
    "like",
    "actual",
    "translation",
    "click",
    "click",
    "click",
    "click",
    "click",
    "click",
    "click",
    "click",
    "marty",
    "although",
    "get",
    "last",
    "part",
    "right",
    "click",
    "marty",
    "get",
    "loves",
    "click",
    "click",
    "click",
    "click",
    "click",
    "funny",
    "understanding",
    "task",
    "least",
    "extent",
    "thing",
    "mall",
    "translation",
    "well",
    "translation",
    "generated",
    "least",
    "got",
    "part",
    "generate",
    "anything",
    "else",
    "translation",
    "generated",
    "correct",
    "absolutely",
    "fumbles",
    "one",
    "today",
    "um",
    "idea",
    "generates",
    "says",
    "guess",
    "like",
    "common",
    "phrase",
    "see",
    "everywhere",
    "english",
    "also",
    "language",
    "canada",
    "seeing",
    "like",
    "kinds",
    "forms",
    "see",
    "like",
    "oh",
    "probably",
    "tries",
    "mahdi",
    "like",
    "every",
    "single",
    "everywhere",
    "tries",
    "create",
    "scenario",
    "interesting",
    "completely",
    "fumbles",
    "despite",
    "know",
    "english",
    "see",
    "kind",
    "like",
    "sentence",
    "extra",
    "word",
    "uh",
    "interesting",
    "note",
    "phrase",
    "sentence",
    "actually",
    "generated",
    "almost",
    "perfectly",
    "well",
    "something",
    "something",
    "common",
    "common",
    "sentence",
    "general",
    "common",
    "phrase",
    "probably",
    "well",
    "last",
    "part",
    "also",
    "interesting",
    "one",
    "word",
    "street",
    "generated",
    "translation",
    "topic",
    "translation",
    "kind",
    "semantically",
    "relate",
    "actually",
    "means",
    "little",
    "idiom",
    "let",
    "go",
    "insights",
    "probably",
    "going",
    "give",
    "information",
    "tips",
    "building",
    "transformer",
    "language",
    "first",
    "wanted",
    "say",
    "create",
    "translator",
    "language",
    "understand",
    "ideally",
    "much",
    "easier",
    "see",
    "transformer",
    "things",
    "correctly",
    "also",
    "things",
    "incorrectly",
    "generating",
    "insight",
    "think",
    "important",
    "better",
    "understand",
    "language",
    "case",
    "people",
    "saying",
    "use",
    "canada",
    "understand",
    "properly",
    "evaluate",
    "otherwise",
    "would",
    "able",
    "come",
    "insights",
    "piggybacking",
    "think",
    "pretty",
    "important",
    "insight",
    "really",
    "seen",
    "anywhere",
    "describe",
    "training",
    "typically",
    "english",
    "character",
    "set",
    "known",
    "call",
    "alphabet",
    "every",
    "character",
    "kind",
    "phonetic",
    "representation",
    "language",
    "like",
    "canada",
    "alpha",
    "syllabary",
    "individual",
    "units",
    "actually",
    "complete",
    "like",
    "syllables",
    "means",
    "even",
    "though",
    "like",
    "example",
    "word",
    "think",
    "typed",
    "character",
    "write",
    "english",
    "would",
    "like",
    "uh",
    "know",
    "like",
    "accent",
    "top",
    "would",
    "like",
    "multiple",
    "characters",
    "english",
    "single",
    "character",
    "kannada",
    "language",
    "however",
    "dealing",
    "tokens",
    "way",
    "tokenizing",
    "data",
    "also",
    "treating",
    "like",
    "multiple",
    "tokens",
    "plus",
    "ah",
    "even",
    "though",
    "current",
    "language",
    "actually",
    "supposed",
    "one",
    "character",
    "treating",
    "two",
    "characters",
    "semantically",
    "makes",
    "sense",
    "create",
    "tokenizer",
    "divvy",
    "entire",
    "know",
    "canada",
    "like",
    "word",
    "sub",
    "characters",
    "rather",
    "divvy",
    "kannada",
    "word",
    "actual",
    "kannada",
    "characters",
    "may",
    "may",
    "combination",
    "two",
    "characters",
    "also",
    "alpha",
    "silveries",
    "type",
    "script",
    "confined",
    "language",
    "canada",
    "many",
    "alpha",
    "syllabares",
    "understanding",
    "writing",
    "style",
    "may",
    "actually",
    "create",
    "translator",
    "meaningful",
    "highly",
    "recommend",
    "try",
    "another",
    "another",
    "insight",
    "mentioned",
    "kind",
    "similar",
    "described",
    "generating",
    "one",
    "character",
    "time",
    "smaller",
    "vocabulary",
    "longer",
    "sentence",
    "length",
    "play",
    "generating",
    "like",
    "word",
    "time",
    "much",
    "larger",
    "vocabulary",
    "smaller",
    "sentence",
    "length",
    "good",
    "mix",
    "worlds",
    "use",
    "something",
    "called",
    "byte",
    "pair",
    "tokenizations",
    "bite",
    "pairing",
    "coatings",
    "like",
    "sub",
    "words",
    "issue",
    "hard",
    "create",
    "bike",
    "pair",
    "encoding",
    "certain",
    "languages",
    "know",
    "really",
    "great",
    "research",
    "online",
    "presence",
    "hard",
    "find",
    "one",
    "language",
    "canada",
    "hence",
    "went",
    "character",
    "tokenizations",
    "illustrate",
    "concepts",
    "ideas",
    "able",
    "create",
    "like",
    "bike",
    "pairing",
    "coatings",
    "languages",
    "input",
    "outputs",
    "think",
    "might",
    "like",
    "good",
    "starting",
    "point",
    "fact",
    "think",
    "exactly",
    "happening",
    "main",
    "paper",
    "lot",
    "research",
    "associated",
    "generative",
    "models",
    "days",
    "another",
    "one",
    "make",
    "sure",
    "training",
    "set",
    "large",
    "variety",
    "words",
    "general",
    "could",
    "see",
    "illustrated",
    "lot",
    "sentences",
    "like",
    "right",
    "marty",
    "like",
    "lot",
    "sentences",
    "kind",
    "go",
    "tobacco",
    "modi",
    "fact",
    "look",
    "data",
    "set",
    "actually",
    "10",
    "000",
    "cases",
    "least",
    "like",
    "well",
    "like",
    "millions",
    "records",
    "like",
    "10",
    "000",
    "cases",
    "least",
    "entire",
    "word",
    "called",
    "matabeko",
    "common",
    "phrase",
    "would",
    "suggest",
    "try",
    "plot",
    "every",
    "single",
    "word",
    "frequency",
    "counts",
    "get",
    "idea",
    "kind",
    "data",
    "set",
    "dealing",
    "whether",
    "catered",
    "like",
    "news",
    "government",
    "articles",
    "politics",
    "catered",
    "like",
    "general",
    "random",
    "sentences",
    "ideally",
    "would",
    "case",
    "general",
    "translators",
    "one",
    "technical",
    "increasing",
    "number",
    "encoder",
    "decoder",
    "units",
    "used",
    "one",
    "keeping",
    "simple",
    "ideally",
    "try",
    "encoder",
    "decoder",
    "units",
    "pick",
    "complexities",
    "intricacies",
    "languages",
    "overall",
    "yeah",
    "model",
    "definitely",
    "learned",
    "something",
    "use",
    "know",
    "languages",
    "instead",
    "canada",
    "well",
    "still",
    "hope",
    "going",
    "available",
    "github",
    "videos",
    "illustrate",
    "general",
    "concept",
    "transformers",
    "work",
    "want",
    "actually",
    "going",
    "thinking",
    "making",
    "full",
    "video",
    "end",
    "end",
    "start",
    "finish",
    "explaining",
    "transformer",
    "neural",
    "networks",
    "want",
    "see",
    "mega",
    "video",
    "please",
    "comment",
    "see",
    "oh",
    "gosh",
    "want",
    "see",
    "mega",
    "video",
    "similar",
    "like",
    "12",
    "videos",
    "probably",
    "know",
    "much",
    "continuous",
    "flow",
    "make",
    "thank",
    "much",
    "watching",
    "going",
    "continuing",
    "wonderful",
    "journey",
    "worlds",
    "landscape",
    "artificial",
    "intelligence",
    "thank",
    "much",
    "see",
    "next",
    "time"
  ],
  "keywords": [
    "another",
    "code",
    "building",
    "translator",
    "using",
    "transformer",
    "neural",
    "network",
    "last",
    "videos",
    "video",
    "though",
    "wanted",
    "actually",
    "go",
    "get",
    "training",
    "model",
    "also",
    "want",
    "insights",
    "see",
    "entire",
    "data",
    "everything",
    "give",
    "account",
    "point",
    "issue",
    "kind",
    "like",
    "two",
    "think",
    "values",
    "would",
    "one",
    "time",
    "really",
    "know",
    "community",
    "well",
    "case",
    "thank",
    "much",
    "even",
    "could",
    "let",
    "part",
    "thing",
    "going",
    "use",
    "encoder",
    "decoder",
    "reason",
    "10",
    "epochs",
    "set",
    "200",
    "000",
    "english",
    "sentences",
    "translate",
    "language",
    "called",
    "kannada",
    "good",
    "character",
    "generating",
    "generate",
    "sentence",
    "end",
    "say",
    "try",
    "translation",
    "says",
    "although",
    "least",
    "commonalities",
    "words",
    "way",
    "means",
    "right",
    "cases",
    "number",
    "probably",
    "might",
    "able",
    "pick",
    "increasing",
    "taro",
    "software",
    "engineers",
    "career",
    "hard",
    "many",
    "questions",
    "learning",
    "still",
    "truth",
    "actual",
    "whereas",
    "generated",
    "translates",
    "something",
    "complex",
    "example",
    "name",
    "common",
    "interesting",
    "pretty",
    "meanings",
    "lot",
    "different",
    "characters",
    "things",
    "fact",
    "general",
    "word",
    "need",
    "larger",
    "vocabulary",
    "length",
    "tokens",
    "click",
    "marty",
    "phrase",
    "canada",
    "every",
    "single",
    "create",
    "understand",
    "ideally",
    "insight",
    "alpha",
    "units",
    "may",
    "languages"
  ]
}