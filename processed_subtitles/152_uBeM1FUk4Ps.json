{
  "text": "The following content is\nprovided under a Creative\nCommons license.\nYour support will help\nMIT OpenCourseWare\ncontinue to offer high quality\neducational resources for free.\nTo make a donation or\nview additional materials\nfrom hundreds of MIT courses,\nvisit MIT OpenCourseWare\nat ocw.mit.edu.\nPROFESSOR: OK.\nWell, last time I\nwas lecturing, we\nwere talking about\nregression analysis.\nAnd we finished up talking\nabout estimation methods\nfor fitting regression models.\nI want to recap the method\nof maximum likelihood,\nbecause this is really\nthe primary estimation\nmethod in statistical\nmodeling that you start with.\nAnd so let me just\nreview where we were.\nWe have a normal linear\nregression model.\nA dependent variable\ny is explained\nby a linear combination\nof independent variables\ngiven by a regression\nparameter beta.\nAnd we assume that there are\nerrors about all the cases\nwhich are independent\nidentically distributed\nnormal random variables.\nSo because of that relationship,\nthe dependent variable vector\ny, which is an\nn-vector, for n cases,\nis a multivariate\nnormal random variable.\nNow, the likelihood function is\nequal to the density function\nfor the data.\nAnd there's some\nambiguity really\nabout how one manipulates\nthe likelihood function.\nThe likelihood function\nbecomes defined once we've\nobserved a sample of data.\nSo in this expression for\nthe likelihood function\nas a function of beta\nand sigma squared,\nwe're considering evaluating\nthe probability density\nfunction for the\ndata conditional\non the unknown parameters.\nSo if this were simply a\nunivariate normal distribution\nwith some unknown mean\nand variance, then\nwhat we would have is\njust a bell curve for mu\ncentered around a\nsingle observation y,\nif you look at the\nlikelihood function\nand how it varies with\nthe underlying mean\nof the normal distribution.\nSo this likelihood\nfunction is-- well,\nthe challenge really\nin maximum estimation\nis really calculating\nand computing\nthe likelihood function.\nAnd with normal linear\nregression models,\nit's very easy.\nNow, the maximum\nlikelihood estimates\nare those values that\nmaximize this function.\nAnd the question is, why\nare those good estimates\nof the underlying parameters?\nWell, what those\nestimates do is they\nare the parameter values for\nwhich the observed data is\nmost likely.\nSo we're able to scale\nthe unknown parameters\nby how likely those parameters\ncould have generated these data\nvalues.\nSo let's look at the\nlikelihood function\nfor this normal linear\nregression model.\nThese first two lines here are\nhighlighting-- the first line\nis highlighting that\nour response variable\nvalues are independent.\nThey're conditionally\nindependent\ngiven the unknown parameters.\nAnd so the density of the\nfull vector of y's is simply\nthe product of the density\nfunctions for those components.\nAnd because this is a normal\nlinear regression model,\neach of the y_i's is\nnormally distributed.\nSo what's in there\nis simply the density\nfunction of a normal random\nvariable with mean given\nby the beta sum of independent\nvariables for each i,\ncase i, given by the\nregression parameters.\nAnd that expression\nbasically can be expressed\nin matrix form this way.\nAnd what we have is\nthe likelihood function\nends up being a function\nof our Q of beta, which\nwas our least squares criteria.\nSo the least squares\nestimation is\nequivalent to maximum likelihood\nestimation for the regression\nparameters if we have a normal\nlinear regression model.\nAnd there's this\nextra term, minus n.\nWell, actually, if we're going\nto maximize the likelihood\nfunction, we can also maximize\nthe log of the likelihood\nfunction, because that's\njust a monotone function\nof the likelihood.\nAnd it's easier to maximize the\nlog of the likelihood function\nwhich is expressed here.\nAnd so we're able to\nmaximize over beta\nby minimizing Q of beta.\nAnd then we can maximize\nover sigma squared\ngiven our estimate for beta.\nAnd that's achieved by\ntaking the derivative\nof the log-likelihood with\nrespect to sigma squared.\nSo we basically have this\nfirst order condition\nthat finds the\nmaximum because things\nare appropriately convex.\nAnd taking that derivative\nand solving for zero,\nwe basically get\nthis expression.\nSo this is just\ntaking the derivative\nof the log-likelihood with\nrespect to sigma squared.\nAnd you'll notice\nhere I'm taking\nthe derivative with\nrespect to sigma squared\nas a parameter, not sigma.\nAnd that gives us that\nthe maximum likelihood\nestimate of the error variance\nis Q of beta hat over n.\nSo this is the sum of the\nsquared residuals divided by n.\nNow, I emphasize here\nthat that's biased.\nWho can tell me\nwhy that's biased\nor why it ought to be biased?\nAUDIENCE: [INAUDIBLE].\nPROFESSOR: OK.\nWell, it should be n\nminus 1 if we're actually\nestimating one parameter.\nSo if the independent variables\nwere, say, a constant, 1,\nso we're just estimating a\nsample from a normal with mean\nbeta 1 corresponding to\nthe units vector of the X,\nthen we would have a one\ndegree of freedom correction\nto the residuals to get\nan unbiased estimator.\nBut what if we\nhave p parameters?\nWell, let me ask you this.\nWhat if we had n parameters\nin our regression model?\nWhat would happen if\nwe had a full rank n\nindependent variable matrix\nand n independent observations?\nAUDIENCE: [INAUDIBLE].\nPROFESSOR: Yes, you'd have\nan exact fit to the data.\nSo this estimate would be 0.\nAnd so clearly, if\nthe data do arise\nfrom a normal linear regression\nmodel, 0 is not unbiased.\nAnd you need to have\nsome correction.\nTurns out you need\nto divide by n\nminus the rank of the X\nmatrix, the degrees of freedom\nin the model, to get\na biased estimate.\nSo this is an important\nissue, highlights\nhow the more parameters you add\nin the model, the more precise\nyour fitted values are.\nIn a sense, there's\ndangers of curve fitting\nwhich you want to avoid.\nBut the maximum likelihood\nestimates, in fact, are biased.\nYou just have to\nbe aware of that.\nAnd when you're using\ndifferent software,\nfitting different\nmodels, you need\nto know whether there are\nvarious corrections be\nmade for biasedness or not.\nSo this solves the\nestimation problem\nfor normal linear\nregression models.\nAnd when we have normal\nlinear regression\nmodels, the theorem we\nwent through last time--\nthis is very important.\nLet me just go back and\nhighlight that for you.\nThis theorem right here.\nThis is really a very\nimportant theorem\nindicating what is the\ndistribution of the least\nsquares, now the maximum\nlikelihood estimates\nof our regression model?\nThey are normally distributed.\nAnd the residuals, sum\nof squares, have a chi\nsquared distribution\nwith degrees of freedom\ngiven by n minus p.\nAnd we can look at how\nmuch signal to noise\nthere is in estimating\nour regression\nparameters by calculating a t\nstatistic, which is take away\nfrom an estimate its\nexpected value, its mean,\nand divide through by an\nestimate of the variability\nin standard deviation units.\nAnd that will have\na t distribution.\nSo that's a critical\nway to assess\nthe relevance of different\nexplanatory variables\nin our model.\nAnd this approach will apply\nwith maximum likelihood\nestimation in all\nkinds of models\napart from normal linear\nregression models.\nIt turns out maximum\nlikelihood estimates generally\nare asymptotically\nnormally distributed.\nAnd so these properties here\nwill apply for those models\nas well.\nSo let's finish up these\nnotes on estimation\nby talking about\ngeneralized M estimation.\nSo what we want to consider is\nestimating unknown parameters\nby minimizing some\nfunction, Q of beta,\nwhich is a sum of evaluations\nof another function h,\nevaluated for each of\nthe individual cases.\nAnd choosing h to take on\ndifferent functional forms\nwill define different\nkinds of estimators.\nWe've seen how when h\nis simply the square\nof the case minus its\nregression prediction,\nthat leads to least squares,\nand in fact, maximum likelihood\nestimation, as we saw before.\nRather than taking the\nsquare of the residual,\nthe fitted residual,\nwe could take simply\nthe modulus of that.\nAnd so that would be the\nmean absolute deviation.\nSo rather than summing\nthe squared deviations\nfrom the mean, we could\nsum the absolute deviations\nfrom the mean.\nNow, from a\nmathematical standpoint,\nif we want to solve\nfor those estimates,\nhow would you go\nabout doing that?\nWhat methodology would you\nuse to maximize this function?\nWell, we try and apply\nbasically the same principles\nof if this is a\nconvex function, then\nwe just want to take derivatives\nof that and solve for that\nbeing equal to 0.\nSo what happens when\nyou take the derivative\nof the modulus of y minus xi\nbeta with respect to beta?\nAUDIENCE: [INAUDIBLE].\nPROFESSOR: What did you say?\nWhat did you say?\nAUDIENCE: Yeah, it's\nnot [INAUDIBLE].\nThe first [INAUDIBLE]\nderivative is not continuous.\nPROFESSOR: OK.\nWell, this is not\na smooth function.\nBut let me just plot x_i beta\nhere, and y_i minus that.\nBasically, this is going\nto be a function that\nhas slope 1 when it's positive\nand slope minus 1 when\nit's negative.\nAnd so that will be true,\ncomponent-wise, or for the y.\nSo what we end up\nwanting to do is\nfind the value of the\nregression estimate\nthat minimizes the\nsum of predictions\nthat are below the estimate plus\nthe sum of the predictions that\nare above the estimate given\nby the regression line.\nAnd that solves the problem.\nNow, with the maximum\nlikelihood estimation,\none can plug in minus log the\ndensity of y_i given beta, x\nand sigma_i squared.\nAnd that function simply sums\nto the log of the joint density\nfor all the data.\nSo that works as well.\nWith robust M estimators, we can\nconsider another function chi\nwhich can be defined to have\ngood properties with estimates.\nAnd there's a whole theory\nof robust estimation--\nit's very rich-- which\ntalks about how best\nto specify this chi function.\nNow, one of the problems\nwith least squares estimation\nis that the squares\nof very large values\nare very, very\nlarge in magnitude.\nSo there's perhaps\nan undue influence\nof very large values, very large\nresiduals under least squares\nestimation and maximum\n[INAUDIBLE] estimation.\nSo robust estimators\nallow you to control that\nby defining the\nfunction differently.\nFinally, there are\nquantile estimators,\nwhich extend the mean\nabsolute deviation criterion.\nAnd so if we consider\nthe h function\nto be basically a\nmultiple of the deviation\nif the residual is positive\nand a different multiple,\na complementary multiple if\nthe derivation, the residual,\nis less than 0,\nthen by varying tau,\nyou end up getting\nquantile estimators, where\nwhat you're doing is minimizing\nthe estimate of the tau\nquantile.\nSo this general\nclass of M estimators\nencompasses most\nestimators that we will\nencounter in fitting models.\nSo that finishes the technical\nor the mathematical discussion\nof regression analysis.\nLet me highlight for you--\nthere's a case study that I\ndragged to the desktop here.\nAnd I wanted to find that.\nLet me find that.\nThere's a case study that's been\nadded to the course website.\nAnd this first one is on\nlinear regression models\nfor asset pricing.\nAnd I want you to\nread through that just\nto see how it applies to\nfitting various simple linear\nregression models.\nAnd enter full screen.\nThis case study begins by\nintroducing the capital asset\npricing model, which\nbasically suggests\nthat if you look at the\nreturns on any stocks\nin an efficient\nmarket, then those\nshould depend on the return\nof the overall market\nbut scaled by how\nrisky the stock is.\nAnd so if one looks\nat basically what\nthe return is on the\nstock on the right scale,\nyou should have a simple\nlinear regression model.\nSo here, we just look at\na time series for GE stock\nin the S&P 500.\nAnd the case study guide\nthrough how you can actually\ncollect this data\non the web using R.\nAnd so the case notes\nprovide those details.\nThere's also the\nthree-month treasury rate\nwhich is collected.\nAnd so if you're\nthinking about return\non the stock versus return\non the index, well, what's\nreally of interest is the excess\nreturn over a risk-free rate.\nAnd the efficient\nmarkets models,\nbasically the excess\nreturn of a stock\nis related to the excess\nreturn of the market as\ngiven by a linear\nregression model.\nSo we can fit this model.\nAnd here's a plot of the excess\nreturns on a daily basis for GE\nstock versus the market.\nSo that looks like a\nnice sort of point cloud\nfor which a linear\nmodel might fit well.\nAnd it does.\nWell, there are\nregression diagnostics,\nwhich I'll get to-- well, there\nare regression diagnostics\nwhich are detailed in the\nproblem set, where we're\nlooking at how influential are\nindividual observations, what's\ntheir impact on\nregression parameters.\nThis display here\nbasically highlights\nwith a very simple\nlinear regression\nmodel what are the\ninfluential data points.\nAnd so I've highlighted\nin red those values\nwhich are influential.\nNow, if you look at the\ndefinition of leverage\nin a linear model,\nit's very simple.\nA simple linear model is\njust those observations that\nare very far from the\nmean have large leverage.\nAnd so you can confirm\nthat with your answers\nto the problem set.\nThis x indicates a\nsignificantly influential point\nin terms of the\nregression parameters\ngiven by Cook's distance.\nAnd that definition is also\ngiven in the case notes.\nAUDIENCE: [INAUDIBLE].\nPROFESSOR: By computing\nthe individual\nleverages with a function\nthat's given here,\nand by selecting out those\nthat exceed a given magnitude.\nNow, with this very,\nvery simple model\nof stocks depending\non one unknown factor,\nrisk factor given the market.\nIn modeling equity\nreturns, there\nare many different factors that\ncan have an impact on returns.\nSo what I've done\nin the case study\nis to look at adding\nanother factor which is just\nthe return on crude oil.\nAnd so-- I need to go down here.\nSo let me highlight\nsomething for you here.\nWith GE stock, what would you\nexpect the impact of, say,\na high return on crude oil to\nbe on the return of GE stock?\nWould you expect it to\nbe positively related\nor negatively related?\nOK.\nWell, GE is a stock that's\njust a broad stock invested\nin many different industries.\nAnd it really reflects the\noverall market, to some extent.\nMany years ago,\n10, 15 years ago,\nGE represented maybe 3% of\nthe GNP of the US market.\nSo it was really highly related\nto how well the market does.\nNow, crude oil is a commodity.\nAnd oil is used to drive cars,\nto fuel energy production.\nSo if you have an\nincrease in oil prices,\nthen the cost of essentially\ndoing business goes up.\nSo it is associated with\nan inflation factor.\nPrices are rising.\nSo if you can see here,\nthe regression estimate,\nif we add in a factor of\nthe return on crude oil,\nit's negative 0.03.\nAnd it has a t value\nof minus 3.561.\nSo in fact, the market, in\na sense, over this period,\nfor this analysis, was not\nefficient in explaining\nthe return on GE; crude oil\nis another independent factor\nthat helps explain returns.\nSo that's useful to know.\nAnd if you are clever about\ndefining and identifying\nand evaluating\ndifferent factors,\nyou can build\nfactor asset pricing\nmodels that are\nvery, very useful\nfor investing and trading.\nNow, as a comparison\nto this case study,\nalso applied the same\nanalysis to Exxon Mobil.\nNow, Exxon Mobil\nis an oil company.\nSo let me highlight this here.\nWe basically are\nfitting this model.\nNow let's highlight it.\nHere, if we consider\nthis two-factor model,\nthe regression\nparameter corresponding\nto the crude oil factor is\nplus 0.13 with a t value of 16.\nSo crude oil definitely\nhas an impact\non the return of Exxon Mobil,\nbecause it goes up and down\nwith oil prices.\nThis case study closes\nwith a scatter plot\nof the independent variables\nand highlighting where\nthe influential values are.\nAnd so just in the same way that\nwith a simple linear regression\nit was those that were far\naway from the mean of the data\nwere influential, in a\nmultivariate setting-- here,\nit's bivariate-- the\ninfluential observations\nare those that are very\nfar away from the centroid.\nAnd if you look at one of the\nproblems in the problem set,\nit actually goes\nthrough and you can\nsee where these\nleveraged values are\nand how it indicates influences\nassociated with the Mahalanobis\ndistance of cases\nfrom the centroid\nof the independent variables.\nSo if you're a visual\ntype mathematician as\nopposed to an algebraic\ntype mathematician,\nI think these\nkinds of graphs are\nvery helpful in understanding\nwhat is really going on.\nAnd the degree of influence\nis associated with the fact\nthat we're basically taking\nleast squares estimates,\nso we have the quadratic\nform associated\nwith the overall process.\nThere's another\ncase study that I'll\nbe happy to discuss after\nclass or during office hours.\nI don't think we have time\ntoday during the lecture.\nBut it concerns\nexchange rate regimes.\nAnd the second case study\nlooks at the Chinese yuan,\nwhich was basically pegged\nto the dollar for many years.\nAnd then I guess through\npolitical influence\nfrom other countries,\nthey started\nto let the yuan vary\nfrom the dollar,\nbut perhaps pegged\nit to some basket\nof securities-- of currencies.\nAnd so how would you determine\nwhat that basket of currencies\nis?\nWell, there are\nregression methods\nthat have been\ndeveloped by economists\nthat help you do that.\nAnd that case study goes\nthrough the analysis of that.\nSo check that out to see how\nyou can get immediate access\nto currency data and be\nfitting these regression models\nand looking at the\ndifferent results\nand trying to evaluate those.\nSo let's turn now\nto the main topic--\nlet's see here-- which\nis time series analysis.\nToday in the rest\nof the lecture,\nI want to talk about univariate\ntime series analysis.\nAnd so we're thinking of\nbasically a random variable\nthat is observed over time and\nit's a discrete time process.\nAnd we'll introduce you\nto the Wold representation\ntheorem and definitions\nof stationarity\nand its relationship there.\nThen, look at the classic\nmodels of autoregressive\nmoving average models.\nAnd then extending those\nto non-stationarity\nwith integrated autoregressive\nmoving average models.\nAnd then finally, talk about\nestimating stationary models\nand how we test\nfor stationarity.\nSo let's begin from\nbasically first principles.\nWe have a stochastic process,\na discrete time stochastic\nprocess, X, which consists\nof random variables indexed\nby time.\nAnd we're thinking\nnow discrete time.\nThe stochastic behavior\nof this sequence\nis determined by specifying\nthe density or probability mass\nfunctions for all finite\ncollections of time indexes.\nAnd so if we could specify\nall finite.dimensional\ndistributions of\nthis process, we\nwould specify this\nprobability model\nfor the stochastic process.\nNow, this stochastic process\nis strictly stationary\nif the density function for\nany collection of times,\nt_1 through t_m, is equal to\nthe density function for a tau\ntranslation of that.\nSo the density function for any\nfinite-dimensional distribution\nis stationary, is constant\nunder arbitrary translations.\nSo that's a very\nstrong property.\nBut it's a reasonable\nproperty to ask for if you're\ndoing statistical modeling.\nAnd what do you want to do\nwhen you're estimating models?\nYou want to estimate\nthings that are constant.\nConstants are nice\nthings to estimate.\nAnd parameters of\nmodels are constant.\nSo we really want the underlying\nstructure of the distributions\nto be the same.\nThat was strict\nstationarity, which\nrequires knowledge of\nthe entire distribution\nof the stochastic process.\nWe're now going to introduce\na weaker definition, which\nis covariance stationarity.\nAnd a covariance\nstationary process\nhas a constant mean,\nmu; a constant variance,\nsigma squared; and a\ncovariance over increments tau,\ngiven by a function gamma of\ntau, that is also constant.\nGamma isn't a constant function,\nbut basically for all t,\ncovariance of X_t, X_(t+tau)\nis this gamma of tau function.\nAnd we also can introduce\nthe autocorrelation function\nof the stochastic\nprocess, rho of tau.\nAnd so the correlation\nof two random variables\nis the covariance of those\nrandom variables divided\nby the square root of the\nproduct of the variances.\nAnd Choongbum I think\nintroduced that a bit.\nin one of his lectures,\nwhere we were talking\nabout the correlation function.\nBut essentially, the\ncorrelation function\nis if you standardize the\ndata or the random variables\nto have mean 0-- so\nsubtract off the means\nand then divide through by\ntheir standard deviations.\nSo those translated variables\nhave mean 0 and variance 1.\nThen the correlation\ncoefficient is the covariance\nbetween those standardized\nrandom variables.\nSo this is going to come up\nagain and again in time series\nanalysis.\nNow, the Wold\nrepresentation theorem\nis a very, very powerful theorem\nabout covariance stationary\nprocesses.\nIt basically states that if\nwe have a zero-mean covariance\nstationary time\nseries, then it can\nbe decomposed into two\ncomponents with a very\nnice structure.\nBasically, X_t can be\ndecomposed into V_t plus S_t.\nV_t is going to be a linearly\ndeterministic process, meaning\nthat past values of\nV_t perfectly predict\nwhat V_t is going to be.\nSo this could be like a\nlinear trend or some fixed\nfunction of past values.\nIt's basically a\ndeterministic process.\nSo there's nothing\nrandom in V_t.\nIt's something that's\nfixed, without randomness.\nAnd S_t is a sum\nof coefficients,\npsi_i times eta_(t-i), where\nthe eta_t's are linearly\nunpredictable white noise.\nSo what we have is S_t\nis a weighted average\nof white noise with\ncoefficients given by the psi_i.\nAnd the coefficients psi_i\nare such that psi_0 is 1,\nand the sum of the\nsquared psi_i's is finite.\nAnd the white noise\neta_t-- what's white noise?\nIt has expectation zero.\nIt has variance, given by\nsigma squared, that's constant.\nAnd it has covariance across\ndifferent white noise elements\nthat's 0 for all t and s.\nSo eta_t's are uncorrelated\nwith themselves,\nand of course, they\nare uncorrelated\nwith the deterministic process.\nSo this is really a very,\nvery powerful concept.\nIf you are modeling\na process and it\nhas covariance\nstationarity, then there\nexists a representation\nlike this of the function.\nSo it's a very\ncompelling structure,\nwhich we'll see how it applies\nin different circumstances.\nNow, before getting into the\ndefinition of autoregressive\nmoving average\nmodels, I just want\nto give you an intuitive\nunderstanding of what's going\non with the Wold decomposition.\nAnd this, I think,\nwill help motivate\nwhy the Wold\ndecomposition should exist\nfrom a mathematical standpoint.\nSo consider just some\nunivariate stochastic process,\nsome time series X_t\nthat we want to model.\nAnd we believe that it's\ncovariance stationary.\nAnd so we want to\nspecify essentially\nthe Wold decomposition of that.\nWell, what we could\ndo is initialize\na parameter p, the number\nof past observations,\nin the linearly\ndeterministic term.\nAnd then estimate the linear\nprojection of X_t on the last p\nlag values.\nAnd so what I want to do\nis consider estimating\nthat relationship using\na sample of size n\nwith some ending point t_0\nless than or equal to T.\nAnd so we can consider y\nvalues like a response variable\nbeing given by the successive\nvalues of our time series.\nAnd so our response variables\ny_j can be considered to be x\nt_0 minus n plus j.\nAnd define a y vector and\na Z matrix as follows.\nSo we have values of our\nstochastic process in y.\nAnd then our Z matrix,\nwhich is essentially\na matrix of\nindependent variables,\nis just the lagged\nvalues of this process.\nSo let's apply\nordinary least squares\nto specify the projection.\nThis projection matrix\nshould be familiar now.\nAnd that basically gives\nus a prediction of y hat\ndepending on p lags.\nAnd we can compute the\nprojection residual\nfrom that fit.\nWell, we can conduct\ntime series methods\nto analyze these residuals,\nwhich we'll be introducing here\nin a few minutes, to specify\na moving average model.\nWe can then have estimates of\nthe underlying coefficients\npsi and estimates of\nthese residuals eta_t.\nAnd then we can evaluate whether\nthis is a good model or not.\nWhat does it mean to be\nan appropriate model?\nWell, the residual should\nbe orthogonal to longer lags\nthan t minus s, or\nlonger lags than p.\nSo we basically shouldn't\nhave any dependence\nof our residuals on lags\nof the stochastic process\nthat weren't included\nin the model.\nThose should be orthogonal.\nAnd the eta_t hats should be\nconsistent with white noise.\nSo those issues\ncan be evaluated.\nAnd if there's\nevidence otherwise,\nthen we can change the\nspecification of the model.\nWe can add additional lags.\nWe can add additional\ndeterministic variables\nif we can identify\nwhat those might be.\nAnd proceed with this process.\nBut essentially that is\nhow the Wold decomposition\ncould be implemented.\nAnd theoretically, as\nour sample gets large,\nif we're observing this time\nseries for a long time, then\nwell certainly the\nlimit of the projections\nas p, the number of lags\nwe include, gets large,\nshould be essentially\nthe projection\nof our data on its history.\nAnd that, in fact, is the\nprojection corresponding to,\ndefining, the\ncoefficient's psi_i.\nAnd so in the limit, that\nprojection will converge\nand it will converge\nin the sense\nthat the coefficients of\nthe projection definition\ncorrespond to the psi_i.\nAnd now if p goes to\ninfinity is required,\nnow p means that there's\nbasically a long term\ndependence in the process.\nBasically, it doesn't\nstop at a given lag.\nThe dependence\npersists over time.\nThen we may require\nthat p goes to infinity.\nNow, what happens when\np goes to infinity?\nWell, if you let p go\nto infinity too quickly,\nyou run out of\ndegrees of freedom\nto estimate your models.\nAnd so from an\nimplementation standpoint,\nyou need to let p/n\ngo to 0 so that you\nhave essentially more\ndata than parameters\nthat you're estimating.\nAnd so that is required.\nAnd in time series\nmodeling, what we\nlook for are models where\nfinite values of p are required.\nSo we're only estimating a\nfinite number of parameters.\nOr if we have a moving\naverage model which\nhas coefficients that\nare infinite in number,\nperhaps those can be defined by\na small number of parameters.\nSo we'll be looking for\nthat kind of feature\nin different models.\nLet's turn to talking\nabout the lag operator.\nThe lag operator is\na fundamental tool\nin time series models.\nWe consider the operator L\nthat shifts a time series back\nby one time increment.\nAnd applying this\noperator recursively,\nwe get, if it's operating\n0 times, there's no lag,\none time, there's\none lag, two times,\ntwo lags-- doing\nthat iteratively.\nAnd in thinking of these,\nwhat we're dealing with\nis like a transformation on\ninfinite dimensional space,\nwhere it's like\nthe identity matrix\nsort of shifted by\none element-- or not\nthe identity, but an element.\nIt's like the identity\nmatrix shifted\nby one column or two columns.\nSo anyway, inverses\nof these operators\nare well defined in terms\nof what we get from them.\nSo we can represent\nthe Wold representation\nin terms of these lag\noperators by saying\nthat our stochastic\nprocess X_t is\nequal to V_t plus this\npsi of L function,\nbasically a\nfunctional of the lag\noperator, which is a potentially\ninfinite-order polynomial\nof the lags.\nSo this notation is\nsomething that you\nneed to get very\nfamiliar with if you're\ngoing to be comfortable with\nthe different models that\nare introduced with\nARMA and ARIMA models.\nAny questions about that?\nNow relating to\nthis-- let me just\nintroduce now, because this\nwill come up somewhat later.\nBut there's the impulse\nresponse function\nof the covariance\nstationary process.\nIf we have a stochastic process\nX_t which is given by this Wold\nrepresentation, then\nyou can ask yourself\nwhat happens to the innovation\nat time t, which is eta_t,\nhow does that affect\nthe process over time?\nAnd so, OK, pretend that you are\nchairman of the Federal Reserve\nBank.\nAnd you're interested in the GNP\nor basically economic growth.\nAnd you're considering\nchanging interest rates\nto help the economy.\nWell, you'd like to\nknow what an impact is\nof your change in\nthis factor, how\nthat's going to affect the\nvariable of interest, perhaps\nGNP.\nNow, in this case,\nwe're thinking\nof just a simple covariance\nstationary stochastic process.\nIt's basically a process that\nis a random-- a weighted sum,\na moving average of\ninnovations eta_t.\nBut the question is, basically\nany covariance stationary\nprocess could be\nrepresented in this form.\nAnd the impulse\nresponse function\nrelates to what is\nthe impact of eta_t.\nWhat's its impact over time?\nBasically, it affects\nthe process at time t.\nThat, because of the\nmoving average process,\nit affects it at t plus\n1, affects it at t plus 2.\nAnd so this impulse\nresponse is basically\nthe derivative of the\nvalue of the process\nwith the j-th previous\ninnovation is given by psi_j.\nSo the different\ninnovations have an impact\non the current value given by\nthis impulse response function.\nSo looking backward,\nthat definition\nis pretty well defined.\nBut you can also\nthink about how does\nan impact of the\ninnovation affect\nthe process going forward.\nAnd the long-run\ncumulative response\nis essentially what is the\nimpact of that innovation\nin the process ultimately?\nAnd eventually, it's\nnot going to change\nthe value of the process.\nBut what is the value to\nwhich the process is moving\nbecause of that one innovation?\nAnd so the long run\ncumulative response\nis given by basically the\nsum of these individual ones.\nAnd it's given by the\nsum of the psi_i's.\nSo that's the polynomial of\npsi with lag operator, where we\nreplace the lag operator by 1.\nWe'll see this\nagain when we talk\nabout vector\nautoregressive processes\nwith multivariate time series.\nNow, the Wold\nrepresentation, which\nis a infinite-order moving\naverage, possibly infinite\norder, can have an\nautoregressive representation.\nSuppose that there is\nanother polynomial psi_i\nstar of the lags, which we're\ngoing to call psi inverse of L,\nwhich satisfies the fact if you\nmultiply that with psi of L,\nyou get the identity lag 0.\nThen this psi inverse,\nif that exists,\nis basically the\ninverse of the psi of L.\nSo if we start with psi of\nL, if that's invertible,\nthen there exists\na psi inverse of L,\nwith coefficients psi_i star.\nAnd one can basically take\nour original expression\nfor the stochastic process,\nwhich is as this moving average\nof the eta's, and express it\nas this essentially moving\naverages of the X's.\nAnd so we've essentially\ninverted the process\nand shown that the\nstochastic process can\nbe expressed as an infinite\norder autoregressive\nrepresentation.\nAnd so this infinite order\nautoregressive representation\ncorresponds to that intuitive\nunderstanding of how\nthe Wold representation exists.\nAnd it actually works with the--\nthe regression coefficients\nin that projection several\nslides back corresponds\nto this inverse operator.\nSo let's turn to some\nspecific time series\nmodels that are widely used.\nThe class of autoregressive\nmoving average processes\nhas this mathematical\ndefinition.\nWe define the X_t to be equal\nto a linear combination of lags\nof X, going back p\nlags, with coefficients\nphi_1 through phi_p.\nAnd then there are\nresiduals which\nare expressed in terms of a\nq-th order moving average.\nSo in this framework, the\neta_t's are white noise.\nAnd white noise, to reiterate,\nhas mean 0, constant variance,\nzero covariance between those.\nIn this representation, I've\nsimplified things a little bit\nby subtracting off the\nmean from all of the X's.\nAnd that just makes the formulas\na little bit more simpler.\nNow, with lag operators, we\ncan write this ARMA model\nas phi of L, p-th order\npolynomial of lag L given\nwith coefficients 1,\nphi_1 up to phi_p,\nand theta of L given\nby 1, theta_1, theta_2,\nup to theta_q.\nThis is basically\na representation\nof the ARMA time series model.\nBasically, we're\ntaking a set of lags\nof the values of the stochastic\nprocess up to order p.\nAnd that's equal to a weighted\naverage of the eta_t's.\nIf we multiply by the inverse\nof phi of L, if that exists,\nthen we get this\nrepresentation here,\nwhich is simply the\nWold decomposition.\nSo the ARMA models basically\nhave a Wold decomposition\nif this phi of L is invertible.\nAnd we'll explore\nthese by looking\nat simpler cases\nof the ARMA models\nby just focusing on\nautoregressive models\nfirst and then moving\naverage processes\nsecond so that\nyou'll get a better\nfeel for how these things are\nmanipulated and interpreted.\nSo let's move on to the p-th\norder autoregressive process.\nSo we're going to consider\nARMA models that just have\nautoregressive terms in them.\nSo we have phi of L X_t\nminus mu is equal to eta_t,\nwhich is white noise.\nSo a linear combination of\nthe series is white noise.\nAnd X_t follows then a linear\nregression model on explanatory\nvariables, which are\nlags of the process X.\nAnd this could be expressed\nas X_t equal to c plus the sum\nfrom 1 to p of phi_j X_(t-j),\nwhich is a linear regression\nmodel with regression\nparameters phi_j.\nAnd c, the constant term, is\nequal to mu times phi of 1.\nNow, if you basically take\nexpectations of the process,\nyou basically have\ncoefficients of mu coming in\nfrom all the terms.\nAnd phi of 1 times mu is the\nregression coefficient there.\nSo with this\nautoregressive model,\nwe now want to go over what are\nthe stationarity conditions.\nCertainly, this\nautoregressive model\nis one where, well,\na simple random walk\nfollows an autoregressive\nmodel but is not stationary.\nWe'll highlight that\nin a minute as well.\nBut if you think\nit, that's true.\nAnd so stationarity is something\nto be understood and evaluated.\nThis polynomial\nfunction phi, where\nif we replace the\nlag operator L by z,\na complex variable, the\nequation phi of z equal to 0\nis the characteristic\nequation associated\nwith this autoregressive model.\nAnd it turns out that we'll\nbe interested in the roots\nof this characteristic equation.\nNow, if we consider\nwriting phi of L\nas a function of the\nroots of the equation,\nwe get this expression\nwhere you'll\nnotice if you multiply\nall those terms out,\nthe 1's all multiply out\ntogether, and you get 1.\nAnd with the lag operator\nL to the p-th power,\nthat would be the product\nof 1 over lambda_1\ntimes 1 over lambda_2,\nor actually negative 1\nover lambda_1 times\nnegative 1 over lambda_2,\nand so forth-- negative\n1 over lambda_p.\nBasically, if there are\np roots to this equation,\nthis is how it would\nbe written out.\nAnd the process\nX_t is covariance\nstationary if and\nonly if all the roots\nof this characteristic equation\nlie outside the unit circle.\nSo what does that mean?\nThat means that the norm\nmodulus of the complex z\nis greater than 1.\nSo they're outside\nthe unit circle\nwhere it's less\nthan or equal to 1.\nAnd the roots, if they are\noutside the unit circle,\nthen the modulus of the\nlambda_j's is greater than 1.\nAnd if we then consider\ntaking a complex number\nlambda, basically\nthe root, and have\nan expression for 1 minus\n1 over lambda L inverse,\nwe can get this series\nexpression for that inverse.\nAnd that series will exist and\nbe bounded if the lambda_i are\ngreater than 1 in magnitude.\nSo we can actually compute\nan inverse of phi of L\nby taking the inverse\nof each of the component\nproducts in that polynomial.\nSo in introductory\ntime series courses,\nthey talk about\nstationarity and unit roots,\nbut they don't\nreally get into it,\nbecause people don't\nknow complex math,\ndon't know about roots.\nSo anyway, but this\nis just very simply\nhow that framework is applied.\nSo we have a\npolynomial equation,\nthe characteristic equation,\nwhose roots we're looking for.\nThose roots have to\nbe outside the unit\ncircle for stationarity\nof the process.\nWell, it's basically\nconditions for invertibility\nof the process, of the\nautoregressive process.\nAnd that invertibility renders\nthe process an infinite-order\nmoving average process.\nSo let's go through\nthese results\nfor the autoregressive\nprocess of order one,\nwhere things-- always start\nwith the simplest cases\nto understand things.\nThe characteristic equation\nfor this model is just 1\nminus phi z.\nThe root is 1/phi.\nSo lambda is greater than\n1-- if the modulus of lambda\nis greater than 1,\nmeaning the root\nis outside the unit circle,\nthen phi is less than 1.\nSo for covariance stationarity\nof this autoregressive process,\nwe need the magnitude of phi\nto be less than 1 in magnitude.\nThe expected value of X is mu.\nThe variance of X\nis sigma squared X.\nThis has this form, sigma\nsquared over 1 minus phi.\nThat expression is\nbasically obtained\nby looking at the infinite order\nmoving average representation.\nBut notice that if\nphi is positive,\nthen the variance\nof X is actually\ngreater than the variance\nof the innovations.\nAnd if phi is less than 0,\nthen it's going to be smaller.\nSo the innovation variance\nbasically is scaled up a bit\nin the autoregressive process.\nThe covariance matrix is\nphi times sigma squared\nX. You'll be going through\nthis in the problem set.\nAnd the covariance of X is phi\nto the j power sigma squared X.\nAnd these expressions can\nall be easily evaluated\nby simply writing out the\ndefinition of these covariances\nin terms of the original\nmodel and looking\nat what terms are independent,\ncancel out, and that proceeds.\nLet's just go\nthrough these cases.\nLet's show it all here.\nSo we have if phi\nis between 0 and 1,\nthen the process experiences\nexponential mean reversion\nto mu.\nSo an autoregressive\nprocess with phi between 0\non 1 corresponds to a\nmean-reverting process.\nThis process is\nactually one that\nhas been used theoretically\nfor interest rate models\nand a lot of theoretical\nwork in finance.\nThe Vasicek model is\nactually an example\nof the Ornstein-Uhlenbeck\nprocess,\nwhich is basically a\nmean-reverting Brownian motion.\nAnd any variables\nthat exhibit or could\nbe thought of as\nexhibiting mean reversion,\nthis model can be\napplied to those\nprocesses, such as interest rate\nspreads or real exchange rates,\nvariables where one can\nexpect that things never\nget too large or too small.\nThey come back to some mean.\nNow, the challenge\nis, that usually\nmay be true over\nshort periods of time.\nBut over very long\nperiods of time,\nthe point to which you're\nreverting to changes.\nSo these models tend to\nnot have broad application\nover long time ranges.\nYou need to adapt.\nAnyway, with the AR\nprocess, we can also\nhave negative\nvalues of phi, which\nresults in exponential mean\nreversion that's oscillating\nin time, because the\nautoregressive coefficient\nbasically is a negative value.\nAnd for phi equal to 1, the Wold\ndecomposition doesn't exist.\nAnd the process is the\nsimple random walk.\nSo basically, if\nphi is equal to 1,\nthat means that basically just\nchanges in value of the process\nare independent and identically\ndistributed white noise.\nAnd that's the\nrandom walk process.\nAnd that process, as was\ncovered in earlier lectures,\nis non-stationary.\nIf phi is greater than 1, then\nyou have an explosive process,\nbecause basically the\nvalues are scaling up\nevery time increment.\nSo those are features\nof the AR(1) model.\nFor a general autoregressive\nprocess of order p,\nthere's a method-- well, we\ncan look at the second order\nmoments of that process, which\nhave a very nice structure,\nand then use those to\nsolve for estimates\nof the ARMA parameters, or\nautoregressive parameters.\nAnd those happen to be\nspecified by what are called\nthe Yule-Walker equations.\nSo the Yule-Walker equations\nis a standard topic\nin time series analysis.\nWhat is it?\nWhat does it correspond to?\nWell, we take our original\nautoregressive process\nof order p.\nAnd we write out the\nformulas for the covariance\nat lag j between\ntwo observations.\nSo what's the covariance\nbetween X_t and X_(t-j)?\nAnd that expression is\ngiven by this equation.\nAnd so this equation for gamma\nof j is determined simply\nby evaluating the expectations\nwhere we're taking\nthe expectation of X_t in the\nautoregressive process times\nthe fix X_(t-j) minus mu.\nSo just evaluating\nthose terms, you\ncan validate that\nthis is the equation.\nIf we look at the equations\ncorresponding to j equals 1--\nso lag 1 up through\nlag p-- this is\nwhat those equations look like.\nBasically, the left-hand side\nis gamma_1 through gamma_p.\nThe covariance to\nlag 1 up to lag p\nis equal to basically\nlinear functions\ngiven by the phi of\nthe other covariances.\nWho can tell me what the\nstructure is of this matrix?\nIt's not a diagonal matrix?\nWhat kind of matrix is this?\nMath trivia question here.\nIt has a special name.\nAnyone?\nIt's a Toeplitz matrix.\nThe off diagonals are\nall the same value.\nAnd in fact, because of the\nsymmetry of the covariance,\nbasically the gamma of 1 is\nequal to gamma of minus 1.\nGamma of minus 2 is\nequal to gamma plus 2.\nBecause of the\ncovariant stationarity,\nit's actually also symmetric.\nSo these equations allow\nus to solve for the phis\nso long as we have estimates\nof these covariances.\nSo if we have a\nsystem of estimates,\nwe can plug these in in\nan attempt to solve this.\nIf they're consistent\nestimates of the covariances,\nthen there will be a solution.\nAnd then the 0th\nequation, which was not\npart of the series\nof equations--\nif you go back and look\nat the 0th equation, that\nallows you to get an estimate\nfor the sigma squared.\nSo these Yule-Walker\nequations are the way\nin which many ARMA\nmodels are specified\nin different statistics packages\nand in terms of what principles\nare being applied.\nWell, if we're using unbiased\nestimates of these parameters,\nthen this is applying\nwhat's called\nthe method of moments principle\nfor statistical estimation.\nAnd with complicated models,\nwhere sometimes the likelihood\nfunctions are very hard\nto specify and compute,\nand then to do optimization\nover those is even harder.\nIt can turn out that\nthere are relationships\nbetween the moments of the\nrandom variables, which\nare functions of the\nunknown parameters.\nAnd you can solve for basically\nthe sample moments equalling\nthe theoretical moments\nand you apply the method\nof moments estimation method.\nEconometrics is rich with many\napplications of that principle.\nThe next section goes through\nthe moving average model.\nLet me highlight this.\nSo with an order\nq moving average,\nwe basically have a polynomial\nin the lag operator L,\nwhich is operated\nupon the eta_t's.\nAnd if you write out\nthe expectations of X_t,\nyou get mu.\nThe variance of X_t,\nwhich is gamma 0,\nis sigma squared times 1 plus\nthe squares of the coefficients\nin the polynomial.\nAnd so this feature,\nthis property here is due\nto the fact that we have\nuncorrelated innovations\nin the eta_t's.\nThe eta t's are white noise.\nSo the only thing that comes\nthrough in the square of X_t\nand the expectation of\nthat is the squared powers\nof the etas, which\nhave coefficients\ngiven by the theta_i squared.\nSo these properties are left--\nI'll leave you just to verify,\nvery straightforward.\nBut let's now turn to the\nfinal minutes of the lecture\ntoday to accommodating\nnon-stationary behavior\nin time series.\nThe original approaches\nwith time series\nwas to focus on\nestimation methodologies\nfor covariance\nstationary process.\nSo if the series is not\ncovariance stationary,\nthen we would want to\ndo some transformation\nof the data, of the\nseries, into a stationary\nso that the resulting\nprocess is stationary.\nAnd with the\ndifferencing operators,\ndelta, Box and Jenkins\nadvocated moving\nnon-stationary trending\nbehavior, which\nis exhibited often in\neconomic time series,\nby using a first difference,\nmaybe a second difference,\nor a k-th order difference.\nSo these operators are\ndefined in this way.\nBasically with the\nk-th order operator\nhaving this\nexpression here, this\nis the binomial expansion\nof a k-th power,\nwhich can be useful.\nIt comes up all the time\nin probability theory.\nAnd if a process has\na linear time trend,\nthen delta X_t is going to\nhave no time trend at all,\nbecause you're\nbasically taking out\nthat linear component by\ntaking successive differences.\nSometimes, if you\nhave a real series\nand you look at the difference,\nit appears non-stationary,\nyou look at first differences,\nthat can still not\nappear to be growing\nover time, in which case\nsometimes the second\ndifference will result\nin a process with no trend.\nSo these are sort of\nconvenient tricks,\ntechniques to render\nthe series stationary.\nAnd let's see.\nThere's examples here of\nlinear trend reversion models\nwhich are rendered\ncovariance stationary\nunder first differencing.\nIn this case, this is an\nexample where you have\na deterministic time trend.\nBut then you have reversion\nto the time trend over time.\nSo we basically have\neta_t, the error\nabout the deterministic trend,\nis a first order autoregressive\nprocess.\nAnd the moments here\ncan be derived this way.\nLeave that as an exercise.\nOne could also consider\nthe pure integrated process\nand talk about\nstochastic trends.\nAnd basically,\nrandom walk processes\nare often referred\nto in econometrics\nas stochastic trends.\nAnd you may want to try and\nremove those from the data,\nor accommodate them.\nAnd so the stochastic\ntrend process is basically\ngiven by the first difference\nX_t is just equal to eta_t.\nAnd so we have essentially\nthis random walk\nfrom a given starting point.\nAnd it's easy to verify it if\nyou knew the 0th point, then\nthe variance of the t-th time\npoint would be t sigma squared,\nbecause we're summing t\nindependent innovations.\nAnd the covariance between\nt and lag t minus j\nis simply t minus\nj sigma squared.\nAnd the correlation between\nthose has this form.\nWhat you can see is that this\ndefinitely depends on time.\nSo it's not a\nstationary process.\nSo this first differencing\nresults in stationarity.\nAnd the end difference\nprocess has those features.\nLet's see where we are.\nFinal topic for\ntoday is just how\nyou incorporate non-stationary\nprocess into ARMA processes.\nWell, if you take\nfirst differences\nor second differences\nand the resulting process\nis covariance\nstationary, then we\ncan just incorporate that\ndifferencing into the model\nspecification itself, and define\nARIMA models, Autoregressive\nIntegrated Moving\nAverage Processes.\nAnd so to specify\nthese models, we\nneed to determine the order\nof the differencing required\nto move trends,\ndeterministic or stochastic,\nand then estimating\nthe unknown parameters,\nand then applying model\nselection criteria.\nSo let me go very\nquickly through this\nand come back to it the\nbeginning of next time.\nBut in specifying the\nparameters of these models,\nwe can apply maximum\nlikelihood, again,\nif we assume normality of\nthese innovations eta_t.\nAnd we can express\nthe ARMA model\nin state space\nform, which results\nin a form for the\nlikelihood function, which\nwe'll see a few lectures ahead.\nBut then we can apply limited\ninformation maximum likelihood,\nwhere we just condition on the\nfirst observations of the data\nand maximize the likelihood.\nOr not condition on the first\nfew observations, but also\nuse their information as well,\nand look at their density\nfunctions, incorporating\nthose into the likelihood\nrelative to the stationary\ndistribution for their values.\nAnd then the issue\nbecomes, how do we\nchoose amongst different models?\nNow, last time we talked about\nlinear regression models,\nhow you'd specify a\ngiven model, here, we're\ntalking about autoregressive,\nmoving average,\nand even integrated\nmoving average processes\nand how do we specify\nthose, well, with the method\nof maximum likelihood,\nthere are procedures\nwhich-- there are measures of\nhow effectively a fitted model\nis, given by an\ninformation criterion\nthat you would want to minimize\nfor a given fitted model.\nSo we can consider\ndifferent sets of models,\ndifferent numbers of\nexplanatory variables,\ndifferent orders of\nautoregressive parameters,\nmoving average parameters,\nand compute, say,\nthe Akaike information criterion\nor the Bayes information\ncriterion or the\nHannan-Quinn criterion\nas different ways of judging\nhow good different models are.\nAnd let me just finish\ntoday by pointing out\nthat what these\ninformation criteria are\nis basically a function of the\nlog likelihood function, which\nis something we're\ntrying to maximize\nwith maximum\nlikelihood estimates.\nAnd then adding some penalty\nfor how many parameters\nwe're estimating.\nAnd so what I'd like you to\nthink about for next time\nis what kind of a penalty\nis appropriate for adding\nan extra parameter.\nLike, what evidence is\nrequired to incorporate\nextra parameters, extra\nvariables, in the model.\nWould it be t statistics\nthat exceeds some threshold\nor some other criteria.\nTurns out that these are\nall related to those issues.\nAnd it's very interesting\nhow those play out.\nAnd I'll say that for those\nof you who have actually\nseen these before, the\nBayes information criterion\ncorresponds to an\nassumption that there\nis some finite number of\nvariables in the model.\nAnd you know what those are.\nThe Hannan-Quinn criterion\nsays maybe there's\nan infinite number of\nvariables in the model,\nbut you want to be\nable to identify those.\nAnd so anyway, it's a\nvery challenging problem\nwith model selection.\nAnd these criteria can\nbe used to specify those.\nSo we'll go through\nthat next time.\n",
  "words": [
    "following",
    "content",
    "provided",
    "creative",
    "commons",
    "license",
    "support",
    "help",
    "mit",
    "opencourseware",
    "continue",
    "offer",
    "high",
    "quality",
    "educational",
    "resources",
    "free",
    "make",
    "donation",
    "view",
    "additional",
    "materials",
    "hundreds",
    "mit",
    "courses",
    "visit",
    "mit",
    "opencourseware",
    "professor",
    "well",
    "last",
    "time",
    "lecturing",
    "talking",
    "regression",
    "analysis",
    "finished",
    "talking",
    "estimation",
    "methods",
    "fitting",
    "regression",
    "models",
    "want",
    "recap",
    "method",
    "maximum",
    "likelihood",
    "really",
    "primary",
    "estimation",
    "method",
    "statistical",
    "modeling",
    "start",
    "let",
    "review",
    "normal",
    "linear",
    "regression",
    "model",
    "dependent",
    "variable",
    "explained",
    "linear",
    "combination",
    "independent",
    "variables",
    "given",
    "regression",
    "parameter",
    "beta",
    "assume",
    "errors",
    "cases",
    "independent",
    "identically",
    "distributed",
    "normal",
    "random",
    "variables",
    "relationship",
    "dependent",
    "variable",
    "vector",
    "n",
    "cases",
    "multivariate",
    "normal",
    "random",
    "variable",
    "likelihood",
    "function",
    "equal",
    "density",
    "function",
    "data",
    "ambiguity",
    "really",
    "one",
    "manipulates",
    "likelihood",
    "function",
    "likelihood",
    "function",
    "becomes",
    "defined",
    "observed",
    "sample",
    "data",
    "expression",
    "likelihood",
    "function",
    "function",
    "beta",
    "sigma",
    "squared",
    "considering",
    "evaluating",
    "probability",
    "density",
    "function",
    "data",
    "conditional",
    "unknown",
    "parameters",
    "simply",
    "univariate",
    "normal",
    "distribution",
    "unknown",
    "mean",
    "variance",
    "would",
    "bell",
    "curve",
    "mu",
    "centered",
    "around",
    "single",
    "observation",
    "look",
    "likelihood",
    "function",
    "varies",
    "underlying",
    "mean",
    "normal",
    "distribution",
    "likelihood",
    "function",
    "well",
    "challenge",
    "really",
    "maximum",
    "estimation",
    "really",
    "calculating",
    "computing",
    "likelihood",
    "function",
    "normal",
    "linear",
    "regression",
    "models",
    "easy",
    "maximum",
    "likelihood",
    "estimates",
    "values",
    "maximize",
    "function",
    "question",
    "good",
    "estimates",
    "underlying",
    "parameters",
    "well",
    "estimates",
    "parameter",
    "values",
    "observed",
    "data",
    "likely",
    "able",
    "scale",
    "unknown",
    "parameters",
    "likely",
    "parameters",
    "could",
    "generated",
    "data",
    "values",
    "let",
    "look",
    "likelihood",
    "function",
    "normal",
    "linear",
    "regression",
    "model",
    "first",
    "two",
    "lines",
    "highlighting",
    "first",
    "line",
    "highlighting",
    "response",
    "variable",
    "values",
    "independent",
    "conditionally",
    "independent",
    "given",
    "unknown",
    "parameters",
    "density",
    "full",
    "vector",
    "simply",
    "product",
    "density",
    "functions",
    "components",
    "normal",
    "linear",
    "regression",
    "model",
    "normally",
    "distributed",
    "simply",
    "density",
    "function",
    "normal",
    "random",
    "variable",
    "mean",
    "given",
    "beta",
    "sum",
    "independent",
    "variables",
    "case",
    "given",
    "regression",
    "parameters",
    "expression",
    "basically",
    "expressed",
    "matrix",
    "form",
    "way",
    "likelihood",
    "function",
    "ends",
    "function",
    "q",
    "beta",
    "least",
    "squares",
    "criteria",
    "least",
    "squares",
    "estimation",
    "equivalent",
    "maximum",
    "likelihood",
    "estimation",
    "regression",
    "parameters",
    "normal",
    "linear",
    "regression",
    "model",
    "extra",
    "term",
    "minus",
    "well",
    "actually",
    "going",
    "maximize",
    "likelihood",
    "function",
    "also",
    "maximize",
    "log",
    "likelihood",
    "function",
    "monotone",
    "function",
    "likelihood",
    "easier",
    "maximize",
    "log",
    "likelihood",
    "function",
    "expressed",
    "able",
    "maximize",
    "beta",
    "minimizing",
    "q",
    "beta",
    "maximize",
    "sigma",
    "squared",
    "given",
    "estimate",
    "beta",
    "achieved",
    "taking",
    "derivative",
    "respect",
    "sigma",
    "squared",
    "basically",
    "first",
    "order",
    "condition",
    "finds",
    "maximum",
    "things",
    "appropriately",
    "convex",
    "taking",
    "derivative",
    "solving",
    "zero",
    "basically",
    "get",
    "expression",
    "taking",
    "derivative",
    "respect",
    "sigma",
    "squared",
    "notice",
    "taking",
    "derivative",
    "respect",
    "sigma",
    "squared",
    "parameter",
    "sigma",
    "gives",
    "us",
    "maximum",
    "likelihood",
    "estimate",
    "error",
    "variance",
    "q",
    "beta",
    "hat",
    "sum",
    "squared",
    "residuals",
    "divided",
    "emphasize",
    "biased",
    "tell",
    "biased",
    "ought",
    "biased",
    "audience",
    "inaudible",
    "professor",
    "well",
    "n",
    "minus",
    "1",
    "actually",
    "estimating",
    "one",
    "parameter",
    "independent",
    "variables",
    "say",
    "constant",
    "1",
    "estimating",
    "sample",
    "normal",
    "mean",
    "beta",
    "1",
    "corresponding",
    "units",
    "vector",
    "x",
    "would",
    "one",
    "degree",
    "freedom",
    "correction",
    "residuals",
    "get",
    "unbiased",
    "estimator",
    "p",
    "parameters",
    "well",
    "let",
    "ask",
    "n",
    "parameters",
    "regression",
    "model",
    "would",
    "happen",
    "full",
    "rank",
    "n",
    "independent",
    "variable",
    "matrix",
    "n",
    "independent",
    "observations",
    "audience",
    "inaudible",
    "professor",
    "yes",
    "exact",
    "fit",
    "data",
    "estimate",
    "would",
    "clearly",
    "data",
    "arise",
    "normal",
    "linear",
    "regression",
    "model",
    "0",
    "unbiased",
    "need",
    "correction",
    "turns",
    "need",
    "divide",
    "n",
    "minus",
    "rank",
    "x",
    "matrix",
    "degrees",
    "freedom",
    "model",
    "get",
    "biased",
    "estimate",
    "important",
    "issue",
    "highlights",
    "parameters",
    "add",
    "model",
    "precise",
    "fitted",
    "values",
    "sense",
    "dangers",
    "curve",
    "fitting",
    "want",
    "avoid",
    "maximum",
    "likelihood",
    "estimates",
    "fact",
    "biased",
    "aware",
    "using",
    "different",
    "software",
    "fitting",
    "different",
    "models",
    "need",
    "know",
    "whether",
    "various",
    "corrections",
    "made",
    "biasedness",
    "solves",
    "estimation",
    "problem",
    "normal",
    "linear",
    "regression",
    "models",
    "normal",
    "linear",
    "regression",
    "models",
    "theorem",
    "went",
    "last",
    "time",
    "important",
    "let",
    "go",
    "back",
    "highlight",
    "theorem",
    "right",
    "really",
    "important",
    "theorem",
    "indicating",
    "distribution",
    "least",
    "squares",
    "maximum",
    "likelihood",
    "estimates",
    "regression",
    "model",
    "normally",
    "distributed",
    "residuals",
    "sum",
    "squares",
    "chi",
    "squared",
    "distribution",
    "degrees",
    "freedom",
    "given",
    "n",
    "minus",
    "look",
    "much",
    "signal",
    "noise",
    "estimating",
    "regression",
    "parameters",
    "calculating",
    "statistic",
    "take",
    "away",
    "estimate",
    "expected",
    "value",
    "mean",
    "divide",
    "estimate",
    "variability",
    "standard",
    "deviation",
    "units",
    "distribution",
    "critical",
    "way",
    "assess",
    "relevance",
    "different",
    "explanatory",
    "variables",
    "model",
    "approach",
    "apply",
    "maximum",
    "likelihood",
    "estimation",
    "kinds",
    "models",
    "apart",
    "normal",
    "linear",
    "regression",
    "models",
    "turns",
    "maximum",
    "likelihood",
    "estimates",
    "generally",
    "asymptotically",
    "normally",
    "distributed",
    "properties",
    "apply",
    "models",
    "well",
    "let",
    "finish",
    "notes",
    "estimation",
    "talking",
    "generalized",
    "estimation",
    "want",
    "consider",
    "estimating",
    "unknown",
    "parameters",
    "minimizing",
    "function",
    "q",
    "beta",
    "sum",
    "evaluations",
    "another",
    "function",
    "h",
    "evaluated",
    "individual",
    "cases",
    "choosing",
    "h",
    "take",
    "different",
    "functional",
    "forms",
    "define",
    "different",
    "kinds",
    "estimators",
    "seen",
    "h",
    "simply",
    "square",
    "case",
    "minus",
    "regression",
    "prediction",
    "leads",
    "least",
    "squares",
    "fact",
    "maximum",
    "likelihood",
    "estimation",
    "saw",
    "rather",
    "taking",
    "square",
    "residual",
    "fitted",
    "residual",
    "could",
    "take",
    "simply",
    "modulus",
    "would",
    "mean",
    "absolute",
    "deviation",
    "rather",
    "summing",
    "squared",
    "deviations",
    "mean",
    "could",
    "sum",
    "absolute",
    "deviations",
    "mean",
    "mathematical",
    "standpoint",
    "want",
    "solve",
    "estimates",
    "would",
    "go",
    "methodology",
    "would",
    "use",
    "maximize",
    "function",
    "well",
    "try",
    "apply",
    "basically",
    "principles",
    "convex",
    "function",
    "want",
    "take",
    "derivatives",
    "solve",
    "equal",
    "happens",
    "take",
    "derivative",
    "modulus",
    "minus",
    "xi",
    "beta",
    "respect",
    "beta",
    "audience",
    "inaudible",
    "professor",
    "say",
    "say",
    "audience",
    "yeah",
    "inaudible",
    "first",
    "inaudible",
    "derivative",
    "continuous",
    "professor",
    "well",
    "smooth",
    "function",
    "let",
    "plot",
    "beta",
    "minus",
    "basically",
    "going",
    "function",
    "slope",
    "1",
    "positive",
    "slope",
    "minus",
    "1",
    "negative",
    "true",
    "end",
    "wanting",
    "find",
    "value",
    "regression",
    "estimate",
    "minimizes",
    "sum",
    "predictions",
    "estimate",
    "plus",
    "sum",
    "predictions",
    "estimate",
    "given",
    "regression",
    "line",
    "solves",
    "problem",
    "maximum",
    "likelihood",
    "estimation",
    "one",
    "plug",
    "minus",
    "log",
    "density",
    "given",
    "beta",
    "x",
    "squared",
    "function",
    "simply",
    "sums",
    "log",
    "joint",
    "density",
    "data",
    "works",
    "well",
    "robust",
    "estimators",
    "consider",
    "another",
    "function",
    "chi",
    "defined",
    "good",
    "properties",
    "estimates",
    "whole",
    "theory",
    "robust",
    "estimation",
    "rich",
    "talks",
    "best",
    "specify",
    "chi",
    "function",
    "one",
    "problems",
    "least",
    "squares",
    "estimation",
    "squares",
    "large",
    "values",
    "large",
    "magnitude",
    "perhaps",
    "undue",
    "influence",
    "large",
    "values",
    "large",
    "residuals",
    "least",
    "squares",
    "estimation",
    "maximum",
    "inaudible",
    "estimation",
    "robust",
    "estimators",
    "allow",
    "control",
    "defining",
    "function",
    "differently",
    "finally",
    "quantile",
    "estimators",
    "extend",
    "mean",
    "absolute",
    "deviation",
    "criterion",
    "consider",
    "h",
    "function",
    "basically",
    "multiple",
    "deviation",
    "residual",
    "positive",
    "different",
    "multiple",
    "complementary",
    "multiple",
    "derivation",
    "residual",
    "less",
    "0",
    "varying",
    "tau",
    "end",
    "getting",
    "quantile",
    "estimators",
    "minimizing",
    "estimate",
    "tau",
    "quantile",
    "general",
    "class",
    "estimators",
    "encompasses",
    "estimators",
    "encounter",
    "fitting",
    "models",
    "finishes",
    "technical",
    "mathematical",
    "discussion",
    "regression",
    "analysis",
    "let",
    "highlight",
    "case",
    "study",
    "dragged",
    "desktop",
    "wanted",
    "find",
    "let",
    "find",
    "case",
    "study",
    "added",
    "course",
    "website",
    "first",
    "one",
    "linear",
    "regression",
    "models",
    "asset",
    "pricing",
    "want",
    "read",
    "see",
    "applies",
    "fitting",
    "various",
    "simple",
    "linear",
    "regression",
    "models",
    "enter",
    "full",
    "screen",
    "case",
    "study",
    "begins",
    "introducing",
    "capital",
    "asset",
    "pricing",
    "model",
    "basically",
    "suggests",
    "look",
    "returns",
    "stocks",
    "efficient",
    "market",
    "depend",
    "return",
    "overall",
    "market",
    "scaled",
    "risky",
    "stock",
    "one",
    "looks",
    "basically",
    "return",
    "stock",
    "right",
    "scale",
    "simple",
    "linear",
    "regression",
    "model",
    "look",
    "time",
    "series",
    "ge",
    "stock",
    "p",
    "case",
    "study",
    "guide",
    "actually",
    "collect",
    "data",
    "web",
    "using",
    "case",
    "notes",
    "provide",
    "details",
    "also",
    "treasury",
    "rate",
    "collected",
    "thinking",
    "return",
    "stock",
    "versus",
    "return",
    "index",
    "well",
    "really",
    "interest",
    "excess",
    "return",
    "rate",
    "efficient",
    "markets",
    "models",
    "basically",
    "excess",
    "return",
    "stock",
    "related",
    "excess",
    "return",
    "market",
    "given",
    "linear",
    "regression",
    "model",
    "fit",
    "model",
    "plot",
    "excess",
    "returns",
    "daily",
    "basis",
    "ge",
    "stock",
    "versus",
    "market",
    "looks",
    "like",
    "nice",
    "sort",
    "point",
    "cloud",
    "linear",
    "model",
    "might",
    "fit",
    "well",
    "well",
    "regression",
    "diagnostics",
    "get",
    "well",
    "regression",
    "diagnostics",
    "detailed",
    "problem",
    "set",
    "looking",
    "influential",
    "individual",
    "observations",
    "impact",
    "regression",
    "parameters",
    "display",
    "basically",
    "highlights",
    "simple",
    "linear",
    "regression",
    "model",
    "influential",
    "data",
    "points",
    "highlighted",
    "red",
    "values",
    "influential",
    "look",
    "definition",
    "leverage",
    "linear",
    "model",
    "simple",
    "simple",
    "linear",
    "model",
    "observations",
    "far",
    "mean",
    "large",
    "leverage",
    "confirm",
    "answers",
    "problem",
    "set",
    "x",
    "indicates",
    "significantly",
    "influential",
    "point",
    "terms",
    "regression",
    "parameters",
    "given",
    "cook",
    "distance",
    "definition",
    "also",
    "given",
    "case",
    "notes",
    "audience",
    "inaudible",
    "professor",
    "computing",
    "individual",
    "leverages",
    "function",
    "given",
    "selecting",
    "exceed",
    "given",
    "magnitude",
    "simple",
    "model",
    "stocks",
    "depending",
    "one",
    "unknown",
    "factor",
    "risk",
    "factor",
    "given",
    "market",
    "modeling",
    "equity",
    "returns",
    "many",
    "different",
    "factors",
    "impact",
    "returns",
    "done",
    "case",
    "study",
    "look",
    "adding",
    "another",
    "factor",
    "return",
    "crude",
    "oil",
    "need",
    "go",
    "let",
    "highlight",
    "something",
    "ge",
    "stock",
    "would",
    "expect",
    "impact",
    "say",
    "high",
    "return",
    "crude",
    "oil",
    "return",
    "ge",
    "stock",
    "would",
    "expect",
    "positively",
    "related",
    "negatively",
    "related",
    "well",
    "ge",
    "stock",
    "broad",
    "stock",
    "invested",
    "many",
    "different",
    "industries",
    "really",
    "reflects",
    "overall",
    "market",
    "extent",
    "many",
    "years",
    "ago",
    "10",
    "15",
    "years",
    "ago",
    "ge",
    "represented",
    "maybe",
    "3",
    "gnp",
    "us",
    "market",
    "really",
    "highly",
    "related",
    "well",
    "market",
    "crude",
    "oil",
    "commodity",
    "oil",
    "used",
    "drive",
    "cars",
    "fuel",
    "energy",
    "production",
    "increase",
    "oil",
    "prices",
    "cost",
    "essentially",
    "business",
    "goes",
    "associated",
    "inflation",
    "factor",
    "prices",
    "rising",
    "see",
    "regression",
    "estimate",
    "add",
    "factor",
    "return",
    "crude",
    "oil",
    "negative",
    "value",
    "minus",
    "fact",
    "market",
    "sense",
    "period",
    "analysis",
    "efficient",
    "explaining",
    "return",
    "ge",
    "crude",
    "oil",
    "another",
    "independent",
    "factor",
    "helps",
    "explain",
    "returns",
    "useful",
    "know",
    "clever",
    "defining",
    "identifying",
    "evaluating",
    "different",
    "factors",
    "build",
    "factor",
    "asset",
    "pricing",
    "models",
    "useful",
    "investing",
    "trading",
    "comparison",
    "case",
    "study",
    "also",
    "applied",
    "analysis",
    "exxon",
    "mobil",
    "exxon",
    "mobil",
    "oil",
    "company",
    "let",
    "highlight",
    "basically",
    "fitting",
    "model",
    "let",
    "highlight",
    "consider",
    "model",
    "regression",
    "parameter",
    "corresponding",
    "crude",
    "oil",
    "factor",
    "plus",
    "value",
    "crude",
    "oil",
    "definitely",
    "impact",
    "return",
    "exxon",
    "mobil",
    "goes",
    "oil",
    "prices",
    "case",
    "study",
    "closes",
    "scatter",
    "plot",
    "independent",
    "variables",
    "highlighting",
    "influential",
    "values",
    "way",
    "simple",
    "linear",
    "regression",
    "far",
    "away",
    "mean",
    "data",
    "influential",
    "multivariate",
    "setting",
    "bivariate",
    "influential",
    "observations",
    "far",
    "away",
    "centroid",
    "look",
    "one",
    "problems",
    "problem",
    "set",
    "actually",
    "goes",
    "see",
    "leveraged",
    "values",
    "indicates",
    "influences",
    "associated",
    "mahalanobis",
    "distance",
    "cases",
    "centroid",
    "independent",
    "variables",
    "visual",
    "type",
    "mathematician",
    "opposed",
    "algebraic",
    "type",
    "mathematician",
    "think",
    "kinds",
    "graphs",
    "helpful",
    "understanding",
    "really",
    "going",
    "degree",
    "influence",
    "associated",
    "fact",
    "basically",
    "taking",
    "least",
    "squares",
    "estimates",
    "quadratic",
    "form",
    "associated",
    "overall",
    "process",
    "another",
    "case",
    "study",
    "happy",
    "discuss",
    "class",
    "office",
    "hours",
    "think",
    "time",
    "today",
    "lecture",
    "concerns",
    "exchange",
    "rate",
    "regimes",
    "second",
    "case",
    "study",
    "looks",
    "chinese",
    "yuan",
    "basically",
    "pegged",
    "dollar",
    "many",
    "years",
    "guess",
    "political",
    "influence",
    "countries",
    "started",
    "let",
    "yuan",
    "vary",
    "dollar",
    "perhaps",
    "pegged",
    "basket",
    "securities",
    "currencies",
    "would",
    "determine",
    "basket",
    "currencies",
    "well",
    "regression",
    "methods",
    "developed",
    "economists",
    "help",
    "case",
    "study",
    "goes",
    "analysis",
    "check",
    "see",
    "get",
    "immediate",
    "access",
    "currency",
    "data",
    "fitting",
    "regression",
    "models",
    "looking",
    "different",
    "results",
    "trying",
    "evaluate",
    "let",
    "turn",
    "main",
    "topic",
    "let",
    "see",
    "time",
    "series",
    "analysis",
    "today",
    "rest",
    "lecture",
    "want",
    "talk",
    "univariate",
    "time",
    "series",
    "analysis",
    "thinking",
    "basically",
    "random",
    "variable",
    "observed",
    "time",
    "discrete",
    "time",
    "process",
    "introduce",
    "wold",
    "representation",
    "theorem",
    "definitions",
    "stationarity",
    "relationship",
    "look",
    "classic",
    "models",
    "autoregressive",
    "moving",
    "average",
    "models",
    "extending",
    "integrated",
    "autoregressive",
    "moving",
    "average",
    "models",
    "finally",
    "talk",
    "estimating",
    "stationary",
    "models",
    "test",
    "stationarity",
    "let",
    "begin",
    "basically",
    "first",
    "principles",
    "stochastic",
    "process",
    "discrete",
    "time",
    "stochastic",
    "process",
    "x",
    "consists",
    "random",
    "variables",
    "indexed",
    "time",
    "thinking",
    "discrete",
    "time",
    "stochastic",
    "behavior",
    "sequence",
    "determined",
    "specifying",
    "density",
    "probability",
    "mass",
    "functions",
    "finite",
    "collections",
    "time",
    "indexes",
    "could",
    "specify",
    "distributions",
    "process",
    "would",
    "specify",
    "probability",
    "model",
    "stochastic",
    "process",
    "stochastic",
    "process",
    "strictly",
    "stationary",
    "density",
    "function",
    "collection",
    "times",
    "equal",
    "density",
    "function",
    "tau",
    "translation",
    "density",
    "function",
    "distribution",
    "stationary",
    "constant",
    "arbitrary",
    "translations",
    "strong",
    "property",
    "reasonable",
    "property",
    "ask",
    "statistical",
    "modeling",
    "want",
    "estimating",
    "models",
    "want",
    "estimate",
    "things",
    "constant",
    "constants",
    "nice",
    "things",
    "estimate",
    "parameters",
    "models",
    "constant",
    "really",
    "want",
    "underlying",
    "structure",
    "distributions",
    "strict",
    "stationarity",
    "requires",
    "knowledge",
    "entire",
    "distribution",
    "stochastic",
    "process",
    "going",
    "introduce",
    "weaker",
    "definition",
    "covariance",
    "stationarity",
    "covariance",
    "stationary",
    "process",
    "constant",
    "mean",
    "mu",
    "constant",
    "variance",
    "sigma",
    "squared",
    "covariance",
    "increments",
    "tau",
    "given",
    "function",
    "gamma",
    "tau",
    "also",
    "constant",
    "gamma",
    "constant",
    "function",
    "basically",
    "covariance",
    "gamma",
    "tau",
    "function",
    "also",
    "introduce",
    "autocorrelation",
    "function",
    "stochastic",
    "process",
    "rho",
    "tau",
    "correlation",
    "two",
    "random",
    "variables",
    "covariance",
    "random",
    "variables",
    "divided",
    "square",
    "root",
    "product",
    "variances",
    "choongbum",
    "think",
    "introduced",
    "bit",
    "one",
    "lectures",
    "talking",
    "correlation",
    "function",
    "essentially",
    "correlation",
    "function",
    "standardize",
    "data",
    "random",
    "variables",
    "mean",
    "0",
    "subtract",
    "means",
    "divide",
    "standard",
    "deviations",
    "translated",
    "variables",
    "mean",
    "0",
    "variance",
    "correlation",
    "coefficient",
    "covariance",
    "standardized",
    "random",
    "variables",
    "going",
    "come",
    "time",
    "series",
    "analysis",
    "wold",
    "representation",
    "theorem",
    "powerful",
    "theorem",
    "covariance",
    "stationary",
    "processes",
    "basically",
    "states",
    "covariance",
    "stationary",
    "time",
    "series",
    "decomposed",
    "two",
    "components",
    "nice",
    "structure",
    "basically",
    "decomposed",
    "plus",
    "going",
    "linearly",
    "deterministic",
    "process",
    "meaning",
    "past",
    "values",
    "perfectly",
    "predict",
    "going",
    "could",
    "like",
    "linear",
    "trend",
    "fixed",
    "function",
    "past",
    "values",
    "basically",
    "deterministic",
    "process",
    "nothing",
    "random",
    "something",
    "fixed",
    "without",
    "randomness",
    "sum",
    "coefficients",
    "times",
    "linearly",
    "unpredictable",
    "white",
    "noise",
    "weighted",
    "average",
    "white",
    "noise",
    "coefficients",
    "given",
    "coefficients",
    "1",
    "sum",
    "squared",
    "finite",
    "white",
    "noise",
    "white",
    "noise",
    "expectation",
    "zero",
    "variance",
    "given",
    "sigma",
    "squared",
    "constant",
    "covariance",
    "across",
    "different",
    "white",
    "noise",
    "elements",
    "0",
    "uncorrelated",
    "course",
    "uncorrelated",
    "deterministic",
    "process",
    "really",
    "powerful",
    "concept",
    "modeling",
    "process",
    "covariance",
    "stationarity",
    "exists",
    "representation",
    "like",
    "function",
    "compelling",
    "structure",
    "see",
    "applies",
    "different",
    "circumstances",
    "getting",
    "definition",
    "autoregressive",
    "moving",
    "average",
    "models",
    "want",
    "give",
    "intuitive",
    "understanding",
    "going",
    "wold",
    "decomposition",
    "think",
    "help",
    "motivate",
    "wold",
    "decomposition",
    "exist",
    "mathematical",
    "standpoint",
    "consider",
    "univariate",
    "stochastic",
    "process",
    "time",
    "series",
    "want",
    "model",
    "believe",
    "covariance",
    "stationary",
    "want",
    "specify",
    "essentially",
    "wold",
    "decomposition",
    "well",
    "could",
    "initialize",
    "parameter",
    "p",
    "number",
    "past",
    "observations",
    "linearly",
    "deterministic",
    "term",
    "estimate",
    "linear",
    "projection",
    "last",
    "p",
    "lag",
    "values",
    "want",
    "consider",
    "estimating",
    "relationship",
    "using",
    "sample",
    "size",
    "n",
    "ending",
    "point",
    "less",
    "equal",
    "consider",
    "values",
    "like",
    "response",
    "variable",
    "given",
    "successive",
    "values",
    "time",
    "series",
    "response",
    "variables",
    "considered",
    "x",
    "minus",
    "n",
    "plus",
    "define",
    "vector",
    "z",
    "matrix",
    "follows",
    "values",
    "stochastic",
    "process",
    "z",
    "matrix",
    "essentially",
    "matrix",
    "independent",
    "variables",
    "lagged",
    "values",
    "process",
    "let",
    "apply",
    "ordinary",
    "least",
    "squares",
    "specify",
    "projection",
    "projection",
    "matrix",
    "familiar",
    "basically",
    "gives",
    "us",
    "prediction",
    "hat",
    "depending",
    "p",
    "lags",
    "compute",
    "projection",
    "residual",
    "fit",
    "well",
    "conduct",
    "time",
    "series",
    "methods",
    "analyze",
    "residuals",
    "introducing",
    "minutes",
    "specify",
    "moving",
    "average",
    "model",
    "estimates",
    "underlying",
    "coefficients",
    "psi",
    "estimates",
    "residuals",
    "evaluate",
    "whether",
    "good",
    "model",
    "mean",
    "appropriate",
    "model",
    "well",
    "residual",
    "orthogonal",
    "longer",
    "lags",
    "minus",
    "longer",
    "lags",
    "basically",
    "dependence",
    "residuals",
    "lags",
    "stochastic",
    "process",
    "included",
    "model",
    "orthogonal",
    "hats",
    "consistent",
    "white",
    "noise",
    "issues",
    "evaluated",
    "evidence",
    "otherwise",
    "change",
    "specification",
    "model",
    "add",
    "additional",
    "lags",
    "add",
    "additional",
    "deterministic",
    "variables",
    "identify",
    "might",
    "proceed",
    "process",
    "essentially",
    "wold",
    "decomposition",
    "could",
    "implemented",
    "theoretically",
    "sample",
    "gets",
    "large",
    "observing",
    "time",
    "series",
    "long",
    "time",
    "well",
    "certainly",
    "limit",
    "projections",
    "p",
    "number",
    "lags",
    "include",
    "gets",
    "large",
    "essentially",
    "projection",
    "data",
    "history",
    "fact",
    "projection",
    "corresponding",
    "defining",
    "coefficient",
    "limit",
    "projection",
    "converge",
    "converge",
    "sense",
    "coefficients",
    "projection",
    "definition",
    "correspond",
    "p",
    "goes",
    "infinity",
    "required",
    "p",
    "means",
    "basically",
    "long",
    "term",
    "dependence",
    "process",
    "basically",
    "stop",
    "given",
    "lag",
    "dependence",
    "persists",
    "time",
    "may",
    "require",
    "p",
    "goes",
    "infinity",
    "happens",
    "p",
    "goes",
    "infinity",
    "well",
    "let",
    "p",
    "go",
    "infinity",
    "quickly",
    "run",
    "degrees",
    "freedom",
    "estimate",
    "models",
    "implementation",
    "standpoint",
    "need",
    "let",
    "go",
    "0",
    "essentially",
    "data",
    "parameters",
    "estimating",
    "required",
    "time",
    "series",
    "modeling",
    "look",
    "models",
    "finite",
    "values",
    "p",
    "required",
    "estimating",
    "finite",
    "number",
    "parameters",
    "moving",
    "average",
    "model",
    "coefficients",
    "infinite",
    "number",
    "perhaps",
    "defined",
    "small",
    "number",
    "parameters",
    "looking",
    "kind",
    "feature",
    "different",
    "models",
    "let",
    "turn",
    "talking",
    "lag",
    "operator",
    "lag",
    "operator",
    "fundamental",
    "tool",
    "time",
    "series",
    "models",
    "consider",
    "operator",
    "l",
    "shifts",
    "time",
    "series",
    "back",
    "one",
    "time",
    "increment",
    "applying",
    "operator",
    "recursively",
    "get",
    "operating",
    "0",
    "times",
    "lag",
    "one",
    "time",
    "one",
    "lag",
    "two",
    "times",
    "two",
    "lags",
    "iteratively",
    "thinking",
    "dealing",
    "like",
    "transformation",
    "infinite",
    "dimensional",
    "space",
    "like",
    "identity",
    "matrix",
    "sort",
    "shifted",
    "one",
    "element",
    "identity",
    "element",
    "like",
    "identity",
    "matrix",
    "shifted",
    "one",
    "column",
    "two",
    "columns",
    "anyway",
    "inverses",
    "operators",
    "well",
    "defined",
    "terms",
    "get",
    "represent",
    "wold",
    "representation",
    "terms",
    "lag",
    "operators",
    "saying",
    "stochastic",
    "process",
    "equal",
    "plus",
    "psi",
    "l",
    "function",
    "basically",
    "functional",
    "lag",
    "operator",
    "potentially",
    "polynomial",
    "lags",
    "notation",
    "something",
    "need",
    "get",
    "familiar",
    "going",
    "comfortable",
    "different",
    "models",
    "introduced",
    "arma",
    "arima",
    "models",
    "questions",
    "relating",
    "let",
    "introduce",
    "come",
    "somewhat",
    "later",
    "impulse",
    "response",
    "function",
    "covariance",
    "stationary",
    "process",
    "stochastic",
    "process",
    "given",
    "wold",
    "representation",
    "ask",
    "happens",
    "innovation",
    "time",
    "affect",
    "process",
    "time",
    "ok",
    "pretend",
    "chairman",
    "federal",
    "reserve",
    "bank",
    "interested",
    "gnp",
    "basically",
    "economic",
    "growth",
    "considering",
    "changing",
    "interest",
    "rates",
    "help",
    "economy",
    "well",
    "like",
    "know",
    "impact",
    "change",
    "factor",
    "going",
    "affect",
    "variable",
    "interest",
    "perhaps",
    "gnp",
    "case",
    "thinking",
    "simple",
    "covariance",
    "stationary",
    "stochastic",
    "process",
    "basically",
    "process",
    "random",
    "weighted",
    "sum",
    "moving",
    "average",
    "innovations",
    "question",
    "basically",
    "covariance",
    "stationary",
    "process",
    "could",
    "represented",
    "form",
    "impulse",
    "response",
    "function",
    "relates",
    "impact",
    "impact",
    "time",
    "basically",
    "affects",
    "process",
    "time",
    "moving",
    "average",
    "process",
    "affects",
    "plus",
    "1",
    "affects",
    "plus",
    "impulse",
    "response",
    "basically",
    "derivative",
    "value",
    "process",
    "previous",
    "innovation",
    "given",
    "different",
    "innovations",
    "impact",
    "current",
    "value",
    "given",
    "impulse",
    "response",
    "function",
    "looking",
    "backward",
    "definition",
    "pretty",
    "well",
    "defined",
    "also",
    "think",
    "impact",
    "innovation",
    "affect",
    "process",
    "going",
    "forward",
    "cumulative",
    "response",
    "essentially",
    "impact",
    "innovation",
    "process",
    "ultimately",
    "eventually",
    "going",
    "change",
    "value",
    "process",
    "value",
    "process",
    "moving",
    "one",
    "innovation",
    "long",
    "run",
    "cumulative",
    "response",
    "given",
    "basically",
    "sum",
    "individual",
    "ones",
    "given",
    "sum",
    "polynomial",
    "psi",
    "lag",
    "operator",
    "replace",
    "lag",
    "operator",
    "see",
    "talk",
    "vector",
    "autoregressive",
    "processes",
    "multivariate",
    "time",
    "series",
    "wold",
    "representation",
    "moving",
    "average",
    "possibly",
    "infinite",
    "order",
    "autoregressive",
    "representation",
    "suppose",
    "another",
    "polynomial",
    "star",
    "lags",
    "going",
    "call",
    "psi",
    "inverse",
    "l",
    "satisfies",
    "fact",
    "multiply",
    "psi",
    "l",
    "get",
    "identity",
    "lag",
    "psi",
    "inverse",
    "exists",
    "basically",
    "inverse",
    "psi",
    "start",
    "psi",
    "l",
    "invertible",
    "exists",
    "psi",
    "inverse",
    "l",
    "coefficients",
    "star",
    "one",
    "basically",
    "take",
    "original",
    "expression",
    "stochastic",
    "process",
    "moving",
    "average",
    "eta",
    "express",
    "essentially",
    "moving",
    "averages",
    "x",
    "essentially",
    "inverted",
    "process",
    "shown",
    "stochastic",
    "process",
    "expressed",
    "infinite",
    "order",
    "autoregressive",
    "representation",
    "infinite",
    "order",
    "autoregressive",
    "representation",
    "corresponds",
    "intuitive",
    "understanding",
    "wold",
    "representation",
    "exists",
    "actually",
    "works",
    "regression",
    "coefficients",
    "projection",
    "several",
    "slides",
    "back",
    "corresponds",
    "inverse",
    "operator",
    "let",
    "turn",
    "specific",
    "time",
    "series",
    "models",
    "widely",
    "used",
    "class",
    "autoregressive",
    "moving",
    "average",
    "processes",
    "mathematical",
    "definition",
    "define",
    "equal",
    "linear",
    "combination",
    "lags",
    "x",
    "going",
    "back",
    "p",
    "lags",
    "coefficients",
    "residuals",
    "expressed",
    "terms",
    "order",
    "moving",
    "average",
    "framework",
    "white",
    "noise",
    "white",
    "noise",
    "reiterate",
    "mean",
    "0",
    "constant",
    "variance",
    "zero",
    "covariance",
    "representation",
    "simplified",
    "things",
    "little",
    "bit",
    "subtracting",
    "mean",
    "x",
    "makes",
    "formulas",
    "little",
    "bit",
    "simpler",
    "lag",
    "operators",
    "write",
    "arma",
    "model",
    "phi",
    "l",
    "order",
    "polynomial",
    "lag",
    "l",
    "given",
    "coefficients",
    "1",
    "theta",
    "l",
    "given",
    "1",
    "basically",
    "representation",
    "arma",
    "time",
    "series",
    "model",
    "basically",
    "taking",
    "set",
    "lags",
    "values",
    "stochastic",
    "process",
    "order",
    "equal",
    "weighted",
    "average",
    "multiply",
    "inverse",
    "phi",
    "l",
    "exists",
    "get",
    "representation",
    "simply",
    "wold",
    "decomposition",
    "arma",
    "models",
    "basically",
    "wold",
    "decomposition",
    "phi",
    "l",
    "invertible",
    "explore",
    "looking",
    "simpler",
    "cases",
    "arma",
    "models",
    "focusing",
    "autoregressive",
    "models",
    "first",
    "moving",
    "average",
    "processes",
    "second",
    "get",
    "better",
    "feel",
    "things",
    "manipulated",
    "interpreted",
    "let",
    "move",
    "order",
    "autoregressive",
    "process",
    "going",
    "consider",
    "arma",
    "models",
    "autoregressive",
    "terms",
    "phi",
    "l",
    "minus",
    "mu",
    "equal",
    "white",
    "noise",
    "linear",
    "combination",
    "series",
    "white",
    "noise",
    "follows",
    "linear",
    "regression",
    "model",
    "explanatory",
    "variables",
    "lags",
    "process",
    "could",
    "expressed",
    "equal",
    "c",
    "plus",
    "sum",
    "1",
    "p",
    "linear",
    "regression",
    "model",
    "regression",
    "parameters",
    "c",
    "constant",
    "term",
    "equal",
    "mu",
    "times",
    "phi",
    "basically",
    "take",
    "expectations",
    "process",
    "basically",
    "coefficients",
    "mu",
    "coming",
    "terms",
    "phi",
    "1",
    "times",
    "mu",
    "regression",
    "coefficient",
    "autoregressive",
    "model",
    "want",
    "go",
    "stationarity",
    "conditions",
    "certainly",
    "autoregressive",
    "model",
    "one",
    "well",
    "simple",
    "random",
    "walk",
    "follows",
    "autoregressive",
    "model",
    "stationary",
    "highlight",
    "minute",
    "well",
    "think",
    "true",
    "stationarity",
    "something",
    "understood",
    "evaluated",
    "polynomial",
    "function",
    "phi",
    "replace",
    "lag",
    "operator",
    "l",
    "z",
    "complex",
    "variable",
    "equation",
    "phi",
    "z",
    "equal",
    "0",
    "characteristic",
    "equation",
    "associated",
    "autoregressive",
    "model",
    "turns",
    "interested",
    "roots",
    "characteristic",
    "equation",
    "consider",
    "writing",
    "phi",
    "l",
    "function",
    "roots",
    "equation",
    "get",
    "expression",
    "notice",
    "multiply",
    "terms",
    "1",
    "multiply",
    "together",
    "get",
    "lag",
    "operator",
    "l",
    "power",
    "would",
    "product",
    "1",
    "times",
    "1",
    "actually",
    "negative",
    "1",
    "times",
    "negative",
    "1",
    "forth",
    "negative",
    "1",
    "basically",
    "p",
    "roots",
    "equation",
    "would",
    "written",
    "process",
    "covariance",
    "stationary",
    "roots",
    "characteristic",
    "equation",
    "lie",
    "outside",
    "unit",
    "circle",
    "mean",
    "means",
    "norm",
    "modulus",
    "complex",
    "z",
    "greater",
    "outside",
    "unit",
    "circle",
    "less",
    "equal",
    "roots",
    "outside",
    "unit",
    "circle",
    "modulus",
    "greater",
    "consider",
    "taking",
    "complex",
    "number",
    "lambda",
    "basically",
    "root",
    "expression",
    "1",
    "minus",
    "1",
    "lambda",
    "l",
    "inverse",
    "get",
    "series",
    "expression",
    "inverse",
    "series",
    "exist",
    "bounded",
    "greater",
    "1",
    "magnitude",
    "actually",
    "compute",
    "inverse",
    "phi",
    "l",
    "taking",
    "inverse",
    "component",
    "products",
    "polynomial",
    "introductory",
    "time",
    "series",
    "courses",
    "talk",
    "stationarity",
    "unit",
    "roots",
    "really",
    "get",
    "people",
    "know",
    "complex",
    "math",
    "know",
    "roots",
    "anyway",
    "simply",
    "framework",
    "applied",
    "polynomial",
    "equation",
    "characteristic",
    "equation",
    "whose",
    "roots",
    "looking",
    "roots",
    "outside",
    "unit",
    "circle",
    "stationarity",
    "process",
    "well",
    "basically",
    "conditions",
    "invertibility",
    "process",
    "autoregressive",
    "process",
    "invertibility",
    "renders",
    "process",
    "moving",
    "average",
    "process",
    "let",
    "go",
    "results",
    "autoregressive",
    "process",
    "order",
    "one",
    "things",
    "always",
    "start",
    "simplest",
    "cases",
    "understand",
    "things",
    "characteristic",
    "equation",
    "model",
    "1",
    "minus",
    "phi",
    "root",
    "lambda",
    "greater",
    "1",
    "modulus",
    "lambda",
    "greater",
    "1",
    "meaning",
    "root",
    "outside",
    "unit",
    "circle",
    "phi",
    "less",
    "covariance",
    "stationarity",
    "autoregressive",
    "process",
    "need",
    "magnitude",
    "phi",
    "less",
    "1",
    "magnitude",
    "expected",
    "value",
    "x",
    "mu",
    "variance",
    "x",
    "sigma",
    "squared",
    "form",
    "sigma",
    "squared",
    "1",
    "minus",
    "phi",
    "expression",
    "basically",
    "obtained",
    "looking",
    "infinite",
    "order",
    "moving",
    "average",
    "representation",
    "notice",
    "phi",
    "positive",
    "variance",
    "x",
    "actually",
    "greater",
    "variance",
    "innovations",
    "phi",
    "less",
    "0",
    "going",
    "smaller",
    "innovation",
    "variance",
    "basically",
    "scaled",
    "bit",
    "autoregressive",
    "process",
    "covariance",
    "matrix",
    "phi",
    "times",
    "sigma",
    "squared",
    "going",
    "problem",
    "set",
    "covariance",
    "x",
    "phi",
    "j",
    "power",
    "sigma",
    "squared",
    "expressions",
    "easily",
    "evaluated",
    "simply",
    "writing",
    "definition",
    "covariances",
    "terms",
    "original",
    "model",
    "looking",
    "terms",
    "independent",
    "cancel",
    "proceeds",
    "let",
    "go",
    "cases",
    "let",
    "show",
    "phi",
    "0",
    "1",
    "process",
    "experiences",
    "exponential",
    "mean",
    "reversion",
    "mu",
    "autoregressive",
    "process",
    "phi",
    "0",
    "1",
    "corresponds",
    "process",
    "process",
    "actually",
    "one",
    "used",
    "theoretically",
    "interest",
    "rate",
    "models",
    "lot",
    "theoretical",
    "work",
    "finance",
    "vasicek",
    "model",
    "actually",
    "example",
    "process",
    "basically",
    "brownian",
    "motion",
    "variables",
    "exhibit",
    "could",
    "thought",
    "exhibiting",
    "mean",
    "reversion",
    "model",
    "applied",
    "processes",
    "interest",
    "rate",
    "spreads",
    "real",
    "exchange",
    "rates",
    "variables",
    "one",
    "expect",
    "things",
    "never",
    "get",
    "large",
    "small",
    "come",
    "back",
    "mean",
    "challenge",
    "usually",
    "may",
    "true",
    "short",
    "periods",
    "time",
    "long",
    "periods",
    "time",
    "point",
    "reverting",
    "changes",
    "models",
    "tend",
    "broad",
    "application",
    "long",
    "time",
    "ranges",
    "need",
    "adapt",
    "anyway",
    "ar",
    "process",
    "also",
    "negative",
    "values",
    "phi",
    "results",
    "exponential",
    "mean",
    "reversion",
    "oscillating",
    "time",
    "autoregressive",
    "coefficient",
    "basically",
    "negative",
    "value",
    "phi",
    "equal",
    "1",
    "wold",
    "decomposition",
    "exist",
    "process",
    "simple",
    "random",
    "walk",
    "basically",
    "phi",
    "equal",
    "1",
    "means",
    "basically",
    "changes",
    "value",
    "process",
    "independent",
    "identically",
    "distributed",
    "white",
    "noise",
    "random",
    "walk",
    "process",
    "process",
    "covered",
    "earlier",
    "lectures",
    "phi",
    "greater",
    "1",
    "explosive",
    "process",
    "basically",
    "values",
    "scaling",
    "every",
    "time",
    "increment",
    "features",
    "ar",
    "1",
    "model",
    "general",
    "autoregressive",
    "process",
    "order",
    "p",
    "method",
    "well",
    "look",
    "second",
    "order",
    "moments",
    "process",
    "nice",
    "structure",
    "use",
    "solve",
    "estimates",
    "arma",
    "parameters",
    "autoregressive",
    "parameters",
    "happen",
    "specified",
    "called",
    "equations",
    "equations",
    "standard",
    "topic",
    "time",
    "series",
    "analysis",
    "correspond",
    "well",
    "take",
    "original",
    "autoregressive",
    "process",
    "order",
    "write",
    "formulas",
    "covariance",
    "lag",
    "j",
    "two",
    "observations",
    "covariance",
    "expression",
    "given",
    "equation",
    "equation",
    "gamma",
    "j",
    "determined",
    "simply",
    "evaluating",
    "expectations",
    "taking",
    "expectation",
    "autoregressive",
    "process",
    "times",
    "fix",
    "minus",
    "mu",
    "evaluating",
    "terms",
    "validate",
    "equation",
    "look",
    "equations",
    "corresponding",
    "j",
    "equals",
    "1",
    "lag",
    "1",
    "lag",
    "p",
    "equations",
    "look",
    "like",
    "basically",
    "side",
    "covariance",
    "lag",
    "1",
    "lag",
    "p",
    "equal",
    "basically",
    "linear",
    "functions",
    "given",
    "phi",
    "covariances",
    "tell",
    "structure",
    "matrix",
    "diagonal",
    "matrix",
    "kind",
    "matrix",
    "math",
    "trivia",
    "question",
    "special",
    "name",
    "anyone",
    "toeplitz",
    "matrix",
    "diagonals",
    "value",
    "fact",
    "symmetry",
    "covariance",
    "basically",
    "gamma",
    "1",
    "equal",
    "gamma",
    "minus",
    "gamma",
    "minus",
    "2",
    "equal",
    "gamma",
    "plus",
    "covariant",
    "stationarity",
    "actually",
    "also",
    "symmetric",
    "equations",
    "allow",
    "us",
    "solve",
    "phis",
    "long",
    "estimates",
    "covariances",
    "system",
    "estimates",
    "plug",
    "attempt",
    "solve",
    "consistent",
    "estimates",
    "covariances",
    "solution",
    "0th",
    "equation",
    "part",
    "series",
    "equations",
    "go",
    "back",
    "look",
    "0th",
    "equation",
    "allows",
    "get",
    "estimate",
    "sigma",
    "squared",
    "equations",
    "way",
    "many",
    "arma",
    "models",
    "specified",
    "different",
    "statistics",
    "packages",
    "terms",
    "principles",
    "applied",
    "well",
    "using",
    "unbiased",
    "estimates",
    "parameters",
    "applying",
    "called",
    "method",
    "moments",
    "principle",
    "statistical",
    "estimation",
    "complicated",
    "models",
    "sometimes",
    "likelihood",
    "functions",
    "hard",
    "specify",
    "compute",
    "optimization",
    "even",
    "harder",
    "turn",
    "relationships",
    "moments",
    "random",
    "variables",
    "functions",
    "unknown",
    "parameters",
    "solve",
    "basically",
    "sample",
    "moments",
    "equalling",
    "theoretical",
    "moments",
    "apply",
    "method",
    "moments",
    "estimation",
    "method",
    "econometrics",
    "rich",
    "many",
    "applications",
    "principle",
    "next",
    "section",
    "goes",
    "moving",
    "average",
    "model",
    "let",
    "highlight",
    "order",
    "q",
    "moving",
    "average",
    "basically",
    "polynomial",
    "lag",
    "operator",
    "l",
    "operated",
    "upon",
    "write",
    "expectations",
    "get",
    "mu",
    "variance",
    "gamma",
    "0",
    "sigma",
    "squared",
    "times",
    "1",
    "plus",
    "squares",
    "coefficients",
    "polynomial",
    "feature",
    "property",
    "due",
    "fact",
    "uncorrelated",
    "innovations",
    "eta",
    "white",
    "noise",
    "thing",
    "comes",
    "square",
    "expectation",
    "squared",
    "powers",
    "etas",
    "coefficients",
    "given",
    "squared",
    "properties",
    "left",
    "leave",
    "verify",
    "straightforward",
    "let",
    "turn",
    "final",
    "minutes",
    "lecture",
    "today",
    "accommodating",
    "behavior",
    "time",
    "series",
    "original",
    "approaches",
    "time",
    "series",
    "focus",
    "estimation",
    "methodologies",
    "covariance",
    "stationary",
    "process",
    "series",
    "covariance",
    "stationary",
    "would",
    "want",
    "transformation",
    "data",
    "series",
    "stationary",
    "resulting",
    "process",
    "stationary",
    "differencing",
    "operators",
    "delta",
    "box",
    "jenkins",
    "advocated",
    "moving",
    "trending",
    "behavior",
    "exhibited",
    "often",
    "economic",
    "time",
    "series",
    "using",
    "first",
    "difference",
    "maybe",
    "second",
    "difference",
    "order",
    "difference",
    "operators",
    "defined",
    "way",
    "basically",
    "order",
    "operator",
    "expression",
    "binomial",
    "expansion",
    "power",
    "useful",
    "comes",
    "time",
    "probability",
    "theory",
    "process",
    "linear",
    "time",
    "trend",
    "delta",
    "going",
    "time",
    "trend",
    "basically",
    "taking",
    "linear",
    "component",
    "taking",
    "successive",
    "differences",
    "sometimes",
    "real",
    "series",
    "look",
    "difference",
    "appears",
    "look",
    "first",
    "differences",
    "still",
    "appear",
    "growing",
    "time",
    "case",
    "sometimes",
    "second",
    "difference",
    "result",
    "process",
    "trend",
    "sort",
    "convenient",
    "tricks",
    "techniques",
    "render",
    "series",
    "stationary",
    "let",
    "see",
    "examples",
    "linear",
    "trend",
    "reversion",
    "models",
    "rendered",
    "covariance",
    "stationary",
    "first",
    "differencing",
    "case",
    "example",
    "deterministic",
    "time",
    "trend",
    "reversion",
    "time",
    "trend",
    "time",
    "basically",
    "error",
    "deterministic",
    "trend",
    "first",
    "order",
    "autoregressive",
    "process",
    "moments",
    "derived",
    "way",
    "leave",
    "exercise",
    "one",
    "could",
    "also",
    "consider",
    "pure",
    "integrated",
    "process",
    "talk",
    "stochastic",
    "trends",
    "basically",
    "random",
    "walk",
    "processes",
    "often",
    "referred",
    "econometrics",
    "stochastic",
    "trends",
    "may",
    "want",
    "try",
    "remove",
    "data",
    "accommodate",
    "stochastic",
    "trend",
    "process",
    "basically",
    "given",
    "first",
    "difference",
    "equal",
    "essentially",
    "random",
    "walk",
    "given",
    "starting",
    "point",
    "easy",
    "verify",
    "knew",
    "0th",
    "point",
    "variance",
    "time",
    "point",
    "would",
    "sigma",
    "squared",
    "summing",
    "independent",
    "innovations",
    "covariance",
    "lag",
    "minus",
    "j",
    "simply",
    "minus",
    "j",
    "sigma",
    "squared",
    "correlation",
    "form",
    "see",
    "definitely",
    "depends",
    "time",
    "stationary",
    "process",
    "first",
    "differencing",
    "results",
    "stationarity",
    "end",
    "difference",
    "process",
    "features",
    "let",
    "see",
    "final",
    "topic",
    "today",
    "incorporate",
    "process",
    "arma",
    "processes",
    "well",
    "take",
    "first",
    "differences",
    "second",
    "differences",
    "resulting",
    "process",
    "covariance",
    "stationary",
    "incorporate",
    "differencing",
    "model",
    "specification",
    "define",
    "arima",
    "models",
    "autoregressive",
    "integrated",
    "moving",
    "average",
    "processes",
    "specify",
    "models",
    "need",
    "determine",
    "order",
    "differencing",
    "required",
    "move",
    "trends",
    "deterministic",
    "stochastic",
    "estimating",
    "unknown",
    "parameters",
    "applying",
    "model",
    "selection",
    "criteria",
    "let",
    "go",
    "quickly",
    "come",
    "back",
    "beginning",
    "next",
    "time",
    "specifying",
    "parameters",
    "models",
    "apply",
    "maximum",
    "likelihood",
    "assume",
    "normality",
    "innovations",
    "express",
    "arma",
    "model",
    "state",
    "space",
    "form",
    "results",
    "form",
    "likelihood",
    "function",
    "see",
    "lectures",
    "ahead",
    "apply",
    "limited",
    "information",
    "maximum",
    "likelihood",
    "condition",
    "first",
    "observations",
    "data",
    "maximize",
    "likelihood",
    "condition",
    "first",
    "observations",
    "also",
    "use",
    "information",
    "well",
    "look",
    "density",
    "functions",
    "incorporating",
    "likelihood",
    "relative",
    "stationary",
    "distribution",
    "values",
    "issue",
    "becomes",
    "choose",
    "amongst",
    "different",
    "models",
    "last",
    "time",
    "talked",
    "linear",
    "regression",
    "models",
    "specify",
    "given",
    "model",
    "talking",
    "autoregressive",
    "moving",
    "average",
    "even",
    "integrated",
    "moving",
    "average",
    "processes",
    "specify",
    "well",
    "method",
    "maximum",
    "likelihood",
    "procedures",
    "measures",
    "effectively",
    "fitted",
    "model",
    "given",
    "information",
    "criterion",
    "would",
    "want",
    "minimize",
    "given",
    "fitted",
    "model",
    "consider",
    "different",
    "sets",
    "models",
    "different",
    "numbers",
    "explanatory",
    "variables",
    "different",
    "orders",
    "autoregressive",
    "parameters",
    "moving",
    "average",
    "parameters",
    "compute",
    "say",
    "akaike",
    "information",
    "criterion",
    "bayes",
    "information",
    "criterion",
    "criterion",
    "different",
    "ways",
    "judging",
    "good",
    "different",
    "models",
    "let",
    "finish",
    "today",
    "pointing",
    "information",
    "criteria",
    "basically",
    "function",
    "log",
    "likelihood",
    "function",
    "something",
    "trying",
    "maximize",
    "maximum",
    "likelihood",
    "estimates",
    "adding",
    "penalty",
    "many",
    "parameters",
    "estimating",
    "like",
    "think",
    "next",
    "time",
    "kind",
    "penalty",
    "appropriate",
    "adding",
    "extra",
    "parameter",
    "like",
    "evidence",
    "required",
    "incorporate",
    "extra",
    "parameters",
    "extra",
    "variables",
    "model",
    "would",
    "statistics",
    "exceeds",
    "threshold",
    "criteria",
    "turns",
    "related",
    "issues",
    "interesting",
    "play",
    "say",
    "actually",
    "seen",
    "bayes",
    "information",
    "criterion",
    "corresponds",
    "assumption",
    "finite",
    "number",
    "variables",
    "model",
    "know",
    "criterion",
    "says",
    "maybe",
    "infinite",
    "number",
    "variables",
    "model",
    "want",
    "able",
    "identify",
    "anyway",
    "challenging",
    "problem",
    "model",
    "selection",
    "criteria",
    "used",
    "specify",
    "go",
    "next",
    "time"
  ],
  "keywords": [
    "help",
    "professor",
    "well",
    "last",
    "time",
    "talking",
    "regression",
    "analysis",
    "estimation",
    "fitting",
    "models",
    "want",
    "method",
    "maximum",
    "likelihood",
    "really",
    "modeling",
    "let",
    "normal",
    "linear",
    "model",
    "variable",
    "independent",
    "variables",
    "given",
    "parameter",
    "beta",
    "cases",
    "distributed",
    "random",
    "vector",
    "n",
    "function",
    "equal",
    "density",
    "data",
    "one",
    "defined",
    "sample",
    "expression",
    "sigma",
    "squared",
    "evaluating",
    "probability",
    "unknown",
    "parameters",
    "simply",
    "distribution",
    "mean",
    "variance",
    "would",
    "mu",
    "look",
    "underlying",
    "estimates",
    "values",
    "maximize",
    "good",
    "could",
    "first",
    "two",
    "response",
    "functions",
    "sum",
    "case",
    "basically",
    "expressed",
    "matrix",
    "form",
    "way",
    "q",
    "least",
    "squares",
    "criteria",
    "extra",
    "term",
    "minus",
    "actually",
    "going",
    "also",
    "log",
    "estimate",
    "taking",
    "derivative",
    "respect",
    "order",
    "things",
    "get",
    "us",
    "residuals",
    "biased",
    "audience",
    "inaudible",
    "1",
    "estimating",
    "say",
    "constant",
    "corresponding",
    "x",
    "freedom",
    "p",
    "observations",
    "fit",
    "0",
    "need",
    "turns",
    "add",
    "fitted",
    "fact",
    "using",
    "different",
    "know",
    "problem",
    "theorem",
    "go",
    "back",
    "highlight",
    "noise",
    "take",
    "value",
    "deviation",
    "apply",
    "consider",
    "another",
    "h",
    "evaluated",
    "individual",
    "define",
    "estimators",
    "square",
    "residual",
    "modulus",
    "mathematical",
    "solve",
    "negative",
    "plus",
    "specify",
    "large",
    "magnitude",
    "perhaps",
    "criterion",
    "less",
    "tau",
    "study",
    "see",
    "simple",
    "returns",
    "market",
    "return",
    "stock",
    "series",
    "ge",
    "rate",
    "thinking",
    "interest",
    "excess",
    "related",
    "like",
    "nice",
    "point",
    "set",
    "looking",
    "influential",
    "impact",
    "definition",
    "terms",
    "factor",
    "many",
    "crude",
    "oil",
    "something",
    "used",
    "essentially",
    "goes",
    "associated",
    "applied",
    "think",
    "process",
    "today",
    "second",
    "results",
    "turn",
    "talk",
    "introduce",
    "wold",
    "representation",
    "stationarity",
    "autoregressive",
    "moving",
    "average",
    "integrated",
    "stationary",
    "stochastic",
    "finite",
    "times",
    "structure",
    "covariance",
    "gamma",
    "correlation",
    "root",
    "bit",
    "means",
    "coefficient",
    "come",
    "processes",
    "deterministic",
    "trend",
    "coefficients",
    "white",
    "exists",
    "decomposition",
    "number",
    "projection",
    "lag",
    "z",
    "lags",
    "compute",
    "psi",
    "long",
    "infinity",
    "required",
    "infinite",
    "operator",
    "l",
    "identity",
    "anyway",
    "operators",
    "polynomial",
    "arma",
    "impulse",
    "innovation",
    "innovations",
    "inverse",
    "multiply",
    "original",
    "corresponds",
    "phi",
    "walk",
    "complex",
    "equation",
    "characteristic",
    "roots",
    "outside",
    "unit",
    "circle",
    "greater",
    "lambda",
    "j",
    "covariances",
    "reversion",
    "moments",
    "equations",
    "next",
    "differencing",
    "difference",
    "differences",
    "information"
  ]
}