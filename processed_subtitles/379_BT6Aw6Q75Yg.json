{
  "text": "every single machine learning algorithm\nexplained in case you don't know an\nalgorithm is a set of commands that must\nbe followed for a computer to perform\ncalculations or like other\nproblemsolving operations according to\nits formal definition an algorithm is a\nfinite set of instructions carried out\nin a specific order to perform a\nparticular task it's not an entire\nprogram or code it is simple logic to a\nproblem linear regression is a\nsupervised learning algorithm and tries\nto model their relationship between a\ncontinuous Target variable and one or\nmore independent VAR variables by\nfitting a linear equation to the data\ntake this chart of dots for example a\nlinear regression model tries to fit a\nregression line to the data points that\nbest represents the relations or\ncorrelations with this method the best\nregression line is found by minimizing\nthe sum of squares of the distance\nbetween the data points and the\nregression line so for these data points\nthe regression line Looks like this\nsupport Vector machine or svm for short\nis a supervised learning algorithm and\nis mostly used for classification tasks\nbut is also suitable for regression R\ntasks svm distinguishes classes by\ndrawing a decision boundary how to draw\nor determine the decision boundary is\nthe most critical part in svm algorithms\nbefore creating the decision boundary\neach observation or data point is\nplotted in N dimensional space with n\nbeing the number of features used for\nexample if we use length and width to\nclassify different cells observations\nare plotted in a two-dimensional space\nand decision boundary is a line if we\nuse three features decision boundary is\na plane in three-dimensional space if we\nuse more than three features decision\nboundary becomes a hyper plane which is\nreally hard to visualize decision\nboundary is drawn in a way that the\ndistance to support vectors are\nmaximized if the decision boundary is\ntoo close to a support Vector it'll be\nhighly sensitive to noises and not\ngeneralize well even very small changes\nto independent variables may cause a\nmisclassification svm is especially\neffective in cases where number of\ndimensions are more than the number of\nsamples when finding the decision\nboundary svm uses a subset of training\npoints rather than all points which\nmakes it memory efficient on the other\nhand training time increases for large\ndata sets which negatively affects the\nperformance Nave Bas is a supervised\nlearning algorithm used for\nclassification tasks hence it is also\ncalled Nave Bas classifier Nave Bas\nassumes that features are independent of\neach other and there is no correlation\nbetween features however this is not the\ncase in real life this Nave Assumption\nof features being uncorrelated is the\nreason why this algorithm is called\nnaive the intuition behind naive Bay\nalgorithm is the Bas theorem p a is the\nprobability of event a given event B has\nalready occurred PBA is probability of\nevent B given event a has already\noccurred PA is the probability of event\na and PB is the probability of event B\nnaive Bas classifier calculates the\nprobability of a class given a set of\nfeature values the the assumption that\nall features are independent makes knif\nbased algorithm very fast when compared\nto complicated algorithms in some cases\nspeed is preferred over higher accuracy\nbut on the other hand the same\nassumption makes knife Bas algorithm\nless accurate than complicated\nalgorithms logistic regression is a\nsupervised learning algorithm which is\nmostly used for binary classification\nproblems logistic regression is a simple\nyet very effective classification\nalgorithm so it is commonly used for\nmany binary classification tasks things\nlike customer turn spam email website or\nad click predictions are some examples\nof the areas where logistic regression\noffers a powerful solution the basis of\nlogistic regression is the logistic\nfunction also called the sigmoid\nfunction which takes any real value\nnumber and Maps it to a value between 0o\nand 1 Let's consider we have the\nfollowing linear equation to solve\nlogistic regression model takes a linear\nequation as input and uses logistic\nfunction and log odds to perform a\nbinary classification task then we will\nget the f famous shaped graph of\nlogistic regression we can use the\ncalculated probability as is for example\nthe output can be the probability that\nthis email is Spam is 95% or the\nprobability that the customer will click\non the ad is 70% however in most cases\nprobabilities are used to classify data\npoints for example if the probability is\ngreater than 50% the prediction is\npositive class or one otherwise the\nprediction is negative class or zero K\nnearest Neighbors or K&N for short is a\nsupervised learning algorithm that can\nbe used to solve both classification and\nregression tasks the main idea behind\nKNN is that the value of a class or of a\ndata point is determined by the data\npoints around it KNN classifier\ndetermines the class of a data point by\nmajority voting principle for instance\nif K is set to five the classes of five\nclosest points are checked prediction is\ndone according to the majority class\nsimilarly K&N regression takes the mean\nvalue of five CL closest points let's go\nover an example consider the following\ndata points that belong to four\ndifferent classes and let's see how the\npredicted classes change according to\nthe K value it is very important to\ndetermine an optimal K value if K is too\nlow the model is too specific and not\ngeneralized well it also tends to be too\nsensitive to noise the model\naccomplishes a high accuracy on train\nset but will be a poor predictor on new\npreviously unseen data points therefore\nwe are likely to end up with an overfit\nmodel on the the other hand if K is too\nlarge the model is too generalized and\nis not a good predictor on both train\nand test sets this situation is known as\nunderfitting KNN is simple and easy to\ninterpret it does not make any\nassumption so it can be implemented in\nnonlinear tasks KNN does become very\nslow as number of data points increases\nbecause the model needs to store all\ndata points thus it is not memory\nefficient another downside of KNN is\nthat it is sensitive to outliers\ndecision trees work by iteratively\nasking questions to partition data it is\neasier to conceptualize the partitioning\ndata with a visual representation of a\ndecision tree this represents a decision\ntree to predict customer CH first split\nis based on monthly charges amount then\nthe algorithm keeps asking questions to\nseparate class labels the question get\nmore specific as the tree gets deeper\nthe aim is to increase the\npredictiveness as much as possible at\neach partitioning so that the model\nkeeps gaining information about the data\nset randomly splitting the feature does\nnot usually give us the valuable insight\ninto the data set it's the splits that\nincrease purity of nodes that are most\ninformative the purity of a node is\ninversely proportional to the\ndistribution of different classes in\nthat node the questions to ask are\nchosen in a way that increases Purity or\ndecreases impurity but how many\nquestions do we ask when do we stop when\nis our tree sufficient to solve our\nclassification problem the answer to all\nof these questions leads us to one of\nthe most important Concepts in machine\nlearning overfitting the model can keep\nasking questions until all nodes are\npure however this would be a two\nspecific model and would not generalize\nwill it achieves high accuracy with\ntraining set but performs poorly on new\npreviously unseen data points which\nindicates overfitting decision tree\nalgorithm usually does not require to\nnormalize or scale features it is also\nsuitable to work on a mixture of feature\ndata types on the negative side it is\nprone to overfitting and needs to be\nensembled in order to generalize well\nrandom Forest is an ensemble of many\ndecision trees random forests are built\nusing a method called bagging in which\ndecision trees are used as par parel\nestimators if used for a classification\nproblem the result is based on majority\nvote of the results received from each\ndecision tree for regression the\nprediction of a leaf node is the mean\nvalue of the target values in that leaf\nrandom Forest regression takes mean\nvalues of results from decision trees\nrandom forests reduce the risk of\noverfitting and accuracy is much higher\nthan a single decision tree furthermore\ndecision trees in a random forest run in\nparallel so that the time does not\nbecome a bottleneck the success of a\nrandom Forest highly depends on using\nuncorrelated decision trees if we use\nthe same or very similar trees the\noverall result will not be much\ndifferent than the result of a single\ndecision tree random forests achieve to\nhave uncorrelated decision trees by\nbootstrapping and feature Randomness\nbootstrapping is randomly selecting\nsamples from training data with\nreplacement they are called the\nbootstrap samples feature Randomness is\nachieved by selecting features randomly\nfor each decision Tree in a random\nForest the number of features used for\neach tree in a random Forest can be\ncontrolled with maxcore features\nparameter random Forest is a highly\naccurate model on many different\nproblems and does not require\nnormalization or scaling however it is\nnot a good choice for high dimensional\ndata sets compared to fast linear models\ngradient boosted decision trees or gbdt\nfor short is an ensemble algorithm which\nuses boosting methods to combine\nindividual decision trees boosting means\ncombining a learning algorithm in series\nto achieve a strong learner from many\nsequentially connect weak Learners in\nthe case of gbdt the weak Learners are\nthe decision trees each tree attempts to\nminimize the errors of previous tree\ntrees in boosting are weak learners but\nadding many trees in series and each\nfocusing on the errors from the previous\none make boosting a highly efficient and\naccurate model unlike bagging boosta\ndoes not involve bootstrap sampling\nevery time a new tree is added it fits\non a modified version of the initial\ndata set since trees are added\nsequentially boosting algorithms learn\nslowly in statistical learning models\nthat learn slowly perform better gbdt is\nvery efficient on both classification\nand regression tasks and provides more\naccurate predictions compared to random\nForest it can handle mixed type of\nfeatures and no pre-processing is needed\ngbdt does require careful tuning of\nhyperparameters in order to prevent the\nmodel from overfitting K means\nclustering clustering is a way to group\na set of data points in a way that\nsimilar data points are grouped together\ntherefore clustering algorithms look for\nsimilarities or dissimilarities among\ndata points clustering is an\nunsupervised learning method so there is\nno label associated with data points\nclustering algorithms try to find the\nunderlying structure of the data\nobservations or data points in a\nclassification task have labels each\nobservation is classified according to\nsome measurements classification\nalgorithms try to model the relationship\nbetween measurements on observations and\ntheir assigned class then the model\npredicts the class of new observations K\nmeans clustering aims to partition data\ninto K clusters in a way that data\npoints in the same cluster are similar\nand data points in different clusters\nare further apart thus it is a partition\nbased clustering technique similarity of\ntwo points is determined by the distance\nbetween them consider the following 2D\nvisualization of a data set it can be\npartied into four different clusters now\nreal life data sets are much more\ncomplex in which clusters are not\nclearly separated however the algorithm\nworks in the same way K means is an\niterative process it is built on\nexpectation maximization algorithm after\nthe number of clusters are determined it\nworks by executing the following steps\nnumber one it randomly selects the\ncentroids or the center of cluster for\neach cluster then it calculates the\ndistance of all data points to the\ncentroids it assigns the data points to\nthe closest cluster it finds the new\ncentroids of each cluster by taking the\nmean of all data points in the cluster\nand it repeats steps 2 3 and four until\nall points converge and cluster Center\nstop moving K means clustering is\nrelatively fast and easy to interpret it\nis also able to choose the positions of\ninitial centroids in a smart way that\nspeeds up the convergence the one\nchallenge with K means is that the\nnumber of clusters must be predetermined\ncayman's algorithm is not able to guess\nhow many clusters exist in the data if\nthere is a nonlinear structure\nseparating groups in the data K means\nwill not be a good choice DB scan\nclustering partition based and\nhierarchical clustering techniques are\nhighly efficient with normal shaped\nclusters however when it comes to\narbitrary shaped clusters or detecting\noutliers density based techniques are\nmore efficient DB scan stands for\ndensity based spatial clustering of\napplications with noise it is able to\nfind arbitrary shaped clusters and\nclusters with noise the main idea behind\nDB scan is that a point belongs to a\ncluster if it is close to many points\nfrom that cluster there are two key\nparameters of DB scan EPS which is the\ndistance that specifies the neighborhood\ntwo points are considered to be\nneighbors if the distance between them\nare less than or equal to EPs and Min\npts which is the minimum number of data\npoints to define a cluster based on\nthese two parameters points are\nclassified as score Point border point\nor outlier a point is a core point if\nthere are at least Min pts number of\npoints including the point itself in its\nsurrounding area with radius EPS a point\nis a border point if it is unreachable\nfrom a core point and there are less\nthan Min pts number of points within its\nsurrounding area and a point is an\noutlier if it is not a core point and\nnot reach from any core points DB scan\ndoes not require to specify a number of\nclusters beforehand it is robust to\noutliers and able to detect the outliers\nin some cases determining an appropriate\ndistance of neighborhood EPS is not easy\nand it requires domain knowledge\nprinciple components analysis or PCA is\na dimensionally reduction algorithm\nwhich basically derives new features\nfrom the existing ones with keeping as\nmuch information as possible PCA is an\nunsupervised learning algorithm but it\nis also widely used as a pre-processing\nstep for supervised learning algorithms\nPCA deres new features by finding the\nrelations among features in a data set\nthe aim of PCA is to explain the\nvariance within the original data set as\nmuch as possible by using less features\nthe new derived features are called\nprincipal components the order of\nprincipal components is determined\naccording to the fraction of variance of\noriginal data set they explain the\nadvantage of PCA is that a significant\namount of variance of the original data\nset is retained using much smaller\nnumber of features than the original\ndata set principal components are\nordered according to the amount of\nvariants that they explain and that is\nevery common machine learning algorithm\nexplained\n",
  "words": [
    "every",
    "single",
    "machine",
    "learning",
    "algorithm",
    "explained",
    "case",
    "know",
    "algorithm",
    "set",
    "commands",
    "must",
    "followed",
    "computer",
    "perform",
    "calculations",
    "like",
    "problemsolving",
    "operations",
    "according",
    "formal",
    "definition",
    "algorithm",
    "finite",
    "set",
    "instructions",
    "carried",
    "specific",
    "order",
    "perform",
    "particular",
    "task",
    "entire",
    "program",
    "code",
    "simple",
    "logic",
    "problem",
    "linear",
    "regression",
    "supervised",
    "learning",
    "algorithm",
    "tries",
    "model",
    "relationship",
    "continuous",
    "target",
    "variable",
    "one",
    "independent",
    "var",
    "variables",
    "fitting",
    "linear",
    "equation",
    "data",
    "take",
    "chart",
    "dots",
    "example",
    "linear",
    "regression",
    "model",
    "tries",
    "fit",
    "regression",
    "line",
    "data",
    "points",
    "best",
    "represents",
    "relations",
    "correlations",
    "method",
    "best",
    "regression",
    "line",
    "found",
    "minimizing",
    "sum",
    "squares",
    "distance",
    "data",
    "points",
    "regression",
    "line",
    "data",
    "points",
    "regression",
    "line",
    "looks",
    "like",
    "support",
    "vector",
    "machine",
    "svm",
    "short",
    "supervised",
    "learning",
    "algorithm",
    "mostly",
    "used",
    "classification",
    "tasks",
    "also",
    "suitable",
    "regression",
    "r",
    "tasks",
    "svm",
    "distinguishes",
    "classes",
    "drawing",
    "decision",
    "boundary",
    "draw",
    "determine",
    "decision",
    "boundary",
    "critical",
    "part",
    "svm",
    "algorithms",
    "creating",
    "decision",
    "boundary",
    "observation",
    "data",
    "point",
    "plotted",
    "n",
    "dimensional",
    "space",
    "n",
    "number",
    "features",
    "used",
    "example",
    "use",
    "length",
    "width",
    "classify",
    "different",
    "cells",
    "observations",
    "plotted",
    "space",
    "decision",
    "boundary",
    "line",
    "use",
    "three",
    "features",
    "decision",
    "boundary",
    "plane",
    "space",
    "use",
    "three",
    "features",
    "decision",
    "boundary",
    "becomes",
    "hyper",
    "plane",
    "really",
    "hard",
    "visualize",
    "decision",
    "boundary",
    "drawn",
    "way",
    "distance",
    "support",
    "vectors",
    "maximized",
    "decision",
    "boundary",
    "close",
    "support",
    "vector",
    "highly",
    "sensitive",
    "noises",
    "generalize",
    "well",
    "even",
    "small",
    "changes",
    "independent",
    "variables",
    "may",
    "cause",
    "misclassification",
    "svm",
    "especially",
    "effective",
    "cases",
    "number",
    "dimensions",
    "number",
    "samples",
    "finding",
    "decision",
    "boundary",
    "svm",
    "uses",
    "subset",
    "training",
    "points",
    "rather",
    "points",
    "makes",
    "memory",
    "efficient",
    "hand",
    "training",
    "time",
    "increases",
    "large",
    "data",
    "sets",
    "negatively",
    "affects",
    "performance",
    "nave",
    "bas",
    "supervised",
    "learning",
    "algorithm",
    "used",
    "classification",
    "tasks",
    "hence",
    "also",
    "called",
    "nave",
    "bas",
    "classifier",
    "nave",
    "bas",
    "assumes",
    "features",
    "independent",
    "correlation",
    "features",
    "however",
    "case",
    "real",
    "life",
    "nave",
    "assumption",
    "features",
    "uncorrelated",
    "reason",
    "algorithm",
    "called",
    "naive",
    "intuition",
    "behind",
    "naive",
    "bay",
    "algorithm",
    "bas",
    "theorem",
    "p",
    "probability",
    "event",
    "given",
    "event",
    "b",
    "already",
    "occurred",
    "pba",
    "probability",
    "event",
    "b",
    "given",
    "event",
    "already",
    "occurred",
    "pa",
    "probability",
    "event",
    "pb",
    "probability",
    "event",
    "b",
    "naive",
    "bas",
    "classifier",
    "calculates",
    "probability",
    "class",
    "given",
    "set",
    "feature",
    "values",
    "assumption",
    "features",
    "independent",
    "makes",
    "knif",
    "based",
    "algorithm",
    "fast",
    "compared",
    "complicated",
    "algorithms",
    "cases",
    "speed",
    "preferred",
    "higher",
    "accuracy",
    "hand",
    "assumption",
    "makes",
    "knife",
    "bas",
    "algorithm",
    "less",
    "accurate",
    "complicated",
    "algorithms",
    "logistic",
    "regression",
    "supervised",
    "learning",
    "algorithm",
    "mostly",
    "used",
    "binary",
    "classification",
    "problems",
    "logistic",
    "regression",
    "simple",
    "yet",
    "effective",
    "classification",
    "algorithm",
    "commonly",
    "used",
    "many",
    "binary",
    "classification",
    "tasks",
    "things",
    "like",
    "customer",
    "turn",
    "spam",
    "email",
    "website",
    "ad",
    "click",
    "predictions",
    "examples",
    "areas",
    "logistic",
    "regression",
    "offers",
    "powerful",
    "solution",
    "basis",
    "logistic",
    "regression",
    "logistic",
    "function",
    "also",
    "called",
    "sigmoid",
    "function",
    "takes",
    "real",
    "value",
    "number",
    "maps",
    "value",
    "0o",
    "1",
    "let",
    "consider",
    "following",
    "linear",
    "equation",
    "solve",
    "logistic",
    "regression",
    "model",
    "takes",
    "linear",
    "equation",
    "input",
    "uses",
    "logistic",
    "function",
    "log",
    "odds",
    "perform",
    "binary",
    "classification",
    "task",
    "get",
    "f",
    "famous",
    "shaped",
    "graph",
    "logistic",
    "regression",
    "use",
    "calculated",
    "probability",
    "example",
    "output",
    "probability",
    "email",
    "spam",
    "95",
    "probability",
    "customer",
    "click",
    "ad",
    "70",
    "however",
    "cases",
    "probabilities",
    "used",
    "classify",
    "data",
    "points",
    "example",
    "probability",
    "greater",
    "50",
    "prediction",
    "positive",
    "class",
    "one",
    "otherwise",
    "prediction",
    "negative",
    "class",
    "zero",
    "k",
    "nearest",
    "neighbors",
    "k",
    "n",
    "short",
    "supervised",
    "learning",
    "algorithm",
    "used",
    "solve",
    "classification",
    "regression",
    "tasks",
    "main",
    "idea",
    "behind",
    "knn",
    "value",
    "class",
    "data",
    "point",
    "determined",
    "data",
    "points",
    "around",
    "knn",
    "classifier",
    "determines",
    "class",
    "data",
    "point",
    "majority",
    "voting",
    "principle",
    "instance",
    "k",
    "set",
    "five",
    "classes",
    "five",
    "closest",
    "points",
    "checked",
    "prediction",
    "done",
    "according",
    "majority",
    "class",
    "similarly",
    "k",
    "n",
    "regression",
    "takes",
    "mean",
    "value",
    "five",
    "cl",
    "closest",
    "points",
    "let",
    "go",
    "example",
    "consider",
    "following",
    "data",
    "points",
    "belong",
    "four",
    "different",
    "classes",
    "let",
    "see",
    "predicted",
    "classes",
    "change",
    "according",
    "k",
    "value",
    "important",
    "determine",
    "optimal",
    "k",
    "value",
    "k",
    "low",
    "model",
    "specific",
    "generalized",
    "well",
    "also",
    "tends",
    "sensitive",
    "noise",
    "model",
    "accomplishes",
    "high",
    "accuracy",
    "train",
    "set",
    "poor",
    "predictor",
    "new",
    "previously",
    "unseen",
    "data",
    "points",
    "therefore",
    "likely",
    "end",
    "overfit",
    "model",
    "hand",
    "k",
    "large",
    "model",
    "generalized",
    "good",
    "predictor",
    "train",
    "test",
    "sets",
    "situation",
    "known",
    "underfitting",
    "knn",
    "simple",
    "easy",
    "interpret",
    "make",
    "assumption",
    "implemented",
    "nonlinear",
    "tasks",
    "knn",
    "become",
    "slow",
    "number",
    "data",
    "points",
    "increases",
    "model",
    "needs",
    "store",
    "data",
    "points",
    "thus",
    "memory",
    "efficient",
    "another",
    "downside",
    "knn",
    "sensitive",
    "outliers",
    "decision",
    "trees",
    "work",
    "iteratively",
    "asking",
    "questions",
    "partition",
    "data",
    "easier",
    "conceptualize",
    "partitioning",
    "data",
    "visual",
    "representation",
    "decision",
    "tree",
    "represents",
    "decision",
    "tree",
    "predict",
    "customer",
    "ch",
    "first",
    "split",
    "based",
    "monthly",
    "charges",
    "amount",
    "algorithm",
    "keeps",
    "asking",
    "questions",
    "separate",
    "class",
    "labels",
    "question",
    "get",
    "specific",
    "tree",
    "gets",
    "deeper",
    "aim",
    "increase",
    "predictiveness",
    "much",
    "possible",
    "partitioning",
    "model",
    "keeps",
    "gaining",
    "information",
    "data",
    "set",
    "randomly",
    "splitting",
    "feature",
    "usually",
    "give",
    "us",
    "valuable",
    "insight",
    "data",
    "set",
    "splits",
    "increase",
    "purity",
    "nodes",
    "informative",
    "purity",
    "node",
    "inversely",
    "proportional",
    "distribution",
    "different",
    "classes",
    "node",
    "questions",
    "ask",
    "chosen",
    "way",
    "increases",
    "purity",
    "decreases",
    "impurity",
    "many",
    "questions",
    "ask",
    "stop",
    "tree",
    "sufficient",
    "solve",
    "classification",
    "problem",
    "answer",
    "questions",
    "leads",
    "us",
    "one",
    "important",
    "concepts",
    "machine",
    "learning",
    "overfitting",
    "model",
    "keep",
    "asking",
    "questions",
    "nodes",
    "pure",
    "however",
    "would",
    "two",
    "specific",
    "model",
    "would",
    "generalize",
    "achieves",
    "high",
    "accuracy",
    "training",
    "set",
    "performs",
    "poorly",
    "new",
    "previously",
    "unseen",
    "data",
    "points",
    "indicates",
    "overfitting",
    "decision",
    "tree",
    "algorithm",
    "usually",
    "require",
    "normalize",
    "scale",
    "features",
    "also",
    "suitable",
    "work",
    "mixture",
    "feature",
    "data",
    "types",
    "negative",
    "side",
    "prone",
    "overfitting",
    "needs",
    "ensembled",
    "order",
    "generalize",
    "well",
    "random",
    "forest",
    "ensemble",
    "many",
    "decision",
    "trees",
    "random",
    "forests",
    "built",
    "using",
    "method",
    "called",
    "bagging",
    "decision",
    "trees",
    "used",
    "par",
    "parel",
    "estimators",
    "used",
    "classification",
    "problem",
    "result",
    "based",
    "majority",
    "vote",
    "results",
    "received",
    "decision",
    "tree",
    "regression",
    "prediction",
    "leaf",
    "node",
    "mean",
    "value",
    "target",
    "values",
    "leaf",
    "random",
    "forest",
    "regression",
    "takes",
    "mean",
    "values",
    "results",
    "decision",
    "trees",
    "random",
    "forests",
    "reduce",
    "risk",
    "overfitting",
    "accuracy",
    "much",
    "higher",
    "single",
    "decision",
    "tree",
    "furthermore",
    "decision",
    "trees",
    "random",
    "forest",
    "run",
    "parallel",
    "time",
    "become",
    "bottleneck",
    "success",
    "random",
    "forest",
    "highly",
    "depends",
    "using",
    "uncorrelated",
    "decision",
    "trees",
    "use",
    "similar",
    "trees",
    "overall",
    "result",
    "much",
    "different",
    "result",
    "single",
    "decision",
    "tree",
    "random",
    "forests",
    "achieve",
    "uncorrelated",
    "decision",
    "trees",
    "bootstrapping",
    "feature",
    "randomness",
    "bootstrapping",
    "randomly",
    "selecting",
    "samples",
    "training",
    "data",
    "replacement",
    "called",
    "bootstrap",
    "samples",
    "feature",
    "randomness",
    "achieved",
    "selecting",
    "features",
    "randomly",
    "decision",
    "tree",
    "random",
    "forest",
    "number",
    "features",
    "used",
    "tree",
    "random",
    "forest",
    "controlled",
    "maxcore",
    "features",
    "parameter",
    "random",
    "forest",
    "highly",
    "accurate",
    "model",
    "many",
    "different",
    "problems",
    "require",
    "normalization",
    "scaling",
    "however",
    "good",
    "choice",
    "high",
    "dimensional",
    "data",
    "sets",
    "compared",
    "fast",
    "linear",
    "models",
    "gradient",
    "boosted",
    "decision",
    "trees",
    "gbdt",
    "short",
    "ensemble",
    "algorithm",
    "uses",
    "boosting",
    "methods",
    "combine",
    "individual",
    "decision",
    "trees",
    "boosting",
    "means",
    "combining",
    "learning",
    "algorithm",
    "series",
    "achieve",
    "strong",
    "learner",
    "many",
    "sequentially",
    "connect",
    "weak",
    "learners",
    "case",
    "gbdt",
    "weak",
    "learners",
    "decision",
    "trees",
    "tree",
    "attempts",
    "minimize",
    "errors",
    "previous",
    "tree",
    "trees",
    "boosting",
    "weak",
    "learners",
    "adding",
    "many",
    "trees",
    "series",
    "focusing",
    "errors",
    "previous",
    "one",
    "make",
    "boosting",
    "highly",
    "efficient",
    "accurate",
    "model",
    "unlike",
    "bagging",
    "boosta",
    "involve",
    "bootstrap",
    "sampling",
    "every",
    "time",
    "new",
    "tree",
    "added",
    "fits",
    "modified",
    "version",
    "initial",
    "data",
    "set",
    "since",
    "trees",
    "added",
    "sequentially",
    "boosting",
    "algorithms",
    "learn",
    "slowly",
    "statistical",
    "learning",
    "models",
    "learn",
    "slowly",
    "perform",
    "better",
    "gbdt",
    "efficient",
    "classification",
    "regression",
    "tasks",
    "provides",
    "accurate",
    "predictions",
    "compared",
    "random",
    "forest",
    "handle",
    "mixed",
    "type",
    "features",
    "needed",
    "gbdt",
    "require",
    "careful",
    "tuning",
    "hyperparameters",
    "order",
    "prevent",
    "model",
    "overfitting",
    "k",
    "means",
    "clustering",
    "clustering",
    "way",
    "group",
    "set",
    "data",
    "points",
    "way",
    "similar",
    "data",
    "points",
    "grouped",
    "together",
    "therefore",
    "clustering",
    "algorithms",
    "look",
    "similarities",
    "dissimilarities",
    "among",
    "data",
    "points",
    "clustering",
    "unsupervised",
    "learning",
    "method",
    "label",
    "associated",
    "data",
    "points",
    "clustering",
    "algorithms",
    "try",
    "find",
    "underlying",
    "structure",
    "data",
    "observations",
    "data",
    "points",
    "classification",
    "task",
    "labels",
    "observation",
    "classified",
    "according",
    "measurements",
    "classification",
    "algorithms",
    "try",
    "model",
    "relationship",
    "measurements",
    "observations",
    "assigned",
    "class",
    "model",
    "predicts",
    "class",
    "new",
    "observations",
    "k",
    "means",
    "clustering",
    "aims",
    "partition",
    "data",
    "k",
    "clusters",
    "way",
    "data",
    "points",
    "cluster",
    "similar",
    "data",
    "points",
    "different",
    "clusters",
    "apart",
    "thus",
    "partition",
    "based",
    "clustering",
    "technique",
    "similarity",
    "two",
    "points",
    "determined",
    "distance",
    "consider",
    "following",
    "2d",
    "visualization",
    "data",
    "set",
    "partied",
    "four",
    "different",
    "clusters",
    "real",
    "life",
    "data",
    "sets",
    "much",
    "complex",
    "clusters",
    "clearly",
    "separated",
    "however",
    "algorithm",
    "works",
    "way",
    "k",
    "means",
    "iterative",
    "process",
    "built",
    "expectation",
    "maximization",
    "algorithm",
    "number",
    "clusters",
    "determined",
    "works",
    "executing",
    "following",
    "steps",
    "number",
    "one",
    "randomly",
    "selects",
    "centroids",
    "center",
    "cluster",
    "cluster",
    "calculates",
    "distance",
    "data",
    "points",
    "centroids",
    "assigns",
    "data",
    "points",
    "closest",
    "cluster",
    "finds",
    "new",
    "centroids",
    "cluster",
    "taking",
    "mean",
    "data",
    "points",
    "cluster",
    "repeats",
    "steps",
    "2",
    "3",
    "four",
    "points",
    "converge",
    "cluster",
    "center",
    "stop",
    "moving",
    "k",
    "means",
    "clustering",
    "relatively",
    "fast",
    "easy",
    "interpret",
    "also",
    "able",
    "choose",
    "positions",
    "initial",
    "centroids",
    "smart",
    "way",
    "speeds",
    "convergence",
    "one",
    "challenge",
    "k",
    "means",
    "number",
    "clusters",
    "must",
    "predetermined",
    "cayman",
    "algorithm",
    "able",
    "guess",
    "many",
    "clusters",
    "exist",
    "data",
    "nonlinear",
    "structure",
    "separating",
    "groups",
    "data",
    "k",
    "means",
    "good",
    "choice",
    "db",
    "scan",
    "clustering",
    "partition",
    "based",
    "hierarchical",
    "clustering",
    "techniques",
    "highly",
    "efficient",
    "normal",
    "shaped",
    "clusters",
    "however",
    "comes",
    "arbitrary",
    "shaped",
    "clusters",
    "detecting",
    "outliers",
    "density",
    "based",
    "techniques",
    "efficient",
    "db",
    "scan",
    "stands",
    "density",
    "based",
    "spatial",
    "clustering",
    "applications",
    "noise",
    "able",
    "find",
    "arbitrary",
    "shaped",
    "clusters",
    "clusters",
    "noise",
    "main",
    "idea",
    "behind",
    "db",
    "scan",
    "point",
    "belongs",
    "cluster",
    "close",
    "many",
    "points",
    "cluster",
    "two",
    "key",
    "parameters",
    "db",
    "scan",
    "eps",
    "distance",
    "specifies",
    "neighborhood",
    "two",
    "points",
    "considered",
    "neighbors",
    "distance",
    "less",
    "equal",
    "eps",
    "min",
    "pts",
    "minimum",
    "number",
    "data",
    "points",
    "define",
    "cluster",
    "based",
    "two",
    "parameters",
    "points",
    "classified",
    "score",
    "point",
    "border",
    "point",
    "outlier",
    "point",
    "core",
    "point",
    "least",
    "min",
    "pts",
    "number",
    "points",
    "including",
    "point",
    "surrounding",
    "area",
    "radius",
    "eps",
    "point",
    "border",
    "point",
    "unreachable",
    "core",
    "point",
    "less",
    "min",
    "pts",
    "number",
    "points",
    "within",
    "surrounding",
    "area",
    "point",
    "outlier",
    "core",
    "point",
    "reach",
    "core",
    "points",
    "db",
    "scan",
    "require",
    "specify",
    "number",
    "clusters",
    "beforehand",
    "robust",
    "outliers",
    "able",
    "detect",
    "outliers",
    "cases",
    "determining",
    "appropriate",
    "distance",
    "neighborhood",
    "eps",
    "easy",
    "requires",
    "domain",
    "knowledge",
    "principle",
    "components",
    "analysis",
    "pca",
    "dimensionally",
    "reduction",
    "algorithm",
    "basically",
    "derives",
    "new",
    "features",
    "existing",
    "ones",
    "keeping",
    "much",
    "information",
    "possible",
    "pca",
    "unsupervised",
    "learning",
    "algorithm",
    "also",
    "widely",
    "used",
    "step",
    "supervised",
    "learning",
    "algorithms",
    "pca",
    "deres",
    "new",
    "features",
    "finding",
    "relations",
    "among",
    "features",
    "data",
    "set",
    "aim",
    "pca",
    "explain",
    "variance",
    "within",
    "original",
    "data",
    "set",
    "much",
    "possible",
    "using",
    "less",
    "features",
    "new",
    "derived",
    "features",
    "called",
    "principal",
    "components",
    "order",
    "principal",
    "components",
    "determined",
    "according",
    "fraction",
    "variance",
    "original",
    "data",
    "set",
    "explain",
    "advantage",
    "pca",
    "significant",
    "amount",
    "variance",
    "original",
    "data",
    "set",
    "retained",
    "using",
    "much",
    "smaller",
    "number",
    "features",
    "original",
    "data",
    "set",
    "principal",
    "components",
    "ordered",
    "according",
    "amount",
    "variants",
    "explain",
    "every",
    "common",
    "machine",
    "learning",
    "algorithm",
    "explained"
  ],
  "keywords": [
    "every",
    "single",
    "machine",
    "learning",
    "algorithm",
    "case",
    "set",
    "perform",
    "like",
    "according",
    "specific",
    "order",
    "task",
    "simple",
    "problem",
    "linear",
    "regression",
    "supervised",
    "model",
    "one",
    "independent",
    "equation",
    "data",
    "example",
    "line",
    "points",
    "method",
    "distance",
    "support",
    "svm",
    "short",
    "used",
    "classification",
    "tasks",
    "also",
    "classes",
    "decision",
    "boundary",
    "algorithms",
    "point",
    "n",
    "space",
    "number",
    "features",
    "use",
    "different",
    "observations",
    "way",
    "highly",
    "sensitive",
    "generalize",
    "well",
    "cases",
    "samples",
    "uses",
    "training",
    "makes",
    "efficient",
    "hand",
    "time",
    "increases",
    "sets",
    "nave",
    "bas",
    "called",
    "classifier",
    "however",
    "real",
    "assumption",
    "uncorrelated",
    "naive",
    "behind",
    "probability",
    "event",
    "given",
    "b",
    "class",
    "feature",
    "values",
    "based",
    "fast",
    "compared",
    "accuracy",
    "less",
    "accurate",
    "logistic",
    "binary",
    "many",
    "customer",
    "function",
    "takes",
    "value",
    "let",
    "consider",
    "following",
    "solve",
    "shaped",
    "prediction",
    "k",
    "knn",
    "determined",
    "majority",
    "five",
    "closest",
    "mean",
    "four",
    "noise",
    "high",
    "new",
    "good",
    "easy",
    "outliers",
    "trees",
    "asking",
    "questions",
    "partition",
    "tree",
    "amount",
    "much",
    "possible",
    "randomly",
    "purity",
    "node",
    "overfitting",
    "two",
    "require",
    "random",
    "forest",
    "forests",
    "using",
    "result",
    "similar",
    "gbdt",
    "boosting",
    "means",
    "weak",
    "learners",
    "clustering",
    "clusters",
    "cluster",
    "centroids",
    "able",
    "db",
    "scan",
    "eps",
    "min",
    "pts",
    "core",
    "components",
    "pca",
    "explain",
    "variance",
    "original",
    "principal"
  ]
}