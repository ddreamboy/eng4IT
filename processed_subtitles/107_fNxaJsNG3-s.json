{
  "text": "LAURENCE MORONEY: Hi, and\nwelcome to this series on Zero\nto Hero for natural language\nprocessing using TensorFlow.\nIf you're not an expert\non AI or ML, don't worry.\nWe're taking the concepts\nof NLP and teaching them\nfrom first principles.\nIn this first lesson, we'll talk\nabout how to represent words\nin a way that a computer\ncan process them,\nwith a view to later training\na neural network that\ncan understand their meaning.\nThis process is\ncalled tokenization.\nSo let's take a look.\nConsider the word \"listen,\"\nas you can see here.\nIt's made up of a\nsequence of letters.\nThese letters can be\nrepresented by numbers\nusing an encoding scheme.\nA popular one called ASCII\nhas these letters represented\nby these numbers.\nThis bunch of numbers can then\nrepresent the word listen.\nBut the word silent has\nthe same letters, and thus\nthe same numbers, just\nin a different order.\nSo it makes it hard for us to\nunderstand sentiment of a word\njust by the letters in it.\nSo it might be easier,\ninstead of encoding letters,\nto encode words.\nConsider the sentence\nI love my dog.\nSo what would happen\nif we start encoding\nthe words in this\nsentence instead\nof the letters in each word?\nSo, for example, the\nword \"I\" could be one,\nand then the sentence \"I love\nmy dog\" could be 1, 2, 3, 4.\nNow, if I take another sentence,\nfor example, \"I love my cat,\"\nhow would we encode it?\nNow we see \"I love my\" has\nalready been given 1, 2, 3,\nso all I need to\ndo is encode \"cat.\"\nI'll give that the number 5.\nAnd now, if we look\nat the two sentences,\nthey are 1, 2, 3,\n4 and 1, 2, 3, 5,\nwhich already show some form\nof similarity between them.\nAnd it's a similarity\nyou would expect,\nbecause they're both\nabout loving a pet.\nGiven this method of encoding\nsentences into numbers,\nnow let's take a look at some\ncode to achieve this for us.\nThis process, as I mentioned\nbefore, is called tokenization,\nand there's an API for that.\nWe'll look at how to\nuse it with Python.\nSo here's your first\nlook at some code\nto tokenize these sentences.\nLet's go through\nit line by line.\nFirst of all, we'll need\nthe tokenize our APIs,\nand we can get these from\nTensorFlow Keras like this.\nWe can represent our\nsentences as a Python array\nof strings like this.\nIt's simply the \"I love my\ndog\" and \"I love my cat\"\nthat we saw earlier.\nNow the fun begins.\nI can create an instance\nof a tokenizer object.\nThe num_words parameter\nis the maximum number\nof words to keep.\nSo instead of, for example,\njust these two sentences,\nimagine if we had hundreds\nof books to tokenize,\nbut we just want\nthe most frequent\n100 words in all of that.\nThis would automatically\ndo that for us\nwhen we do the next\nstep, and that's\nto tell the tokenizer to\ngo through all the text\nand then fit itself\nto them like this.\nThe full list of words is\navailable as the tokenizer's\nword index property.\nSo we can take a\nlook at it like this\nand then simply print it out.\nThe result will be this\ndictionary showing the key\nbeing the word and the value\nbeing the token for that word.\nSo for example, my\nhas a value of 3.\nThe tokenizer is\nalso smart enough\nto catch some exceptions.\nSo for example, if we\nupdated our sentences to this\nby adding a third sentence,\nnoting that \"dog\" here\nis followed by an\nexclamation mark,\nthe nice thing is\nthat the tokenizer\nis smart enough to spot this\nand not create a new token.\nIt's just \"dog.\"\nAnd you can see\nthe results here.\nThere's no token for\n\"dog exclamation,\"\nbut there is one for \"dog.\"\nAnd there is also a new\ntoken for the word \"you.\"\nIf you want to try\nthis out for yourself,\nI've put the code\nin the Colab here.\nTake it for a spin\nand experiment.\nYou've now seen how\nwords can be tokenized,\nand the tools in\nTensorFlow that handle\nthat tokenization for you.\nNow that your words\nare represented\nby numbers like\nthis, you'll next\nneed to represent your sentences\nby sequences of numbers\nin the correct order.\nYou'll then have data ready for\nprocessing by a neural network\nto understand or maybe\neven generate new text.\nYou'll see the\ntools that you can\nuse to manage this sequencing\nin the next episode,\nso don't forget to hit\nthat subscribe button.\n[MUSIC PLAYING]\n",
  "words": [
    "laurence",
    "moroney",
    "hi",
    "welcome",
    "series",
    "zero",
    "hero",
    "natural",
    "language",
    "processing",
    "using",
    "tensorflow",
    "expert",
    "ai",
    "ml",
    "worry",
    "taking",
    "concepts",
    "nlp",
    "teaching",
    "first",
    "principles",
    "first",
    "lesson",
    "talk",
    "represent",
    "words",
    "way",
    "computer",
    "process",
    "view",
    "later",
    "training",
    "neural",
    "network",
    "understand",
    "meaning",
    "process",
    "called",
    "tokenization",
    "let",
    "take",
    "look",
    "consider",
    "word",
    "listen",
    "see",
    "made",
    "sequence",
    "letters",
    "letters",
    "represented",
    "numbers",
    "using",
    "encoding",
    "scheme",
    "popular",
    "one",
    "called",
    "ascii",
    "letters",
    "represented",
    "numbers",
    "bunch",
    "numbers",
    "represent",
    "word",
    "listen",
    "word",
    "silent",
    "letters",
    "thus",
    "numbers",
    "different",
    "order",
    "makes",
    "hard",
    "us",
    "understand",
    "sentiment",
    "word",
    "letters",
    "might",
    "easier",
    "instead",
    "encoding",
    "letters",
    "encode",
    "words",
    "consider",
    "sentence",
    "love",
    "dog",
    "would",
    "happen",
    "start",
    "encoding",
    "words",
    "sentence",
    "instead",
    "letters",
    "word",
    "example",
    "word",
    "could",
    "one",
    "sentence",
    "love",
    "dog",
    "could",
    "1",
    "2",
    "3",
    "take",
    "another",
    "sentence",
    "example",
    "love",
    "cat",
    "would",
    "encode",
    "see",
    "love",
    "already",
    "given",
    "1",
    "2",
    "3",
    "need",
    "encode",
    "cat",
    "give",
    "number",
    "look",
    "two",
    "sentences",
    "1",
    "2",
    "3",
    "4",
    "1",
    "2",
    "3",
    "5",
    "already",
    "show",
    "form",
    "similarity",
    "similarity",
    "would",
    "expect",
    "loving",
    "pet",
    "given",
    "method",
    "encoding",
    "sentences",
    "numbers",
    "let",
    "take",
    "look",
    "code",
    "achieve",
    "us",
    "process",
    "mentioned",
    "called",
    "tokenization",
    "api",
    "look",
    "use",
    "python",
    "first",
    "look",
    "code",
    "tokenize",
    "sentences",
    "let",
    "go",
    "line",
    "line",
    "first",
    "need",
    "tokenize",
    "apis",
    "get",
    "tensorflow",
    "keras",
    "like",
    "represent",
    "sentences",
    "python",
    "array",
    "strings",
    "like",
    "simply",
    "love",
    "dog",
    "love",
    "cat",
    "saw",
    "earlier",
    "fun",
    "begins",
    "create",
    "instance",
    "tokenizer",
    "object",
    "parameter",
    "maximum",
    "number",
    "words",
    "keep",
    "instead",
    "example",
    "two",
    "sentences",
    "imagine",
    "hundreds",
    "books",
    "tokenize",
    "want",
    "frequent",
    "100",
    "words",
    "would",
    "automatically",
    "us",
    "next",
    "step",
    "tell",
    "tokenizer",
    "go",
    "text",
    "fit",
    "like",
    "full",
    "list",
    "words",
    "available",
    "tokenizer",
    "word",
    "index",
    "property",
    "take",
    "look",
    "like",
    "simply",
    "print",
    "result",
    "dictionary",
    "showing",
    "key",
    "word",
    "value",
    "token",
    "word",
    "example",
    "value",
    "tokenizer",
    "also",
    "smart",
    "enough",
    "catch",
    "exceptions",
    "example",
    "updated",
    "sentences",
    "adding",
    "third",
    "sentence",
    "noting",
    "dog",
    "followed",
    "exclamation",
    "mark",
    "nice",
    "thing",
    "tokenizer",
    "smart",
    "enough",
    "spot",
    "create",
    "new",
    "token",
    "dog",
    "see",
    "results",
    "token",
    "dog",
    "exclamation",
    "one",
    "dog",
    "also",
    "new",
    "token",
    "word",
    "want",
    "try",
    "put",
    "code",
    "colab",
    "take",
    "spin",
    "experiment",
    "seen",
    "words",
    "tokenized",
    "tools",
    "tensorflow",
    "handle",
    "tokenization",
    "words",
    "represented",
    "numbers",
    "like",
    "next",
    "need",
    "represent",
    "sentences",
    "sequences",
    "numbers",
    "correct",
    "order",
    "data",
    "ready",
    "processing",
    "neural",
    "network",
    "understand",
    "maybe",
    "even",
    "generate",
    "new",
    "text",
    "see",
    "tools",
    "use",
    "manage",
    "sequencing",
    "next",
    "episode",
    "forget",
    "hit",
    "subscribe",
    "button",
    "music",
    "playing"
  ],
  "keywords": [
    "processing",
    "using",
    "tensorflow",
    "first",
    "represent",
    "words",
    "process",
    "neural",
    "network",
    "understand",
    "called",
    "tokenization",
    "let",
    "take",
    "look",
    "consider",
    "word",
    "listen",
    "see",
    "letters",
    "represented",
    "numbers",
    "encoding",
    "one",
    "order",
    "us",
    "instead",
    "encode",
    "sentence",
    "love",
    "dog",
    "would",
    "example",
    "could",
    "1",
    "2",
    "3",
    "cat",
    "already",
    "given",
    "need",
    "number",
    "two",
    "sentences",
    "similarity",
    "code",
    "use",
    "python",
    "tokenize",
    "go",
    "line",
    "like",
    "simply",
    "create",
    "tokenizer",
    "want",
    "next",
    "text",
    "value",
    "token",
    "also",
    "smart",
    "enough",
    "exclamation",
    "new",
    "tools"
  ]
}