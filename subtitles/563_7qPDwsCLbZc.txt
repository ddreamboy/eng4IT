it's super exciting
last week openai released gpt3 the
largest
language model ever released we had a
fantastic conversation about this today
now the most exciting thing about the
model isn't its sheer size
but that it's only been trained to do
language modeling
but it can do any number of nlp tasks
out of the box without any fine tuning
and that
working in this kind of precision has
never been seen before
and it raises many questions about the
future of building
nlp models and how we will interact with
them
in the future gpt3 is the biggest
language model ever produced 175
billion parameters you need a super
computer
to train it and openai has done
exactly that gpt-3 is a 175 billion
parameter
neural network 10 times larger than the
biggest artificial neural network that
had ever been trained before now
so how do they train such an enormous
model this is powered by the zero two
optimizations
in microsoft's open source deep speed
library
there's been a lot of research in
efficiently scaling training to hundreds
or thousands of gpus
the foundational ideas of this research
are data parallelism
where we copy the model on each gpu and
then send each gpu a different batch of
data
and model parallelism where we split the
model up onto different gpus
model parallelism is a necessary
component of this because we can only
fit a 1.4 billion parameter model
on a single nvidia v100 gpu that has 32
gigabytes of memory
so the next step in this story was the
development of pipeline parallelism
most famously implemented in google's
g-pipe which was used to hit
state-of-the-art image net accuracy
with 557 million parameters
g-pipe partitions the mini batches such
that different subsets of a model
split on multiple gpus has something to
do while they're waiting for the
sequential forward and backward passes
through the network to finish in a
similar vein xero recognizes the
inefficiency
with how data parallelism partitions
training memory across accelerators
if we're splitting our model across
multiple accelerators we don't need each
one to hold the entire set of optimizer
states
gradients and parameters so to solve
this inefficiency
xero uses a dynamic scheduling of
optimizer state
gradients and parameters to communicate
between multiple gpus
the first iteration of xero had produced
turing nlg
a 17 billion parameter language model
that continued to improve the perplexity
metric
for auto regressive language modeling
from the previous iteration which was
nvidia's megatron lm
at 8.3 billion parameters this was the
result of implementing the optimizer
state memory reductions
in zero one well zero two further
implements the gradient and parameter
optimizations as well
the bar chart in their blog post
highlights the enormous gains from zero
one
to zero two which evidences the training
differences between turing nlg and gpt3
gpt3 is an autoregressive language model
as opposed to a denoising autoencoder
like birth
and in today's conversation we talk
about some of the comparative
differences
between those two architectures of
language model there's been such an
incredible amount of progress in
language processing
especially using neural networks kind of
started in about 2013 when mickelov
introduced word to vec
and that was a distributed
representation for words which was
learned in a self-supervised way based
on the context
and then we had lstm type language
models and they were learning a
sequential
representation of tokens through time
and there were bidirectional variants of
this
a really cool thing that happened in
about 2018 was jeremy howard
and sebastian ruder released ulm fit and
that introduced transfer learning to
natural language processing
word to vect technically is transfer
learning but it was transfer learning in
the sense of
the language model was learning
dependencies between a whole bunch of
tokens
not individual tokens and then of course
it could be fine-tuned on a downstream
task
attention is all you need introduce
transformers and transformers are these
incredible
kind of routing machines they can learn
dependencies between
any tokens in the input and then it goes
through this successive
clever routing system which is learned
as part of the training process
in the original recurrent neural
networks the tokens and the input
sequence couldn't directly attend to
each other
and that meant that those models
suffered from catastrophic forgetting
and also they were quite difficult to
train because they had
vanishing and exploding gradients so
transformers came along and it was a
different paradigm
you could learn a complex hierarchy of
relationships
directly between the tokens it was no
longer this sequential
representation where you only knew what
happened at one
time step backwards it was a real
paradigm shift
and straight away these language
processing models produce
state-of-the-art results
across the board generally speaking
there are two architectural patterns
that we see with these transformers
architectures
the first is the auto-regressive pattern
where the model is just predicting the
next word and the next word and the next
word and the
the answer from the previous prediction
gets fed into the model the next time
around
the other type of model is what's called
a denoising autoencoder like
burton roberta and excel net and what
these do is
you feed in an input sentence and then
you typically you add some noise to it
and then you say what you expect
so these things seem to be appropriate
for things like question answering when
you need to
point to spans in your input sequence
for example if you're doing
in-span question answering the
auto-regressive models are quite
attractive because
you can just keep generating data
forever
so they seem appropriate for natural
language generation one of the things i
wanted to explore today was the
commercial utility of this
is this something that can help us in
industry or is this something which is
mainly an academic endeavor and only in
reach for
the very top tech companies like
microsoft and google
now yannick did an incredible video
about gpt3 last week on his channel
and he's starting to develop a bit of a
name for himself online
lightspeed yannick you might say within
hours of the paper being released he had
the video
out on his youtube channel and it was
even linked by
andre capathi the director of ai at
tesla so
it's incredible i've cut about five
minutes of the most descriptive elements
out of yannick's video on gbt3 and i'm
going to play them now
i think it will serve as a good
refresher for the topic this paper is
basically an investigation into
what you can do with giant language
models now this language model is an
order of magnitude larger than anyone
has ever
built a language model and it can do
some absolutely crazy things
this paper is basically an investigation
into what you can do
with giant language models now this
language model is an
order of magnitude larger than anyone
has ever
built a language model and it can do
some absolutely crazy things
so we'll basically go over the
architecture over what the model does
and over the experimental results it
turns out that if you train a language
model on enough data
it is able to solve nlp
tasks that it has never seen just out of
the box
so a language model let's just take an
example this
sentence right here just the sentence as
such like
third humans do not require large
supervised data sets to learn
most language tasks but this is an
english sentence
and a language model would be a model
that if you cross out a portion
from the end here like this right here
it would be able to tell you what comes
next
so in a language model you would input
this part right here
and it will tell you the next word is
data sets
so that's basically all the language
model does and
once you've trained one you can
basically generate word
after word after word from it or you can
ask it
a question like which word is most
likely to
come next or more likely so a language
model is nothing but a model
that can kind of generate language in a
probabilistic way
and the cool thing about language models
is that you can train it
on any sort of text data and that's what
they do here
so they train a language model on giant
amounts
of data just compare this to a language
model
like bert bert required
this much flops to train and these this
is a log scale
so this is right here this is several
orders of magnitude
a larger and bigger model and is trained
for
way longer on this text so naturally it
is going to be a lot better at
language modeling you can see right here
the size of these models that they
trained on remember the previous largest
language model the touring nlg of
microsoft had something like 17 billion
parameters so would be comparable to
this right here whereas gpt-3 has
175 billion parameters
which this is absolutely crazy it's an
order of magnitude higher
than anything that's ever existed and if
you look at the
last gpt the gpt-2 model that if you
remember i've made a video about it
is too dangerous to be released well now
it has been released but was too
dangerous to be released
it clocked in at about 1.5 billion
parameters
but essentially this is an
auto-regressive language model so it's
not like bert
it's not bi-directional it is
auto-regressive it goes from left to
right
always produces the next word it is like
gpt-2 they even say this they say
we use the same model and architecture
as
gpt2 they just have more layers
and wider layers and more data to train
it on
so with something like bird you would do
first pre-train so there you would this
is the language modeling right here this
pre-training phase
where you teach bert about the english
language by just feeding it a lot of
data
and then second you had a step called
fine tuning
so on the second one you'd have
something like
the task you're actually interested in
and let's say the task you're actually
interested in
is sentiment classification so in
sentiment classification you have like a
sentence like blah blah blah
and you want to know is that a positive
sentiment like is a happy sentence or is
it a sad sentence
and you would have a database of labeled
instances of that so
in this database you'd have a bunch of
sentences and for each one
you would know is it good is it is it
positive or is it negative
and then you'd have like a smaller test
set right here
you would basically take this
pre-trained model train it
on this data set in a supervised machine
learning way
and then test it on this test set right
here this is called fine tuning
what they are interested in is basically
to take the pre-trained model
and directly go and evaluate it on the
test data set in a sort of a zero-shot
fashion
your language model that you pre-trained
and you just
input the following text you input what
they call a task description
and a prompt so this is the input and
you
simply ask the model as a language model
to predict the next word it's just what
comes here
now what you're counting on is basically
that in the training data the model has
seen a structure like this enough to
understand what's going on
so that in the training data somewhere
in the internet there was this structure
of
translate something to something and
then there would be a word here of
something and you know it kind of has to
realize that this goes here
like that the next word so basically
what you're asking it is
if you were to find this
text on a website or on wikipedia or in
any of the books data set if you were to
find
this piece of text what would be
the next word in that piece of text you
simply input this
as a string so not only do you have the
task description
and the prompt right here but you also
have one
example and the example so the example
is going to come from the training data
set
of the task that you're interested in
but the important part is you never
train on it you never explicitly
train on that example you simply put it
in the context so you simply put this
string so translate english to french
new line
see other loot is
new line cheese is what you simply input
that string
into the model as a language model and
you ask it
what's the next word right here
okay so i hope i hope this is clear this
is
what they call kind of one shot
generalization
and by one shot they basically mean you
simply provide
this thing in the context of the model
as a language model now the the
advantage here is immediately clear
that you only have to train one model
then
and then basically at inference time you
can just
input the task description
and the sort of training data for the
task
into its its evaluation context
and the task itself i think what it does
is it will simply
take all of this
and it will go to its own training data
which it has
stored in its weights and it will filter
the training data
and basically take out the the things
that sort of
pattern match sort of regex match in a
fuzzy way
to this context and then it will kind of
interpolate these training examples in
order to come up with the answer i don't
think there is reasoning happening here
i was looking at some of the comments
actually on the video and some of them
make for interesting reading
yannick says he gets better comments on
his youtube videos than he does
from conferences when he submits his
papers here are some of the reviews
so as you can see this is a janik's
video that he published
and it's been viewed nearly 28 000 times
1.2 000 up votes and nine people
down voted it you should be ashamed of
yourselves
 you alex bravo chips in with t6
one trillion text to text transfer
transformer the next model coming out of
google
mallow marsh comments that just looking
at the wall of authors at the beginning
is making me sweat some person with an
incomprehensible
youtube name and a human adversarial
example as a profile picture
adds i don't think the intuition of the
model essentially just storing or the
training data
in a quasi lookup table is correct if
anything
the model acts as a very elaborate
compression algorithm
also modeling the semantic structure of
the language
needed to pass the natural language
model input certainly is achieved in a
way
that doesn't resemble a plane lookup
table okay so
he seemed to lose his grammatical um
fidelity towards the end of that message
but you get the idea and yannick replies
yes there's an argument to be made for
that i'm not only saying they're plain
look up tables
but more like fuzzy look up and
interpolation tables
my main point is that all of these tasks
where the model performs well
can be explained by lookup and
interpolation and there's
none where the model succeeds where
you'd have to say it was due to
reasoning abilities
and this seems to be the really clever
thing about gpt3 what it does
is it internalizes all of the
information you give it
and it deconstructs it and packs it into
some internal representation
and then when you ask it to go and do
something for you it will
go and find all of the information it
was trained on and it will reconstruct
it by interpolating between all of the
internal structures that it learned
super clever stuff happy teacher says
wow
it's incredible how fast you are i agree
with you happy teacher
it really is gary blauer says
continuing to expand the class of
problems which can be solved by pattern
recognition
won't get us all the way to artificial
general intelligence
but it's very interesting and impressive
nonetheless this is a remarkable example
he then goes on to quote chalet without
crediting charlay i might say
any problem can be treated as a pattern
recognition problem if your training
data covers a sufficiently dense
sampling of the problem space
what's interesting is what happens when
your training data
is sparse indeed
poodle chen says i like to think of
language models as really smart parrots
they repeat what you've said
thank you poodle chen we like that dodon
ko
also unwittingly quotes francois chalet
175 billion parameters sometimes i feel
that that's like trying to reach the
moon by just building higher and higher
skyscrapers until we get there
he said reach the moon twice i think
this might have been generated with gpt3
russian person says i think that a lot
of people missed the news
that microsoft fired all of their
journalists right off gpt-3
indeed they did don't need them anymore
andrew owens says
not agi enjoy watching your take on
these papers
i usually watch the video at 1.75 times
normal speed
it makes me feel more comfortable with
your sloppy handwriting
i can apologize on behalf of yannick his
handwriting is
sean hardy says phenomenal analysis you
really make this field
approachable to pre-university students
like myself
christian garcia says i was thinking
about the addition being memorized
argument which i totally agree with
and it reminded me that we humans also
tend to replace a lot of logic with
memory
for example multiplication tables
anecdotally i think i've memorized
various combinations of numbers that add
up to ten
five plus five six plus four seven plus
three deep learning still needs a good
way to do logic reasoning
but what if i have a vast amount of
knowledge that is a good portion of
human-like intelligence
well this is one of the things that we
talked about in the show today
that there are so many things that
initially we need to reason about using
our system too
but then it seems to get distilled and
baked into a system one program
borislav dizodzo says sorry to crash the
mensa
party but isn't the prediction of the
next word inherently a shallow target
i don't know how to exactly formulate a
deeper thought target
but perhaps a whole sentence prediction
with blur
or some other and better similarity
measure would do the trick
and the trick would only work if
transformers were inherently good at
modeling
reasoning some other thoughts that come
to mind is the prediction of
any sentence that would take place in
the future
and the further in the future if the
text it takes place the bigger the
reward
since i don't have the super computers
and the youtube comments are not cited
maybe you can just tell me why i'm wrong
or silently implement something if you
think i'm right
and yannick says well the problem with
blur is that it's not differentiable
so you'd essentially have to do
reinforcement learning and people have
done that
as for predicting longer sequences you
would have to back propagate through
multiple invocations of this already
giant model
so that's out of the question to predict
a single word
that's further in the future than the
next word and you can do that
but it will likely increase your
variance more than it will benefit your
model
good ideas though and good old herp
derpingsson
her your comments are always legendary
that you say a single part of dl
research is just
flexing how many gpu hours you can
afford
very true unfortunately i can't afford
very many
i hope you have fun of us analyzing the
hype and the stories around gpt3
and we do get into a fair bit of various
topics here
so i had fun doing this remember to like
comment and subscribe and we'll see you
back
next week hello folks and welcome back
to the machine learning street talk
youtube channel
uh with me tim scarf and my two
compadres
yannick kilcher and connor shorten
it it's been a really exciting week
because open ai have released
yet another paper microsoft have
introduced some new approaches for
training these huge
language models as you guys have done
videos about this on your youtube
channels but
last week yannick you actually made a
video on gpt3
and how many views has it got 20 000 20
23 000 as of now it's crazy
that is incredible is that your most
popular video to date
no no attention is all you need is the
most popular
by far but it's been up since uh 2017 or
so
awesome so what did you guys think about
gbt3
i was surprised just by by
the sheer publicity of it because so
gpt2
i think a lot of the publicity was sort
of manufactured by this fact that
it was oh it's too dangerous to release
and
uh you remember all of this right it's
it's like oh
we're concerned about the ethical you
know the bad applications if we release
this is so dangerous
it seems that none of that is necessary
with gpt3 it just
kind of took off by itself and
i think the most surprising thing was
that
people probably thought that this trend
that you could just scale up
would have to end at some point and i
think gpt3
just not only does it show that you can
push it another
order of magnitude but it kind of
fortifies the trend
that we could probably do this another
two or three orders of magnitude i i
think that's a lot of
not only you know it's not only this
order of magnitude
it it is such an indication for
more and more and more scaling and
that's still going to benefit
us a lot is it showing signs of slowing
down
yeah it's not that's the crazy part
right it's it's it's not the truth it's
almost it's it's like pretty much a
complete trajectory down like they have
this plot
of the perplexity and it just goes down
and you might
think that their you know their largest
model kind of breaks the trend a bit but
then if you look at the training curve
it's not
it doesn't even look converged yet and
they haven't even even gotten through a
full epoch
on their big corpus yeah
so thomas wolfe we did a video about his
presentation on youtube and we still
haven't put it live so
eventually we'll put that live and he
cited uh
yeovil you know one of the israeli
researchers and he was talking about
this
phase change in language models so they
asked it questions like uh
years later than this year this year is
later than this year and they created
this kind of trick
showing how good the language model was
at answering
questions and and of course it's just
memorization i think these
these folks were arguing that there's a
phase change
past a certain point where it's actually
learning to reason
but of course it doesn't even know that
a year is a number it's just it's just a
token
and if you train on enough data it will
know
that 1976 is after 1945.
so what's it doing really memorization
and generalization
generally these language models they
just memorize in their parameters but
it's still
useful right even if it is memorizing it
that's kind of the most criticism i've
gotten on my video is that i argue
that there is a you know that it's
basically just memorizing the training
data and i
might maybe i should have expressed that
more precisely what i mean
basically is that it is memorizing the
training data in like a fuzzy way
so it's mem it's memorizing little
snippets it's memorizing grammar
constructs and so on
uh that's what i that's what i mean by
memorizing kind of
in this distributed way but very much
tied to the training data and there
thereby it can like just use these
memorized
you know i take the grammar construct
from this sentence and i know that these
two
people often occur together and i know
this fact and so on
so it's i think the memory this this
sort of memorization
will lead to generalization because what
you essentially learn is how to
interpolate
between the what you've seen in training
you're saying
generalization but the thing that you
said in your video
was that it's not reasoning it's
memorizing
now what you've just described is quite
interesting because you're saying okay
it's not memorizing
it's deconstructing and it's cleverly
interpolating between different
instances of reconstructed sentences
and that will make it generalize at what
point does that become reasoning or will
it never become reasoning
well it's philosophy right but but they
have some interesting experiments in the
paper where they
for example show that the model can
unscramble
a word so you you know you take the
scram you give the
some examples right this scrambled word
unscrambled scrambled word on scramble
and they say look
or at least that's the argument i don't
know their precise formulation but
someone might say look the model has
learned what it means to
unscramble a word and i'm
all i'm saying is that if you have like
a perfect language model
and or you condition on these things it
will just output
whatever word has the highest
probability given those word pieces
right so it's absolutely not surprising
to me
and it doesn't mean that it has learned
to reason a much better experiment
would be to learn to scramble a word
right so so it's the same experiment if
the model has learned to reason
across the english language really
understand what it means
to scramble a word and can actually
infer this from the same context
learning
it should be as good scrambling as
unscrambling
words so are you arguing it should be
symmetric so if it can do one thing it
should be able to do this yes it should
be
quite symmetric in that but so any
asymmetry in scrambling versus
unscrambling would come from the fact
that it is a very good english language
model right because it can unscramble
there's a that's two effects that's for
one it understands what unscrambling
means
and the second effect is it just knows a
lot of english words
but if you are going to scramble words
now
your knowledge of english doesn't help
you because the scrambled word
is probably not an english word so you
now need to rely on the
understanding of what scrambling means
and therefore if the model has learned
to reason i would expect
it to perform scrambling as much as
unscrambling but
unfortunately they don't do this
experiment and i would be very
very surprised if the model could do the
scrambling i think there's a reason why
the paper shows unscrambling yeah either
way do you think it's ever seen
something like that in the training data
set
it uses inevitably and scrambled how
could it have ever seen something like
that
in the training data but all it needs to
know
right is is that is that the word pieces
often occur together it sees the
scrambled version of inevitably
and like the highest likelihood word
after that is the same word
but in absence of the same you can't
produce the same word because it's not a
word
so you'll produce the word in the
correct order
i i think that's much more what's going
on than it has like seen some website
where you scramble and unscramble things
the mechanics of this is quite
interesting because we we spoke before
we had this philosophical discussion
about
our frame of reference being a function
of the convex hull of all the different
words that we have you know these words
are
shared placeholders to things that we
that we have a common understanding of
so straight away the the frame of
reference of these language models is
truncated by virtue of the fact that it
only has these word peace and beddings
now the other thing is i think it's a
truncation
in the formulation of the language model
it's just predicting the next word
and it seems a little bit bizarre to me
that you're
doing all of these examples you know one
shot and two shot learning so you're
saying here's an example of something
now continue
so i'm thinking doesn't that limit the
amount of use cases
i think it's because of the complexity
of the decoding in the output space like
if you only have to predict the next
token
doesn't that give you a much less
computationally complex output space
than having to reconstruct the entire
input with the
predictions of the mass well i think the
transformers architecture has a
quadratic layer-wise time complexity
now the computationally the auto
aggressive models are quite nice because
they can just keep predicting forever
because you just put the
the previous prediction into the input
of the next one and
you could just you could generate an
article of any size
whereas with a bert type model there are
huge limitations obviously they have 512
inputs and
but it does allow you to do things that
you can't do with an auto aggressive
model like for example you can do
question answering or be it on a very
small input
i think because in invert all the
weights
need to be able to contribute to all the
outputs right because you have one
output
for each element in the sequence and in
gpt3
the entire apparatus of weights can just
be focused on predicting that one
next word yeah on the subject of having
scaled this up we have this auto
regressive compared to the mass language
model right
just talking about the difference
between say gbt and then burt and then
what each of them are still good for
i still think the specialized
architecture i think bert will always be
better at
sentence classification right so there's
going to be uses for the different
specialized architecture
bert has this bi-directional context and
i think there is
an assertion there that on things like
question and answering you need
a bi-directional context one thing that
i didn't get from watching
your video on gpt3 yannick was what was
the input size
how many tokens was it i think it was
2048
yeah exactly yeah oh that's huge yeah
yeah so what do we think with the
reformer
and transformer xl do you think if we
have a longer context window
say twenty thousand forty eight how i'm
not a power two but just a random number
but like um does that make this a lot
stronger
well the transformers xl was
significantly better
because of that reduction in context
fragmentation
it probably depends on the task right
but with respect to
bi-directionality i'm not i'm not
yeah okay i mean in that case bert is
more powerful because it's like an
all-one
encoder whereas as a gpt-2
is this has this autoregressive property
but it is all also
sort of bi-directional it is
unidirectional in the way it produces
output right but it can still attend to
everything
and probably something like reformer or
long-former or whatever that they trade
off
this ability to attend to anything in a
precise way
by trade this off for then getting
longer input
it probably very much depends on the
task and it might
just work because if you think about it
in natural language does really every
single
you know position need to attend to
every other
position it's probably not probably
you'll have like some hierarchical
structure in your sentences
and whatnot i think we simply
haven't found out yet how to make that
concrete
to say oh you know you
because basically language is a tree
sort of
because you can parse it in various ways
but then also sometimes it's really
important what that one word is
so there needs to be an organization
where you mainly attend to this tree
structure but can also attend
to the individual words but then you
don't want to attend to all the words
because
that would be computationally hard
and we're back to we can't learn sparse
things
the regressive models there is some kind
of a filter right to to stop the model
from cheating
and to stop tokens from seeing things
that are ahead of them
now i don't think that exists in the
bert type architecture
you just give it some text and maybe
like a noise from a masked version of
the same text
now architecturally something like
question answering is reasonably
straightforward to do in in the
bi-directional architecture
because you can say you could do in-span
question answering by
putting in a span of text and and a
question and saying
this is this is where the answer should
be and you could train it on that
downstream so you wouldn't be able to do
that on an auto aggressive model
no no but you could actually do the same
with bird that you do with gpd3 and that
you simply input the thing into bird
just take the very last
language model prediction and just shift
all the things by one append that and
so you could do the producing the same
with bird but you're absolutely right
you couldn't do this the gpt3 cannot
point to its own input like bird can
that's that's the power of bird yeah
well ask
gpt3 in language what you want would
want to know
right i think just one more interesting
thing on the auto aggressive and the
mass language modeling is like spam bur
and how we're masking like
spans instead of just tokens and i think
that's kind of interesting too how much
knowledge can you pack into the
parameters of a language model they're
looking at just
asking t5 questions based on giving it
no extra context
and they show in appendix c the gains
when you use this
span masking and you see a pretty huge
gain with
using this kind of training where you
mask out a span of tokens
yeah it's is it is this even further
than the full word
masking or is this the same
because so the reasoning i can
understand the reasoning behind the full
word mask it's where you say
instead of just masking word pieces we
mask
if we mask a word piece we mask all the
word pieces that belong to the same word
usually if you just mask one word piece
the other word pieces will give it away
it should be so determined so you don't
really need to know to learn
all of you know sentence structure and
grammar and so on you can just go
on the other word pieces and if you mask
the whole word
you can't do that anymore but is the
span bird is that the same i don't know
yeah i don't know if they explicitly are
searching for that but that is
interesting
yeah i thought it was where they just
masked off a span
of words i can't remember how they
arrived at the length of the span but it
might be in a length of two or three or
something
okay yeah i think they call it like
degeneration how these language
generators they'll repeat the word
you see that all the time do you think
that would be alleviated by having
instead of
predict the next one you'd mask out five
you know and predict five at a time
these are all variations because bert is
a denoising auto encoder
so it's super interesting when we
covered the t5 paper or thomas wolf's
thing
they were talking about all these
different variations on bert and
sometimes
they were masking off words sometimes
they were masking off a span of words
sometimes they were
replacing words or just adding noise
what they all seem to be doing is taking
something that's on the manifold
and pushing it off the manifold
yeah so i think we're trying to get to
the bottom of like gpt versus bert right
yeah well yeah but seems really
interesting because i'm
i'm in industry at the moment and the
number one thing that everyone wants to
do
is knowledge mining so many companies
have
a wealth of information in unstructured
documents
on a data lake or even inside office 365
and what we need to be able to do is is
use these models to
extract useful information out that i'm
interested in
now gpt-3 seems almost entirely useless
for that
i can ask gpt3 who's the queen of
england and it seems to know a lot of
common sense
but if i want to extract out some
semantic information from my documents
about something i care about
it's useless well are you sure because
if if you use something like bert it's
still it still requires like
training data to fine-tune the thing
right whereas in gpt-3 you could just
potentially throw
all of your customer documents all of
your
contracts all of the things in there and
you could just go ahead and
say think about what you need provide
like two three examples of the structure
what you need
don't think these examples that they
provide need to be correct
it would be so interesting another
experiment to do for them is to just
simply provide
structurally the same grammatically or
you know the same things but incorrect
things
and just see what the model comes up i'm
pretty sure the model would still come
up with
what you're looking for because yeah
i've made this argument in my video that
what these models basically do is they
go to the
semantic fuzzily go to the training data
and
filter the training data for whatever
you condition
on this in context learning it's
basically a fil you filter the entire
training data for all the documents that
match
that particular structure and then you
you just run your language model
conditioned on that set of you just
interpolate those
things wouldn't it be awesome
if as a company you just throw
everything in there and then you could
do that
i'm interested to know how brittle it is
with bert for example i get the
impression
that i could fine-tune it even this is
the thing with language models you
you train on a large corpus of text on
the internet it learns some common sense
and then you fine-tune it on your own
domain
my intuition is that if i took a gpt
type model
it wouldn't be as good maybe i'm wrong
so let's say
i take the gpt3 model and somehow i've
got loads and loads of compute that i
can access and i can
i can start to continue to train it on
my own corpus of data at work
and then i ask it questions because you
were saying that it filters
it finds all of the things that that are
similar to that and it cleverly
interpolates between them
but surely that depends on several
factors it depends on
how much stuff it's been trained on it
depends on how
homogeneous the data is it depends on
whether there was a critical mass of
things on on that particular topic that
it learned
do you have any intuition on what those
factors are sure absolutely i mean
the the sheer number of data i think
ultimately the compressed size of the
training corpus is over half
a terabyte is that correct if i remember
from the
paper and that's half a terabyte of text
right we're used to
big data sets but this is text right it
was
tex you can compress text to like
minuscule things so this is a giant
amount
i think it's like a trillion tokens or
something
and yeah chances that you will have this
in your company
are slim well isn't that a reason not to
use it then because if i start training
i'm in i'm in banking
and i want to learn about financial
defaults or
people's financial prospectus documents
or something and i put it in there
would it be a drop in the ocean would it
it wouldn't even move the needle
probably probably i mean it's very hard
to say but
probably yes i was just hypothesizing if
you had if you could
right then maybe it would be super
interesting
to just have your company documents all
in this thing
that just sort of interpolates so what
do you think about fine tuning it's very
intuitive that we have this new data set
when we fine-tune the parameters but
so it doesn't make sense to think that
you could just put all your company
documents in the context and you
intuitively you'd want to fine-tune it
but maybe
fine-tuning itself doesn't make as much
sense as we think
because for one it's going to take a ton
of resources to do so
hasn't gpt-3 already learned the best
representation of words
yeah that's the question it has probably
learned the best representation of words
on the general internet or on wikipedia
or things like this
we're going to see an era where right
now when you fine-tune bert what you do
is you
take the pre-trained checkpoint that
comes from wikipedia
and you fine-tune specifically on your
task right you fine-tune with the label
being
whatever you want maybe we'll see an era
where you take gpt
three or something and then fine-tune as
language model continue the language
model but in you on your data
and then you do this in context learning
to actually answer your task
i mean that that that is entirely
entirely possible
one of the other drawbacks is if i ask
gpt a question it will give me a kind of
abstractive answer probably what i want
in a business context is an extractive
answer
yeah if i mean you can you can trick it
you you can
that's some of what they do in the paper
is where because
if you for example do sentiment
classification you just
you don't want the word you don't want
any word you just want the word either
positive or negative
right so you can actually restrict the
beam search of the output to just those
words so you can just ask it
which one of those is more probable so
you can
you can whenever you can phrase your
question as
let's say a multiple choice between
bunch of answers you can just ask it
which one of these answers is more
probable
and then it will sort of yeah yeah
another paper that i really like is
pattern exploiting training
where they it's the same idea you you
take some expression that you can append
to your task that will
give a better use of the language model
for answering the question
so patterns like with the there's like a
yelp review and then you would
add it was mask to get a label from the
language model so there's a yeah
yes this idea of coming up with these
prompts
but then the problem is how do we have
to manually find these prompts now they
in the pattern exploding training they
try this automatic verbalizer search
where they're trying to find the
patterns automatically but
it still doesn't look like they have any
way of taking the human out of the loop
with what kind of
additional context you give to it to
give it a better answer
abstractively that goes into two
directions of these math examples
which are quite prominent in the paper i
feel and
what a lot of people talk about and i
don't know what are
you views on on what's happening right
there you already said at the beginning
tim like it's it's these are strings
right you
would somehow have to learn the decimal
system
it's so weird and but yet what what are
your thoughts on this
on the math of gpt being able to do math
i don't think it can i completely agree
with your assessment
but but it's not as black and white as
that
it's not just well maybe it is just
memorization but there is a nuance to it
it is learning some kind of internal
structure
we've said before that in days gone past
we would manually create these knowledge
graphs
and inside our own minds maybe there's
two systems that we've spoken about
system one and system two
when we verbalize our knowledge there is
a certain structure to it that we're
capturing
to what extent is the transformers model
replicating that i'm not sure
yeah i mean i've that that's i've also
gotten a lot of pushback on that
and i'm i'm like already half convinced
that there's more happening but still
not
fully convinced but what what i found a
good
a good comment was that humans for
example also
do a lot of this math as memorization
like you you
remember multiplication tables up to you
know
10 10 by 10. it's this is you don't
perform the math you just know it from
strings basically
and all of these of these like low-level
additions you just
you know by heart and it's like the
argument is that
that's just what gpt is is doing
basically learns by heart these
low-level
math and that's why it no longer works
if you then go multiply three
digit numbers yeah like if if you look
at the chart it
it really dies that it works for like
two digit addition and subtraction
but this is basically randomly get you
know you only have 10 possible outputs
so this is randomly guessing it
yeah you covered this in your thing
janet
so surely it's just memorizing because
if it was reasoning
there wouldn't be such a precipitous
change exactly
well that was my point basically if you
think about addition and subtraction
how many websites are there and we're
we're looking at a crawl of the web
right how many websites are there where
there's just like a giant table
and one of the columns is surely going
to be the sum of some other columns
right so so by pure language modeling
you
will learn to add and subtract and and i
think
that's why addition and subtraction are
so so high on there
whereas you have you'd be much more
hard-pressed to find a table where
three-digit multiplication is intrinsic
to the structure
i'm not talking about websites where
someone specifically constructs a math
table
but it's like oh here are the number of
people that live in
uh west west side of town and here are
the number of people that live in the
east side of town and then there's a
column
total number of people right and it's
like so that's how you learn
i love your conception though you can
say that humans are dumb
right because
i think that we do memorize a lot of
knowledge that if you look at the way
that humans learn
we are really efficient about our use of
thinking it's
it's very taxing for us to think so we
quite often
look at the decisions of others to guide
our own behavior
and there are lots of psychological
biases like social proof that do that so
a lot of the time we're not thinking and
also
with this precipitous change in
abilities we we think of human
intelligence as being really general
but it isn't there's lots of things that
we just can't do like
solving traveling salesman problem we
can solve it up to a reasonable number
of cities and then we suddenly are
really crap at it or if we
try and reverse the problem and we solve
the reverse which is finding the longest
possible path
we're really bad at it we're good in 3d
we're really bad in 4d
so it's it's not so different
yeah maybe yeah but i mean the
the quite the the big question i think
that people have is
is is there an explicit like
what we said at the beginning is there
an explicit notion of reasoning going on
inside gpt3 like or or is that
completely absent well it does it does
come back to this system one versus
system two because i think
gpt3 is system one it's exactly the same
as you were describing we do many things
without needing to think about it
consciously because our intelligence has
baked it
into a skill program and that program
almost runs deterministically just like
in gpt3 it's just a deterministic
program
one which of course we couldn't
verbalize it's been baked by this
back propagation program and it's
informed by various inductive priors and
and so on and experience that's been
given but
it's now a fully baked system one
program
it's not reasoning yeah i mean i
i i that's actually a good
characterization is that
if you if you characterize gpd3 as like
reasoning or intelligent basically
you're saying that
system two doesn't exist or something
like that
because i completely agree it's not it's
not
probably not doing that well what do you
think about this
like connectionist idea of kind of like
that we can't with that we store all
like maybe it is like because
to me it seems like the attention on the
intermediate weights could be a way of
reasoning kind of
so i think the fact that this can do the
trend the zero shot transfer
kind of points in this direction that
maybe it is just system one
a little bit yeah that's a good point
like is is the is this attention
computation is
that a form of reasoning like is
that's i don't i don't because it's
transforming the representations
like in a meaningful way it's true
but is it possible to do reasoning in
the context of a neural network
i think the transformation yeah well i
mean to say
another way human beings create you know
imperative programming code
and the constructs in programming code
are things like looping semantics and
symbols and it's it's a logical
one-to-one correspondence with the
verbalization
of how you do something it's an
algorithm
what you do in a deep neural network is
completely different
well we we know that these things are
like can compute arbitrary
functions so from like a purely powerful
power analysis they should be able to do
anything but
i think here's the notion of where
people think
that something like a reasoning should
happen
like it happens in your head like you
have to sit down and go like
okay logical step one logical step two
logical step three right
so we can't we almost can't imagine a
system
that it does reasoning that doesn't do
that we think like some there's got to
be like a module in there to retrieve a
memory
and then a module to update that and
then a forward simulation or something
maybe you maybe the this is just a
human-centric view and the
reasoning like the function that is
actually being computed the exact
function not an approximation
the exact function that's being computed
can also be represented by simply
forward pass through attention layers
it's entirely possible i don't know but
then there's a difference between
representing it because i think system
two is
the intelligence that creates the system
one skill program
so when you have a new situation
which is similar to something that you
haven't seen before and you need to
create a new skill program
that's when you need the reasoning yeah
it's debatable how much you can do that
or how much
you simply go more abstract like if you
see
there's a situation you've never seen
before i'm sure you've seen it before in
some abstract way
like okay you've never been to london
but you have been to a new city
before right you've never you've never
been
in a in in i don't know in in the
philippines but i have been in a country
before that where my
the culture is not mine and so i kind of
know i have to
adapt in that way you know the question
is can you
really generalize
to situations that you haven't seen
before ever
or is it just that you can generalize
in to situations and transfer their
abstract properties
well in that situation you would have a
taxonomy of skill programs which you
have learned elsewhere
and they would all have a degree of
generalizability
and you could fall back on those skill
programs
and your system too would learn to
recombine them
as you as you acted in that environment
yeah i have another kind of funny
example so we were drive we were headed
to trader joe's and as is common now you
see this long line outside of trader
joe's because they're you know filtering
it to keep it
empty in there so we come up and there's
no line and so
quickly we're like what's happened why
is there no line outside of trader joe's
but it's not like we've never seen no
line outside of trader joe's before
it's not like it's lined up with aliens
or the building is on fire
so like we can be like whoa this is out
of distribution but i've seen this
before so it's not like it's blown my
mind
but then it's like so with our neural
networks when they see some crazy added
distribution thing and
it blows their mind right like gpt3
could at least maybe
start trying to say something about it
like our classifiers they just give you
this
awful prediction but the abstractive
generation models they can at least
start trying to tell you what i think
i'm seeing right like
moving towards that if you want let's
let's play with that give let's let's
come up with an with an example
of something that would be completely
out of distribution what would gpt3 do
we had this i think when we talked about
benjo's thing in that
is human consciousness is it generative
or discriminative because
in in all of these models that we have
at least
it's sort of they assume that their
input comes from the same
distribution as the training
distribution so
they will have a very hard time to
assess that the input that they're
getting
is very improbable right now gpt3 could
at least
say something like that what are getting
at this context
that's improbable so yeah it makes it
makes a case that gpt3 could actually be
be like whoa this context isn't
something i expect right but then
again if you give it a novel task it is
also
something that it doesn't necessarily
expect so i
i don't know but it's a good point that
gpd3 might actually
be able to recognize out of distribution
well my intuition is if you give it
an example which is well off the
manifold
and presumably there there are a
combinatorially many
permutation there are many things way
off the manifold and there are things on
the manifold
if you gave it something off the
manifold it would just
not know what to do what would it do
just just generate garbage
most likely it's the same as humans do
you just put them in a situation where
they're completely lost
what do they do either they like freeze
or they run or they cry i mean it's
because there's a difference because
humans
first of all we i would imagine we have
a taxonomy of skill programs but our
skill
programs generalize so much more whereas
the
deep learning manifold that we've
learned you know we had that energy
based surface that we were talking about
it's it's very very small if you give it
something just slightly off the manifold
you're lost what do you think about
like so this is one paper you can see my
screen
where they're trying to like they mix in
the data with like
explain the task would be like explain
nli premise and then so they want you to
not only give the answer but then
explain this do you think this could be
a promising direction towards like added
distribution like
explanation and then you can get a sense
of it
if you said something like explain and
then you give something that's
completely
unexplainable or yeah but
maybe it's like a way of getting a sense
of
how they're thinking maybe i mean that
would still that would then require
that the human interpret what the model
says
right it's because the model would just
output the words i
cannot explain this or something like
that
i'm not sure if that's the same because
for the model that would still be
in distribution because the the
fact that it produces that still means
it can actually answer the question
but i guess it's a different question
from asking what happens if the
input itself is like completely
out of distribution for the model and
then i would argue it's just going to
fall back to
because the pro the conditioning
information is so weak
because there's no signal it's just
going to fall back on its prior and
that's the english language
so it's probably just going to produce a
probable sentence
like i don't know i think
these when when google introduced this i
heard a public talk about this when they
introduced this
autorespond not autoresponder quick
response for gmail
right these three things you can click
one by one and they
they tried i think they tried generative
first and so on but then
it turned out that it would always start
with sorry
for the late reply
[Laughter]
it was like whatever whatever input it's
like sorry for the late response
yes so that's that's maybe when the the
prior is too overwhelming and and
in a case where the input just doesn't
make sense it's just gonna fall back to
the prior
it's really interesting that you bring
that up because that would have been my
expectation surely these models would
just do something like that all the time
it's surprising to me that when you look
at the gpt3 paper
it just gives incredible results
yes apparently with no hacking or
tweaking or massaging
is that surprising to you very
well in in some somewhat it's it's so
surprising
like it's not surprising if you just do
language modeling because all it has
is that context right and if it wants to
do better than
random it better going to start paying
attention to that context
because that's the only way it gets a
better like a lower loss
but that then what you're saying what
that it then actually generalizes to
give other contexts and it does
something useful not just falls back
it's
it would be interesting to
experimentally see
how far you can push it how far you can
push garbage into that context until it
actually
breaks until it just always gets back to
the most probable sentence
is it possible to conflate an auto
aggressive model and
an encoded you know like a denoising
auto encoder type model
or to bro to broaden the question are we
barking up the right tree
do you think this type of architecture
is what we should be looking at well
i guess it depends on what you want to
do right this is this is a this is an
enormously good language model because
it can throw all its power into
predicting the next word and not like
bert you know has to like predict all
the words at the same time
and whether the two sentences are follow
one another
uh this can just this can be like full
blast
next word is the only thing that matters
i suppose my intuition is that a
bi-directional model would learn
more about the structure of the language
but it's easier to train an
auto-aggressive model
is that fair maybe at like scale it
doesn't matter anymore like
i mean like you know once you scale up
bi-directional
you know unidirectional it's all okay
now that we've trained on
tons of data with bur
there is this horrible quadratic layer
wise time complexity
so they had to truncate the input size
to 512.
if that restriction wasn't there i would
i would imagine
if you could train a much longer input
size in a bi-directional model surely it
would give much better results than an
auto aggressive model
well the the the the other aggressive
one has the same
complexity like it's it's also a
transformer it has the quadratic
complexity just as much
and but it has a much longer impact so
if you think of the like
original transformer of the original
attention is all you need paper
you have two inputs right you have the
encoding
part and the decoding part and those
those have internal attention
so in in that case the encode if we
translate to this to our thing the
encoding part would be
this context whatever you give us this
in context
and the decoding part would be whatever
you have output
so far now you can construct this in
very different ways
but at best you're going to to basically
have your the size of the thing so in
where in bert
everything could attend to everything in
every single layer
now you have attention internally but
that only like
that gets you like to d square half or
or d hat the
half squared it's not like you're
you're significantly reducing the
complexity of the transformer here
so it has it has the same limitations
and the reason it can take
2 000 tokens as input is just because
microsoft has built like
this big of a computer that's the reason
are there any trade-offs to having a
longer input size
is longer at some point worse
it's a good question because with a
models like lstms
the answer was definitely yes there is a
point at which you
have so much information that you know
you can only encode
in this much hidden state so at some
point you overload
i still think yes and transformers at
like if you throw
information and information in there
especially correlated information
that is going to at some point be
detrimental but
because something like a cnn for example
has a receptive field
which means you can feed in as much
information as you want and it will only
use this receptive field
is it a similar concept with
transformers that even if you fed in a
huge amount of information
because of the attention mechanism it
would only pay attention to
the substructures and the symmetries and
and it would ignore
everything else so it kind of wouldn't
matter if you fed it more information
yes possibly i mean as a like the
problem i see is really when you
input correlated information and you
might you might know this from
simply doing linear regressions if you
have correlated features or logistic
regressions
it's horrible because you could
technically if you have two very
correlated features you could
technically predict the output from one
or the other so and then through
due to noise in you know your stochastic
procedure
in one step is going to push one up and
the other one down
and then the other step it's going to do
the exact opposite so they're like
it's like ah should i pay attention to
this no to that no to this no to that
but this works well
but that works well as well and you'll
just end up with like a very fuzzy very
confused model
um so i think there's definitely a point
where too much
information conditioning becomes
detrimental but
well i think they even show that in the
paper where more examples doesn't help
like with the demonstrations and
and also i think that's the reason why
they also
limit the training corpus because if you
you have to filter out
the garbage samples right and and that's
sort of the same thing where you want
high quality
data where you can actually predict
something
because another observation is just like
in images
cnn's had this inducted prior that
presupposed that
local connectivity between pixels was
important
and global connectivity was not
important and
with documents for example if you look
on average how long is a document it's
probably not that big
so at some point if you had a huge
receptive field you would be
wasting all of those potential
attention points between tokens because
no document is that long
maybe there are some documents out there
that are huge but maybe if you had an
infinite receptive field and documents
could attend between each other when
they're talking about similar things
i guess i'm just trying to reason i'm
trying i'm trying to play with this a
little bit because at some point if you
go past the average document
length you would be wasting that
representational capacity
but maybe if you just infinitely
increased it it would be a good thing
again
because you would encounter other things
that you could attend to in a useful way
well well here's the point if
i guess if you had much longer receptive
field you could input more than
one document but the question is
which documents do you input together
because you can't just
take two random ones because what you'll
teach the model is that
the second one follows the first one
right that's
it it thinks like oh this is a sequence
that happens in nature
and if you just input two like random
things after one another it's
it's also more confused probably and
like keep in mind these transformers
they don't have an explicit
actually they don't have an explicit
constraint
on the input length you could actually
technically input
any in the same transformer so invert
right now you can
you could input uh sequence of length 10
or a sequence of length 10 000. now the
first problem with the 10
000 sequence length is that the position
encodings
have only been trained up to 512.
but that's a that's like a minor point
the major point is that your gpu is
going to explode
but in the same model that the
transformer has no notion of the
sequence length it is it is a set comp
set computation algorithm that
just sees elements of the set
yeah yeah that's exactly right so that
these positional encodings are kind of
like
these sincoidal functions and that's the
only way it knows that one thing was
before another thing
so you were right you can only put in
one document at a time because
you can't really impute an ordinal
or a relationship between documents i
mean maybe you could you could invent it
you could say well it comes from the
wikipedia bucket and
it was this one was created on a
wednesday and this one on thursday but
but the interesting thinking point
though is
what if you could get the
layers of a transformer model to attend
to
the layers of another document did you
see what i mean so
then here's another similar document and
i want this to be able to attend to that
document
and i could dynamically pull the other
document out because i've encoded it
and i could do some locally sensitive
hashing or something would that be a
good way to go
well that's it sounds like awesome if
you could if you could do if you could
like
forward prop and then reach because i've
i've
basically come up with and this has been
done so people in
my very helpful like my comments are the
most helpful
my youtube comments are like way more
helpful than any reviews
of my papers or any any of that so
so people have pointed out this has been
done where i said
surely you know if the model is is is
learning to interpolate the training
dates in a fuzzy way
then you should be able for each output
you generate
to pinpoint back to the like here are
the
five training examples that led to me
making this decision right now maybe we
can spin this a bit further
and say okay i could do that i could
like forward prop and then i could
retrieve the training documents that are
most relevant
put those in the context and
you know be able to explicitly attend to
them
not only through the weights but it it's
it's sort of like
right now we're trying to distill all
this knowledge and
what i'm arguing is mem fuzzy
memorization of the training data
into the weights of the transformer but
if you were to build an architecture
like by programming what you would have
is like a database
where you could retrieve things
you know and then attend to them so
we're we're basically trying to build
the logic
and the the knowledge itself into these
weight connections where i think what's
missing here
are explicit memory modules where
you store and retrieve information and
have the
transformer itself do the computation on
that information
rather than both so that be like
matching the query with the nearest
neighbor in the training data or would
it be like
i have this this path through my
transformer
made the most influence on this
prediction so i'm gonna see like
what had caused the biggest like
magnitude gradient change during the
training
like which mini batch i don't i don't i
have no clue like i
i'm just think like this but what you're
saying tim is
is like super interesting if if that
were i feel that's
sort of out of reach right now out of
the computational
right capabilities but it's like super
fun to think about this could be
yeah the next step in these transformers
i'd love to explore this because you
said
i want to know here's the output i got
which examples that i trained on have
led to this output
now in vision models you did that thing
on chris ola's
feature visualization and you can say
well here's an output neuron and i'm
going to
solve an optimization problem to
generate the input which maximally
activates that neuron and that's quite a
good way of going backwards in vision
architectures
and i'm not sure how your setting would
work here on the transformers model
would you find the intermediate
representations for all the different
inputs and then you would trace back
from the output and you would then
do a vector search and you would find
the ones that
that were most similar because you were
talking about something quite
interesting
you said basically it's a memorization
machine and what if we actually took
this to the next level and we created
some symbolic reasoning and we
explicitly
made it into a memorization machine so
we took in
a representation we we stored it in
memory
we turned it into a computer program
essentially a kind of hybrid
would that be more interesting and would
it give us more transparency in the way
that you want
yeah i mean that's so that's one of the
one of the
the next thoughts you can spin from this
exactly a direction like this basically
what my
my argument was that if i train whatever
a logistic regression or a
one of the smaller vision models what
they learn is
map features to outputs in the in the
way they have to they have to abstract
but as we go larger and larger and
larger amount of weights
and and just just from you know from the
trend of
doing larger models from the output of
what these models can produce
i strongly feel that they are more and
more
actually memorizing the training data in
in the sort of fuzzy way i described
and where as opposed to extracting
features and and predicting based on
those so
i maybe the most basic imagination
would be something like during training
like build a reverse index from weights
to training examples where you said oh i
just passed this training example
and it influenced this weight here a lot
right
now index just from this weight index
back and maybe you could update that
during
training you see other samples index
update that weight even more
you can like kick the first one out
again
but then you basically have this reverse
index and then if you forward prop
your test sample or your context your
question
you would simply observe maybe the
the self gradient the gradient on its
own output or you would observe like the
forward prop signal
and see ah these connections here
they're really important
they're activated right now let's go
back in the reverse index
and see which training examples led to
and i feel this could be
first of all explainability wise
that that could be massive and again
this has been done like people tell me
this has been done i haven't read it but
uh if you're interested check out work
that has been done it could be
explainability-wise super interesting
because you'll see ah okay
here's what the model you know basically
you can
it's it's explained by example it's like
oh this output yeah you know here are
the things i i
base it on and second it could actually
be the next sort of
search engine right if you like the
fuzzy search engine through your
documents you throw your documents in
there
and then you know you just type
something
and it will just pop up oh here are
things that i found on your computer
that are relevant you've got a continuum
on one side of the continuum is it's
just memorizing stuff
and in your gbt3 video you gave an
example
of an utterance and and it was very very
close to an article that you could find
on google
what if it's more sophisticated what if
it
is abstractly generating tech and
being far more nuanced and intelligent
than we realize
how would you measure that that's a hard
question
i i i i don't know
pro probably you would measure it by
yeah how abstractly it is interpolating
the training data because ultimately
if if you see that it is really working
on the level of the idea and the
abstract concepts
it's hard to devise a test for this
but i think in the outputs of gpt3 right
now you can
clearly see in these news articles for
example that it has
probably just taken a bunch of these and
pushed them together with grammatical
structures that it has learned
and yeah it's very hard to test
for that i i wouldn't know yeah because
all of our metrics for this
are rubbish yes and
it makes me think what if we went back
to first principles and and had a much
simpler language model and we generated
our own
corpus using some grammar and we could
come up with some kind of a level of
abstractive text generation and we could
understand
which factors influenced it i just
suppose
at the moment if we're just using this
huge corpus and this huge language model
it's impossible for us to because it
would just generate anything
and it and it would be really good and
it's impossible for us to reason about
or measure
the abstractness or intelligence of that
generation
sure but then i mean as i i
agree but then if you if you do this
kind of thing on your abstract task
where you know how it's created then the
question is
you know so what how like what does that
have to do with the real world
yeah it's the eternal question between
people that work on toy problems
that they understand and people that
work on the real world
like we don't know what the real world
is yeah i think this is a great example
of this where they have
they construct this toy data set where
it's charlie is big charlie is blue and
then you learn how to combine the rules
and then
so then they start to scale this simple
toy thing up into more complex language
and then they try to take it to the
question generation
but i do agree with tim i think we need
better generalization probes i mean like
better tests even if it's like a toy
data set that can you know layer
level up its complexity to you know
match something like squad or
a more complex data set yeah
yeah i just have no intuition how we
would even start because one
one thing we could do let's assume that
all of our training data was
perfectly spaced on the language
manifold and
we could start by inputting an article
of course what it's going to do is
complete that article
and then we did a mixture of two
articles that were next to each other on
the manifold and presumably it would
interpolate halfway between those two
articles and do you see what i mean
we need to come up with some framework
of reasoning about its behavior and it
feels to me that we're a million miles
away from doing that yeah
absolutely we have like nearest neighbor
probes
kind of that you know i think that's the
most popular visualization is we'd love
to go see what's the most similar
document in our
in our representation or i think with
images is a little easier to think about
you know which dog image is the most
similar to this but then you have this
entanglement between
the k nearest neighbors it's on top of
this representation i think
yeah and interesting have you could you
this radioactive data
i think i've seen this before but it
that seems like very much
something just from the title something
that would go into the direction of
yeah i thought this paper was really
interesting it's it's like so you take
your like
uh batch your sgd batch and you're gonna
like
put some signature in the data such that
it comes out
into the model like you put these little
encodings
like you know just like kind of how we
put these little encryptions on our
images like a watermark
you know that doesn't show so maybe like
i was thinking about
listening to your idea that maybe we
could like sign our mini batches of text
data
in the token tokenizations
maybe something like that yeah that's
smart i mean
this this here is probably also for if
you want to detect if
your personal data was used to train a
particular model or something like this
i mean that's
yeah but it is yeah it could be a way to
find your way back to the original
the training examples that gave rise to
something
yeah that's quite smart yeah it's a bit
like adding a watermark
yes yeah my intuition is though even in
a cnn architecture but in the
transformers architecture as well that
they're very clever at taking shortcuts
and your watermark would just represent
a different path through the network it
wouldn't necessarily
help you locate the thing that you're
interested in
it would just be a different thing in
the network and if you use the watermark
in a lot of different places it would be
quite a
well well-trodden path through the
network and it wouldn't
if it was on a picture of a mountain or
an article about a mountain
there wouldn't be a pool between that
article and the watermark you created if
that makes sense
i imagine there's a lot of overlap
between like if you can easily find an
adversarial example you can also easily
encode the watermark so
you would have to trade off like it's
gonna be you can't you can have like
adversarial robustness and then also be
able to put these watermarks through the
network right
yeah i mean the watermark is gonna if if
if your goal is really that
if someone trains on this data i'm going
to spot this in the output
and i guess the the watermark is almost
equal to an adversarial example
because what you want to do is kind of
align the features such that
you know the output is very
very weird i haven't read that
particular paper but
i'm pretty sure the mechanisms are going
to be exactly the same as adversarial
examples so i was going to ask you
yannick about the other comments you got
because i noticed on your chalet
video i was surprised to see some people
uh laying into chile and chile is my
spiritual
deity so i was very pissed off about
that it turns out that there are some
other people that have different
opinions about measuring intelligence uh
who would have thought it but
what comments did you get about the gpt3
paper
what on on the charlay thing no
nothing to do with shirley just what
comments on the gpt3 paper
well i was just saying i i i was
exasperated
i read your comments yeah because
chalet i would just like to say
officially that chalet
is the is the man he is the one of the
pillars of the deep learning community
and if anyone wants to have a go at
chalet you have to come through me first
that's very very normal i know i mean
i've got to do my bit
for for france and for sure but we'll
we'll get back to chile next week i
don't
we should keep our powder dry on chili
we are we are going to
do chalet justice next week but in the
meantime
yeah what comments did you get on gpt3 i
guess most
most people with like substantial
comments were
were talking about these there's a
reasoning ability
either agreeing or disagreeing of what
it
you know is it doing reasoning or not
and specifically
like the math part you know is you know
is that
is it feasible that it learns this as
a language modeling task or not yeah
it's just a lot of a lot of opinions and
very i mean i've i've learned a lot just
by
reading these comments so that was
pretty that was pretty cool
i thought i think i got i got no zero
comments on this
news thing right because what i so
if people haven't seen this what i did
was basically gpt
three was asked to complete this news
article about
i think the mormon church or something
now ordaining lgbtq ministers and then
there was a split in the church and so
on
so it was asked to complete this news
article from like just the title
and like the first paragraph or so
and i could basically show that if
i could find because they say well we've
de-duplicated the training data
such that this news article wasn't
in the training data that we got the
title and subtitle from but i was
basically able to show that
like a lot of other i think i've shown
one but
i found like a bunch of news article
about the same thing or books
that that quoted not quoted but not
verbatim but
basically books that had as a source
that news article that were using like
extremely similar language
to describe like same sentence structure
a couple of words
switched where i feel the the
deduplication efforts
of so if if they show look you can
produce this news article
i'm not super convinced because i think
it's kind of seen almost that news
article before
so you know there's there's a couple of
things there though one thing that
i think is is good is that was quite a
specific event
i would imagine that there were not many
news articles talking about
that event so i'm impressed that on a
huge language model
because you know we were talking about
the specificity
and diversity and scale needed to
to move the needle on a large language
model because i'm in the industry and i
wanted to learn about financial defaults
or something i'm in banking
and we asserted that my corpus wouldn't
push the needle
this seems to suggest to me that you can
push the needle
with a small number of training examples
presumably it's learning a taxonomy so
there are lots of concepts in there it
knows about what lgbt is it knows about
churches it knows it
you know so it's all of these things are
activating in the network and
they're being kind of combined together
and
it's learned that and it can retrieve it
given one example that's
not i'm not sure that it learns what
these concepts are
i mean the the other extreme would be to
simply say here's the title
this is the training example that or
these are the three training examples
that match that title now just
interpolate them
whatever they are and this is actually i
can make a credible claim that that's
happening because one of these articles
who i found was from google books and
google books is one of the corpuses that
they have
used in a in a prom like in a dominant
fashion they
use like different weights to different
corp
corpora corporacies corporate
okay and and google books was pretty
weighted up so
it's arguably it has seen that
particular example multiple
times and given that it has so much
weight
right it's possible that it has used a
bunch of these weights specifically for
that or similar
similar articles and doesn't really know
what lgbtq
doesn't really know what mormon means in
in
in that sense right so yeah that's kind
of the two extremes and
i have not gotten many comments about
this which
i interpreted as no one's no one wants
to debate me about it
there's also the observation that you
know we were saying that
most humans are um like drones they
don't really think for themselves
for example if i was to go out and write
an article on cnns right now
what would i do i would go and read a
book on cnns and i would go and read
some articles on cnns and i and i would
do exactly what gbt3 is doing
so you can argue that there is an agency
and
autonomy has limits yes but it's not the
same
as as a news reporter that you know
sees things happening and then
synthesizes
you know this news story right it's not
because there are als
all there are already so many texts
about cnns
there's nothing really left to do about
interpolating them
but a new a journalist that you know you
give a title and you say please or even
if you're if you're
allowed to invent an essay she says
here's a title of an essay
right you are going to come up with
something much
less interpolated than gpt3
i would argue you're going to come up
with something that's
more interpolated maybe in the concept
space you're like i'm going to tell a
story it has a beginning there's a
conflict and then conflict gets resolved
but i'm not yeah i love the way you
articulated that
because if i'm writing about cnns for
example
you know we were talking about the
convex whole and if it's
if it's a topic which has been maturing
for a significant period of time
that convex hole will solidify
as you say if a new event happens
tomorrow and i started blogging about it
that thing would take shape and it would
evolve and sometimes things change over
time
our politics and our views about things
change over time
yeah i mean i mean that's i mean that's
an entirely different problem with
things like gpt3 right
here as you crawl the entire internet
and so first of all
to the point before 500 gigabytes of
text data like really a lot of stuff is
going to be repeated over and over
like no deduplication can save you from
the fact that you know there's like
there's a lot of stuff that's kind of
over and over and over in there okay but
second of all of course it's a snapshot
it's like
some of these things are old some of
these things are
from sources that are sketchy uh
some of you know it's it's so that
there's an entirely different
aspect and i think that's one of the
aspects that you know in gpt2
and general in the you know the fairness
community that's that's very prevalent
if you
if you input data from 1950
you're going gonna get output from 1950
and and that's
so that's a question i had for you what
what are your thoughts on the
on the dangers of of these mod because
in gpt2 especially but i think
in gpthree the broader impact statement
is five pages long or so
on like these things like let's say fake
news generation
with that and so on what are your
thoughts on this
oh and i'm just gonna say that i do
think that you know a lot of it has to
be a publicity play don't you think like
most people don't really know how to
read these papers i it's just
not most people but a lot of people and
i think they want to keep this
like mystery to what they can do at
least like the eyes of the tech
investors
arguably it's going to be a short time
until this is available
you know publicly i mean think of things
like deep fakes right
it it's it's been like maybe a year
from the point where it was you know
kind of working in academia to the point
of where i can just
you know have an app on my phone be like
deep
coming back to what you said a second
ago when we train classification models
we we have more rigor about
you know stratifying the the training
set for example
you know we would balance the client the
the cats and the dogs
because we don't want dogs to be over
representative and dominant in the model
and that level of due diligence is
impossible to do in the natural language
processing model
first of all some articles will just be
doubled or tripled or quadrupled
does that mean that they have more
impact but the other thing is if you
look at all of the different
directions that the model learns for
example
gender vectors or racial bias or it
might be learning
lots and lots of things about 1950s
england but not things about
now and once we develop these really
sophisticated machine learning models
they are they're so heterogeneous and
diverse we couldn't possibly
balance their training in any meaningful
way
yeah yeah i mean that makes sense
but do we want to if you think about it
the convex whole of our physical reality
it has a shape
and that shape is evolving
well the question is just if i have a
snapshot
of that that's that's always going to be
out of date let's let's just care about
care about the date
right now historic versus current right
it's
like any snapshot of the of the internet
is necessarily going to be out of date
because
i've kept even if i downloaded the whole
internet yesterday
it is it's out of date by at least one
day
that's given that every website was
written yesterday
so even everything's evolving but
you know that that's an interesting
trade of those so there's the time thing
now if we only trained it on the data
from yesterday
it would be super up to date and it
would know loads of gender pronouns and
so on
but it wouldn't have the scale
so there's a trade-off between i want to
have
more training data but inevitably i'm
going to have to give it
older information but there's also the
specificity and personalization aspect
so it would also be good if i
partitioned the data so it was only in
london in the united kingdom and then it
would know about
all of the colloquialisms and all the
things that we're talking about
so there are always trade-offs to be
made yes maybe that's another
actually thing for this reverse index
thing where you could say
i condition i only want the weights
where
these particular training subsets
mattered for training or something like
this it would be
i mean this is kind of what they do with
this in context learning where they say
you can sort of condition it
but what you're saying is basically i
would i want the model
that acts as if i only had trained it
on data from london
i mean that's going to be very hard but
like a cool idea
well yeah that's personalization but
then there's bias as well
yeah so so what what if i want to remove
meaningfully gender bias
well you first of all i have to come up
with like a definition
which people are still arguing
over what that even means um but i guess
presuming you have a definition i i also
think that
there i mean this is still an actively
developing field this whole
fairness bias removal and so on there
are lots of
opinions and techniques i i i'm not
qualified
too much to give something here but
going back to my
initial question like
specifically let's talk about the
dangers of fake news
like this this is brought up it was
brought up in gpt2
and it's brought up in this broader
impact statement here i believe i
actually haven't
read it fully but i think that was one
of the points
and it seems like this is a you know a
thing that people say
oh this can be used to generate fake
news
and my question is is that a problem
well let's say you have like a not even
gpt3 but like a gam that can produce
like this video
yeah that could be a huge problem right
because you could make these videos of
like these
crazy things that are happening and then
hit the social media thing and then
outrage well exactly right that's that's
one of the things
i see i feel with the fake news
it is people are people already know
that whatever is written must not be
true
i think people with video people most
people think if they see a video
it might you know be deceptively cut or
something but certainly the
the pixel information is what actually
happened but
most people are probably aware that word
information
must not represent like it's possible
that it
doesn't represent the truth like some
someone could be just
writing you know crap and
fake news right someone could be writing
a lie
and it seems to me that
the argument you'd have to make is that
the ability to now automatically do this
instead of
manually is somehow dangerous
so do you do you think i don't know i'm
i have yet to hear
an argument that this automatically
generating
of fake news is a is a dangerous thing
it's not a dangerous thing because we
have authoritative
sources so it doesn't matter if you can
generate realistic looking text
people know that you're not an
authoritative source i think the real
problem is bias
just to use another example what if we
felt as a society that we should have
diversity
in colors of car you know there should
be
green cars and yellow cars and orange
cars
and at the moment people have been
buying more orange cars and we think
it's incredibly unfair
and the kind of society we want to move
to is where there are more blue cars
so what we need to do is we need to
stratify our language models to make
sure
that blue cars are represented um
diversely well they're like they're like
10 different layers of this problem so
first of all i mean this is an unpopular
opinion but i've been arguing this
we should we should like this term of d
biasing or unbiasing and so on like what
you're describing is
biasing like let's be like it has a bad
rep a bad connotation but statistically
you want to bias the model because what
you want to do is
say okay here is a world that i would
like to have right i would here is my
worlds that i aspire to right and
i want my money to conform to that world
right i
i and that's irrespective in the case
what you're saying of with the cars
and that's absolutely has nothing to do
with what the real world looks like
like you simply say i want this
distribution of cars that's what i
aspire to that's my ideal
and therefore i will bias my model
towards that now that's a different
thing from saying
the model isn't representing the world
as it is right for example i've seen a
paper recently that
makes a credible case that
regularization
is one of the drivers or can be one of
the drivers
of bias in models where let's say
it's actually very simple right you you
have the ferraris and the lamborghinis
then the drive the ferrari drivers and
the lamborghini drivers
and you want make a model that predicts
the accident probability
now it just so happens that the ferrari
drivers are a bit more reckless
and and they do slightly higher
accidents right and now i train my
logistic regression and it tells me okay
60 40. cool but now i train my logistic
regression with an l1 penalty
and i say i want my model to be you know
explainable
i want my explainable model and so i
want it to be sparse i want
the least amount of variables to be
contributing to
what's the model going to say the model
is going to say ferrari drivers
bad lamborghini drivers good right
so i think there are there are two very
different
concepts here one is one and i think we
should
frankly separate the subfields here
because one says
these models by some by how we train
them
by what loss functions we use by
what data we input to a certain degree
are not representing
the the data as it occurs in the world
like they are not representing the the
cumulative distribution of data and the
other
the entirely different field is saying
here is a state that i want to achieve
how can i make the model achieve that so
in the one case you can
legitimately speak about d biasing or
unbiasing in the other case it's
actually
biasing but to play devil's advocate
people asian would say there are
structures of interlocking oppressions
in society
and people have been moving away from
buying blue cars because their
perception is that blue cone
don't work very well and the existing
world model tells them
not to buy blue cars so even though we
want to move our
buying blue cars that there are all of
these blockers in the way
okay there i can again think of like two
different
sub genres here where you could
you could so one is this this
compounding effect where you say
because the world is in a certain way we
train our models the models might be you
know a bit regularized so they're a bit
biased
and the models actually inform the
people that then will only go
more into this direction which will
train the models if we update them more
into that direction and so on so there's
there's that which is a problem of
course if you use model
and then train on data that comes out of
the process you're going to
just amplify and amplify in in certain
cases like
you can argue that it doesn't always
happen but
i think that's not exactly what you were
saying what you were saying is
there is like in society there for
whatever reason there is a process
that's steering people away from blue
cars
right okay let's say that the yellow car
lobby the evil yellow car lobby
um is steering people away from blue
cars
and that's not right and it shouldn't be
because blue cars are exactly as good as
yellow cars or like a more concrete
example that the grain lobby
right grain lobby breakfast is the most
important meal of the day that's a
marketing slogan
like people think it's a health slogan
but it's it's a marketing slogan to sell
cereal
and you can say well you know actually
lunch lunch is as important you
shouldn't just eat breakfast and skip
lunch you can just eat lunch and skip
breakfast and and the whole society
somehow
has this wrong notion but and here is
the
the point that's still in that second
camp that described initially
because the data of the world the state
of the world
is such that people eat breakfast
that people you know eat more breakfast
because they think breakfast is the most
important meal of the day and
what you're describing is still a world
that you would like to
get to where people don't differentiate
between breakfast and lunch
i like that another way to verbalize
this is politically you have
conservativism
versus progressivism and conservatives
want us to have a strong memory
about how things have always been and
progressivists want us to forget
everything we've
we've learned over generations and just
to
dynamically evolve very quickly well so
as a thought experiment because
there's there's a lot there's a lot to
disagree here
but just just to continue this thought
these models
the reason you could argue they are
unethical is because they
increase our memory so what they do is
if we train them on concepts from the
1950s
they will stop us from as i said in
london now all of the cool kids are
using all of these colloquialisms and so
on
and if they use that on their cvs they
won't get the job because the model is
still trained on the 1950s
so there's an argument to just get rid
of the model or to somehow
impute stuff into the model the war
against ai will be
fought with sentences like okay boomer
model
well i've well to compare it to
conservatism i would
because i guess a conservative would say
they memorize
things because they want to learn from
the things that they've memorized and
not necessarily that
they would that they would always stick
to them and maybe a progressive would
say
i still remember but i want to change
the things that are
have not worked and but you i mean you
were right in that
a model from the olden days would
actually
just i mean what it does is
it sees every data point equally and
it is going to aggregate over everything
that it you give to it
and it's going to think that the average
of what it sees is the world today
which is not true right the average of
what it sees is actually
1975. and there's this discrepancy
is that the model the model has no
notion that this data point
is out of date if you gave that to the
model
then i like your analogy in my
opinion become much more relevant
because we're getting to the age-old
thing that we do in machine learning
we start to incorporate
inductive priors or temperature
parameters
one thing we might do for example is
decide that recency is important
we do this with online ridge regression
for example we have a sliding window
we we assume that recent information is
more important than old information
because
maybe we're going into a new regime now
and the old information won't help us
predict stuff about the new information
but we are also throwing away a lot of
knowledge so
as machine learning practitioners we
need to tweak all of these
knobs and levers and we need to
establish what the best representation
of this data is
and it seems like a completely arbitrary
process absolutely like i think
in these in these types of models and i
don't think that has been considered in
language models yet but
this must exist because there has to be
like a notion of
i can extract timeless principles from
data no matter how old it is right
that's why people go back to
marcus aurelius and and plato and
aristotle because
they had they just wrote down timeless
things that
are as applicable today as they were
in in their days not everything that
they've written down is applicable
today so these models i feel
if they're trained on ever growing
amounts of data that comes from
different
time periods and whatnot they should
have a notion
of of what sort of information they must
extract and
that seems very hard like that means
you'd have to understand wait a minute i
i can probably write this down on my
smartphone nowadays and
marcus aurelius even though marcus
aurelia says i should write it down on
my clay tablet
but i should write it down nonetheless
like that's the timeless part
without grounding that's going to be
super hard
yeah emanuel kant for example was racist
he had some incredible ideas on
philosophy he introduced
consequentialism for example
but he was a racist and now we look back
and we say well um
we well it doesn't discredit his other
ideas
but that's that's something that he
thought which we now no longer think is
is is right and maybe that might happen
to
some of these older philosophers as well
maybe we'll decide that
this particular viewpoint is no longer
something which is relevant
yeah but how do you make how do you make
that distinction between
this is i mean even that's an even
more complicated thing because that is
somehow
on the level of abstraction that is
actually an abstract
idea racism right that we changed our
mind on right we've we've somehow
we've somehow as a society gotten
past that and and recognized more of the
individual value of every human being
and so on and
you can argue that has also gone with
science because a lot of these things
were based on like junk science
but also it's like society
evolves and and the consciousness of
society changes and
i mean how are you going to teach that
to a model like it's
it's one thing that it says oh you know
clay tablets aren't a thing anymore
because
there are no more clay tablets but
now it must also recognize wait society
changed its mind about this particular
abstract issue
it's it's going to be a long time before
we have
really solid methods of dealing with
that thing
and i don't i don't know what to do
about this
i mean as you say there is this immense
trade-off
if you want more data you're probably
going to have to go back in history
and you're just going to increase that
history that recency or on recency bias
but in a way there is a recency bias
because we are producing exponentially
more data
all the time yes even now the
when the philosophers were around in the
medieval times not much information
has gone on record from that time so if
anything we need to over sample from
that period
well but how how much of the data we
produce is just
copies of the philosopher's thing but
what you could hope is that their
copies adjusted to the modern times
right that would be the perfect actually
the perfect training data would be
people
you know taking taking marcus aurelius's
work
but translating it to the modern time
says what marcus aurelius says
is that you should write down uh your
dreams
and you could do you you might do so on
your smartphone
i've no i don't think marcus aurelius
was into dream
journaling yeah that's if like that
would be the perfect training data
because
you'd have this exponential production
of data but you keep
this timeless knowledge around but that
requires humans
right that requires i
i'm not sure how the machine itself
could do that
it shouldn't matter that we've got so
much oversampling and duplication of
data now because we're just learning a
language manifold
but i want to be able to do with
language like what we
can do with gans and faces you know now
you can
i want it to have more hair and i want
it to have a beard and glasses and so on
and perhaps we can do the same thing
with language we can have slide bars to
say well i want it to be a bit more
medieval
i want it to be a bit more united
kingdom
and we can start to get control over
this manifold in dimensions that we care
about
yeah i think the best way to do that is
to fine-tune the model though
right like you just start fine-tuning i
want you to write positive articles
right and then score it with reinforced
learning or auxiliary classifiers
yeah i mean this in gans this only works
because
these are on this face data set right
and the faces
they can only have like 10 to 20
attributes if you're if you're
if you're serious right that that are
really significantly different they're
like all super
pretty aligned and and you can have you
know your hair
color and your whether you have glasses
or not but
ultimately if i gave you a language
model and say
here are 100 relevant latent directions
but i'm not going to tell you what they
are
it's just those are the directions of
the 100
principal components basically of
maximal variants that are also
disentangled
you'd have no clue what you would be
doing you're like
it's like me using like a video editing
software like um
[Laughter]
yes that was a bit of a bombshell then i
mean it's incredible
i think there are the the big parts of
this development are yet
in front of us and also like big
challenges like
uh like this big corpora we have no clue
we don't even have a clue what's in them
and how do we deal with the fact that
you know information is old and
things like this it's crazy i don't know
okay well
i think that is a suitable place to
round off the episode
yeah thanks such folks next week we're
going to be back with francoise chalet
the fact the fabulous frenchman don't
forget uh we are on all
major audio podcast platforms now as
audio like as sound
because we sound so beautiful
we do made it to all the platforms so if
you enjoy that more
yeah check us out there fantastic see
you next week folks
yeah cool peace out peace
