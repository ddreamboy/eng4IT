hello everyone and welcome back to my
another video well this video right now
will be a little different from previous
ones well uh recently I was
investigating and checking the war to
vac 2 framework for solves for wise
learning of speech representation well
that's kind of paper from 2020 and you
if you are related or doing something in
speech recognition test probably you
already heard it well this is a second
version of this kind of paper there was
of course the first version War to vac 2
simply and this is release paper from uh
Facebook and basically uh what is what
this is is that uh kind of paper that
tells us how they released a model that
is right now is one of the best in
speech recognition models overall well
it's three years already for for this
Mundo but still it's one of the best
models we can use so basically uh in
this tutorial I'll demonstrate you how
easy we can fine tune this model with
our own data for speech recognition
tasks so I'm not going to cover the
theory and the modal specifications
itself I'm mainly gonna give you a code
that I use to fine tune our model on my
own data with myself my own tokenizer
and Etc the this will be pretty simple
and we are not going deep into the
theory or the coding stuff I'll simply
gonna cover it in short so basically uh
that's pretty nice model because uh what
we need to do is simply we read about
your data we receive this kind of raw
audio and we simply feed this data
straight into this model and it has a
conclusion neural networks inside
Transformer encoder layer and there it
gives us a output well basically this
model is focused on CTC loss and that's
what I'm gonna use in this kind of
tutorial and demonstrate how to use it
as simple as it is and we don't need to
throw in this form model from scratch we
can use the pertrained model and we
fine-tune on this model so basically
this paper demonstrates uh what they did
and they explained why they receive such
great results and Etc and we're gonna
use it basically we we're gonna find
tunics of course there is some results
of data that are labeled labeled they do
the some comparison on it but I don't
think that you are so interested on this
so basically this model is trained and
published on hugging face and we can use
it from this and this is the base and
you can see that's the Facebook and it
gives us some explanations and of course
in in hugging phase you can find some
explanations how they train this model
and Etc so I'll give you a video
tutorial how to train this model and how
to run the inference when you want to
deploy it and if you don't want to use
iport or pytorch installation itself
there so what we got to do is simply
jump into the code on of my ml2 package
that I was working recently and so on so
you might see that I have what worked
two torch tutorial here and you might
heard it right right now I'm not going
to use tensorflow and I will use a pie
torch and at the end of this tutorial I
will explain you why and that's going to
be pretty simple so what are the
requirements from this tutorial is that
you need torch at least this version
because I tested on it and I'm not sure
if it will work on higher versions and
you need Transformers so you could
download this pre-trained model you need
this mltu package that I recently
released but I'll update the version
that you're gonna use and of course on
the next and there might be on an extra
on time
okay let's import this that's right and
let's go straight to the training code
uh here and I'll give you explanation
what I do here so basically uh I import
many stuff most of the stuff is from my
mltu package as you may see there is a
model CTC law that I use data provider
metrics to track character error rate
and board error rate and of course some
callbacks are really stopping bottom of
cosine model 2 on the next answer board
so we could track the metrics how we
train them Etc and model checkpoint and
of course there are some augmentations
related to audio So Random audio noise
random Audio Pitch and Time Stretch I
implemented these few augmentation to
improve uh our data to scale it so our
model would uh fit it better on it so
basically what we do here I still have
as you can see above to vac 2 for CTC
see that basically will be from
Transformers library that we should have
installed while using this and there's a
few more functions from pytorch so but
you don't need to focus on it right now
so what I'm doing here as you might see
I download a simple kind of LG speech
data set uh that's pretty huge data set
that has 30 000 uh samples of speech
that are labeled and what we need to do
we simply need to download them and I
use this kind of function in to download
it so it will be placed in dataset ldot
speech metadata and so on so if I open
my data sets you can see that I have
this data already downloaded and
it's pretty simple so let's move on
so next I have some here a vocabulary
that this is the usual stuff that have
this kind of characters inside this uh
data set so what I need to do I need to
pre-process this data set so I read this
metadata path method there's a file that
uses separator of the following and I
simply read the strings of
transcriptions in and what I do here so
uh I joined this kind of labels and I
use a lower as you can see this this
means that I don't want to have a
capitalized characters I lower all of
them that's for Simplicity reason
because of course when we are trying to
recognize speech we don't care if it's a
capital or not so simply it's way easier
and to train the model when all
characters are lower and this simplifies
our vocabulary that we need to use
so great so then I use a data provider
that's very simple to tensorflow data
provider and uh actually I inherited
everything from the tensorflow data
provider but you might see that I it has
some additional parameters as workers
use multi-processing or not and Etc so
this time when we are working with a
sound data it's really hard on the CPU
side because it needs to load all this
audio from our disk and if we need to
augment it it's even harder so usually
if you have a strong CPU so I recommend
to use it with multi-processing but if
it if it's there's something wrong it it
can't you be used on multiple on CPU it
will be used as threadpool executor and
next and Etc it simply iterates all our
data
so what we do here so we skip the
validation I have the configurations
here that you can dig in and check what
it has and how many apple I want to
train what will be my batch size would
be learning rates and Etc uh what will
be warm up epochs uh whether I want to
use mixed Precision or not and Etc
there's many different things and this
config is saved along the model
trainable mode so let's move on and as
you might see I I'm gonna read use this
data preprocessors this is audio reader
this means that it will read out you
have a sample rate of 16 000 and it will
create a specific audio object of my
mltu object then we need to use label
indexer that will label our characters
into the integer representation and then
we need to use this batch post
processors so when we are training our
model we need to have
audios for example it Audios in one
batch so when we want to train our model
all these audio should be the same
length this means that we use audio
padding and I recommend use to use this
or on batch and
then it will be padded to maximum size
of possible length in the audios and the
same we apply for labels and we don't
need to pad for example if we have
sentences with uh 20 characters we don't
need to add it to 100 characters
it will be a little harder to train our
model so for efficiency it's better to
pad only to the maximum length that
exist in our batch so this is the
purpose of this and this is as I
mentioned is whether we want to use
multiprocessing or not
and if we are training our model on some
weaker CPU it might not handle this very
much for example on my computer
I can't use multi-processing on windows
at least but if I try to run this on
Linux it works so it depends on what
operating system you have what you have
and Etc and this is kind of very good
worker that when we work with audio
because it will work in a background
load everything for a trainable model
for us in the background that's pretty
amazing how it works and there of course
we split our data into validation and
training and we're gonna save this along
the model what our training and volition
data sets so this mean this is simply
for validating our trainable model and
if we want we can use augmentation and I
am not using it because it eats a lot of
CPU Papa in this it's pretty hard to
train it takes way more time to train
augmented audios so the crucial part
right now is here we need to create this
uh about to vac model from a pytorch
Transformers so basically we've used
both to Vector for CTC and we use from
pre-trainer and I as you see I see I I'm
using per trained name and this is the
specific name from hugging faces and
we're simply gonna download this model
and load it on and what I'm doing here
differently is that hidden States I'm
changing my book but the vocabulary size
and I ignore mismatches this means that
I'm changing the head of this model and
simply put my own head that will be
trainable for classification and when we
feed the forward pass here we input our
audio data audio batches and Etc and we
simply
receive the outputs and we use the
logins from these outputs into our into
the logsoft max function and whole
output will go to the CTC loss of course
you can implement this differently but I
for me I found this is pretty good
implementation so I follow with it so
here we create this custom model of
pytorch and we have it here and I put
this on GPU because I'm totally not
recommend to train try to train it on
CPU because you're gonna need to wait a
month to train it I believe and of
course if you want to train it on pretty
large data set you need kind of a very
strong GPU or use multiple gpus and of
course pretty strong CPU remember that
train or out Transformers on how the
it's it takes a lot of time so it's up
to you so now next we defined a bar map
cosine design and I'll explain what it
does but it's pretty necessary to to use
with Transformers so it means that we
start from really low learning rate and
every Epoch we increase this learning
rate to faster and better train our
motor so then I use the tensorboard
that's pretty self-explainable we use
early stopping we track the validation
character erroring and we of course we
could track the word error read but I
chose to use Charter error rate it's up
to you you can you what you want to do
so next I use the model checkpoint this
means that I'll save my models or as
Mundo PT this is the weights and I will
save the best weights according to
validation character error rate that's
what I'm gonna use
and of course at the end uh I wanna say
my best Moodle into on the next format
so I don't need to save the pytorch mode
because it's pretty hard when you I'm
using a pie torch to load back the
trainer model on another platform for
example if I want to use this on
Raspberry Pi it's pretty impossible to
load this bag without knowing the
architecture or without installing the
pie torch so I chose to convert the
model to uninx and then I it's very
simply for me to run this on any other
device I I want so that's it and here
you might see that I have a custom model
object that it handles all the training
stuff the callbacks the metrics and Etc
so here what I do here I feed to this
model are a model as you can see custom
model that's my buff to back model I use
CTC lows and
uh that's kind of custom lost and we
here use my our vocabulary for for this
kind of data set and then I use Adam B
Optimizer you can use Simple Adam uh it
work both for well well but it was shown
that Adam W Works slightly better so why
not using it and then I want to track my
character error rate and what error rate
that's kind of usual when we are working
the towards uh audio or any recognition
related to speech words characters
doesn't matter then I use a mixed
Precision we can find it using configs
and usually if you if you have good GPU
it will dramatically increase the
training for you you two times least and
it will take less memory on GPU so we
can use larger batches but it's up to
you I I chose to use this mixer
precision as true and then I I saved is
are train and validation CSV files from
my split so later I could
test my how my module works that I would
train and convert on the next so I want
to use this on the same validation data
because of course on train data it will
work well and that's it we call the fit
and we waited to finish well I didn't
want to train it right now because if I
start training my CPU jumps to 100 and I
might face a blue screen or simply uh
the video recording might start lagging
insanely but to prove that it works I
can start it immediately and kill it
when it starts streaming so let's run
and let's see how it keeps loading
something here okay and as you can see
it takes one half
1474 batches to train and that's great
and as you can see it doesn't handle
multiple processing so it's switching to
multi threading that's as I expected
because I know that it doesn't work on
my machine like that and within the
first batch it will be pretty good
okay
and as you can see it it keeps training
and it showed us chart the error rate
word error rate and we simply need to
wait until it completes this stuff but
right now because I'm recording it's way
slower and I believe it might be lagging
on my side so I don't want this to
continue training while I'm recording so
great I'll kill it because I already
trained this model and I already tested
everything out
that skill kill
this stuff okay I'll simply kill this
one so great and I believe you're
interested how it trained how you can
prove it okay uh I have a pre-trained
model and if I go to my models I have
moved to work too and here is my train
model that already has a PT weights and
model on next let's train motor but
first before going to the test part
let's look at the tensorboard how it
trained and I have attention board for
exactly that what I showed you and let's
maximize this one and you might see that
it started training and there was 10
epoths of warm up and it wasn't
improving at all so that's the worst
part with this model because we when we
it's training for such epos and it's not
decreasing we are not sure whether it's
training or not but look at this uh
after 12 epos it dramatically dropped
somewhere here and you might see that
train is you checked Android of training
is 0.21 and validation is only 0.01 so
that's a huge difference and if we
scroll to the end you might see that uh
it dramatically decreased and let's look
at the word array because that's really
towards and you might see that at the
end our training uh was only two percent
error and validation was 1.8 error so
this means that it works on our eight
audio data and our loss looks exactly
the same so now let's look at our
learning right so that's exactly what I
was talking what is this uh uh warm up
so this means that you might see that it
starts increasing the learning rate to
some learning rate we Define so this is
one point one e minus five is my
learning rate that it should have
achieved during the warm up and it
achieved this and it will train it to
continue training and it was decreasing
our word our rate was decreasing
everything was just great
and its training actually lost looks
pretty the same and we don't need to
look at it it was simply also with high
high and then dropped dramatically so
great it works so right now you might
ask what prove it that it works okay
great why not so if I go to my
moodles here I have a test dot Pi script
that I created and here is my own X
inference model and CTC decoder and
character and what are calculation
scripts so what I do here so simply here
I have my model trainer you can see four
three four uh and I load this model on
the next with my following object and
what we do here how we do this
prediction
so uh here is our audio path and the
true label so maybe we are interested to
to see what is the label great not
problem let's print it out here print
labor and it will print in terminal for
us and right now I read this volition
CSP file that it was saved along the
model here and we're gonna iterate
through it to see whether it works or
not and we'll see what are the true
labels of it and we are not interested
in the prediction because uh this error
rate is pretty low so why not we might
know not to notice the difference
accurately without having time into it
so great and if I I run this for all
this data set and we will see what is
the charter and what error rate for my
labels through labels and Etc so let's
run this simply and you'll see that it
it's pretty nice
and it works so simply what we do we use
our live librosa to load this up and we
load this audio we have this audio raw
data that we put into this kind of
prediction model and it expands them or
runs the inference and use a CTC decoder
to decode the text and that's it and you
might see here is the output of my uh
mood off and you might see that there is
Charter error rate word error rate what
are the results and Etc and let's stop
this right now I'm not interested
actually and let's move on
uh and we can see what are the true
labels uh how I see it's not the perfect
I see it cropped me uh capitalized
letters because all all sentences start
from uh from a capital so that's my
problem I need to change this train
script to prove it that it works and
basically you might see that I that's my
mistake I teached more model to predict
uh Speech without capitalized letters so
uh yeah that's my mistake I need to look
at it closer so anyway I'll fix this
training code for you
uh I don't know maybe I'll train a model
model but I'm not recording another
tutorial right now because I'm not
interested to do so
and of course
um I'll publish all the model VR into my
GitHub repository and Etc you can find
the link in the video description below
if you want and there everything is but
here is the idea how simple it is to
fine tune this buff to Vector model
and you can use the audio whatever you
want so basically
it works as you can see and if you can
try to record your audio to and use the
prediction but it will be pretty hard
and it will be not that accurate because
uh this kind of data set that I used to
train is very specific with a accent of
people that were reading books and it
might be not that great for you but
still it works and you can try to use
your own data to fine-tune this model
and you will see that it works pretty
nicely so but I don't want to invest a
lot of time into explaining this stuff
here so basically it works and if you
like this video please don't say
hesitate to smash the like button
subscribe and if you have any questions
drop the comment below and you will see
how to solve this so basically let's go
back to one stuff if you remember I
asked I'm not I do not recognize to
train this on tensorflow
and there is a huge uh problem with this
because we also we can load the
Transformers the same model
uh that we use in pytorch but there's a
minor difference that training takes
around five times longer I don't know
why
it used the same uh CPU same GPU uh as
as that by torch but it trains way
slower and I can't explain this
so I don't know why we would like to
train this model on tensorflow if we can
faster train it on pytotch and we if we
anyway are going to deploy this into
some on an X format and use it so uh
it's up to you you can try to train this
on tensorflow but I do not recommend to
do so I recommend using only pytorch so
it's up to you so that's it about this
introduction with war to vac and this
code you can find on my GitHub Link in
the description and try to train your
own monal or throw you can try to run my
own model that well I'll open upload
link also into our description so that's
it about this and
and that's it we'll see you in our next
video tutorial thank you again for
watching and we'll see you next time bye
