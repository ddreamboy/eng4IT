python is the most popular programming
language in the field of data science
python provides a range of libraries
that help you perform data analysis data
visualization model building faster and
with more efficiency in this full course
video you will learn the top python
libraries for data science you will
first get an overview of the top 5
python libraries for data science then
we will deep dive into learning the
numpy library in detail the numpy
library is mainly used for numerical
computation
next you will learn about the most
popular data manipulation library that
is spanned as moving further you will
learn data visualization using the
matplotlib library then you will look at
how to scrape data from the web using
beautiful soup library and finally you
will learn about the tensorflow library
provided by google so let's get started
python is the most widely used
programming language today when it comes
to solving data science tasks and
challenges python never ceases to
surprise its audience most data
scientists out there are already
leveraging the power of python every day
hi i'm abeksha from simply learn and
well after some thought and a bit more
research i was finally able to narrow
down my choice of top python libraries
for data science what are they let's
find out so let's talk about this
amazing library tensorflow which is also
one of my favorites so tensorflow is a
library for high performance numerical
computations with around 35 000 github
comments and a vibrant community of
around 1500 contributors and it's used
across various scientific domains it's
basically a framework where we can
define and run computations which
involves tensors and tensors we can say
are partially defined computational
objects again where they will eventually
produce a value that was about
tensorflow let's talk about the features
of tensorflow so tensorflow is majorly
used in deep learning models and neural
networks where we have other libraries
like torch and theano also but
tensorflow has hands down better
computational graphical visualizations
when compared to them also tensorflow
reduces the error largely by 50 to 60
percent in neural machine translations
it's highly parallel in a way where can
train multiple neural networks and
multiple gpus for highly efficient and
scalable models this parallel computing
feature of tensorflow is also called
pipelining also tensorflow has the
advantage of seamless performance as
it's backed by google it has quicker
updates frequent new releases with the
latest of features now let's look at
some applications tensorflow is
extensively used in speech and image
recognition text based applications time
series analysis and forecasting and
various other applications involving
video detection so favorite thing about
tensorflow that it's already popular
among the machine learning community and
most are open to trying it and some of
us are already using it now let's look
at an example of a tensorflow model in
this example we will not dive deep into
the explanation of the model as it is
beyond the scope of this video so here
we're using amnest dataset which
consists of images of handwritten digits
handwritten digits can be easily
recognized by building a simple
tensorflow model let's see how when we
visualize our data using matplotlib
library the inputs will look something
like this then we create our tensorflow
model to create a basic tensorflow model
we need to initialize the variables and
start a session then after training the
model we can validate the data and then
predict the accuracy this model has
predicted 92 percent accuracy let's see
which is pretty well for this model so
that's all for tensorflow if you need to
understand this tutorial in detail then
you can go ahead and watch our deep
learning tutorial from simply learn as
shown in the right corner interesting
right let's move on to the next library
now let's talk about a common yet a very
powerful python library called numpy
number is a fundamental package for
numerical computation in python it
stands for numerical python as the name
suggests it has around 18 000 comments
on github with an active community of
700 contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numpy addresses the slowness problem
partly by providing these
multi-dimensional arrays that we talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now let's talk about
features of number it's very easy to
work with large arrays and mattresses
using numpy numpy fully supports object
oriented approach for example coming
back to nd array once again it's a class
possessing numerous methods and
attributes ndra provides for larger and
repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods i
always wanted to get rid of loops and
vectorization of numpy clearly helps me
with that now let's talk about the
applications of numpy numpy along with
pandas is extensively used in data
analysis which forms the basis of data
science it helps in creating the
powerful n-dimensional array whenever we
talk about numpy the mention of the
array we cannot do it without the
mention of the powerful n-dimensional
array also number is extensively used in
machine learning when we are creating
machine learning models as in where it
forms the base of other libraries like
sci-fi scikit-learn etc when you start
creating the machine learning models in
data science you will realize that all
the models will have their bases numpy
or pandas also when number is used with
scipy and matplotlib it can be used as a
replacement of matlab now let's look at
a simple example of an array in numpy as
you can see here there are multiple
array manipulation routines like there
are basic examples where you can copy
the values from one array to another we
can give a new shape to an array from
maybe one dimensional to we can make it
as a two dimensional array we can return
a copy of the array collapsed into one
dimension now let's look at an example
where this is a jubilee notebook and we
will just create a basic array and uh
for detailed explanation you can watch
our other videos which targets on these
explanations of each libraries so first
of all whenever we are using any library
in python we have to import it so now
this np is the areas which we will be
using let's create a simple array
let's look what is the type of this
array
so this is an end array type of array
also let's look what's the shape of this
array
so this is a shape of the array now here
we saw that we can expand the shape of
the array
so this is where you can change the
shape of the array using all those
functions now let's create an array
using arrange functions if i give
arrange 12 it will give me a 1d array of
12 numbers like this now we can reshape
this array
to 3 comma 4 or we can write it here
itself
so this is how our range function and
the reshape function works for numpy now
let's discuss the next library which is
scipy so this is another free and open
source python library extensively used
in data science for high level
computations so this library as the name
suggests stands for scientific python
and it has around 19 000 comments on
github with an active community of 600
contributors it is extensively used for
scientific and technical computations
also as it extends numpy it provides
many user-friendly and efficient
routines for scientific calculations now
let's discuss about some features of
scipy so scipy has this collection of
algorithms and functions which is built
on the numpy extension of python
secondly it has various high level
commands for data manipulation and
visualization also the ndmh function of
scipy is very useful in
multi-dimensional image processing and
it includes built-in functions for
solving differential equations linear
algebra and many more so that was about
the features of sci-fi now let's discuss
its applications so cyber is used in
multi-dimensional image operations it
has functions to read images from disk
into number arrays to write arrays to
discuss images resize images etc solving
differential equations fourier
transforms then optimization algorithms
linear algebra etc let's look at a
simple example to learn what kind of
functions are there inside by here i'm
importing the constants package of scipy
library so in this package it has all
the constants
so here i'm just mentioning c or h or
any and this library already knows what
it has to fetch like speed of light
planck's constant etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science life
cycle yes pandas is the most popular and
widely used python library for data
science along with numpy and matplotlib
the name itself stands for python data
analysis with around 17 000 comments on
github and an active community of 1200
contributors it is heavily used for data
analysis and cleaning as it provides
fast flexible data structures like data
frames cvs which are designed to work
with structured data very easily and
intuitively now let's talk about some
features of pandas so panas offers this
eloquent syntax and rich functionalities
like there are various methods in pandas
like drop n a fill n a which gives you
the freedom to deal with missing data
also panas provides a powerful apply
function which lets you create your own
function and run it across a series of
data now forget about writing those for
loops while using pandas also this
library's high level abstraction over
low level numpy which is written in pure
c then it also contains these high level
data structures and manipulation tools
which makes it very easy to work with
pandas like their data structures and
series now let's discuss the
applications of pandas so panas is
extensively used in general data
wrangling and data cleaning then pandas
also finds its usage in edl jobs for
data transformation and data storage as
it has excellent support for loading csv
files into its data frame format then
pandas is used in a variety of academic
and commercial domains including
statistics finance neuroscience
economics web analytics etc then pandas
is also very useful in time series
specific functionality like date range
generation moving window linear
regression date shifting etc now let's
look at a very simple example of how to
create a data frame so data frame is a
very useful data structure in pandas and
it has very powerful functionalities so
here i'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
i'm using jupiter notebook again and in
this before using pandas here i'm
importing the pandas library
let me go and run this so in data frame
we can import a file a csv file excel
files there are many functions doing
these things and we can also create our
own data and put it into data frame so
here i am taking random data and putting
in a data frame also i'm creating an
index and then also giving the column
names so pd is the alias we've given
pandas random data of 6x4 index which is
taking a range six numbers and column
name i'm giving as abcd now let's go
ahead and look at it
so here it has created a data frame with
my column name sub abcd my list as six
numbers zero to five and a random data
of six by four so data frame is just
another table with rows and columns
where you can do various functions over
it also i can go ahead and describe this
data frame to see so it's giving me all
these functionalities where count and
mean and standard deviation etc okay so
that was about pandas now let's talk
about next library and the last one so
matplotlib for me is the most fun
library out of all of them why because
it has such powerful yet beautiful
visualizations we'll see in the coming
slides plot and matplotlib suggests that
it's a plotting library for python it
has around 26 000 comments on github and
a very vibrant community of 700
contributors and because of such graphs
and plots that it produces it's majorly
used for data visualization and also
because it provides an object-oriented
api which can be used to embed those
plots into our applications let's talk
about the features of matplotlib the pi
plot module of matplotlib provides
matlab-like interface so matplotlib is
designed to be as usable as matlab with
an advantage of being free and open
source also it supports dozens of
back-ends and output types which means
you can use it regardless of which
operating system you're using or which
output format you wish pandas itself can
be used as droppers around matplotlib's
api so as to drive matplotlib via
cleaner and more modern apis also when
you start using this library you will
realize that it has a very little memory
consumption and a very good runtime
behavior now let's talk about the
applications of matplotlib it's
important to discover the unknown
relationship between the variables in
your data set so this library helps to
visualize the correlation analysis of
variables also in machine learning we
can visualize 95 percent confidence
interval of the model just to
communicate how well our model fits the
data then matlab finds its application
and outlier detection using scatter plot
etc and to visualize the distribution of
data to gain instant insights now let's
make a very simple plot to get a basic
idea i've already imported the libraries
here so this function matplotlib inline
will help you show the plots in the
jupiter notebook this is also called a
magic function i won't be able to
display my plots in the jupiter notebook
if i don't use this function i am using
this function in
numpy to fix random state for
reproducibility now i take my n as 30
and will assign random values to my
variables so this function is generating
30 random numbers here i'm trying to
create a scatter plot so i want to
decide the area
let's
put this so it's just multiplying 30
with random numbers to the power 2 so
that we get the area of the plot which
we will see in just a minute so using
the scatter function and the alias of
matplotlib as plt i've created this if i
don't use this and i have very small
circles as my scatter plot it's colorful
it's nice so that's one very easy plot i
suggest that you explore more of
matplotlib and i'm sure you will enjoy
it let's create a histogram so i'm using
my the style as gg plot and assigning
some values to these variables any
random values
now we are assigning bars and colors and
alignment to the plot and here we get
the graph so we can create different
type of visualizations and plots and
then work upon them using matplotlib and
it's just that simple so that was about
the leading python libraries in the
field of data science but along with
these libraries data scientists are also
leveraging the power of some other
useful libraries for example like
tensorflow keras is another popular
library which is extensively used for
deep learning and neural network modules
keras drafts both tensorflow and theano
backends so it is a good option if you
don't want to dive into details of
tensorflow then scikit learn is a
machine learning library it provides
almost all the machine learning
algorithms that you need and it is
designed to interpolate with numpy and
sci-by then we have cbon which is
another library for data visualization
we can say that seaborne is an
enhancement of matplotlib as it
introduces additional plot types welcome
to numpy my name is richard kirschner
with the simply learn team that's
www.simplylearn.com get certified get
ahead
what's in it for you well today we're
going to do part one of numpy in a
two-part series and we're going to go
over what is numpy installing and
importing numpy numpy array numpy array
versus python list basics of numpy
finding size and shape of any array
range and arrange functions numpy string
functions and then in part two i'll move
on to cover axes array manipulation and
much more
so let's start with what is numpy numpy
is the core library for scientific and
numerical computing in python it
provides high performance
multi-dimensional array object and tools
for working with arrays and i'll go a
step further and say there are so many
other modules in python built on numpy
so the fundamentals of numpy are so
important to latch onto for the python
so you can understand the other modules
and what they're doing numbies main
object is a multi-dimensional array it's
a table of elements usually numbers all
of the same type indexed by a tuple of
position integers in numpy dimensions
are called axes
take a one dimensional array or we have
remember dimensions are also called axes
you can say this is the first axis
0 1 2 3 4 five and you can see down here
it has a shape of six why because
there's six different elements in it in
the one dimension array and they usually
denote that as six comma with an empty
node on there and then we have a two
dimensional array where you can see zero
one two three four five six seven and in
here we have two axes or two dimensions
and the shape is two four so if you were
looking at this as a matrix or in other
mathematical functions you can see
there's all kinds of importance on shape
we're not going to cover shape today but
we will cover that in part two did you
know that numpy's array class is called
nd array for numpy data array now we're
going to take a detour here because
we're working in python and two of my
favorite tools in python is the jupiter
notebook and then i like to use that
sitting on top of anaconda and if you
flip over to jupiter.org that's
j-u-p-y-t-e-r dot org you can go in here
you can install it off of here if you
don't want to use the anaconda notebook
but this is the jupiter setup the
documentation on the jupiter jupiter
opens up in your web browser that's what
makes it so nice is it's portable the
files are saved on your computer they do
run in ipython or iron python and you
can create all kinds of different
environments in there which i'll show
you in just a minute i myself like to
use anaconda that's www.anaconda.com
if you install anaconda it will install
the jupiter notebook with the anaconda
separate and you can install jupiter
notebook and it'll run completely
separate from anaconda's jupiter
notebook and you can see here i've now
opened up my anaconda navigator what i
like about the navigator and this is a
fresh install on a new computer which is
always nice i can launch my jupyter
notebook from in here i can bring other
tools so the anaconda does a lot more
and under environments i only have the
one environment and i can open up the
terminal specific to this environment
this one happens to have python37 in it
the most current version as of this
tutorial and the open terminal if you're
going to do your pip installs and stuff
like that for different modules you can
also create different environments in
here so maybe you need a python 36
python python35 you can see we're having
a nice framework like anaconda really
helps so you don't have to track that on
your own in the jupiter notebook in your
different jupiter notebook setups we'll
go ahead and launch this jupiter
notebook and then i've set my browser
window for a default of chrome so it's
going to open up in chrome and you can
see here this opens up a folder on my
computer we have a couple different
options on here remember i set the
environment up as python 3.7 you would
install any additional modules that
aren't already installed in your python
on this and it keeps them separate so
you do have to for each environment
install the separate modules so they
match the environment on there and in
here we have a couple things we can look
up what's running
you have your different clusters again
this is i just installed this on a new
machine so i just have the one a couple
things in here that were run on here
recently and what we go on here is we
then have on the upper right new and
from the pull down menu you'll see
python 3. and this will open up a new
window
and now we're in jupiter python so this
is a python window and we'll just do a
print
and this of course is hello world
and we'll run that and it prints out
hello world in the command line there's
a couple special things you have to know
we're not going to do today which is on
graphics if you've never seen this
one of the things you can do you can
also do a equals hello world and if you
just put the a in there now if you do a
bunch of these we have a equals hello
world b equals goodbye world and you put
a b a and return b you'll only run the
last one but you can see here if you put
the variable down here it will show you
what's in that variable
and that has to do with the jupiter
notebook inline coding so that's not
basic python that's just jupiter
notebook shorthand which you'll see in a
little bit
so back to our numpy numpy array versus
python list python list being the basic
list in your python why should we use
numpy array when we have python list
well first it's fast the numpy array has
been optimized over years and years by
multiple programmers and it's usually
very quick compared to the basic python
list setup it's convenient so it has a
lot of functionality in there that's not
in the basic python list and it also
uses less memory so it's optimized both
for speed and memory use
and let's go ahead and jump into our
jupyter notebook since we're coding best
way to learn coding is to code just like
the best way to learn how to write is
right and the best way to learn how to
cook is cook so let's do some coding
here today and just like any modules we
have to import numpy we almost always
import it as np that is such a standard
so you'll see that very commonly we can
just run that and now we have access to
our numpy module inside our python and
then the most common thing of course is
to go and create a number array
and in here we can send it a regular
list
and so we'll go ahead and send this a
regular array let's do one two three to
make it simple and then i'm just going
to type in a and we'll run this
as you can see down here the output is
an array of one two three
and we could also do
print
just a reminder that this is an inline
command so that wouldn't work if you're
using a different editor you can see
that it's an array one two three but
we'll go and leave it as a
kind of a nice feature so you can see
what you're doing really quick in the
jupyter notebook
and just like all your other standard
arrays i can go
a of 0
which is going to be a value of
of course we do a of 1. you go all the
way through this
i have 1 has a value of 2 in it
so whether using the numpy array or the
basic python list that's going to be the
same that should all look pretty
familiar and be pretty straightforward
remember the first value is always zero
and when we set on there so let's take a
look why we're using numpy because we
went over the slide a little bit but
let's just take a look and see what that
actually looks like and what we want to
look at is the fact that it's fast
convenient and uses less memory so let's
take a glance at that in code and see
what that actually looks like when we're
writing it in python and what the
differences are
and to do this i'm going to go ahead and
import a couple other modules we're
going to import the time module so we
can time it and we're going to import
the system module so that we can take a
look at how much memory it uses and
we'll go and just run those so those are
imported
so we'll do b equals oh range of 1 yeah
one thousand is fine
and so that's going to create a list of
one thousand zero to nine hundred ninety
nine remember it starts at zero and it
stops right at the one thousand without
actually going to the one thousand
and let's go ahead and print and we want
system dot get size of
and we'll pick any integer because we
have you know zero to
a thousand we'll just throw one in there
five it doesn't matter because whatever
integer we put in there is going to
generate the same value because we're
looking at the size of how how much
memory it stores an integer in
and then we want to have the link of the
b that's how many integers are in there
and if we go ahead and execute this and
run this in a line we'll see
oops i did that wrong comma
if we multiply them together
we'll see it generates 28 000 so that's
the size we're looking at is 28 000 i
believe that's bytes that sounds about
right
so let's go ahead and create this in
numpy
and we'll go with c equals np
and this is a range
so that's the number command do the same
thing that we were just doing in a list
and we'll also use the same value on
there the 1000
and then once we've created the
c
value of c for np dot a range
let's go ahead and print
and we can do that by doing c dot size
times c dot
item size
when it's very similar we did before we
did get the size of so the c size is the
size of the array and each item size
just reversed
so it's the size of an integer five item
size it's going to be the integers and c
size and let's just take a look and see
what that generates
and wow okay we got 4 000 versus 28 000.
that's a significant difference in
memory how much memory we're using with
the array and then let's go ahead and
take a look at speed let's do um oh
let's do size we tried this with lower
values and it would happen so fast that
the npra kept coming up with
zero because it just rounded it off
so size and let's create an l1
moles
range of size
and we'll do an l2
we'll just set up to the same thing it's
also range
of size on there there we go
and then we can do on a1
equals
np dot
a range size
and then let's do an a
2 equals np dot
a range
we'll keep it the same size
and what we're going to do is we're
going to take these two different arrays
and we're going to perform some basic
functions on them
but let's go ahead and just load these
up now we'll go ahead and run this so
those are all set in memory
except for the typo here
quickly fix that
there we go so these are now all loaded
in here and let's do a start
equals
time dot time
so it's just going to look at my clock
time and see what time it is
and then we'll do result equals and
let's do
let's say we got
an array and we're going to say
let's do some addition here x
plus y
for x comma y
and
we'll zip it up here
two different arrays so here's our two
different arrays we're gonna multiply
each of the individual things on here l1
l2
there we go so that should add up each
value so l1 plus l2 each value in each
array
then we want to go ahead and print
and let's say
python list took
and then we'll do
time
dot
time we'll just subtract the start out
of there so time whoops i messed up on
some of the quotation marks on there
okay there we go
time
minus the start
and we'll convert that to second so
we'll go because in milliseconds or
times one thousand
and let's hit the run on there it's kind
of fun because you also get a view while
we're doing this
of some ways to manipulate the script
and as you can see also my bed typing
there we go okay so we'll go ahead and
run this
and we can see here that the python list
took 34
actually i have to go back and look at
the conversion on there but you can see
it takes roughly 0.34 of a second and we
go ahead and print the result in here
too
let's do that
we'll run that just so you can see what
the what kind of data we're looking at
and we have the zero two four six eight
so it's just adding them together it
looks pretty straightforward on there
and if we scroll down to the bottom of
the answer again we see python list took
46 a little different time on there
depending on what
core because i have this is on an eight
core computer so it just depends on what
core it's running on what else is
pulling on the computer at the time
and let's go back up here and do our
start time
paste that into here
and this time we're going to do a result
equals and this is really cool notice
how elegant this is so straightforward
this is a lot of reason people started
using numpy is because i can add the two
arrays together by simply going a1 plus
a2
it makes a lot of sense both looking at
it and it's just very
convenient remember that slide we're
looking at fast convenient and less
memory so look how convenient that is
really easy to read real easy to see
and i don't know if we don't need to
print the result again so let's just go
ahead and print
the time on here and we'll borrow this
from the top part
because i really am a lazy typer
and this isn't the python list this is
the numpy list or number array
and let's go ahead and see how that
comes out and we get 2.99
so let's take a look at these two
numbers 46 versus 2.99 so we'll just
round this up to 3.
that's a huge difference that's that's
like
more than 10 times faster that's like 15
times roughly at a quick glance i'd have
to go do the math to look at it and it's
going to vary a little bit depending on
what's running in the background the
computer obviously so we've looked at
this and if we go back here we found out
it's much faster yes there's different
going to be different speeds depending
on what you're doing with the array very
convenient easy to read and it uses less
memory so that's the core of the numpy
that's why a lot of people base so many
other modules on numpy and why it's so
widely used so we did glance at a couple
operations when we were looking at speed
and size
let's dive into a little bit more into
the basic operations
and these are always nice to see i mean
certainly you want to go get a cheat
sheet if you're using it for the first
time you know look things up google is
your friend
we did this with the most basic numpy
dot array or np dot array
and we'll go ahead and create an array
let's do
pairs
one comma
two and then let's do a three comma four
and if we can do that let's do five
comma six
there we go and if we go ahead and take
this
and run this i can go ahead and do our a
down here so it's in line and i'll print
that out
you can see it makes a nice array for us
so we have a and if you look at that we
have three different objects each with
two values in them and hopefully you're
starting to think well how many
dimensions or indexes is that and you'll
see three by two so let's go ahead and
take a look and let's go how about a dot
in dimensions speaking of which
we'll run that and we have two
dimensions for each object
and then we can do the item size so a
dot
we saw this earlier we looked up how
many items it was up here where we
wanted to multiply item size times the
actual size of the object so the memory
is being used versus the item size
and we should see four there
memory is compressed down that's always
a good thing
and then the shape the shape is so
important when you're working with data
science
and you're moving it from
one format to another
so we have our shape we just talked
about that we have three by two
three rows by two objects in each one
generally i don't look too much at the
size but the dimensions i'm always
looking up this is nice you can automate
it so you might be converting something
you might need to know how many
dimensions are going into the
next machine learning package so that
you can automatically just have it send
that information over
so we looked at a shape
let's go and create a slightly different
array np dot array
let's go ahead and just do as our
original
setup here
and one of the features we can do which
is really important is we can do d type
equals in this case let's do
np
float
64. and so what we've done is converting
all of these into a float and we type in
a
and now instead of having one two three
four five six you see they're all float
values one dot zero there's no actual
zero in there just so it's a one dot or
the one period two three period four
period five period six period
and this again data science i don't know
how many times i've had to convert
something from an integer to a float so
that's going to work correctly in the
model i'm using
so very common features to be aware of
and to be able to get around and use
and we'll also do let's just curiosity
item size
we'll go ahead and run that
and we see that it doubled in size so
it's not a huge increase well doubling
is always a big increase in computers
but it's not a huge increase compared to
what it would be if you're running this
in the python list format
and then we did the shape earlier
without having it set to the float64
let's go ahead and do a shape with it
set to 64. and it should be the same
three comma two so it all matches
so we've gone through and remember if
you really if this is all brand new to
you
according to the cambridge study at the
cambridge university if you're learning
a brand new word in a foreign language
the average person has to repeat it
163 times before it's memorized
so a lot of this you build off of it so
hopefully you don't have to repeat it
163 times but we did manage to repeat it
at least twice here if not a little bit
more
and let's go ahead and take this we're
going to look at one more setup on here
and let me just take this last statement
here on the converting our properties of
our data
and instead of float 64
let's do complex let's just see what
that looks like and let's go ahead and
print that out
and run it
and so we now have a complex data set up
and you'll see it's denoted by the one
dot plus zero dot j
and if we flip over here and do a basic
search for numpy data types
better to go to the original web page
but pull up a bunch of these you can see
there's a whole list of different number
data types
shorthand complex we have complex
complex 64 complex 128
complex number represented by 264 bit
floats real and imaginary components
one option on there float 16 float32
float shorthand for float64 most
commonly used and of course all the
different ones that you can possibly put
into your numpy array so we covered a
basic addition up there we're comparing
how fast it runs but some very basic
components how to set up a numpy array
how many dimensions it has item size
data type item again we went to item
size and there's also
the
shape probably one of the more used i
used a shape all the time very commonly
used
and then down here you can see where we
actually created a numpy complex data
type
so let's look at some other features in
numpy one of them is you could do numpy
dot
zeros
and we're gonna do three comma four
there we go and we'll go ahead and run
this and you can see if i do np dot
zeros i create a numpy array of zeros
this is really important i was building
my own neural network and i needed to
create an array where i initialized the
weights and i want them all to be the
same weight in this case i wanted them
to start off as zero for the particular
project i was working on and there's
other options that you can do numpy ones
and we'll do the same thing three comma
four we'll run that
and you can see i've created an array of
numpy ones in this case it comes out as
a float array
and this is an interesting to note
because we have let's go back to our
python and do lrange5
and we'll print the l so there's our
list
and if i run that
it doesn't create the range until after
the fact until you actually execute it
that's an upgrade in python python27
actually created the array zero one two
three four this one actually creates the
script and then once it's used it then
actually generates the array
and if we do that in numpy a range
remember that from before
and if we do a numpy a range 5
and let's do l equals
or we can just leave it as numpy that's
fine there we go just run that
you can see there we actually get an
array zero one two three four for the
value the numpy arrange a range five
generates the actual array
and for part one we're going to do just
one more section on basic
setup
and we're going to concatenation
do a concatenation now example
there we go we're going to do strings
let's take a look at strings what's
going on with there and let's do
oh let's see print
let's do an np
character something new here
and we're going to add
and then here's our brackets for what
we're going to add
oh and let's say
let's do
hello
comma hi
and
in the brackets on there let's create
another one
and this one's going to be
a b c
and we'll do
x y z so we're just creating some
randomly making some up on here and then
we'll go ahead and just print this
if we run that and come down here and of
course make sure all your brackets are
open and closed correctly
and then you can see in here when we
concatenate the example in numpy
it takes the two different arrays that
we set up in there and it combines the
hello with the abc and the high with xyz
and if we can also do something like
print
oh let's do
np character dot
multiply
so there's a lot of different functions
in here again you can look these up it's
probably good to look them all up and
see what they are but it's good to also
just see them in action let's do hello
space comma three
and we'll run this one
run that without the error and you'll
see it does hello hello hello so we
multiplied it by three and we can also
let's just take this whole thing here
instead of retyping it
and we can do character
center so instead of multiply let's do
center
and over here
keep our hello going
take the space out of there
and let's do center at
20 and
fill character
equals and we'll fill it with dashes
so if we run this
you can see it prints out the hello with
dashes on each side and we keep going
with that we can also in addition to
doing the fill function we can play with
capitalize we can title we can do
lowercase we can do uppercase we can
split split line strip join these are
all the most common ones and let's go
ahead and just look at those and see
what those look like each one of them
here we're going to do the hello world
all-time favorite of mine i would like
to say hello universe and you can see
here we did a capital h with the world
but so we want to capitalize so
capitalize is the first one in the array
so we get hello world on there and we
can also take this
and instead of capitalizing
another feature in here is title and
let's just change this to how are we
doing
how
are you doing instead of
do you and let's run that
and you can see here because we created
as a title it capitalizes the first
letter in each word
and in this one we're going to do
character lower
two different examples here we have an
array we have hello world all
capitalized and we have just hello and
you can see that one is an array and one
is just a string if we run that you get
a an array with hello world lowercase
and hello lowercase
and if we're going to do it that way we
can also do it the opposite way there's
also upper
and let's paste those in there and you
can see here we have
character.upper opposite there
python.data and that will do python is
easy
hopefully you're starting to get the
picture that most of the python and the
scripting is very simple
it's when you put the bigger picture
together and starts building these
puzzles and somebody asks you hey i need
the first letter capitalized unless it's
the title and then we have you start
realizing that this can get really
complicated so numpy just makes it
simple and we like that
and so in this case we did python data
it's all uppercase python is easy like
shouting in your messenger python is
easy
and then if you're ever processing text
and tokenizing it
a lot of times the first thing you do is
we just split the text and we're just
going to run this
np.character.split are you coming to the
party if we do that returns an array of
each of the individual words are you
coming to the party splitting it by the
spaces
and then if we're going to split it by
spaces we also need to know how to split
it by lines
and just like we have the basic split
command we also have split lines
hello and you'll see here the scoop in
for our new line
and when we run that if you're following
the split part with the words you should
see hello how are you doing the two
different lines are now split apart
and let's just review three more before
we wrap this up commonly used string
variable manipulations we have strip and
in this case we have
nina admin anita and we're going to
strip a off of there let's see what that
looks like
and then you end up with nin diminished
it basically takes up all leading and
trailing letters in this case we're
looking for a
more common would be a space in there
but it might also be punctuation or
anything like that that you need to
remove from your letters and words
and if we're going to strip and clean
data we also need to be able to reformat
it or join it together so you see here
we have a character join we'll go ahead
and run this
and it has on the first one it splits
he's the letters up by the colon and the
second one by the dash and you can see
how this is really useful if you're
processing in this case a date we have
day month year year month date very
common things to be have to always
switch around and manipulate depending
on what they're going into what you're
working with
and finally let's look at one last
character string we're going to do
replace
if you're doing misinformation this is
good pulling news articles replacing
is and what in this case we're just
doing here's a good dancer and we're
going to replace is with was
and you can see here he was a good
dancer hopefully that's not because he
had a bad fall he just was from like you
know 1920s and has gotten old
so there we go we covered a lot of the
basics in numpy as far as creating an
array very important stuff here when
you're feeding it in how do we know the
shape of it the size of it what happens
when we convert it from a regular
integer into a float value as far as how
much space it takes we saw that that
doubled it item size you have your n
dimensions and probably the most used is
shape and we'll cover more on shape in
part two so make sure you join us on
part two there's a lot of important
things on shaping in there and setting
them up we also saw that you can create
a
zeros based array you can create one
with ones if we do a range you can see
how it is a lot easier to use to create
its own range or a range as it is in
numpy
you saw how easy it was to add two
arrays we saw that earlier just plus
sign then we got into
doing strings and working with strings
and how to concatenate so if you have
two different arrays of strings you can
bring them together we also saw how you
can fill so you can add a nice headline
dash dash dash
we saw about capitalize the first letter
we saw about turning it into a title so
all the first letters are capitalized
doing lower case on all the letters
upper for all the letters just lower and
upper nice abbreviation we also covered
how to split the character set how to
strip it so if you want to strip all the
a's out from leading ai a's and ending
a's or spaces you can do that very
easily also how to join the data sets so
here's a character join option for your
strings and finally we did the character
replace so last time we covered part one
where we went over the difference
between the python array and the numpy
array and why it's both easier to use
uses less memory and resources and is
also faster than the python list we also
went over a number of the basic features
in there like looking up the min the max
the median how you can go ahead and
create some very basic arrays fill them
all with zeros fill them all with ones
look up the size the shape so we covered
a lot we covered a range which is
equivalent of python well sort of the
equivalent of python list range and then
we looked a lot into characters working
with the np character and how to
capitalize center it change it to a
title lowercase uppercase splitting
stripping joining and replacing
characters so what's in it for you today
we're going to go over array
manipulation we're going to go over
numpy arithmetic operations slicing
arrays iterating over arrays array
concatenation splitting arrays numpy
histogram using matplot library and a
few other useful functions in the numpy
and then we'll do a practice examples at
the end and hopefully you've already got
your jupiter notebook i like to use it
through anaconda but certainly you just
use a direct jupiter notebook now let's
go ahead and dive in there since we're
going right into part two which is
getting some coding going under our belt
and here in our jupiter notebook we can
go under new and create a new folder
python 3.
i think i forgot to do this last time
but we could just do the
control plus plus which in any browser
enlarges the page makes it a lot easier
to see always a nice feature another
beautiful benefit of using jupiter
notebook
and let me go ahead and show you a neat
thing we can do in jupiter this is nice
if you're working with people and you're
doing this as a demo on a large screen
i'm going to do the hashtag or pound
symbol array manipulation kind of a
title that we're working on and then i'm
going to call this cell cell type
markdown as opposed to code and you'll
see it highlights it here and then if i
run it it just turns it into array
manipulation and then we're specifically
going to be working on array
manipulation changing shape to start
with
and we'll go ahead and mark this cell
also a markdown so has a nice little
look there and then it comes up and you
can see it just like i said it just
highlights it and makes it very in bold
print just making it easier to read not
a python thing but a jupiter thing
that's good to know about especially if
you're working with the shareholders
since they're investing money in you of
course the first thing we do is import
we're going to import numpy as in p that
should be standard by now by now you you
start a python program you're doing some
data science numpy is just something you
bring in there and let's go ahead and
create our array and we're going to do
that as the np dot a range remember
that's a zero well we're going to do
zero to nine
and then we'll print
a little title on the original array
we'll just print that array a remember
from the first lesson so we have our
array which is 0 1 2 3 4 5 6 7 8.
and let's add a print space in between
let's create a second array b but we
want this to reshape array a and what
does that mean
and the command is simply reshape and
then we have nine items in here and this
is so important right now so be very
aware if i did some weird numbers in
here it's not gonna work
and we want multiples of nine we know
that three times 3 is 9
so we're going to reshape
our a array by 3 by 3 and then we're
going to print well let's give it a
title
oops i have too many brackets in there
modified array and then let's go ahead
and print
our b
and let's see what that looks like
and as we come down here you can see
we've taken this and it's gone from
0 1 2 3 4 5 6 7 8 to an array
of arrays and we have 0 1 2 three four
five six seven eight
and so we split this into three by three
and you can guess that if i tried to
reshape this let's just do a five by
three
which is fifteen
that's going to give me an error so it's
not going to work you're not going to
reshape something unless the shape all
the the data in there matches correctly
so we can take this 9 this flat 9 and
they call it flex it's just a single
array and we can reshape it into a 3x3
array and first you might think matrixes
which this is used for that definitely i
use it a lot in graphing because they'll
come in that i have an array that's x y
comma x y one y one comma x two y two
and so the shape of it might be 2 by the
length of the number of points
and i need to separate that into
x flat ray and a y flat array you can
see this can be very easy to reshape the
array doing that and we can of course go
back we can do b let me do a print
and we'll do b dot
latin remember i said it's called a
flatten array
and if we run that
you'll see it just goes back to the
original one it takes this 0 1 2 3 4 5 6
7 8 and flattens it back to a single
array
and then one other feature to be aware
of is if we flatten it one of the
commands we can put in there is order
let me just go ahead and do that order
equals
f strangely enough f
stands for fortran
the whole fortran days i remember
actually studying fortran programming
language
in this case you'll see that it uses the
first like 036
is the order so instead of flattening it
like we had before zero one two three
four five six seven eight it now does
zero three six one four seven two five
eight
and if you go to the numpy array page
you can see here that they have the
flatten you just open up the numpy and d
array flattened setup to look it up and
they have three different options they
have c
f and a
and it's whether to flatten in c which
was based on how the c code works for
flattening originally worked which is
row major
fortran which is column major or
preserve the column fortran ordering
from a so whatever it was in the default
is the
c version so the default that you saw
you could put orders equal c and it'd
have the same effect as we saw there
before you could even do order equals a
that would also have the same effect
because that's the default so really the
only other thing you really need to
change on here is to change it to c if
you need it
and you can see right here or f i mean
not c the only thing you really want to
change it to is to your f for the
fortron order which then does it by
column versus by row and let's look at
here we go
reshape
so let's create a range of 12
and let's reshape it
and we'll do 4 comma 3 for this one and
remember this is numpy i forgot the np
there
in p dot arrange
and we can type in just a for print or
you can do full print a and of course in
jupyter notebook even have a little
extra print at the beginning we run this
we'll see we create a nice array of zero
one two it's reshaped it so we have
four rows and three columns or you could
call that three columns and four rows
zero one two three four five six seven
eight nine ten eleven
but this one is so important we'll do np
transpose
a let's go ahead and run that
and it helps if i get all the s's in
there don't leave an s out and you'll
see here we've taken our array if you
remember correctly we had 0 1 2 3 4 five
six seven eight nine ten eleven
and we've swapped it so we've gone from
a three by four or a four by three to a
three by four
and this really helps if you're looking
at like a huge number of rows and the
data all comes in like let's say this is
your features in row one your features
in row two
and this is x y z well when you go to
plot it you send it all of x in one
array all the white and not one array in
all z in another array
and so it's really important that we can
transpose this rather quickly
this is kind of a fun thing i can
highlight it and do brackets around it
if you remember correctly
because we're in jupiter it doesn't
matter where we do the print or not
it'll automatically print it for us and
you see if i hit the run button it comes
up at the same exact thing
and let's play with the reshape and you
know let's zoom this up a little bit
here
make that even bigger so you can really
see what's going on and let's play with
the reshape just a little bit more we'll
do b equals
np dot a range let's do 8
and reshape
we'll do 2 comma 4.
let's go ahead and
print b
and then run that
and you'll see we have now the two rows
this is a bit more like so we have four
maybe two rows of four things so this
might be all of our x components and our
y components so we can switch it back
and forth real easy
important to know here whether we do 2
comma 4 or in the case of 4 comma 3
this has 12 elements and so however you
split it up it's got to equal 12.
so four times three equals twelve that's
pretty straightforward same thing down
here
two times four equals eight
if i change this and let's say i do two
comma three let's just run that in and
you'll find we get an error because
you can't split eight up into two rows
by three
you have to pick something that it can
split up and arrange it in so let's go
ahead and run that and just for fun
let's go
reshape our b again if i can type
reshape our b again and what else goes
into eight well we could do
two by two by two
so we can take this out to three
different dimensions
and then of course if we um because this
is going to come out you you as a
variable we can just go ahead and run it
and it'll print it we can also do a
print statement on there just like we
did before and you'll see we have two
different groups of two variables of two
different dimensions so 2 by 2 by 2. and
let's go ahead and assign this to a
variable c equals b reshape
and let's do something a little
different
let's roll the axes roll
axes
and we'll take our c
and do two comma one
and if we go ahead and run this it's
going to print that out oops
hit a wrong button there let's do that
one again and you roll the axis and you
can see that we now have a set of zero
one two three four five six seven we now
have the zero two one three four six
five seven
so what's going on here we're taking and
we're rolling the numbers around and
let's just simplify this we'll just do
it with c comma one and run that and so
if we roll a single axis you got 0 1
and then it rolled the 4 5
up and then we have 2 3 6 7 and if we do
2
let me see what happens there this is
one of those things you really have to
play with and start filling what it's
doing
we've now taken 0 2
4 6 1 3 5 7 so you can see we've now
rolled by two digits instead of rolling
the one set up we now rolled two digits
up there and so if we go back and we do
the one
so we've rolled it up zero one four five
and then we're gonna take the two in
there and we've rolled the zero one two
three four five and six seven
so we start rolling these things around
on here there's a lot of different
things you can do on this
but it's another way to manipulate the
numbers on your numpy
and finally let's go ahead and swap
axes
we'll do c
and let's just go ahead and run that
it's going to give me an error on there
that's because it requires multiple
arguments left out the arguments so now
we can swap and we get the zero two one
three four six five seven so you can see
everything's been swapped around
so next thing we want to go over is we
want to go over numpy
arithmetic operations
how can we take these and use these let
me just go ahead and put this cell as a
markdown there we go
we'll run that so it has a nice thing
all right nice title on there that's
always helpful
and let's start by creating two arrays
we'll do a as an ep np
range a range nine and let's reshape
this
three by three so by now you should be
saying this reshape stuff and this
should all look pretty familiar we have
our zero one two 3 4 5 6 7 8 on there
and let's create a second one b
and this time instead of doing a range
let's do n p array we'll just create a
straight up array
and we'll do an array of three objects
so it's going to be three by one and if
we go ahead and print a b out let me run
that this is actually pretty common to
have something like this where you have
a three by whatever it is in a three by
three array when you're doing your math
you kind of have that kind of setup on
there
and what we can do is we can go um
np dot add
a b
don't forget we can always put a print
statement on there
so if we add it you'll see that it just
comes in there and it goes okay we're
adding 10 to everything and we could
actually do something more i'll make it
more interesting 11 10 11 12. so let's
change b's now 10 11 12 and let's run
that
and you can see that we have
10 and then you had 1
plus 11 is 12
2 plus 12 is 14
13 so 10 plus 3 is 13 11 plus 4 is 15
and 12 plus 5 is 17 and so on
we'll put this back since that's how the
original setup was let's do 10 by 10 by
10 and run that and run that and get the
original answer and if you're going to
add them together we need to go ahead
and subtract
a b
and we run that
we get minus 10 minus 9 minus 8 just
like you would expect
so we have our subtraction 0 minus 10 is
minus 10 and so on and if you're going
to add and subtract you can guess what
the next one is we're going to multiply
and we'll multiply
a b
and this should be pretty
straightforward you should expect this
if we multiply 10 times 0 we got 0 10
times 10 is 10 and so on
and finally if you're going to multiply
what's the last one we got is divide
what happens we do divide a by b
and we run this
and we're going to get 0 and this is
0 divided by 10 is 0 1 divided by 10 is
0.1 2 divided by 10 is 0.2 and so on and
so on so the math is pretty
straightforward it just makes it very
easy to do the whole setup and again if
we went this and let's say i'll just
change this up here instead of 10 we do
a hundred
and
make this a thousand there we go
if we run that and then we do the add
you can see we got 10
plus 100 plus a thousand
same thing with the subtract
same thing with the multiply
and then you can also see the same thing
here with the divide so a lot of control
there with your array and your math
again let's set this back to 10 oops
it's right up here wrong section
there we go 10. i'll just go ahead and
run these
and get back to where we were
and this brings us to our next section
which is slicing and let's put in our
just make this a cell cell type markdown
and when we run that of course it gives
us a nice looking slicing there and
slicing means we're just going to take
sections of the array so let's create an
array in p a range
let's just do
20.
and if you remember if we do a
we have a 0 to 19.
and then we can do a and remember we can
always print these this can always be
put in a print but because i'm in
jupiter if you're doing a demo in
jupiter that is it's just so great that
you have all these controls on here so
we can slice four on and this should
look familiar because this is the same
as a python and a lot of other different
scripting languages if we do four go
zero one two
three that's the first four in the thing
and the skip sum and starts with this
one the first four skip then from there
on
you can also do the opposite
and go till the fourth one if we run
that we get zero one two three quite the
opposite on there we can do a single
item so we can pick object number five
on the list run that and five happens to
be five because that's the order they're
in and then this one's interesting so i
can do s equals slice
and let's create a slice here
and let's do two comma nine comma
yeah let's leave it two on there so
we'll create an s slice on here and then
if we take our array and we do array of
s we're taking our slice in there and
let's go ahead and run that
and let's take a look and see what it
generated here first off we started with
two so we have two at the beginning
we're going to end at nine which happens
to be eight so it stops before the nine
remember when we're doing arrays in
python and then we step two so two four
six eight
we could do this as three let me run
that and you can see how the changes to
five eight and we could do this as uh
let's leave this at three and if we
change this to ten
oops let's make it twelve there we go
when we run that we have two five eight
eleven so that's pretty straightforward
it's a very nice feature to have on here
we can slice it
and take different parts of the series
right out of the middle so now that
we've accessed the different pieces of
our array
let's get into iterating iteration and
this is interesting because my sister
who runs a college data science
division the first question she asks
is
how do you go through data and she's
asking can you do you know how to
iterate through data do you know how to
do a basic for loop you know how to go
through each piece of the data and in
numpy they have some cool controls for
that this is a mark down there we go and
run it
it's called the nd iter i'm not sure
what the nd stands for but in d iter for
iterator
so before we do that though let's create
an array or something we can actually
iterate through
we'll call it a equals np
a range
let's do something a little funny here
or funky
and we'll do 0 45
5. i'm not sure
why the guys in the back picked this
particular one it's kind of a fun one
and if i run that we do this you can see
we get 0 5 10 15 20 25 30 35 40. that's
what this array looks like
and that's just from our slice you could
this is just a slice that's all that is
is we created a slice of 0 45 0 to 45
step 5. and so we can do with this we
can also do a equals the shape
let's go ahead and take and reshape this
and since there's nine variables in
there we'll do a reshape it three by
three so if we run that oops missed
something there that is the a
that really helps
so if we do the a reshape
and we'll go ahead and print that out
we get 0 5 10 15 20 25 30 35 40.
and then we simply do 4 x in
our numpy nd
of a
colon and we'll just go ahead and print
x
and let's see what happens here when we
run through this and we print each one
of those
it goes all the way through the whole
array so it's the same thing we just saw
before we got 0 5 10
15 20 25 30 35 40. so it prints out each
object in the array so you can go
through and view each one of these
and certainly if you remember you could
also flatten the array and just do for
a and that also and get the same result
there's a lot of ways to do this but
this is the proper way with the nd
iterator because it'll
minimize the amount of resources needed
to go through each of the different
objects in the numpy array
and hopefully you asked this question i
just did that
and the question is how can i change
this instead of doing each object so
first of all let's go ahead and take my
cell type and we mark that down and run
it
and so we're going to work on iteration
order c style and f style remember c
because it came from the c programming
and f because it came from the old
fortran programming so let's give us a
reminder i will do a print a
and we'll do four x in np
iterate
a
but we also want to do this in a
specific order and you know what i'm a
really lazy typer so let's go back up
here
this is the nd iterator i knew it was
missing the nd part of a
and let's do order
equals
c
we'll print x on there and let's do that
again and this time order
order equals
f
there we go order equals f let's go
ahead and run this
and see what happens here and the first
thing you're going to notice our
original array 0 5 10 15 20 25 30 35 40.
when we do order c that's the default
0 5 10 15 20 and so on and then when you
come down here
you'll see f order f is 0 15 30 so it
takes the first digit of each on the sub
arrays or the second dimension and then
it goes into the second one 520 35 10 25
40. so slightly different order for
iterating through it if you need to do
that so we've covered reshaping we
covered math we've covered iteration
we've covered a number of things the
next section we want to go ahead and go
over
is going to be joining arrays so we need
to bring them together let me go ahead
and take the cell and make it a markdown
cell type markdown there we go and run
that so let's work on joining arrays so
we can bring them together and what
different options we have
and let's do uh we'll do an np array one
two
comma three four
we'll go ahead and print
let's do oops
first
these rays aren't that big so let's just
go ahead and keep it all on one line a
so if we run this first array one two
three four whoops i forgot that it
automatically wraps it when you do it
this way so we'll go ahead and keep it
separate
and print
a there we go
and let's go ahead and do a
b and we'll do five six seven eight and
notice i'm keeping the same shape on
these two arrays
depending on what you're doing those
shapes have to match
and let's go ahead and print
second array
do a print
b
i'll go ahead and run that oops
missed something up there let me fix
that real quick
when i was reformatting it to go on
separate lines i messed that up there we
go run all right so we have first array
one two three four second array five six
seven eight
and we'll put a carried return on there
and the keyword we use is concatenate
and if you're familiar with linux it
usually means you're adding it to the
end
on there and we're going to do what they
call a long axis zero so we have
concatenate a b along axis zero let's go
ahead and run that and see what that
looks like
so we have one two three four five six
seven eight so now we have an array that
is four by two as a nice shape of four
by two one here
and if we're going to do it along the
axis 0
you should guess what the next one is
we're going to do it along the one axis
and let's see how those differ from each
other let's just go ahead and run that
and again all we're doing is adding in
the axis equals one so we have our
concatenate we have a b and then axis
one remember a couple things one these
are the same shape so we have a two by
two
same dimensions going in there you're
going to get an error if you're
concatenating and they're not if you
have something that instead of one two
is uh one two three four five six with
the five six seven eight they'll give
you an error on there in fact let's take
a look and see what happens when we do
that let me just take this
one two three
three four five and let's run that and
if we come down here oh we got there it
says all the input three dimensions
except for the concatenation axes m
must match exactly
so it'll let you know if you mess up
that's always a good thing let's go
ahead and take this back here and let's
go ahead and run that
and so we have our zero axes which is
one two three four five six seven eight
and we bring them together and you'll
see a very different setup here when we
do it along the axis one
we end up with instead of
four by two we end up with a two by four
one two five six three four seven eight
and this is changing which axes we're
going to go ahead and concatenate on
what i find is when you're talking about
the concatenate or the joining arrays
you really got to play with these for a
while to make sure you understand what
you mean by the axes it looks very
intuitive when you're looking at it
actually zero one two three four five
six seven eight axes one is then
splitting in a different way one two
five six three four seven eight
when you're actually using real data you
start to really get a feel for what this
means and what this does
so if we're going to do that let's go
ahead and look at splitting the array
and do that into mark down and run it
there we go so you have a nice little
title there
and we'll go ahead and create an array
of nine let's do np split
we'll do a and we're gonna split it by
three
let's just see what that looks like so
if we split it we get an array zero one
two three four five we get three
separate arrays on here and remember
we're looking at let me just print a up
here
so we're looking at zero one two three
four five six seven eight and then we
can split it into three separate arrays
and let's take this we're gonna do this
right down here just move the a split
down here instead of the 3 let's do 4
comma 5. put that in brackets
and so we do it this way we have 0 1 2 3
4 5 six seven eight and that's kind of
interesting i wasn't sure what to expect
on that but we get when you split it a
by four comma five you get a totally
different setup on here as far as the
way it split the array
and to understand how this works i'm
going to change the five to a seven
and this will visually make this a
little bit more clear
so we had four and five it went zero one
two three four five six seven eight and
you see the markers four and five when
we do four and seven i get zero one two
three four five six
seven eight
and so what you're looking at here is
the first markers this is going to go to
four
so there's our first split at the four
the marker of four and then the second
split is going to be at position seven
and this is the same thing here
four position five that's why we're
splitting it in those two sections we
could also do it seventh let's just see
what that looks like run
and you can see i now have zero one two
three four five six seven eight
so we can split in all kinds of
different ways and create a different
set of
multiple arrays on here and split it all
kinds of different ways
and before we get into the graphs and
other
miscellaneous stuff
let's go ahead and look at resizing the
array i'm going to take the cell and set
the cells a mark down and run it
give us a nice title there and we'll do
an array uh an input array of one two
three and four five six here i'm just
gonna just print
let's go print
a dot
shape and we'll go ahead and run that
whoops hit a wrong button there
hit the comma instead of the dot so we
have a shape of two comma three here
and this is important to note because we
start resizing it it's going to mess
with different aspects of the shape
and so we'll go ahead and do a print
scoop in for a blank line there we go
let's do b
equals np dot resize
we're going to resize a
and let's resize it with
three by two
and then we'll just go ahead and print
b
and print
b period shape not a comma
i'll run that
oops forgot the quotation marks around
the end we'll go ahead and run that and
let's just see what that looks like so
we have
one two three four five six our original
array with a shape of two three
and then we want to go ahead and resize
it by three two and we end up with one
two three four five six and we end up
with the shape of three two that
shouldn't be too much of a surprise
you know we got six elements in there we
can resize it by two three was the
original one and then we're actually
just reshaping is how that kind of comes
out as when you resize it like that
but what happens if we do something a
little different
and let's go ahead and just take this
whole thing and copy it down here so we
see what that looks like
and instead of doing 3 2 remember last
time i did the to reshape it
i messed with the numbers and it gave me
an error
when you resize it you don't have to
match the numbers they don't have to be
the same dimensions so we instead of
going from a 2 3 to a 3 2 we can resize
it to a 3 3. so let's take a look and
see how it handles that and we come down
here to 3 3
we end up with 1 2 3 4 5 6 and it
repeats one two three
so it actually takes the data and just
adds a whole other block in there based
on the original data and repeating it
all right now at this point you know
we've been looking at tons of numbers
and moving stuff around
we want to go ahead and do is get a
little visual here because that
certainly you can picture all the
different
numbers on there but let's look at
histogram let's put this into a
histogram let me go ahead and run that
and to do that we're going to use the
matte plot library so from matplot
library we're going to import pi plot as
plt
that's usually the notation you see for
pi plot
so if you ever see plt in a code it's
probably pi plot in the matplot library
and then the guys in the back did a nice
job and gals too guys and gals back
there
our team over at simply learn put
together a nice array for me 20 87 4 40
53 with a bunch of numbers that way we
had something to play with
and what we want to do is we want to
plot
the histogram now remember a histogram
says how many times
different numbers come in and then we're
going to put them in bins and we have
been 0 to 20 to 40 to 60 to 80 to 100.
you might in here with the matplot
library they call them bins you might
hear the term buckets where they put
them in buckets that's a really common
term
then we want to give it a title so the
way it works is you do your plt.hist for
histogram your plt title and your plt
show
and we're doing just a single array in
here in the numpy array of a and let's
go ahead and run this piece of code
taking a moment to come out there says
figure size so it's generating the graph
and you can see we have let's just take
a look at this and go down a size there
we go okay so now we can see we're
taking a look at here
so between 0 and 20
we have three values so we have a 20
here we have a 4
and a 11 and a 15.
zero one two three it's actually four
values but they start at zeros remember
we always count from zero up
and from twenty to forty we got twenty
this is one forty 42
3
4
5 6.
and so you can see in the histogram it
shows that the most common numbers
coming up
is going to be between the 40 and 60
range least common between the 80 and
100 this looks like a age demographics
is what this looks like to me and you
can see where they would have put it in
the buckets of different age groups
which would be a nice way of looking at
this
histograms are so important so powerful
when you're doing
demos and explaining your data so being
able to quickly put a histogram up that
shows
what's common and how it's trending is
really important
and using that with a numpy
is really easy
and you know what let's take the same
data and i want to show you why we do
bins or why we have buckets of data i'm
used to calling it buckets why we have
bins let's do it instead of by 20 let's
do it by tens and see what happens
and what happens when you do it by tens
is you miss out on the you can see a
nice curve here on the first one
and on the second one it looks like a
ladder going up and a plummet a ladder
going up and a plummet and a ladder
going down
so the first would be more indicative of
an age group and the second one would be
what you would get if you divide it
incorrectly you wouldn't see the natural
trend of
i don't know what this would be maybe
how much food they eat hopefully not
because i'm in 50 so i'm right in the
middle there that which means i get a
ton of food compared to everybody else
but it's some kind of democrat maybe
it's mental maybe it's knowledge because
we we hit a certain point and we start
losing our marbles start leaking out or
something so you start off knowing
something and then as you get older you
grow more but you see here we lose that
you lose that continuity in the thing if
you split the histogram into too many
bins or too many buckets
and if you actually plotted this by the
individual numbers it would just be a
bunch of dots
on the graph it wouldn't mean a whole
lot
and we've looked at graphs there are
terms that are a ton of useful functions
in numpy
i'm sure there's even new ones that are
going to be in here but let's just cover
some important ones you really need to
know about if you're using the numpy
framework
one of them is line space function this
is generating data so we have a line
space we have 1 3 10 and when we do that
we end up with
10 numbers so if you count them there's
10 numbers they're between 1 and 3 and
they're evenly spaced we get 1 1.222 but
these are all there's a total of 10 here
and it's right between the 1 and three
range
that can be there's a lot of uses for
that but they're probably more obscure
than a lot of the other common numpy
arrays set up
a real common one is to do summation so
we'll do summation where you do in this
case we create a numpy array of one of
um
two different arrays one two three or
two different dimensions one two three
three four five
and we're going to sum them up under
axes zero which is your columns and if
you remember correctly columns is the
one plus three two plus four three plus
five so we have three columns and if we
change this we'll just flip this to one
we get two numbers so we get one two
three all added together which equals
six and three plus four plus five which
equals twelve we'll set this back to
zero
there we go since it was just we're
looking to actually zero
and these probably could have been some
of these communities are math section
square root and standard deviation two
very important
tools we use throughout the machine
learning process
in data science
and simply we take the np array we have
again the one two three four five six
three four five i don't know why i need
to keep recreating it probably could
just kept it but we can take the square
root of a so it goes through and it
takes the square root of all the
different terms in a
and we can also take the standard
deviation how much they deviate in value
on there
and there's a rabble function we can run
that
and np array is x we're going to do x
equals say we change it from a to x
x equals
ravel and this sets it up as columns so
we have one two three four five this is
all columns on here very similar to the
flattened function
so they kind of look almost identical
but we also the option of doing a ravel
by column
and then another one is log so you can
do mathematical log on your array in
this case we have 1 2 3
and we'll find the
log base 10 for each of those three
numbers
there's a couple of them they don't you
can't just do any number here after log
but there is also log base 2.
log base 10 is pretty commonly used on
here
there we go
before we go let's have a little fun
let's do a little practice session here
on some more challenging questions so
you start to think how this stuff fits
together right now we just looked at all
the basics and all the basic tools you
have
so let's do some numpy practice examples
and let's start by figuring out how do
you plot say a sine wave in numpy how
what would that look like and so in this
project we wouldn't have to do this
because i've already run these but we'd
want to go ahead and import our numpy as
np and import our matplot library piplot
as plt so we get our tools going here
and then we'll break it into two
sections because we need our x y
coordinates in here
so first off let's create our x
coordinates
and our x coordinates we're going to set
to an a range
and we want this error a range since
we're doing
sine and cosine it's going to be between
0 and 0.1
and then we use our np and we actually
can look up numpy stores pi so you have
the option just pulling pi in there
directly from numpy it has a few other
variables that it stores in there that
you can pull from there but we have
numpy pi and we generate a nice range
here let's go ahead and run this
and just out of curiosity let's see what
x looks like i always like to do that so
we have point one point two point three
point four so we're going uh zero to
in this case nine point four three times
numpy pi
pi is like three point something
something something so that makes sense
it should be about nine and we're doing
intervals of 0.1 so we create a nice
range of data and then we need to create
our y variable and so y
is going to simply equal np our numpy
dot sine
of x
and then once we have our x and y
and if we print let's go and just print
y
see how that will do this let's do this
so it looks print
x print y so we basically have two
arrays of data so we have like our
x-axis and our y-axes going on there
and this is simply a plt dot plot
because we're going to plot the points
and we'll do x comma y
and then we want to actually see the
graph so we'll do plot dot show
and we'll go ahead and run that
and you see we get a nice sine wave and
here's our number 0 through nine
and here's our sine value which
oscillates between minus one and one
like we expected to
then for the next challenge
let's create a six by six two
dimensional array and let one and zero
be placed alternatively across the
diagonals
oh that's a little confusing so let's
think about that we're going to create a
six by six
two dimensional so the shape is six by
six two dimensional array
and let one and zero be placed
alternatively across the diagonals
now if you remember from lesson one we
can fill a whole numpy array with zeros
or ones or whatever so we're gonna do np
create a numpy zeros and we're gonna do
a six by six and we'll go ahead and make
sure it knows it's an integer even
though it's usually the default and just
real quick let's take a look and see
what that looks like so if i run this
you can see i get
six by six grid so
six by six zero zero zero zero zero
now if i understand this correctly when
they say ones and zero placed
alternatively across the diagonals
they want the center diagonal maybe
that's going to stay zero all the way
down
and then the next diagonal will be ones
all the way across diagonally and then
the next one zeros the next one ones the
next one zeros and so on hopefully you
can see my mouse lit up there and
highlighting it
so let's take a little piece of code
here
and we'll do z
one colon colon two comma colon colon
two
equals one and wow that's a mouthful
right there so let's go ahead and run
this and see what that's doing and so
what we're doing is we're saying hey
let's look at in this case row one
there's one and then we're gonna go
every other row two so we're gonna skip
a row so skip here skip here skip here
so we're going down
this way and we're going every other row
going this way it's hard to highlight
columns
so you can see right here where the that
we're not touching each row like this
row right here is not being touched okay
so we're going to start with row one and
then we're going to skip a row and
another one and so we're going every two
rows and then in every two rows we're
looking at every two
starting with the beginning that's what
this thing blank means so we're gonna
start with the beginning and we're gonna
look at all of them but we're gonna skip
every two
so starting with row one
we look at all the rows but we do we do
it by two steps so we go one skip one
you know one skip one one skip one one
if you left this out it'd do every one
this would just be ones in fact let's
see what this looks like if i go like
this
and run it
you can see that i just get ones
so
this notation allows us to go down
each row row by row and we're going to
do every other row
set up on there and so if we're going to
start with row one we also
control z
try that there we go we'll start with
row zero again we're going to go each
row step two so we'll start with row
zero
and we'll go every other row
and this time we'll start with one
column one
and again we go every other one going
down
step that's what that step two is and
skipping every other one we're gonna set
that equal to one so let's see what that
looks like
and you can see here we get our answer
we get 0 1 0 0 but it has the ones going
in diagonals
on every other diagonal and 0 on every
other one
a little bit of a brain teaser that one
trying to get that one to work out so
you can see how you can arrange your
rows and here's your step in your
different access on there
and then the next one is
find the total number and locations of
missing values in the array the first
challenge is to create some missing
numbers
so let's create our ray z we're going to
do numpy dot random.rand 10 comma 10.
and before we do the second part let me
just take the second part out
and let's just see what that looks like
so let's run that
and there we go so we have a 10 by 10
random array it randomly is picking out
numbers
and
next we want to go ahead and take our
random integer size equals five
and then we're going to do a random
random 10 size equals five
so in the z we're going to select a
number of random spaces here and set
them equal to null value
and let's go ahead and run that so you
can see what that looks like and if we
look at the array
we've created one two
three
four
there should be a fifth one in here my
eyes may be failing me so we've created
a series of out because zero zero to
five zero one two three four so we've
got five there are different null values
on here
and
this is kind of a neat notation to
notice that we can generate
random integers size equals five so this
generates
five by five miniature grid inside of
this to tell it where to put the nands
at so that's kind of a cool little thing
you can do
and we want to look up and see how many
null values are in there
and this is simply just
np is none of z
simple
so if it is none then we want to sum it
up so we're going to sum up all of the
different null values on there now let's
do one one more feature in here which is
really cool
let's go ahead and print the indexes so
np arg where np is
nan of z so we're going to create our
own another np array and let's run this
and we'll see here that comes up with
the four indexes
so we did count four of them up there
it tells you where they are one nine two
zero four six five four
and then let's go ahead and run this
again run run there we go this time i
got five that's what get for random
numbers another fun one
that i always like to do it's very
similar because we have n p is not z dot
sums we're summing the number of
nands and we can get the indexes and you
can reshape the indexes but you can also
just do we'll do an inds where np is nan
of z
and let's just print let's print that
print inds let's see what that looks
like
and it's very similar we have we have
zero one three zero six three eight six
nine three
but i've split it into two different
arrays
so we have our x and our y kind of
coordinates going there and what i can
now do is i can now do z
inds
equals and at this point you can also
instead of getting the sum you can get
the means or the all the numbers and
that kind of thing or the average as it
is
so that'd be one thing you could do and
you could pick up the average that's
very common in data science to get the
average and just use that for a value
but we'll go and just set it to zero and
then let's go ahead and print our z
and run that
and you can see we come down here
we have wherever there was a null value
it is now
zero
and you can set this to whatever you
want this is another way to replace data
or help clean data
depending on what it is you're doing
so wow we covered a lot of stuff so
quick rehash going over everything we
went into there we looked at array
manipulation changing the shape
how to switch that around we even had
the flattened down there which remember
we have another command lower that's
similar we could change the order by f
remember f stands for fortran very
strange connotation but there's c and f
c is the standard and f switches it to a
different order
to be honest i usually have to look it
up because i almost never use f but when
you need it you're like oh my gosh it
was the other order let's do a quick
google so we talked about reshape making
sure that the dimensions are the same
you don't want to have like something
that has 12 objects in it and reshape it
to
see 11 and 5 because it doesn't work it
doesn't divide into 12. we can transpose
so we can switch them so we can go from
a 4 by three to a three by four oops i
did it the other way around
three by four to four by three
we covered reshaping the array we did
the roll the axes you can do some weird
things with swapping and rolling axes
and transposing the numbers
we dug a little bit into the arithmetic
so we talked about
adding we talked about subtracting
multiplying dividing
and you know at this point it's so
important we just look up the numpy
mathematics and you can see here they
have just about everything your
trigonometry
uh your hyperbolic functions roundings
sums products differences
there are so many all these different
miscellaneous mathematical connotations
so you know google it go to the main
numpy page and look at the different
setups you can do on there
so we covered that
and we did slicing how to break it apart
we did iterating over the array we
covered
joining arrays and how to concatenate
remember concatenate just means add on
to it so in this case how are you adding
b onto a is how you'd read that from
linux you should catch the concatenate
because that's used regularly there
splitting the array we talked about how
to split the array in different ways so
you can split it in
array of arrays all kinds of different
ways to split the array up how to resize
it and remember resize does not have to
have the same shape
but if you resize it
it will take the data and begin at the
beginning and add new rows on if the
size is bigger if it's smaller it
truncates it it just cuts the end off
we looked at how to do a histogram and
how to plot that
uh we mentioned bind buckets or bins as
they call them in pipelot and then we
covered a lot of other useful functions
in numpy talked about the line space
setup for doing
numbers in a series
how to sum the axes up again that's part
of the mathematical formulas there that
we looked at there's a sum there's also
means and median
all of those you can compute in num b
and you can also do the square root and
standard deviation
the ravel function very similar to the
flat
to be honest i almost always just use
the flat but you know the ravel has its
own kind of functionality that it does
and then we went into some numpy
practice examples we challenged you to
create a sine wave in numpy and how to
do that we're kind of looking for that a
range remember how we do the a range and
you can
have your beginning value your n value
which they did is three times pi number
pi
and we're going to do intervals of 0.1
and then y just equals the numpy sign of
x there's our math from the math page we
were just looking at remember that it's
right at the top
and finally we went down here we had
this kind of a little brain teaser how
to do diagonal zeros and ones
playing with the different connotations
of z of the numpy array
and then we did a random size and we
played a little bit with how to with the
null values playing with null values
if you're doing any data science you
know null values are like a headache
what do you do with them big sets of
data you get rid of them small debt sets
of data you have to factor something in
there like figure out the average or the
median there and then replace it with
that pandas really is a core python
module you need for doing data science
and data processing there's so many
other modules that come off of it there
it actually sits kind of on numpy so if
you've already had our numpy array
hopefully you've already gone through
the numpy tutorial one and two so today
we're going to cover what is pandas
we'll discuss series we'll discuss basic
operations on series then we'll get into
a data frame itself basic operations on
the data frame file related operations
on a data frame visualization and then
some practice examples roll up our
sleeves and get some coding underneath
there and let's start with just some
real general what is pandas pandas is a
tool for data processing which helps in
data analysis it provides functions and
methods to officially manipulate large
data sets
now this is a step down from say using
spark or hadoop in big data so we're not
talking about big data here but we are
talking about pandas when there is some
connections there's like an interface
going on with that so there is
availability but you really should know
your pandas because if you're working in
big data you'll know there's data frames
well pandas is a data frame primarily it
has a couple different pieces we'll look
at here and if you've never worked with
data frames before a data frame is
basically like an excel spreadsheet you
have rows and columns you can access
your data either by the row or the
column and you have an index and
different that kind of set up and we'll
dig more into that as we get deeper into
pandas but think of it as like a giant
excel spreadsheet that's optimized to
run a larger data on your computer
and then i said it that it's a data
frame so the data structures in pandas
are series one dimensional arrays and
then we have data frame two dimensional
array and it really centers around the
data frame the series just happens to be
part of that data frame and here's a
closer look at a pandas series series is
a one dimensional array with labels it
can contain any data type including
integers strings floats python objects
and more so it's very diverse if you
remember from numpy we studied they had
to be all uniform not in pandas and
pandas we can do a lot more and pandas
actually kind of sits on numpy so you
really need to know both of those if you
haven't done the numpy tutorials and you
can see here we have our index one two
three four five and then our data a b c
d and e very straightforward it's just
two columns and we have a nice index
label and a column label for the data
and then a data frame is a
two-dimensional data structure with
labels we can use labels to locate data
and you can see here we had if we go
back one we had our index one two three
four five so in each one of these series
they would share the same index over
there the row index so you have your row
index df dot index and then you have a
column index df.columns and this should
look like i said this would be really
familiar if you've done any work with
spreadsheets excel so it kind of
resembles that this does make it a lot
easier to manipulate data and add
columns delete columns move them around
same thing with the rows so you have a
lot of control over all of this
now we're of course going to do this in
our jupiter notebook you can use any of
your python editors but i highly suggest
if you haven't installed jupyter and
haven't worked with it it is probably
one of the best ways for easily
displaying a project you're working on i
skip between a lot of different user
interfaces or ides for editing my python
and it's just simply jupiter.org
j-u-p-y-t-e-r.org
and then i always let mine sit on
anaconda anaconda.com
and just real quick we'll open that up
for you oops offline mode don't show me
that again
but you can see here that i have
different tools that i can actually
install in my anaconda including the
jupiter notebook which comes by default
and then i have access to the
environments
and again that's
anaconda.com named after the very large
one of the largest world's largest
snakes and then jupiter notebook in this
case jupiter.org and when we're in our
i'm going to go in here to our jupiter
notebook and we're going to go ahead and
just do new and a python 3
and this will open up a python 3
untitled folder
so diving right in let's go ahead and
give this a title pandas tutorial and
we'll go up to cell and we'll change the
cell type to mark down so it doesn't
execute it as actual code
one of those wonderful tools when you
have jupyter notebooks so you can do
demos with this and let's go ahead and
import
pandas
and usually people just call it pd that
has become such a standard in the
industry so we'll go ahead and run that
now we have our pandas has been imported
into our jupyter notebook
and then oh we can go ahead and let me
do the control plus since it's internet
explorer i can enlarge it very easily so
you have a nice pretty view oops too big
there we go and whenever you're working
with a new module it's good to check
your version of the module in pandas you
just use the in this case pd dot
underscore underscore version underscore
underscore that's actually pretty common
in most of our python modules there's
different ways to look up the version
but that's one of the more common ones
and we'll go ahead and run that we get
.23.4
and if we go to the pandas site we see
0.23.4 as the latest release and of
course a reminder that if you're going
to environment you need to install it so
you'll need to pip install pandas if
you're using the pip installer we'll go
and close out of that
and the first thing you want to do is
we're going to work with series a lot of
stuff you do in series you can then do
on the whole data set we need to do what
create one we need to manipulate it
take pieces of it so query it query it
delete so you can delete different parts
of it so we want to do all those things
with the series and we'll start with the
series and then almost all the code in
fact all the code does transfer right
into
the actual data table so we go from a
series of a single list of one column
and then we'll take that and we'll
transfer that over to the whole table
and we'll start by creating let's put up
there we go
creating a series from
list
and let's just call this arr equals
and we'll do 0 1 2 3 4. if you remember
from our last one we could easily do
r equals
range of 5 which would be 0 to 4. but
we'll do r equals 0 to 4 and we'll call
this s1 and we'll go pd
and
series is capitalized this one always
throws me is which letters do you
capitalize on these modules they're
getting more and more uniform but you
got to watch that with python
and we're just going to go ahead and do
arr
so we're just going to take this python
list and we're going to turn it into a
series
and then because we're in jupiter we
don't have to put the print statement we
can just put s1 and it'll print out this
series for us
and let's go ahead and run that and take
a look
and you'll see we have two rows of
numbers so the first one is the index
now it automatically creates the index
starting with zero unless you tell it to
do differently so we get 0 index row 0
is 0 1 1 2 2 3 3 4 4. and because it's a
series it doesn't need a title for the
column there's only one column so why
title it
and this also lets you know that it's a
data type of integer 64. so we print
this out this is our series our basic
series we've just created
and let's do a second series
pd and we'll use the same
data list and let's go ahead and do
order we'll give it an order
equals oh let's do it this way
let's go
index
equals order
and it helps if we actually give it an
order so we'll do order
equals and let's do one two three four
five so instead of starting with zero
we're going to give it an order starting
with one we're going to run that and
we'll go ahead and print it out down
here s2
and we'll see that we now have an index
of 1 2 3 4 5 and that represents 0 1 2 3
4 in the series and we're still data
type integer 64. and very common as
you're missing with numpy arrays is we
can import our numpy as np remember that
from our numpy tutorials we can go ahead
and create a numpy out of random with
the random numbers of five
and let's just see what that end looks
like so we can see what our number looks
like so we have some nice random float
values here 2.33 so on and that's from
our last tutorial the numpy tutorial one
and two and instead of calling it order
let's call it index
and we're going to set our index equal
to a b c d and e i want to show you that
the index doesn't have to be an integer
so it can be something very different
here and then let's go ahead and create
our we'll just use s2 again and here's
our np for numpy
series capital s
and n is our
np for numpy
pd for pandas there we go switching my
anachronisms so we have pd.series of n
and we want to do our index
equals our index we just created
and then let's go ahead and see what
that looks like s2 is a print it and
let's run that
and we can see here we have a nice
series going on a b c d and e for our
indexes so instead of being 0 1 2 3 or 4
we can make this index whatever we want
and you can see the numbers here going
down that we randomly generated from the
number array so we use numpy to create
our
panda series right here
and so continuing on with creating our
series this one i use so often we create
a series from a dictionary so we have
our dictionary in this case we went
ahead and did a of 1 b is 2 c of 3 d4
ef5 so each one of those is a key and
then a value and then we're going to use
oh let's use s3 equals pd for pandas
series
and then we want to go ahead and just do
d in here
print out s3 here and let's go ahead and
run this and you can see we got a is 1 b
is two c is three d is four e is five
and it's still of integer 64 because the
actual data is one two three four five
and it's all integer 64 type 64. and the
last thing we want to do in the creating
section of our series is to go ahead and
modify the index because we're going to
start modifying all this data so let's
start with modifying the index of the
series and if you remember let's do a
print this time s1
i'll go ahead and run this and the
reason i did print is because it only
prints out the last variable so if i put
s1 up here and we're going to do another
variable back down lower it won't print
the first one just the last one and
we're going to go ahead and take s1
the index and we're just going to set it
equal to a new index and obviously
the number of objects in our index has
to equal the number of objects in our
data and then because it's the last
variable we can go ahead and just do an
s1 and let's run that and you can see
how we went from 0 to 0 0 1 2 3 4 as our
index we've now altered it to a b c d
and e
so this would be much more readable or
might be representational of a larger
database you're working with
so cool tools we've covered creating a
database based on
our basic array python array we've
showed you how to reset the index
then we showed you how to use a numpy
array so you can put a numpy array in
there it's all the same you know
pd.series a numpy array and then we can
set the index on there and the same
thing with the dictionary so it's very
versatile how it pulls in data and you
can pull in data from different sources
and different setups and create a new
series very easily in
the pandas and then we looked on
changing your index so now we have a new
index on here
and then we want to go ahead and do some
selection let's do some basic
slicing most common thing you'll
probably do on here and we'll just do s1
this notation should start to look
really familiar again this is going to
put an output so i'd usually it doesn't
change s1 this just selects it so we
might do a equals s1 and then print a
and you'll see that it just looks at the
first three zero one two and we can do
the same thing by not having the a in
there i'll go ahead and take that out
but just a reminder that it's not
actually changing s1 it's just viewing
s1 so simple slicing on here and we can
likewise do an append so before we do a
pen let's just do a quick kind of fun
one we'll do two minus one and you'll
see it covers everything but the e of
course you can do minus two on this side
so one another way to select it is to go
how far from the end and likewise we can
do a two here
cde to the end so it starts at the
second one and another way we can do
this is we can do a minus two over here
and that looks at just the last two in
the slice so you can see how easy it is
to slice the data and of course
there's no reason to do this but you
could select all of them
if you wanted to view all of them on
there helps 32 there's not 32 so it's
just going to show the first three there
we go and then we can also append so i
can take and oh let's create another
series and append one to it and if you
remember we had s3 there's our s3 and we
have our s1 we'll go ahead and do s1
and let's go ahead and do
oh let's call it s4
equals s1
a pin
s3
so we're just going to combine those two
into s4
and if we go ahead and print s4 on here
you'll now see that we have a b c d e a
b c d e 0 1 2 3 1 2 3 4 5 because we
started the data at one so very easy to
append one series to the next
and if we're going to append one series
to the next we need to go ahead and drop
or delete one and drop is a key word for
that and let's just do e our index e and
so if i run this
you'll see that it'll print it out and
a b c d there's no e
and remember all these changes if i type
in s4 again
you'll see that s4 still has e in it so
this change does not affect the series
unless you tell it to so i'd have to do
like x s4 equals s4 dot drop e and
there's another way to do that which
we'll show you later on let me just cut
this one out there we go
all right so we've covered all kinds of
cool tools here we have appending we
have slicing we did all the creating
stuff earlier as you can see here on the
setup how easy it is to manipulate the
series so next what we want to get into
is we want to get into
operations that happen on the series let
me go ahead and change this cell to
mark down there we go and run that
so series operations what can we do with
the series
and let's start by creating a couple
arrays we'll call it array one and we'll
do zero through seven and array two six
through six seven eight nine five i
don't know why we threw the five on the
end let's go ahead and run those so
those load up into jupiter
and we'll do this a little backwards
we're going to do s5 equals a panda
series of array two so i'm doing this in
reverse and then when we do s5 you'll
see that we have zero to four it
automatically assign the index
67895
for our series
and let's go ahead and do the same and
we'll call this s6 and we'll set this
equal to pd series for
our first array
and if we do an s6 down here to print it
out
we'll see something similar i got zero
through six
zero one two three four five seven for
the data so those are two series we just
created series six five and six
and one of the first things we can do is
we can add one series to the next so i
can do s5 dot add s6 and let's see what
that generates and just a quick thing if
you never use pandas what do you think
is going to happen with the fact that
this only has five different values in
it and this one has seven values
so let's see what that does
and we end up with 6 8 10 12 9 and it
goes oh i can't add this there's nothing
there so it gives us a null return very
different than the numpy that would have
given you an error this instead tells
you there's no value here because we
couldn't generate one so we can easily
add s5 dot add s6 and likewise we can do
s5 dot
sub for subtract
s6
and we'll run that and on the add the
subtract and you guessed it we're going
to do multiply and divide next again you
can see there's null values where it
can't subtract the two because there's
no values there to subtract we can also
do s5 multiply mul they're all three
letters on these that's one of the ways
to remember how they figured out the
code for this so remember these are all
three letters mole we'll go ahead and
run this
and you can you can see how they're
multiplied together and then we can also
do the s5 div three letters again
s6
and run that
and you'll see here this goes to
infinity because we have zero in the
wrong position so it actually gives you
a whole different answer here that's
important to notice and then in the null
values because there's no data and it
can't actually produce an answer off of
an old off of missing data
and since we're in data science let's do
s6
median so let's look at the median data
which is simply median sorry for those
who are following the three letters
because median is not three letters and
you can see an s6 is 3.0 and let's do a
print here and we'll do median
or average
s6
and let's print max
comma s6
and just like median there's max value
and if we're going to have a max value
we should also have a minimum value so
let's pop in minimum
we'll go ahead and run this
and you're starting to see something
that would be generated like say an r
where you're starting to get your
different statistics we have a medium
value of 3 max value of 7 and a minimum
value of 0. and what it does when it
hits these null values if there is no
values in there because we could still
do that we could actually you know what
let's go up here and do
let's pick this one where we multiplied
let's go s7
equals
i'll go and print the s7 just so i keep
it nice and uniform so i still have my
s7 down there and run it
and then i want to take the s7
because s7 now has
null values and an infinity value and
let's see what happens
this is going to be interesting because
i want to see what it does with infinity
and we end up with a median of 6 maximum
of 27 and minimum of 0. which is correct
it drops those values so when it gets to
there and it doesn't know what to do
with them it just drops those values and
then it computes it on the remaining
data on there so it's important to know
when you're making these computations
you're looking at min and max and median
you're not going to know that there's no
values unless you double check your data
for the null values it's a very
important thing to note on there so just
a real quick
review on there we've done our created
our pd series and we've gone ahead and
done addition subtraction multiplication
division all those are three letters so
sub min div add
and then we looked at median maximum and
minimum so we're going to go ahead and
jump into the next big topic which is to
create a data frame so now we're going
to go from series and we're going to
create a number of series and bundle
them together to make a data frame
there we go cell type markdown let me go
and run that so we have a nice title on
there it's always good to have a good
title all right so our first data frame
we'll jump in with some stuff that looks
a little complicated we'll break it down
first i'm going to create some dates
and you know what let's just go ahead
and do this i want you to see what that
looks like what i'm creating here i've
created a series of dates pd date range
and we're going to use these for the
index okay so when you look at this
you'll see that it's just
basically it comes out kind of like a
basic python list or a numpy array
however you want to look at it with our
different dates going down and we've
generated six of them and it's going to
have whatever time it is right now on
your on the thing for the date for the
time that's that time stamp right there
and then you'll see we have 11 19 2008
11 20 11 19 and looking into the future
there so that's all this is is
generating a series of dates that we're
going to use as our index and this is a
pandas command so we have a date range
which is nice that's one of the tools
hidden in there in the pandas that you
can use
and next we're going to use numpy to go
ahead and generate some random numbers
in this case we'll do the
np.random.random
in 6 comma 4. you can look at this as
rows and columns as we move it into the
pandas and of course you could reshape
this if you had those backwards on your
data but we want the six to match the
rows and we have six periods so our
indexes should match along with the rows
on there and then you know before we do
the next one let's go ahead and just
print out our numpy arrays and see what
that looks like here we have it one two
three four by one two three four five
six four by six
so it's a nice little setup on there and
since working with data frames can be
very visual let's give our columns we
have four columns and we're going to
give them names a b c and d
so now we have columns on there also and
then let's put this all together in a
data frame and we can actually you know
what let's do this since i did it with
everything else let's go ahead and do
columns and you can see there's our
columns on there
and we'll go ahead and do df1 equals
pandas dot
data frame and note that the d and the f
are capitalized series it was just the s
and i always highlight this because you
don't know how many times these things
get retyped when you forget what's
capitalized on there it's a minor thing
you'll pick it up right away if you do a
lot of it and the first thing we want to
do is we want to go ahead and take our
numpy array because that's what we're
going to create our data frame off of is
the numpy array and then we want our
index equal to our dates so there's our
index in there and then we also have
columns equals
columns
and then finally let's see what that
looks like remember we had all the
different data that just looks like a
jumble of data we have our column names
and everything else our numpy array kind
of just a jumble array over there four
by six you could sort of read it but
look how nice this looks i mean this is
you come into a board meeting you're
working with your
shareholders
this is pretty readable this is you know
this is our date this is our a b c d
whatever it is maybe it's one of these
dates has your leads
closures lost leads total dollar made
you know whatever it is fits in a
business maybe it's measurements on some
scientific equipment whether searching
material you know where this is like
higher the temperature low of the day
humidity of the date whatever it is so
you can see that we can really create a
nice clear chart and it looks just like
a spreadsheet you know we have our rows
and we have our columns and we have our
data in there now this one i use all the
time if we're going to create we can
create it like you saw here with our
numpy array very easy to do that and
reshape it you can also create it with a
dictionary array so here we have some
data let me just go down a notch so you
can see all the data on there we have an
animal in this case cat cat snake dog
dog cat snake cat dog we have the age so
we have an array of ages we have the
number of visits
and the priority was it a high priority
yes
no
and then we're going to take that we're
going to create some labels we have a b
c d e f g h i and what i want you to
notice on this is we have a title animal
and then we have basically a python list
and these lists they don't necessarily
have to be equal because we can have
non-data you know np.nand numpy array
null value but we want to go ahead and
create labels that are equal to the
number in the list so a
the first cat b the second cat c the
snake d the dog and so on so we'll go
ahead and create our labels which we're
going to use as an index
and we'll call this df let's do it this
way we'll call this df2
equals pd for pandas
data frame
and then we have our data just like we
did before and we have our index equals
labels
and if we're going to go from there
let's go ahead and print it out so we
can see what that looks like df2 so
let's go ahead and run that another
again you have a nice very clean chart
to look at we've gone from this mess of
data here to what looks like a very
organized spreadsheet very visual and
easy to read
animal age visits priority and then a
through j cats and all your different
animals so on and so on and then when
you do programming a lot of times it's
important to know what the data types
are so we can simply do
df2 d types
and if we run that
we can see that our animal
is an object because it's just a string
but it comes in as an object age is a
float64 integer 64 and then priority
again is just an object
and exploring this this one's very
popular let's go df two
head
and if we print that out the df2 head
returns the first five and we can change
this you don't have to do five you might
want to just look at the top two maybe
you want to look at
let's see oh let's do six so maybe we'll
look at just the top six in the database
in your data frame
and you can actually this creates
another data frame so i could have a df3
equal to df2 and this now takes the df2
and just the first six values
so if we do df3
run get the same answer
and if we do it the head of the data we
can also do the tail it's the same thing
df tail you can look at the last
we'll just do the tail which by default
does five the last five and of course
you can just look at the last three of
those real quick just to see what's at
the end of the data and this is i get to
tell i love doing the tail of one
because i'll have like the index or
something like that and it will just
show me the last whatever the last entry
was looking at stock values and i might
want to look at just the last five days
of the stock values i can do that with
the data frame tail
and some other key things to look up
are the index so we can do df2 dot index
and i want you to notice that this isn't
a call function so if i put the brackets
on the end it'll give me an error
because index is not callable it's just
an object in there
so we do
df2.index there's also columns
so we can go ahead and let's do a let's
print this
remember the first one is not going to
show unless i print it and then df2
column so now we can see we have our
indexes
and we have our columns listed here
df2.columns animal age visits priority
it tells you what kind of object it is
or what kind of data type it is and
they're both object
and then finally df2 dot
values and again there's no brackets on
the end of df2.values
because this is an actual object it's
not a callable function so we'll go
ahead and run that
and it creates just displays a nice
array a very easy way to convert this
back to a numpy array basically so
before i go into the next section let's
just take a quick look at what we
covered so far with the data frame we
came up here we created our data frame
we did it from a numpy array first
setting the columns and the index the
index is setting it up is the same as
when we set up the series so that should
look very familiar so is the whole
format the numpy array the index dates
and the columns columns and remember in
our numpy array we're looking at row
comma column so six rows four columns is
how that reads in the data frame
and we went ahead and also did that from
a dictionary in this case animal was the
column name with all the date data
underneath that column and then age with
that data visits that data priority that
data and then of course we added our
labels in there for our index so there's
no difference in there but it
automatically pulled the column names
important to know when you're dealing
with a data frame and importing a data
frame this way
and then we did looking up d type we
looked at head and tail looking at your
data really quick
we also did index and columns and values
and note these don't have the brackets
on the end
so the next thing we want to do is go
ahead since we're dealing with data
science is we want to go and describe
the data so we have
df2.described to do that and we're going
to manipulate it in just a minute but
let's just see what this generates
and you can see right here we have age
and visits
so looking at our data from up above let
me just go all the way up here
animal age visits priority
and it does a nice job generating your
age versus visits which has all the data
you have your account your means your
standard deviation your minimum value
25 or in this group 50 75 and your
maximum value so this look familiar as a
data science setup with your describe
for a quick look at your data frame data
so let's start manipulating this data
frame moving stuff around and we'll
start with transposing and it is simply
capital t for transpose
and when we run that it flips the
columns and the indexes so now the
indexes are all column names and the
columns are all indexes animal age
visits priority
so if we had come in here with our data
shaped wrong up above where we had a 4x6
we can quickly just swap it if we had it
backwards not a big deal and we can also
sort our data something that you can't
do which is more difficult to do with a
lot of other packages in the data frame
it's really easy to do take our data
frame df2 and we're going to sort
underscore values
by equals age and so when we run this
you'll see the default is ascending so
we have 0.52 2.53 and everything else is
organized so if you look at your indexes
they've been moved around because each
index it moves a whole row not just the
one piece of data is not being sorted so
very quick way to sort by age are
different data in the data frame
and in addition to sorting it we can
also slice the data frame so i can do
df2 and this should look familiar from
earlier we'll just do one
to three so we're going to pull out oops
it does help if i use a df instead of
just d and we're going to pull up just
between one and three so we have not
zero which is a we have b which is two
or b which is one and c which is two so
one two and then it does not include
three which is the standard in python
and we can even do something like this
we can combine them which is always fun
because remember this returns a data
frame so if i take df2 dot
sort
values and we'll do by
equals age
this is just kind of fun and then i'm
going to slice it
there we go double check my typing and
run it
and now you should see fa because fa are
now 1 and two on there
so you can very quickly create a whole
string on here which narrows it you know
that you can sort it then slice it and
do all kinds of fun things with your
data frame we'll just go back to the
original one run there we go and if we
can slice it by row we can also query
the data frame so we can do df2 and this
is a little different because i'm going
to create an array within an array and
in this case we're going to look at oh
let's do
age
comma visits
so look at the different format in here
we have one to three
so we've done this by
slicing by an integer value and then on
here i've done df2 age comma visits in
an array and when i run this
you can see that we get just these two
columns on here we get age and visits so
it's a quick way to select just two
columns or select number of columns
you're working with
and if you stop there we did the slicing
almost identical to slice is i location
which uses the integer location one
comma three
there's a push in pandas to move to this
particular setup
instead of doing just a regular slice
and that's because this can be confusing
when we slice one to three and then we
select age and visits
so there is a push to go ahead and move
to an eye location which does the same
thing you can see here bc it's the same
as up above there's also copy command so
we can do df3 equals df2 copy we're just
going to create a straight copy of it
and of course if we do df3
it'll be the same as a df2 on there so
df3 equals df2.copy
and then let's do df3 dot is null so
we're looking for null values
and this will return a nice map and
you'll see that everything is false
except when you go up here under the cat
or h they had a null there and so if we
go damn a couple up here also underneath
of let's see the dog okay there's a
bunch of nulls in here there's d up here
so let's look at d down here and you'll
see false true there it is there's our
null value so we can create a quick
chart of null values you can use this to
do other things we can leverage that
null value to maybe take an average or
something and fill those null spaces
with data and we can also modify the
location so here's our df3
location
and notice this is location not i
location ilocation has i for integer
location uses the
in this case the variables on the left
and what we can do on here and we'll go
and just set this equal to
1.5
and then let's um i'll pick a spot
let's go back up here where we had let's
do f
a just let's see what are we looking at
oh here we go let's do f and h
and up here f is set to age of 2.0 and
we find out that that's incorrect data
so we go ahead and switch to df3
equal and then we're going to print out
our df3
and if we go to f and age it is now 1.5
so we're just changing the value in the
df3 and this is changing the actual data
frame remember a lot of our stuff we do
a slice
and like it returns another data frame
this changes the actual data frame and
that value in the data frame
so we've covered uh location and eye
location is null making a copy here's
our eye location which is equivalent of
a slice and also selecting columns
so now we want to dive just take a
little detour here and let's look at df3
means
and this is kind of nice because you can
do this you can either do this by as you
can select a single column here by the
way you can just add the column
selection right here like we did before
so we could have age
look up the mean that just creates a
series if i run that there's our age
but if i take that out instead of
selecting it we can do the whole setup
and it has age and visits so why doesn't
it have priority or animal
well those are not integers so it's
really hard
they're non-numerical values so what is
the average i guess you could do a
histogram which probably will look at
that later on but the only two things we
can really look at is age and visits and
we have
the average or the mean on the age is
3.375
and the mean on visits is 1.9
and let's do df3
visits we'll go and steal the visits
again
and remember all those different
functions we looked at for a series well
we can do those here we can do the sum
so if we run that we'll see that these
sum up to 19. we could also look up
minimum if you remember that from before
the minimum is one
max
so all that functionality is here
i'll just go back to summing it up and
adding it all together so real quick
we've shown you how to take the series
operations and put them into the data
frame and then we can actually this is
interesting one we can just do df3 sum
run and you'll see the different
summations on there
it just combines them i like the way it
just combines the strings on there for
priority and animal we've looked at is
null we've also looked at copying along
with the different slices which we
talked about earlier so let's talk about
strings let's dive into the string setup
on there and let's go ahead and create a
string series string equals pd series
and we just put it right in there we
have a c d a a b a c a popped in a null
value cow and owl i don't know why they
picked cal and al in the background
someone must like those animals and of
course we can just do string if we run
that you'll see
leave the r out we'll get an error but
if we put it in there you'll see that we
have a simple series 0 a 1 c 2 d and it
automatically indexes it 0 to 8.
and then we can go string dot lower so
when we're talking about our data frame
in this case or our data series string
in this case we use the string
function str and we're going to make it
lower and if we go ahead and put the
brackets on there and you'll see that
we've gone from capital a capital c so
on to abc and baca cba cow al they were
all lower case already and of course if
you want to go lower you can also do
upper we'll go ahead and run that and
you can see we now have a c d a a baca
everything's capitalized except for the
null value which is still null all right
so we looked at a few basic string you
can see that string functions upper and
lower we're going to jump into a very
important topic i'm even going to give
it its own
header on here because it's such an
important topic what do you do with
missing values
panda has some great tools for that so
we'll dive into those
we'll call we'll work with df4 and if
you remember the df copy from above
we're just going to make a copy of df3
and let's just take a quick look at the
data we're working with oops df3 forgot
the three on there there we go
so here we have our cats snakes and dogs
hopefully not all in the same container
because that would be just probably mean
to all of them so we made a copy we're
going to be working with df4 and the
reason we made a copy is we want to go
ahead and fill the data and we just
simply do fill in a and then we're going
to give it the value we want to put in
there we'll give it the value 4. so i
can run in here and you'll see now that
df4
now has where the n a was is filled with
the value of four same thing down here
a lot of times we'll compute the mean
first so i might do a mean
age
equals df4 and then we want to go ahead
and do age
and dot mean
and then i'll do something like this df4
i only want to select the age
and i want to fill that
with the mean
age i run in there and you'll see that
our df4h now has the means in there just
a quick way of showing you how you can
combine these let me go back to our
original one there we go and run that
and keeping with good practices df5
equals df three dot copy
and we'll print our df5 which should be
the original one
and then on the df5 we can now drop our
missing data
i'm going to simply drop in a
and we're going to use how equals any so
i'm going to drop any row that has
missing data in it and you'll see we had
d here with missing data and h
and then let's go ahead and see what df5
looks like when we do that
there we go and there it is d is gone
and so is h so we create a new data
frame off of this missing those values
now if you have a lot of data dropping
values is a good way to take care of it
because you don't miss some data if you
have not a whole lot of data you're
working with like the iris data set or
something like that or something small
you want to start trying to find a way
to fill that data in so you don't lose
your computational power of the data you
got
so just a quick look at processing null
values
or missing values you can fill them
usually with the means some people use
medium or the mode there's different
ways you can fill it one way is means
and we can also just drop those rows
those are the two main things we do with
missing data
here we go uh we're going to cover next
this is i so love data frames for this
file operations
it saved me so much time because they
have so many different tools for
bringing data in and saving data
so we're looking at the data frame file
operations it's really streamlined i
don't know how many times they'll go on
to
different data downloads and they'll
have panda download standard on there
just because it's so widely used so
let's start with the most common file is
a csv so we have df3 to csv or animal
and let me just show you what the folder
is going into
right now i have some untitled and a few
things in here but nothing labeled
animal so we go ahead and run this
and this has now saved the animal to
my hard drive and you can now see the
animal folder up here and if i let's do
edit with a notepad oh let's open up
with just a regular notepad there we go
or wordpad if i open that up you can see
it's comma separated our titles they
don't have an index on the categories on
the top and the index comma then all the
different data is separated by commas
standard csv file on there and if we're
going to send it to csv and notice the
format is dot 2 underscore csv
and it's just the name of the file we're
sending it to you can also put the
complete path by default it's going to
go whatever the active directory this
program is running on that's why those
other folders are in there so we have
our df3 to csv and then if we're going
to put it in there we want to also get
it back out and we'll call this one df
underscore animal equals pd read
underscore csv
i always have to remember is two
underscore csv and read underscore csv i
always want to do like a capital in
there and not the underscore we're going
in here again is the active directory so
if i now do print out my df animal
and let's just do the ahead we only want
to look at the first three lines so if i
go ahead and run this
we'll see the first three lines and they
should match up here what we saved to
our csv
so very easy to save and import from our
csv files on here
and it turns out df
3 also has a 2 xl they actually have a
lot of different formats but you know
old school
excel was real popular for so long still
is we can go ahead and save it as
animal.xlsx
we're going to call the sheet named
sheet1
and then i can also do df we'll call it
animal2
animal2 and this one's going to come
from in the same format on here there we
go so we still have our animal xlsx
the sheet one that's where it's coming
from index columns equals none so we're
not going to we're going to suppress the
indexing on the columns n a values and
it'll just assign that 0 on up on your
indexes so if it says index columns
equals none that's what it does and then
we've added null values because there's
no values in here and we want to just
make sure that they're marked as n a
and we'll go ahead and just print out
the animal animal 2 there we go and
let's run that let's make this let's
just do the whole thing so we'll go
ahead and run that and it probably
doesn't help that i completely forgot
the read
so animal 2 equals pd.read
excel there we go excel so now we go
ahead and run it
and what we expect is happening here we
have the same data frame on here and if
i flick back to my folder you can now
see that we have the animal one of these
is in excel and one of these is a csv on
here and so there's our two file types
on there and they have other formats
these are just the two most common ones
used and i don't know how many times
i've had stuff from excel i need to pull
out if you've ever played with excel
it's a nightmare in the back end because
of the way they do the indexing
so this just makes it quick and easy to
pull in an excel spreadsheet so we
looked at two different ways to bring
data in and save it to files we've
looked at all kinds of different ways of
manipulating our data set and slicing it
and creating it for our data frame let's
get in there put your visualization
always a big thing at the end because
one it lets you check to see what you
did make sure it looks right and then
also if you're going to show somebody
else it makes it very clear what's going
on if they see something visual so this
is where a really important part of data
science is so let's go ahead and bring
in our tools we're going to do import
numpy as np we want to make sure we have
our amber sign matte plot library in
line this just lets jupiter know that
we're going to print it on this page if
you're using a different ide you don't
really necessarily need that but this
does help it displays correctly in
jupiter notebook and if you remember for
earlier we could create a uh we're gonna
call it ts we're gonna create a pandas
which are cute cuddly creatures versus a
pandem short for pandemonium no so we
have ts equals pd series and we're just
going to create a random
setup of 50. we'll do an index we'll set
it equal to the panda's date range today
periods equals 50 so the 50 should match
and i want you to notice something here
i did not import the matplot library why
because it's already in there pandas
already has its built-in connection and
interface with matplot library so you
don't have to import it
and we'll go ahead and do ts
equals ts dot
cumulative sum we're gonna do the
cumulative sum
so a little reformatting there and we'll
go ahead and plot it and let's take a
look at what that looks like
so we have a nice graph here we have the
dates on the bottom we set this up so we
have a nice range between in this case
minus four to looks like about two maybe
or one minus four and one
so what we've done here we plotted a
basic series just a single row of data
and we've set indexes on there but we
can also do the whole data frame on
there and let's see what that looks like
so first let's go ahead and create the
data frame we have here random numbers
we're going to do 50 by 4 and then we'll
go and create columns a b x and y just
because we can index is the ts.index on
there so we're gonna use the same index
as before
just to keep it nice and uniform we've
already generated the dates to go with
it and then we can do just like we did
with the series
we can also do with the data frame
df equals df cumulative sum
so we're going to sum the whole data
frame and then we'll do simply df plot
and let's put that in and let's go ahead
and run this and look how easy and quick
that was to generate a nice graph with
all the different data on there so we
have our shared index we have the shared
columns and then we have the different
data from each one that we can easily
look at and compare so very quick way of
displaying data you can imagine if you
were working in oh i think i mentioned
stock earlier because i've been doing
some analysis of stock lately so you'd
have your date down here and then you
would have stock a stock b stock x y
whatever it is and you can put them all
on one chart and see how they
what they look like next to each other
and this isn't too far off from what
some of those graphs look like and this
is just randomly generated so stock has
a lot of randomness in it which is one
of the reasons i actually play with it
for doing some of my models on for
testing them out
now there are a lot of features in
pandas so we're going to show you one
more thing on here there are some of the
things like i didn't go too deep we
looked at the top two for importing data
from a csv and from an excel spreadsheet
showed you how to quickly plot the data
there's more settings in there you can
do
we're going to do one more thing down
here and this is kind of a fun one
changes to a markdown and run that
so how would you remove repeated data
using pandas
and this is where you have a data set
that comes in and maybe it's a feeding
from one location and instead of noting
that it's repeated the date like oh
let's go back to stocks that's a good
visual we have the stocks from the 23rd
and it adds another row and it's the
same row it's importing the 23rd again
and again so now you have that data
repeated three times and you need to go
back and figure out how to get rid of it
how do you track that down
so let's start by creating a quick
database our data frame not a database i
keep saying databases a data frame and
we'll just make this data frame has
using our dictionary going in this data
frame only has one data series in it
which is fine so if we do df to print it
out you'll see a
one two two two two four five four five
six seven so on so how would you remove
that well there is a neat feature in
data frames called shift
along with another feature that lets us
select just certain date information
and we'll go with
the location function put that in
brackets remember that from above
location and then in the location let me
just spread this out a little bit so
it's really easy to read in fact i'm
going to go upscale on that since we're
doing some a little bit more complicated
here
what you can see on this on the location
is i have dfa dfa.shift
so this is going to shift up one by
default you can actually change this to
two or three you can even do a minus one
and it shifts the other way but it's
going to shift up by one by default and
it's going to say if that does not equal
df of a
then we want that and if you look down
here we had one two two two two two when
we run this logic on here and we do the
shift
it now gets rid of all the duplicates so
we went from one two two two two four
four five whatever it was here it is one
two two two four four four five five
five six six six to one two four five
six seven eight and you'll see on the
index it just deletes them out of there
so the index stays the same obviously
you don't want the dates to change if
you're working with an index dated setup
so it just deletes those duplicates out
of there this is just a quick way to
introduce you to
one the fact that you can add logic
gates into here and two the eye location
allows you to use shift so there's the
shift function and then the i location
selects that based on true or false
wow so we've actually covered a lot
today in pandas we've really covered
into the basics of selecting your
different series out of your column out
of your data frame how to index rows how
to slice how to plot
hopefully you'll take this beyond that
and start combining these different
things and you can create long strings
and really explore your data generate
some nice graphs if you're in jupyter
notebook it's a great demo to show
others
and i didn't know this about jupiter
notebook you can do this in jupyter
notebook and then you can download and i
always i never really look too closely
at all the downloads
which you now load as an html and post
it to your blog so it's got a neat
feature in there but any of this is
really powerful tool all of this is
really powerful tools for doing your
data science if getting your learning
started is half the battle what if you
could do that for free
visit skill up by simply learn click on
the link in the description to know more
today we're going to study the matplot
library and the python code so what's in
it for you
what is matte plot library types of
plots plotting graphs and sub graphs
adding a graph inside a graph graph
parameters title label legend line
graphs line types color and transparency
canvas grid and axis range 2d plots
scatter step bar fill between radar
chart histogram contour image 3d surface
image and then we'll hit a practice
example pie chart
so let's start with what is matte plot
library map plot library is an open
source drawing library which supports
rich drawing types it is used to draw 2d
and 3d graphics
and there are so many packages in the
matplot library we're going to cover the
basics and there are so many packages
that sit on top of the maplight library
that we can't even cover them all today
but we'll hit the main one so you have a
good understanding of what the matplot
library is and what the basics can do
you can understand your data easily by
visualizing it with the help of matplot
library you can generate plots
histograms bar charts and many other
charts with just a few lines of code and
here we have some basic types of plots
you can see here that we'll go into we
have the bar chart the histogram boy i
use a lot of histograms in my stuff
scatter plot line chart pie chart and
area graph
let's start plotting them and to do this
i'm going to be using jupiter notebook
you can use any of your python
interfaces for programming or scripting
and running it of course we here really
like the jupiter notebook for doing
basic a lot of basic stuff because it's
so visual and in our jupiter notebook
which opens up in this case i'm using
google chrome you can go up here to new
and we'll create a new python 3 and set
that up
if you're not familiar with jupyter
notebook we do have a tutorial that
covers some of the basics of that you'll
look at any of our tutorials i usually
cover a number of them showing how to
set up jupiter and anaconda i myself use
jupiter through anaconda in fact let's
go ahead and open that up and just take
a look at this see what that looks like
you can see your anaconda navigator if
you install it it will automatically
install the jupyter notebook but that
also installs a lot of other things i
know some people like the qt console for
doing python or spyder i've never used
them i actually use notepad plus plus as
one of my editors and then i use the
jupiter notebook a lot because it's so
easy to have a visual while i'm
programming an even simple script in
python i'll take it from the jupyter
notebook and then do a save as you can
always go under file and you can
download as a python program so that
will download it as an actual python
versus the ipython that this saves it as
so let's go ahead and dive in and see we
got going here and let's go ahead and
put matplot library tutorial and i'm
going to turn this cell into a mark down
so it doesn't actually run it
you can see it has a nice little title
there that's all jupiter notebook
and then from mat plot library
let's import
pi lab
back one and then let's go ahead and
just print we'll go pi lab
and the version let's go ahead and run
this so we're going to import our pi lab
module from the matplot library and we
find out that we're in version 1.15.1
always important to note the version
you're in probably i was reading an
article that said the number one thing
that python programmers struggle with is
remembering what version they're working
in and making sure that they're going
from one platform to the other with the
same version and if we're gonna graph
things i think we need some data to
graph so we're going to import numpy as
np now if you're not familiar with numpy
definitely go back and check out our
numpy tutorial there's so many different
things you can do with it dealing with
reshaping the data and creating the data
we're just going to use it to create
some data for us
and there is a lot of ways to create
data but we're going to use the np.line
space so we're going to create a numpy
array and the way you read this is we're
going to create numbers between 0 and 10
and we're going to create 25 of these
numbers so we're just going to divide
that equally up between 0 and 10. and if
we have x coordinates we should probably
have some y coordinates and we'll do
something simple like x times x
plus 2 and let's just take a look we're
going to print x
and print
y
let me go ahead and run this
and let's see we got going on here so we
have our x coordinates which is 0 0.4
0.83 etc and you can look at this as an
xy plot so we have 0 we have 2. we have
0.416 we have
2.17 and just as a quick reminder we're
going to do print np array x comma
y.reshape 25 comma 2. and the reason i
want to do this is i want to show you
something here
a lot of times a program returns
x comma y and it's an array of x comma y
x comma y x comma y
and so when you're working with the pie
plot
you have to separate it out and reshape
it so if i start off with pairs like
this i can reshape them if i know
there's 25 pairs in there i can switch
the 2 and the 25 and this is kind of
goofy but we'll do it anyways reshape so
i'm going to reshape my 25 by 2 back to
2 by 25
and if i run that you'll see i end up
with the same output as the x y the two
different arrays in here
and this is important that we want x and
y separate
again that's all numpy stuff but it's
important to understand that this is a
format that matplot library works with
it works with an array of x's and they
should match your array of y's so each
one has 25 different entities in it
and then for our basic plotting of this
data it only takes one command to draw
graph of this data and so we use our
from up here where we imported pi lab we
take our pi lab and the key under there
is plot for plotting a line and then we
want our x coordinates and our y
coordinates and we'll throw in r and the
r simply means red so we're going to
draw the line in red let me go and run
that
you can actually switch this around if
you wanted to do different there's b
for blue we have a lot of fun yellow
hard to see yellow there we go but we'll
go ahead and stick with red
run
and when you're doing presentations with
these try to be consistent you know if
the business and the shareholders send
you a
spreadsheet and they have losses in red
use red for losses in your graph
try to be consistent use green for
profit for money you don't have to
necessarily use green but it's whatever
they're using whatever the company is
using try to mirror that that way people
aren't going to be confused if you
switch your data around every time one
graph has red for loss and one graph has
blue for loss it gets really confusing
so make sure you're consistent in your
graphs and your coloring and something
to know because we're going to cover
this in a minute this is your canvas
size so we have a canvas here and what
we're going to do next is we're going to
look at sub graphs okay
so let's take our pi lab and create a
sub plot
and one of the things also to know when
we're working with the
matplot library i'm not setting when i
do this this is my drawing canvas the pi
lab so once i've imported the pi lab i'm
drawing my images on there very
important to know and with the subplot
we're going to give it some different
values
and we're going to represent by rows
columns and indexes and let's do one two
one so it's going to be the first row
second column and the index is like you
can stack your graphs and things like
that we don't worry too much about
indexes but rows and columns we want to
go ahead and use row one and column two
and if we're going to have one object we
should probably have two but before we
do that we have to plot data onto the
subplot so the order is very important
and we're going to stick with our x
comma y and let's do this we're going to
add in
a third parameter here remember we did
red we're going to add shorthand dash
dash for dashed lines so this plots the
data into row one column two
and if we're going to do that let's do
up another one
pilab.subplot and if we're going to do
row one let's do
column two and index two
and this time we're going to add g for
green and this denotes a style and if
we're going to set up our pylab
subplot there we go right lab we've got
to go ahead and plot that pi
lab plot
and instead of x y we want y comma x
oops i messed up this is in the wrong
spot there we go we'll move that down
here real quick because that goes in the
plot part so the subplot tells it the
row column and index and the pi plot
tells it what data in this case we
switched them and the color and then the
style shorthand now let's go ahead and
run that
and you'll see it takes this canvas
splits it into
and now we have two different graphs and
we have the red one with dashed lines
and we have the green one which is has
little stars going up and if we take
this and let's just um just for fun
let's change this and run that with an
index of one it puts them both on the
same index it also gives me a warning
because it's a strange way of doing two
subplots there's depreciated there's
another way to do it but most people
just ignore that warning because it's
not going to go away anytime soon now
that's using the same setup what happens
if we do
instead of
this let's change the column on here and
find out what happens
and if we do the column
it didn't really like that on the setup
it just disappears so let's keep our
column as 2 and let's change the row on
the second one to 2
and run that
and you'll see again it kind of squishes
everything together and causes some
issues so let's take the index so these
need a unique index and you can see here
where i made some changes i said row two
and look what happens when i change to
column two so i now have row two column
two index two i squished it up here so
you could put another graph underneath
is what that does and there's all kinds
of different things you really have to
just play with these numbers till you
get a handle on them because
you know you have to repeat it 164 times
according to cambridge university if
it's completely new to you and you can
see right here where there you go three
run there we go but you can see it takes
a little bit sometimes to play with
these and get the numbers right
hopefully hit the wrong one that's why
let's go three there three there run
there we go now it's overlapping so i
have this doubled over here on the right
for now we'll just go ahead and leave
this with the
where we have column and row two and the
two different indexes so they appear
nice and neatly side by side
and then as we just saw as we were
flashing through them we can put them on
top of each other
and let me just highlight that and copy
it down here
paste it down there and here we have one
two one and then we'll do one two one
also for this one and that puts the two
subplots directly on top of each other
gives us that warning and you can see we
now have two different sets of data
graphed on top of each other and you can
also see how it did the indexes since
one of them is from 0 to 10 that's the
green one on the x axis and the other
one is from 0 to 10 on the y-axis so it
took the greatest value of either one
and then used those as a shared value
so let's
next look at operator description and
we'll go ahead and turn this cell into a
markdown and run that so it looks nice
so fig and you remember i talked about
the canvas earlier i briefly mentioned
it we're going to look a little bit more
at the canvas later on but that's what
the figure is fid we're going to add
axes so we're going to initialize a
subplot add the subplot in rows and
columns and all kinds of different
things with this you can do let's look
at that code and see exactly what's
going on and i want you to notice that
there's fig which is the actual canvas
in the matplot library and ax is
commonly used to refer to the subplots
so we're creating subplots you'll see ax
equals plt subplot
earlier we did the pi lab so let's go
ahead and import pi plot from matplot
library and we're going to do it as plt
you'll see that a lot that's really the
standard in the industry is to call it
plt just like pandas is pd and numpy
array is np certainly you can import it
as whatever you want but i would stick
to the standards and we're going to do
the same graph as we did above
with the
pi lab but with the plt so if it looks
familiar there's reason we're doing this
because we want to show you how the
figure part works and working with the
canvas goes but we're going to do the
same plot as we did before and we'll
call it fig and we're going to set that
equal to plot figure so there's our
figure or canvas on there and let's
create a variable called axes and we're
going to set that equal to
fig dot add
axes
and in this we're going to control the
left right the width the height of the
canvas from zero to one
and so we can go ahead and i'm just
going to put some stuff in there i got
point one point eight point eight so
when you're looking at this this is a
zero to one or you could say fifty
percent ten percent eighty eighty
percent but it's a control it's going to
control your left and your right along
with the width and the height so the
width and the height we're gonna use
eighty percent and we're going to have
like a little indent on the left and the
right and this should look familiar from
above x use dot plot
x comma y
and then let's give it a color how about
red since we're recreating the same
graph let's keep it uniform oops and it
helps if i use a axis instead of ax es i
don't know where that came from but this
looks identical to the one we had up
above so here's our axis plot x comma y
of red
same graph same setup but this time
we've added a variable equal to the
figure dot add axes so our plot figures
our canvas our axes is what we're
working in and then our axis.plot x
comma y
and again we can draw sub graphs let me
put that down here
just like we did before and a little
different totation here we're going to
fig comma
axes equal
plt.subplots
and in here it's going to be the number
of rows
we're going to do one row
and columns equals two so if you
remember before that's what we did we
had one row with two different graphs on
it we're going to do the same thing but
know how we did this here's our figure
our canvas and our axes we're going to
create actually two different axes we're
going to create row 1 column 2. and so
axis is an array of information so we
can simply do
4
let's do x in axes
this will now look familiar x dot plot
we're going to do x comma y we'll go
ahead and make a red keep everything
looking the same remember nice uniform
graphs everything looks the same and if
we go ahead and run this
you'll see we get two nice side-by-side
graphs so just as we had before the same
look the same setup
and just for fun let's change in columns
to three we'll run that and now you see
we'll have three on there and let's see
if we make it a little bit more
interesting we'll do in rows equals to
two and you can see down here we're
going to get an attribute error because
it's trying to scrunch everything
together
so it does have a limit how much stuff
you can put in one small space that's
important to know you can fix that by
changing the canvas size which we'll
look at in just a minute and there's
other ways to change it on here but here
we go we can do in rows two and columns
equals one you can see two nice images
right above each other we'll go back to
the original one row two columns side by
side left to right and
we can also
draw a picture
or graph
inside another graph
and that's kind of a fun thing to do
it's important to note that we can layer
our stuff on top of each other which
makes for a really nice presentation
so let's start by uh fig let's create
another figure so we're going to start
over again with our canvas
we set that equal to plt.figure
so there's our new canvas and let's do
axes we'll call it axes one and two axis
one equals fig dot add axes remember
this from earlier
and
this here similar numbers we used before
saying how big this axis is this figure
in the axis is so this is going to be
the big
axes and let's do axes 2 equals
another figure at axes and then
0.2.5.4.3
and if we're going to do this they need
data on them so let's go ahead and plot
some data on our axes so axes1 dot plot
and we'll make this simply x comma y
comma make it red
and then let's go axes2 dot plot
and let's reverse them y comma x comma
green there we go doing what i told you
not to do you shouldn't be swapping axes
around and plotting your data in five
different directions because it's
confusing let's go ahead and run this
and see what this looks like and then
let's talk a little bit about this we
talked about the 0.2.5.4.3
and let me just grab the annotation for
that that's left right width and height
so we have in here that this is going to
be left right so here's our left is
point one in point five and we you know
what let's just play with this a little
bit what happens when i change this to
point one moves it way over to the left
so there's our point one so we can make
this point four run that there we go so
you can see how you can move it around
the branches on here 0.2
0.5 is the
left so that's our right so see what
happens when we do point oh let's make
this point one
that actually is they had it down at
left right i thought this was wrong it's
actually how far from the bottom let me
switch that on here bottom there we go
so we had here on this we can go ahead
and put that back to 0.5 and run that
and this is point three let's make this
point three also and that is the width
and then of course there's the height we
can make that really tiny actually let's
do 0.2
and let's run that and you can see it
changes the height on there we make it
even smaller 0.2 by 0.2 and as you can
see you can get stuck playing with this
to make it look just right it can
sometimes take a little bit
certainly once you have the settings if
you're doing a presentation you try to
keep it uniform unless it doesn't make
sense for the graph you're working on
try to keep the same colors the same
position and the same look and feel
and i mentioned earlier we can adjust
the canvas size so this is from earlier
i just copied it down below we're going
to re-plot the same data we've been
looking at and what we can do is we can
change the figure size to 16 by nine let
me run that and show you what that looks
like so it fills the whole screen and
then if you are normally when you're
working on the screen you don't worry
too much about this but we can set the
dpi to 300
run that
there it goes this is your dots per inch
and if you are doing an output of this
and you're printing a hard copy you want
the higher quality i would suggest
nothing under 300 if it's a professional
print you might get a little less than
that but whenever i'm doing professional
graphics and printing them out on
something 300 dots per inch is kind of
the minimal on there you can go a lot
higher too but keep in mind the higher
you get the more memory it takes the
more lag time and the more resources you
use so usually 300 is a good solid
number to use your dots per inch and you
can see it drills a nice it draws a nice
large canvas here which is 16 by 9 and
then the dpi is 300 on here so it's a
little higher quality and just out of
curiosity i wonder how long it takes to
draw something double that size 600 and
you can see here where at 600 dpi it's
going to take a while there it goes just
because it's utilizing a lot more
graphics on there and let me just go
back to the 300 now we'll actually do
let's do a 100 you're not going to see a
difference on this because it is web
based graphics are pretty low
and up here you saw i did this with the
plot figure this works the same if i do
figure axes subplot figure size and then
we'll go ahead and do axes
dot plot
x comma y comma we'll stick to red
let's go ahead and run this and you
should get almost the same thing here
here's our
axes on the subplot on here with the
fixed size and the dpi let me take this
all out let me just remove all that real
quick run it again there we go now we're
back to our original figure and let's
look at some of the other things you can
do with this
one things we do is we can set a title
for the axis so axis set title you'll
see right here since i put this on the
axis it's the main title for the whole
graph
and if you're going to have a title you
should also label so we can label our x
label and we can set our y label in this
case we're just going to call it x and y
keep it nice and uniform and if we run
this you'll see that we've added a nice
x label and y label whoops where'd they
go and it turns out in this environment
that you have to put it before the title
so let me go ahead and put it before the
title and there's our xy let me run that
and of course we can also do upper size
a little bit and see what's going on a
little better so here we have x label x
if you come down here you'll see our x
label and our y label we can of course
change this to x
label you can change this to y
maybe whatever you want on here of
course and our title graph there we go
run so here we have our title graph our
y label and our x label all set up on
our nice little plot and then before we
move on to the next section let's do one
more thing on here we have a thing
called the legend and we're gonna do
we're gonna set our ax legend label one
label two up here it's a format for it
but let's go down here and actually use
it i'm going to do two different plots
we're going to have axes plot x by x
times x squared and x cubed and if i run
this you'll see it puts two nice graphs
on the setup on there but it's nice to
have a legend telling you what's going
on so for the legend we can actually do
axes since we have the two plots legend
and on here we've created an array
and we have y equals x squared y equals
x cubed you can actually put this as
whatever you want those are just strings
and then location two and let's go ahead
and run this and see what that looks
like and you can see it puts a nice
legend on the upper left hand corner
location two we can do location three
and run it and it drops it down to the
bottom location one i can't remember
where that's at there we go upper right
so each one of these is a number that
refers to the different locations on the
screen zero kinda have to play with them
or look them up to remember where
they're at but they do work it just kind
of moves around depending on where you
want your legend out on there so on this
section we cover the title of the graph
the y labels and legends this is we're
getting into some starting to look
really fancy here so we now have
something we can actually put out you'll
see the title the graph looks a little
fuzzy so i might in a web setup put the
dpi up a couple notches maybe put it at
200 100 might work fine just so you know
something to notice on here when you're
playing with these different things we
had our subplots dpi equals oh let's do
200 to see what that looks like
so you can see now it's a lot clearer
it's also larger so it's a nice little
feature you can throw in there with your
dpi dots per inch
so the next section is let's look at
some graph features we're going to look
at line color transparency size and a
few more things on here and oops i
forgot the main title so we have our
figure in our axis equals our plot and
subplots and i'm going to do a dpi
equals 150 so the graph comes out nice
and large and easy for you to see
and let's go ahead and do three plots on
here we'll do x by x plus one so it's
just going to be a straight line plot
x plus x plus two
and axis dot plot
x x
plus three this looks like we're doing
nearest neighbor setup we're showing how
it uh located data putting your lines on
there between the nearest neighbors
there we go so it draws a nice little
graph with three lines on it one of the
things we can do is we can control the
alpha on this oops and you can actually
see the um when they did these lines it
automatically pulls in different colors
for your setup so there's some automatic
automatic things going on in there and a
lot of times we do that comma r where
we're going to do color equals red
another notation on here let's go ahead
and run this now we have a bright red
line down there and with the matte plot
library you're not limited to red you
can also use the one of many different
color references as you see here with
the pound sign one one five five dd
which just is just blue and we can do
the same thing with another color on
here which it turns out to be green i
can just as easily do this green
blue oops there we go blue and run that
and you'll see here we have red blue and
green and what i want to do is i want to
make this we're going to say what's
called the alpha on this and we're going
to set this equal to 0.5
so this is halfway see-through when i
run this and it's almost going to look
pink because you can see through it and
let's change this just a little bit just
to make this kind of fun let's square it
there we go run it so now we have this
nice square that comes up and you can
see when it crosses it because i plotted
these two lines after it and they have
no alpha the red is behind those lines
or in this case pink because we did the
alpha halfway through so let's go ahead
and do this alpha equals 0.5 and oh you
know what instead of squaring it let's
take it to the 0.5 power that'll be kind
of interesting to see what that does
we'll just go to keep it squared there
we go
and run that and let's go back and look
at this where it crosses over and the
first thing you see right here is on the
blue it's kind of light blue now you can
see how the two colors add together you
get almost a purple on there so i can
clearly see where the red crosses the
blue line and then the green just blanks
it over because i didn't do any
opaqueness no alpha on there
so this is great if you have lots of
data that crosses over and you need to
be able to track those lines better and
we'll go ahead and do this .5 and we'll
run that oops i did
equals 0.5 let me go ahead and run that
and so you can see right here now you
can easily see the red line how it
crosses the green and the blue down here
and if we want to we can do this as the
default is one solid so we can change
this all to point eight let me just do
that
oops 58 there we go run
oops i must have hit a wrong button
there let me try that again i actually
get rid of a bracket and let's go ahead
and run that
and we come down here and look at this
you can still see where it passes behind
them but the green dominates and the
blue dominates because we're now at 80
instead of 50
when you can do less that's kind of fun
although at some point the lanes kind of
fade
so 0.5 is usually the best setting on
there we have a nice pastel here at 0.3
and you can easily see where they cross
over and just like you can play with the
colors we can play with line width and
you know let's do
let's try dpi 100 and see what that
looks like on my screen equals 100 and
we'll go ahead and just take our ax plot
let's do four of these lines just you
can see how they look next to each other
real quick here there we go
if i run this they should all appear the
same it automatically does different
colors on there so let's do color equals
blue
forgot my quotation marks there we go
and we'll go ahead and just make these
all blue just for purposes of being nice
and uniform and then what i want to do
is i want to do the line width
width equals
0.25
and let's just copy and paste that down
here
let's do equals one
about 1.5 and let's do one let's make
this equal to two let's see what that
looks like and we do that you can see it
goes from a very thin line a point five
a one are one point five and two which
is twice the width of the one and if
we're going to do different sizes we had
different colors we had our alpha scheme
let's take this whole thing here
let's paste it down here and do another
one
but instead of line width
let's look at styles and something to
know here you can actually abbreviate
this with lw so line width can also be
point let's just do everything point two
and let's set up a line style we'll do
the first one dashes let me just paste
that down here
so i'm not doing a lot of extra typing
there we go
take this out so we have our dashed
we can do a dash dot let's do the dash
dot here and a colon here there we go
and there's a lot of different options
we'll look at a few more as we go down
for different ways of highlighting data
but when you look at this we have
everything is a line width of two and
now we have a straight line we have a
dashed line or a dot dash and a dot dot
dot line
and then another thing we can add on
here is we're going to do here's our ax
plot and we did x let's do x plus um 4.
so it goes right on the top then do
color black line width 1.5 so it's a
smaller line and we're going to take the
line and we're going to set dashes so
look i've changed some of the notation
here for my line and my ax plot so i can
set my line comma equal to x plot and
then i can change the line settings this
way and when i run this let me run that
on here you'll see the 5 10 15 10
creates a series of dashes
that are buried in link
link in this case they alternate between
a short dash and a long dash we can play
with these numbers curiosity always has
me what happens when you play with the
numbers just to see what they look like
let's do this let's paste this down here
i'll do two of these just because
they're kind of fun to play with and
let's change this from 10 to
3 and we're going to change this one
from 15
to
4. and let's run that and you can see
the differences in the lines oops very a
little bit confusing on there because i
forgot to change the lines are all on
top of each other so let me change that
really quick here and let's run that and
now you can see here's our original
dashed line alternating when i change
these numbers on the second one the very
end value to three you can see now we
have dashes of five let's see i'm going
to guess this is a dash is a five skip
ten dashes of 15 skip three and then it
goes back to the beginning dash is five
dashes skip ten fifteen dashes skip
three and of course the last one we just
switched up a little bit it looks a lot
more uniform because i'm using two sets
of ten or if i did something like this
and change it to thirty it really
becomes pronounced as far as the
distances between them and instead of 4
let's go oh let's put 30 here also 30 by
30 there we go really pronounced on that
one and let's look at one more important
group for plotting our data and in this
we're going to here's our plot we
started with with the x plus one x plus
two x plus three and did it in blue on
this one so three or four different blue
lines
and this property we want to add the
actual plots so you can see where the
plots are on the graph and for that we
might have marker equals o and if we run
this you'll see it puts a dot for each
of these and there's 25 dots because we
have 25 x values so we actually have
zero and each of the different values of
x y are then plotted here with the dots
and we don't want to just limit
ourselves to dots
you can also do
plus sign that's another option dots is
most common i'll actually like the dots
the best if we do the plus sign you see
it puts a nice crosshairs or plus sign
on there and we can do a marker there's
a number of different markers you can
use
and i think this one was it s is another
one
which is a nice square and that's
actually a good one s for square o for
period okay that's just kind of weird so
you can see that probably on these
markers another one is the number one so
if we run that you'll see we now have
these little hatch marks
and let's take oh let's just go with the
o on this one
by the way this works with square really
nicely some stuff we're gonna do here on
just a second let's do
marker
size equals two and
change that to five and run that and you
can see here it puts a nice little tiny
dot versus a the size dot here this is
interesting because it said two i
thought it would be bigger
but if you do 0.5 it gets even smaller
and let's just do 10 to see what that
looks like run that looks huge so marker
size a lot of these are dependent on the
dpi and the setups there's things that
switch around as far as the way the size
shows up you got to be a little careful
when you change one setting it can
change all the other markers and then
let's take our square on here
and we'll do we have marker size so we
also have marker face
we'll set that equal to red of course i
mean change the so it's up one notch
we'll run that whoops must have mistyped
something on here and i did it's marker
face color equals red and so when i run
that you can now see i have the squares
on there with the marker face color of
course we can mix and match these
come down here and we'll make this
instead of let's make this plus seven
and we'll make this
size 15
marker face color
equals
and we'll do what green just because
there we go run very hard to actually
see what's going on there still 25 dots
they kind of overlap as you can see they
print them over each other of course if
we really wanted to make it look
horrible we could just make that really
huge
generally though you want something a
little bit smaller and cuter we'll just
try doing it this way there we go that's
too small to even see the face so four
you can start to see the face on there
around four and maybe an eight eight
might be a good number for this there we
go eight again that all just depends on
what you're trying to show and display
so we've covered a lot of stuff here as
far as our lines we've covered
opaque with our alpha setting on there
give us some nice pastels you can see
how they overlap and how they cross over
we covered the line width different size
on there different formats for the line
itself and these are all you can combine
all these so you can have our line width
equals two line style equals dash you
can bring this down here also to the
markers and then we added markers in
just entered a circle a
plus sign the square a little tick which
uses a one then we had a marker size and
a marker color face and we combine those
you see we get a nice different series
of representations we also briefly
mentioned color where you didn't have to
use like in here we used color black
someplace up here and have to find it we
use the actual number for the color as
opposed to i changed it to red and blue
so you get very precise on the color if
you have very specific color set that
you need to match your website or
whatever you're working on
all those are tools in the matplot
library so we have
one more piece to formatting the graph
so we want to show you and then we have
two big sections we're going to go over
the different graphs that they have
along with a challenge problem so let's
go to the last section we're going to
look at is limits we're going to limit
our data
so this first primer is going to paste
in there we're going to create our
subplots one two so one row two columns
we're gonna do a figure size of ten
comma five this should all look familiar
now since we've done a number of them
and we're gonna go ahead and plot and
this is an interesting notation you
should notice here our axis zero so one
we've used instead of you can just
iterate through them but they're just an
array so it's an array of zero is still
the axes of the first axes out of two
and we're going to plot x
x squared x x cubed lined with two so
we're gonna go ahead and just plot two
graphs right on top of each other
without doing multiple plots on here and
we'll set the grid equal to true one
here let's go ahead and run that and you
can see here our two plots with the x
value going across
and i'm going to do something similar
and by the way as you can just if you
look at it you can see the grid on there
that's all that is
easier to spot the data going across
we're going to take the same
data for axes 1. so we have our plot of
x x squared x and x cubed line with two
and this time we're going to take our
axes one and do y limit
it's actually set underscore y limit
this is the y axis so it's going to be
an array of two two values and we'll do
0 comma
60 i'm just making these numbers up the
guys in the back actually made them up
i'm just using their numbers
we're going to set the x limit
and we'll set the x limit as don't
forget our brackets there
two comma five
so it's the same data going in and but
we're setting a limit on it let's go
ahead and run that and let's see what it
comes out of and here we have the y
limit zero to sixty so we're looking at
just the lower part of this curve here
up to here and we have the x limit two
to five so that starts right here at two
and you can see very different graphs
this is kind of nice because you could
actually put one of these on top of the
other if you wanted to draw focus to one
part of a graph remember how we did that
earlier one inside the other but just a
quick note you can easily limit your
graph and re kind of reshape the way it
looks quite easily and we can also add
that grid down there if you want a grid
we'll run that and add the grid in there
oops i guess you have to do the grid
beforehand
switch that there we go sometimes the
order on this is really important so you
may double check your order when you're
printing these things out it also helps
if i change it to one so in this case
might not be the order i wonder if we'll
go back here as one there we go so it
doesn't matter the order and grid but
you can set the grid for easy viewing
here nice setup on there but you can see
how we can limit the data
so let's start looking at some other 2d
graphs and make this cell a markdown so
we run it as a nice pretty title to it
and let's go ahead and create some data
with an np array we'll do
0 to 5 on here there we go and let's
look at 4 common graphs we'll put them
side by side so we'll do a figure our
axes equals plot subplots one four
columns and then figure size hopefully
it'll fit nicely on here it seems to do
pretty good on here and i'll go and just
run that since we're in there run you'll
see i have my four blank plots on here
and we'll start with axes of zero let's
set title
and we want this to be a scatter plot
a scatter plot just means it has a bunch
of dots on it so here's our axis of zero
dot scatter easy to remember scatter a
bunch of plots on there we'll do our in
we can do x or n there we go and let's
go ahead and do axes set title scatter
already did that we're just gonna do
scatter
that's how you do it on there notice how
you create a scatter plot with simply
with the scatter control and we'll do
let's do the variable x
x
plus let's throw some randomness in here
usually scatter plots are i have a lot
of random numbers connected to them
that's why they do them on there and so
the bigger the x gets the bigger the
randomness so 0.25 times the randomness
and what we should end up doing here is
with the scatter plot and you can see as
you go up it just kind of has some
random numbers and moves up and down the
line
but plus just the points so if you
remember from back up here where we did
marker
this is plotting basically just the
marker so it's a scatter plot
probably less used is a step plot so for
exes one we'll go ahead and do a step
plot so you can see what that looks like
and this time we'll use our n value
instead of x we generated that n value
up here and so for this we have n
n times 2 r n squared n times 2 n
squared line width equals 2 and if we
run that it creates a nice step up
let's see so we've got a scatter plot
we've got a step
plot let's do a bar plot
and we'll use the same formula in n
squared alignment centered because you
can have them left or right with 0.5 and
alpha if you remember correctly that's
how opaque it is
let's see what that looks like on there
so we have some nice you can see here a
nice bar plot it should look very
similar to the step plot but colored in
and we can change the width let's see
what happens we do 0.9 run
and if we take width out completely
run that you can see it starts coming
together on there and we can change the
alpha we can take the alpha out too and
run that so now you have the solid
colors and if we take out the center
and run that
everything you really can't see the
shift on here because that's actually
the default on this but these are common
settings for the bar graph let me just
put them back in there there we go
alignment center and alpha now i can't
say i've used the step craft very much
there's certain other certain i guess
domains of expertise require a step
graph but the scatter plot and the bar
graph very common especially the bar
graph and we'll look at histograms here
in just a minute so i use histograms a
lot especially in data science but this
is nice if you have very concrete
objects somebody how many people are
wearing yellow hats that kind of thing
but if we're going to do that let's go
ahead and do the last one which i see a
lot more in the sciences certainly using
the data science but more like for
mapping i saw publication on
solar flares and they were discussing
the energy and so filling in the graph
gives it a very different look so we're
going to do the fill between
and it's just like you think it'd be
it's filled between but with a
underscore between them and we'll do x
and x squared and x and x cubed and
we'll do color green and alpha again in
case you had other data you want to plot
on there you can see it forms a nice
squared coming up here and also if you
look at the bottom one is your squared
value the upper line is your cubed value
and then it fills in everything in
between
if you remember from calculus this would
be if you had like a car a motor and
efficiency they would talk about the
efficiency going up and the loss and
you're looking for the space or the area
between the two lines so it gives you a
nice visual of that now let's look at a
few more basic two dimensionals so we
have our figure figure size on here
we're going to do a radar chart to be
honest i've never used a radar chart in
business or in data science i can't find
a reason to use one now so the first
line for doing a radar chart we have to
add axes and the figure and with this
this actually creates our
oh let's let's run it so you can see
what it creates it creates a nice looks
like you're on a submarine and you're
tracking the hunt for red october or
something like that and it needs all of
these the polar is the fact that we're
doing polar coordinates
zero zero point six point six has to do
with the size if you take out any of
these things and run them you get just a
box if you take out the other half you
pretty much get nothing in there and if
you change these numbers and change them
a little bit you can see it gets bigger
they had 0.6 on here i'll go ahead and
leave it as one because that's just kind
of fun that's all about the size on here
the height and the width and then let's
create some data t equals np line space
and this is 0 to 2 times np times pi so
if you remember that is the
distance across and we're going to
generate a hundred points
so this is just a thing of data we're
putting together then we simply do an ax
dot plot and in this case let's do t
comma t
which would be a diagonal line on a
regular chart and we'll give it a nice
color equals blue
and line width equals three let's see
what that looks like and we can see here
a spiral coming out remember this would
be just a diagonal line on a regular
chart what happens if we take this and
instead of t
times 0.5 there we go
and you can see it slightly alters the
way it spirals out we could do t times
two spirals that a little quicker so
it's kind of just a fun i've like i said
i've never used a
radar chart it's a column but you can
always think of radar submarine kind of
looks like one of those or in an
airplane
and none of this would be complete if we
didn't discuss histograms oh my gosh do
i use a histogram so much
and we'll use our numpy that we have set
as np to generate oh looks like we have
a hundred thousand variables we're going
to set equal to n and of course we
create our figure and our axes subplots
one two figure size 12 14. so we're
going to look at two different
variations of the histogram and we'll
set a title default histogram set our
title there and then this is simply
hist for histogram and we'll just go
ahead and put in our n in there and let
me run this and see what that looks like
and let's talk about what is going on
here so we generated an array here of
data 1000 random arrays it looks like
they're mostly between -4 and 4
and then it adds up each one it says 0
you have
35 000 that are zero so that's what's
most common on here and we have 20 000
that are somewhere in this range right
here between the minus two and well it
looks like one minus two and somewhere
between zero and 1 there's 30 000
numbers so all this is saying is this is
how common these variables are and this
gives you this point in so many
directions when you're looking at data
science to go ahead and run your
histogram so you should always have your
histogram and you can always put limits
and all the other different things on
your array just like you did on the
other graphs on there and then we're
going to do a cumulative detailed
histogram
and all it is is a histogram let me just
do that
and we set cumulative equal to true
and bins equal 50. and i really want to
highlight the the cumulative equals true
is important but we can now choose how
many bins we have in the first one it
kind of selected them for us in this
case let me go ahead and run this and
you'll see it has that prints the data
out for us and here's our whoops
must have missed oh there we go it
doesn't help that i put it over the old
one there we go okay
so now you have your default histogram
and then we have a cumulative histogram
and we should have 50 steps in there and
let's just find out if that's true not
so much by counting them i'm not going
to count them if you want to you can
count them let's just change it to 10
and see what happens and we see here we
have now 10 counts of that and we could
set that for 5
and run that
and then we have our 5 on there and we
go ahead and take the cumulative equals
true out just so you can see what that
looks like and let me run that on here
too
that looks just like it did before i
think there's what one two three four
five six seven eight they have eight
different bins on here is what the
default came out of
put that back in there run
and so now it should look almost
identical and it does and then we can
put the cumulative back in see what that
looks like with the cumulative
and run that
and we can see how that shifts
everything over and has a slightly
different luck
wait it shifts at all to the right no it
doesn't actually shift it to the right
it's cumulative so it's the total of the
different occurrences and so what that
means is like if you consider this like
for the year of rainfall we have like
day one you had a little bit of rain day
two we have more rain and so if you look
at the number this is a hundred thousand
thirty five thousand so it's
accumulative detail the histogram of the
currents as it grows and rainfall is a
good one because that would be a
cumulative histogram of how much rain
occurred throughout the year and we're
going to look at two more graphs we've
already looked at a bunch of them we
looked at our radar graph we've looked
at scatter step bar fill in
basic plots we've looked at different
ways of showing the data and we can
increase the size of the line the look
the color the alpha setting
so let's look at contour maps let's put
that in there there we go draw a contour
map and before we draw a contour map
we need to go ahead and create data for
it and if you have contours your data is
all going to have three different values
so let's go ahead and create the data
here we have our you'd import your
matplot library your numpy so we have
our numbers array
and we'll import matplot.c
and that's your color maps so you have
all these different color maps you can
look at there's like hundreds of color
maps so if you don't want to do your own
color you can even do your own color map
they're pretty diverse and of course our
plt we're going to our pi plot
and to generate our different data we're
going to create a delta
0.025 and we'll start with x when we're
going to create an array between -3 and
3 and delta increments of 0.025
and we'll have our y we'll do something
similar and then we'll create our x y
into a mesh grid again these are all
numpy commands so if you're not familiar
with these you'll want to go back and
review our numpy tutorial and we'll do
an exponential on here minus x squared
minus y squared for z1 we'll do a z2 so
we have two different areas and z equals
z1 minus z2 times two
so we've created a number of values here
and let me go ahead and run this and
let's plug that in so you can see where
those values are going so once we've set
these we're going to create our figure
and our x from our plt subplots we're
going to create the variable cs and this
is going to be our contour so right here
cs is our contour surface and we're
feeding it x y and z if you remember x y
we created as our x and y components
using our mesh grid and you know what
let's do this just because it's kind of
good to see this let's go ahead and
print
x
and let's print
y and i always like to do this when i'm
working with something that's either is
really complicated in this case is what
we're looking at or you don't understand
yet so we've created a mesh grid we have
x y and when we're done with this we end
up with here's our x
and this set of values and our y so
these are x and y coordinates and then
we've also created z based on our x and
y so we have x capital x capital y and
capital z is our three components x and
y being the coordinates while z is going
to be our actual height since we're
doing a contour map so we created our
contour map from our x y and z
coordinates we want to go ahead and put
in a c label maybe we want to go ahead
and do a title on here
we'll put that in our set title and this
is a
contour there we go contour map
and let's go ahead and run this and see
what that looks like
and you'll see we generated a nice
little contour map there's different
settings you can play with on this but
you can picture this being you're on a
mountain climb and here we have a line
that's represent zero maybe that's sea
level and then moving on up you have
your contours of 0.5 and then minus 1
and different setups little hills i
guess if it's minus that's like a pit so
i guess you're going down into a pit at
minus 5 and minus 1 but on the other
side you can see you're going up in
levels so here's a mountaintop and
here's like a basin of some kind and in
data science this could represent a lot
of things this could also be
representing two different values and
maybe profits and loss i don't know if
i'd ever really do that as a contour map
but i'm sure you can be creative and
find something fun to do with a contour
map and then we're going to look at one
last map which is the 3d map and those
are can be really important as a final
product because they can show so much
additional information that you can't
fit on two-dimensional graphs
there we go draw a 3d image and so we're
going to import from our
mpl toolkits the implant 3d and the axis
3d we're going to import axis 3d this is
what's going to let us work with the 3d
image and this should look familiar
we're going to create another figure
just like we did before figure size 14
by 6 that's a good fit on the screen
we'll go ahead and run that
so we have our figure and let's go ahead
and take our x and we're going to set
that equal to fig dot add subplot that
should also be familiar from earlier and
we're going to work with this sets the
settings for the projection we're going
to use one two one projection 3d and
we'll see what that looks like in just a
minute and we just created some
three-dimensional data here before
where we had x y and z capital x y and z
so we're going to reuse that data we're
just going to use that since it's also
this is also a three-dimensional image
so let's use that for a
three-dimensional graph and we simply do
ax plot
underscore
surface
and our capital x
capital y capital z so there's our data
coming in and we're going to add some
settings in here we're going to do r
stride 4 c stride 4 and line width 0.
i'll show you what that is here in just
a minute let's go ahead and run that so
we can see our graph
and of course it helps if i don't add an
extra comma in there and you can see it
generates this really beautiful
three-dimensional graph so let's take a
little bit time to explore some of these
numbers we have going in here
we have the r stride 4 the c stride 4
and the projection 3d projection 3d is
the important one because that's telling
us that this is a 3d graph here
so what are these first numbers 1 2 1
let's just change one of these i'm going
to change this to 5
and it's going to give me an error let's
change it to 1.
and oh that didn't work let's change
this middle one to three instead and
you're going to see how it starts
reshaping the size and how it fits on
the screen
and we'll change the first one to two
we'll run that one
and again it's changed the dimensions
and the size and how it fits on here
play with these numbers to get a nice
look and feel for it part of it is the
tilt and the angle
i'll do seven on this one
there we go you can see it really
shifted it there but again that changes
the size now fits on the canvas
but we'll leave it at the one
two and just so you get a good look at
what we're talking about here this is
column width and index from before
if we do one one one you can see that it
now spreads it out all the way across
uses the whole set up on there so this
has to do with the size and how big you
want it to be now there's one term that
we didn't cover in this yet but we've
used it throughout the whole setup
and i'm just going to type that down
here even though we're not going to go
into detail and that's the term heat
map you might see that it's kind of
starting to lose ground as far as a
common reference but there sure are a
lot of people still talk about heat maps
what is a heat map well it is simply a
color map that's all it is so if you
ever see the term heat map
that refers to the fact this is in
different colors representing different
heights
that one is in the heat map but you can
see up here we switched into let me go
back up here here we go this one has
different colors for the different
values
a lot of times you'll use like instead
of x and y you might do a heat map
where you have a fourth value and the
fourth value represents the color and so
you'll see this 3d image in a nice
colors represented by a heat map that's
all it is so if you see the term heat
map that only means we're plotting some
of the data in color to make it stand
out or to give it a fourth dimension in
this case so we've covered a lot of
things on matplot and that brings us
cover all the basics so that brings us
to practice example and this is going to
be the challenge for you and let me go
ahead and change our cell
cell type mark down and run that so it
looks pretty
practice example write a path python
program to create a pie chart of the
popularity of programming languages
okay excellent
and if you're going to have a challenge
we need some data and i'll just throw in
our import our map library at the
beginning you should do that
automatically and so for our data to
plot we're going to have our languages
we're going to have python we're going
to have java php javascript c sharp c
plus plus so those are six categories
and then we have our popularity oops
misspelling there popularity we'll give
the first one 22.2 percent java 17.6 and
i don't know if these are real numbers
they pulled
my guess is that they might have just
been made up because i don't know
python's really that much more popular
than the other ones maybe specific to
data science because python is very
popular in data science right now
because it has so many options the only
other program that's highly used and
exclusively for data science is r so
python's big and python also does a lot
more it's a full programming language
where r is primarily for data science
they didn't put r in here so we have
python we have java we have our php and
you can see the different values they've
given it are different percentages
and i did add these up does not add up
to 100 it adds up to 71 percent or
something like that
and then we're going to give colors and
we've chosen these guys in the back
brought in these colors i'm not sure
what these colors are we'll find out in
a minute so i'll be exciting but you can
see they're using the actual color
values you can pull off of a color wheel
or something like that you could have
just as easily done blue red green if
you're too lazy to pick the exact colors
and then let's go ahead and solve this
and see we got here we're going to do
something a little fancy just because we
can
the first thing we're going to do is
we're going to use a variable called
explode and you'll notice that there's
six variables in here so that matches
our six different categories and the
first one we've done is point one and
then zero zero zero zero zero point one
when we put this in here under the
explode in the plot it will actually
push that square out so it's a really
cool feature to highlight certain
information on a pie chart
and this is simply
plt.pi and we're plotting
popularity there we go and before we add
in all the really cool settings for this
let's go ahead and run it and you'll see
we generate a nice flat pie not too
exciting there and then we'll go ahead
and put in all the extras i talked about
explode we can explode one of the values
out so here's our explode equals explode
labels as languages because we want to
know what the different colors mean
here's our colors equals colors
our auto picture and this is standard
print format so that's a python setup on
there and that's just going to put the
value on the pie slice and then we're
going to add shadow because it just
looks cooler with a shadow gives a
little 3d look and we'll do a start
angle of 140. let's go ahead and run
this and take a look and see what comes
out of that
and look how that changes the whole
setup so here's our labels there's our
value we put on there there's our slice
it's pushed out there's our shadow a 3d
effect
and then we started at 140. we could
also rotate this let's just do this
angle
90.
and if we run it
you'll see the blue pie slice has moved
up a little bit we could actually do
actually let's just take the whole
starting triangle out and run it'll
default to zero
this is what it looks like if it
defaulted to zero so depending on where
you want the highlighted slice to appear
usually you want that to appear on the
left because people read left to right
and so it draws a focus onto in this
case python and how great python is i'm
a little biased we're teaching a python
tutorial so it should be understandable
that we're looking at python and one
last reference before we close you can
go over to the map plot library dot pi
plot set up and if you go underneath
there the different functions on there
you can look this up on their website
you'll see a full list and this is why
it's so important to go through a
tutorial like this because this list is
just so massive trying to figure out
like here's our bar plot there's a bar h
you can add barbs there's a box plot we
didn't cover
c labels a totally different kind of for
your contour plot you can set up in
there if you go down here we have our
figures we used on there we showed you
the basics how to do the figure you'll
see some
closer references on those
there's a histogram down here hist
there's also the his 2d makes a 2d
histogram plot h lines all of this these
are all the different commands that are
underneath of here and you can see it's
pretty extensive
we've covered all the basic ones so that
you know have a solid ground to look at
these different options so when you come
to these functions some of them are
going to look a little off or not off
will look unfamiliar but you'll still
have the availability to probably
understand most of this and have a basic
understanding of your matplot library
certainly there are many reasons to be
able to go online and scrape different
websites they range everything from
pulling out different links
to pulling data off of websites as a
data scientist you might need to get
some information off a website that
doesn't have a direct api to pull that
information and in python we have a
wonderful tool when you talk python and
you talk web scraping we're talking
beautiful suit which is a package you
add into your python that you're running
and we can come over here to the website
www.crummy.com
software slash beautiful soup you can
actually read a little bit about it
currently beautiful soup 4 is the
current version if you don't remember
the full website for it you can always
do what i do which is go over and do a
search for beautiful soup official site
it almost always comes up right at the
top and you click on there and it'll
take you to the crummy.com software site
for beautiful soup now we're going to
use our whatever python interface you
want ide i'm going to use jupiter lab
which is built on jupiter notebook
through anaconda so when i open up my
anaconda navigator you'll see that i
have my different tools available again
you might be using a different editor
and that's okay you might be in pycharm
or something like that we don't need to
do this and
jupiter lab is jupiter notebook with
added tabs and some added features it's
basically in beta testing so it's got a
few little glitches when you're saving
things and moving between projects but
for the most part it's a great upgrade
to the jupiter notebook and you can use
them together so you don't have to i
mean it's built on jupiter notebooks
anything you do in jupyter notebook you
can open up in jupiter lab and the first
thing we need to do is we need to go
ahead in this case i'm going under my
environments since it partitions the
environments out and i'm going to open
up a terminal window we have to install
some packages in here to work with now
there's a lot of choices on this i
because of the simplicity we'll be using
conda install now you can use pip
install for the same thing and we're
going to install our beautiful soup four
and you have to type out the whole thing
beautiful soup for you can use a pip
install if you're using a different
environment and i am using python
version
3.6 although according to beautiful soup
they also work on three seven all the
way from 27 through olive 3x now
according to the beautiful soup website
the beautiful soup 4 works on anything
you can install on anything from python
27 all the way through any of the python
3 versions this just happens to be
python 36 because i do there's a lot of
other packages that don't work on three
seven yet and we'll go ahead and run
this install on here and let it go
through its environmental setup and of
course with conda it goes in there and
finds all the dependencies pip doesn't
do as much as far as finding
dependencies but you know exactly what's
on there with pip so if you're doing a
huge distribution you probably want to
use your pip install so you can track
what's going on there with the
conda i like to just let it take over
since this isn't a major distributed
package going out another quick note
between pip and conda is that if you
start on a project in one of these
environments and you're using pip in
there stick with pip if you're using
conda stick with conda they track the
packages and you can run into some
issues where they're not tracking the
same packages and something gets
overwritten so it's important to stay
very consistent with your install on
your environments and we'll also need to
go ahead and install our numpy
environment and our pandas on here so go
ahead and do that if you haven't added
those packages in go ahead and install
those into your environment that you're
working in and of course pandas is just
simply uh install pandas and let's just
install a couple more packages in this
case let's get our install our map plot
library because we're going to plot at
the end since we're going to be
collecting data and for this project
that will be all the packages we'll need
so we can go and close out of our
installer or whatever setup you have and
we'll go back to home and we'll just
launch our jupyter lab and that will
open up in our browser window now if
you're coming from jupiter notebooks and
first time in lab we can go ahead and
just create our first notebook python3
you can also do it under a file launcher
and you'll see new notebook it
automatically opens up and we just click
right on there it'll pop open on the
left and i'll right click this and we'll
rename this we'll rename it just
beautiful and it is a i
n b file on there so that should look
familiar because that's the jupyter
notebook file this is a new one now i
have mults in the past i usually hid
this on the other computer all my notes
for the lesson today but this is my
notes going down and we'll go ahead and
just start going through this and see
what it looks like to do a data pull
from front to end and see how that works
as a data scientist pulling that
information in from the website and the
first thing i want to do is i want to go
ahead and close this side window that
way it looks get the nice full screen
and we can also up the size a little bit
one of the wonderful things about
working in a browser window just do that
control plus thing the packages we
talked about is pandas so we imported
our pandas if you haven't already that's
our data frame if you haven't done our
pandas tutorial definitely worthy of the
time to go through there and understand
pandas because it's such a powerful tool
this basically turns your data into a
spreadsheet data frame our numpy is our
number array uh so it kind of works with
pandas very closely as far as
manipulating data in arrays matplot we
want to go ahead and bring that in our
plt so that we can plot the data at the
end and this line right here that says
matplot library inline is for the
jupiter notebook specifically it tells
it to print that on this page a lot of
the newer versions don't actually
require to have that line they'll still
print it on the page but you should
still include that if you're in the
jupiter lab setup and then we have our
url library.request we're going to
import url open for opening up the
website and then we have our bs4 that's
your beautiful soup for we're going to
import beautiful soup and then our last
one is our re that is for manipulating
our regular expressions so when we get
to that part of importing our data we
have to do a lot of reformatting so it's
something we can use and the re is one
of those tools we'll go ahead and run
this and just bring all that in so this
is all imported all these packages are
now into our web scraping program we're
gonna run now if we're going to dive in
and pull data we should have a nice
website to pull from and let's go ahead
and we'll use the
upper timing.com results for the 2018
martin luther king race and if we take
this you can actually just take this
where did we get this from well you can
go in here and find the website you're
going to scrape from and you'll see
right here it says you just copy that
link right in there that http and this
is the website that we're looking at you
can see right here all the information
that we're looking at let's say we want
wanted to run some statistics on this it
sure would be nice to be able to pull it
off of here and if they don't have a
direct api that means we need to pull it
from their website some of these will
have a download although if you've ever
done it we have a download click and
maybe you're paging through a hundred
websites uh in one case i was uh pulling
all the different united states bills
that are passed to track who voted on
them for a project and you can imagine
that there's you know hundreds and
hundreds of those thousands of these
documents that they voted on who voted
on it goes through the senate goes back
to the congress so i opened up a website
pull all the links off of there that
match a certain criteria and we'll look
at that in just a minute how we go
through the html and then i had to
reformat them or i could hand download
each one one at a time which would just
be a nightmare so it's nice to automate
it in this case we're going to be
pulling up this chart we want to figure
out how to pull this chart off of this
website and so we go back into our
jupiter notebook i've got my url just
our name for it and it's just a string
that's all this is nothing fancy there
you'll notice that on the slashes we now
have forward forward slash you can do a
single forward and hdp is a double
forward this is just how you have to
switch it to match setup in there and
then it's going to go ahead and use the
html equals url open url and that's from
our url library request so it's opening
a link to that website or at least
pointing to it and if we run this this
just sets it up so this is all set up
and then once we've done our setup let's
go ahead and create an object called
soup this will be and if you remember up
here here's our beautiful soup that we
imported from the bs4 and this is the
package that we're working with and so
we're going to do our beautiful soup on
here and on this we need to go ahead and
send it our html so it knows what it's
opening and then the second part is we
have to tell it how the format is coming
in and the most common one for your html
polls is an lxml
setup and so almost all of them you'll
end up using the lxml there's a few
other options and because this is so
common in the newer versions a lot of
times they just leave it out just
because it's already on the default
we'll go ahead and leave it in here just
to remind us that it's there we'll go
ahead and run that and on the newer
versions
they actually default it to the xml
setup in the html we'll just leave it
out and call it html so it's just going
to pull from this url and when we run
that on here we've now created an object
soup that has pulled the website into it
so soup contains the information along
with information on that website and
what's going on so let's just go over
what we did real quick before we start
digging into the actual soup before we
start scooping out stuff we imported our
different
modules that we're going to use with our
package specifically the beautiful soup
we did install the beautiful soup if you
remember correctly you have to call it
beautiful soup 4 specifically so it
knows what you're bringing in and this
line right here is very key from bs4
because that's how it installs the
module we're importing our beautiful
soup and then we found our url in this
case we're going to go pull information
from the martin luther king dream run
and then we set our html to our url open
url and you can see right here we
imported that so here's our url.request
import url open so we're requesting a
connection and once we send that
connection into the beautiful soup it
creates an object called soup and then
this one of course we chose soup just
because it goes with beautiful soup i
guess we could have chosen beautiful and
now we can start extracting information
from our website because we pulled it
down onto our computer under soup now we
can start by looking at the title of the
website soup dot title and if we print
title dot text you'll see this a lot in
beautiful suit because title contains
all kinds of information and if we want
just the text from that title you add
the dot text on the end and you can see
right here we have our 2018 mlk dream
run 5k race results if you look at the
tab that's the actual title up here 2018
mlk dream run 5k race that's what the
title is on the website and then you
might be curious what's in title what's
the whole title that it's storing up
there well let's go ahead and print it
out here's print title and print
title.txt and we run that you can see it
has the html tags title on it and then
the forward slash title to end it and so
we're really just pulling off this piece
of the html code and then we look at the
text inside that particular part of the
html and earlier i mentioned links what
if you want to get all the links off
this page oh that would be fun uh we
could do soup dot and we'll do find
underscore all put this in bracket and
then quotation marks we're going to put
a a is the
key find and you'll start seeing a div
and all the different options you have
for finding these entities in a website
and then let's go ahead and just print
our links and you can see here that it
now shows all the different links in
here that are marked by eggs we did a
find all a and then we can also because
this is a little bit hard to pull off
the h reference so we can also add in
our find all fine tune that in this case
the h reference equals true we'll
actually filter that out and then
finally we might do a four link in links
and we can simply do something like this
for each link we want to actually find
the h reference because we know there's
an h reference in it and if i run this
you can see it just comes through and
prints them out one at a time some of
these are really useful so you might be
looking for something that has https in
it and you know that's a link running to
something else or you might be looking
for the mail to tags you know that's all
the mail addresses but either way you
can easily find all the links in your
html document that you're paging through
and of course
any packages that have evolved over time
you can also do link dot get h reference
which should do the same thing as our
other format and you can see it
certainly does we get the same print out
up here in this particular case we
really want to get the data off the page
uh so let's go ahead and do that let's
see what that looks like and in data
let's call it all rows there we go
equals and then we have our soup dot
find underscore all there's our brackets
and then if we're looking for each row
in a database you'll remember your html
code we're looking for the tag tr so we
want to find all tr and we can take this
and let's go ahead and just take all our
rows and do a print
all rows and about this time you're
going to guess that we're going to get a
huge amount of information just dumped
onto our page and sure enough we do if
you look at this it just kind of goes on
forever but this is an array each row is
considered an array so because of that
we can do something simply as putting
brackets and just print the first let's
do the first five rows so from beginning
to five and you can see here's our first
five rows on here i sometimes like to
just do let's just do row zero and we
see that row zero is finishers finishers
191 and just out of curiosity what's if
that's zero what's row one male okay so
we're starting to see titles going
across here so if we come up here and we
do rows we did what up to 10. let's just
take a look and see what 10 does again
and just take a look at that information
that comes across place
bib name gender age city state chip chip
pace gender and so on so it comes all
the way down here we kind of have an
ending right here and then we have one
and then we actually it looks like we
start to have information so we have our
one or one 1191 max randolph that must
be the name male 29 and so on you start
seeing how the information starts
getting displayed going down so the next
thing we want to do with this i'll go
back up here and just edit the space
we're in so it starts to make a little
bit more sense keep it all together and
so we want to do for each row in all
rows we're looking at what information
are we looking at well we have our th up
here that's the header our td down here
which looks like the individual
information and we really are looking
for
the actual data so we're looking for td
tags in the rows and we can do that
because when you remember when it stores
the row it also stores the tags
underneath that so all rows have all the
different tags in it and you can see
right here as you print each one of
those out and so we look at each row we
can create another variable we'll call
it row list and we'll set this equal to
in this case row because we've already
pulled all the rows out of soup so now
we want to find for each row and in
there we want to find our td and if we
go ahead and just print i'm going to do
it if you notice i changed the indent so
i'm just going to print row list what
this does is the last value to go into
row list our last row is going to print
now and of course make sure you have an
underscore instead of a period when
you're typing so row.find underscore all
td and if we print the last row you can
see i have all the data coming across
here we have our 191 our 1216 zuma ochoa
i hope i said that right female i
believe that's age 40 and so on and then
we can take our row list and there's a
lot of things we can do with the row
list what we'll do for let's do object
or let's just do cell in row list and so
we're going to look at each cell because
this is if you look at this they have
commas separated between the different
objects and then we're going to go ahead
and print cell dot text let's just take
a look and see what that looks like and
we can see here for each row we get 191
there's our 191 there's our 12 16 12 16
our uh individual who's in the race and
so forth all the way down for those
different settings and let's go ahead
and create a new variable up here uh
we'll call this all let's just call it
data we'll keep it simple uh so here's
our data and then we have our row we
take our row we break it up into
individual cells so we'll call this data
row we'll set this empty to an empty row
and we're going to take our cells tab
this we know that each cell generates a
text and so what we want to do is i want
to take my data row let's just replace
that let's take our data row and let's
append our cell dot text so i'm going to
add the each row is going to be a row of
the different text on here and then once
i create each row i want my data which
is going to be everything to append each
row and here's our data row and then if
we go ahead and come down here and let's
just print data now if we were lurking
with large data we'd be very careful
about just throwing all our data on the
page but you can see here we throw the
date on the page and we get finishers
199 male 78 female 113 one and so on and
if you look at this this is the headers
on the file we have finishers uh male
females just like some general
statistics on the first one and then we
have actually uh
an empty data set and then we have our
data that continues which actually the
actual information we're looking for so
we have one 1191 max randolph mill 29
washington dc run time uh one of 78 and
so on on here uh so we could really
quickly get rid of that number of
different ways to do that one of them is
just to do we're gonna set if we do data
two on uh we should get rid of
everything but we wanna keep randolph so
make sure randolph is in there oh we
lost randolph let's try one on there we
go there's max randolph on there uh so
we can just simply do redo our date on
here and we can do data probably want to
do it in all rows from one on but i'm
just going to do my data equals data one
on down here and there's reasons to
split it this way in data science
sometimes you don't want to touch the
original data in case you need it in
case we do need the first row so we'll
put it down here and maybe we'll just
call this titles titles equals data of
zero and so we could do something here
where we print we'll print up our titles
and we'll print our data in this case
instead of one on let's go minus
two let's look at the last two rows of
data so here we have our titles and uh
for some reason just put in finishers of
191 as expecting a little bit more up
there and we have our last couple people
and they look like the data on these
looks just fine on here turns out this
is just some generic statistics up here
so we'll get rid of titles completely
doesn't really do us any good but we
know that data comes in here and we can
look at our data and look at the very
end of the data too the minus two to the
end and we can see it pulls the data in
pretty good we don't have anything too
funky in here we're looking at it looks
pretty clean now you got to be a little
careful because at some point we might
have to come back here and clean up the
data if we get an error for running a
data analysis we might find out there's
some unusual characters or something is
missed in the data itself and you also
notice that everything is a string so
when we're bringing it in we might have
to do some conversions to test it out
and convert them to whatever kind of
data format we're working with so at
this time we want to go ahead and bring
in our pandas
and let's go ahead and call this idea
for data frame we'll set it equal to and
if you remember correctly we imported
pandas as pd and that's standard you'll
see that in most code examples where
they call the import pandas as pd
and it is capital d
capital f for data frame and we're just
going to bring in our data that's what
we called it on here and let's take this
and we'll print now when you're working
with data frames you're usually talking
large amounts of data and so you almost
never want to print the whole data frame
out we're going to go ahead and do that
anyway just so we can see what that
looks like and you can see in here
brings in our data frame coming in here
and we just have a mess of information
this is our data let's go ahead and
print df and see what that looks like in
the data frame and this is nice because
it organizes it into a very easy to read
table and we have they set the label 0 1
two three four five six and so on and
then we have each row uh we have mel uh
seventy eight none none no none going
across when we get all the way down here
we'll see max randall about number three
and the first thing this does is this
flags me that i brought in a bunch of
information up here that we really
didn't want uh it's from three on that
we want and we can clean this up in one
of two ways we can try to clean it up
under the data or we can clean it up
under the data frame depending on what
it is we're trying to do and so to fix
this um i want to go ahead and just
change it up here in the actual data
pull in we don't need that information
so i'll rerun it reload our data from
for on and then when i run this we see
we have max randolph is right at the top
of the list like he should be and we
have all the data going down now with
the data frame remember i said we don't
usually print the whole data frame we'll
go ahead and do df.head and this prints
the first five rows and you can see that
we have 13 columns here's max randall
all the way to theo kinman and i usually
also print df
tail and the reason i like to do these
particular two setups i'm going to
change it just to two rows because you
can do that you can put as many rows as
you want is this good to look at the
first part and the end because those are
usually where you have extra data
brought in uh something's messed up and
you can also see that we have 190 rows
in here and it comes in with our zuma
lisha and they're both on here on the
list so now we have a nice data frame
columns and rows we can easily look at
it we can see the setup on here and we
can look at the names and everything now
at some point you might be looking at
these individual columns and find
different information that needs to be
re-edited if you can you try to do it
with the whole column under pandas you
can up in the upper part of the code
where you went from cell to cell or row
to row you can look at individual cells
maybe find a marker in that cell that's
something specific like remove all
colons or semicolons or something and
there are brackets there's a lot of
options in there but you'll find that
this one actually comes in pretty clean
on here all the way down and the next
thing we really want to do is we want to
look at the headers i don't know about
you but doesn't make any sense to me
when i have column one i don't know what
1191 is or 1080. need name kaiser runner
i'm guessing that's column two is names
third one it looks like male or female
probably age but i don't want to guess i
want this to bring in my column so i
know exactly what i'm looking at so how
can we make beautiful soup do that for
us well let's take our column headers
we're going to set that equal to our
soup find underscore all and then we're
going to look for our headers our th
files and since we're in jupiter lab in
this case jupiter notebook i can just
type out column headers if it's the last
variable i have listed it will
automatically print it so it's kind of a
shorthand and we can see right here we
have place i'm guessing that's bib name
gender age city state chip time chip
pace and so on so we have all our
headers right there i shouldn't have to
type them all in and we'll go ahead and
do it before we'll go ahead and do a
header list equals our empty array and
then we can do for a column in column
headers and we can take our header list
and just a pin and what do we want we
want the text from the column so we'll
just do column.text and then if we come
down here and we print
our header list let's see what that
looks like if we did it right we should
get a nice list of all the different
column headings we want so we have place
bib name and so on and then pandas just
because pandas is so cool we can simply
do df columns equals our header from our
header list we simply said df column set
to df headers and then if i print
df.head we'll take a look at that and
we'll see right here it has nicely
placed our values on here place bib name
gender age and so on so very quickly
we've created this nice data frame we
have the data displayed in nice rows and
columns and easy to read and then as a
data scientist the first thing we want
to know is the info what is in these
columns and rows and headers and you see
right here they all come up with
non-null object there's a big flag so if
i want to do anything with this these
are all coming through as strings or an
object i usually mean strings in this
case that they're a string variable and
we have you can quickly read through
this 191 entries date columns total 14
columns there's a total of 14 columns in
the data and it shows you all the
different names and what type of column
they are and it's probably good also to
look at the shape of the data df.shape
we'll go ahead and just run that you see
it's 191 by 14 14 columns 191 entries
this is more like a we look at a numpy
array 191 by 14 for the shape and
remember this is a variable so if i put
it on if it's a last variable or last
value in the set of cells jupiter
automatically prints it out so if you're
in a different ide you want to go ahead
and use the print statement on here uh
then one of the things you'd want to
also go through we'll create a second
one df2 equals df dot drop in a now the
axis is automatically equal to zero so a
lot of times you'll see something like
axes equal zero comma how equals any
axis equals 0 is default that means
we're looking at going down the rows you
could look at the column going across
let's remove the how any that's just
going to confuse you the axis is is
whether you're going down the columns or
if you're looking at a row by row by row
by row or you could be looking at it by
column by column by column this would
drop any column and it would drop off
the n a in any column and how
equals and we want any i always confuse
all in any because they both start with
a all means that all of them have to be
non-value where any means that any of
them can be there to drop it so this
would drop any column with a null value
in it but we want zero you know drop any
value with a null value and then because
0 is always the default we'll just leave
it out and then it's curious as to what
the shape is now did we lose anything
was there any null values in our df2
that we dropped from the df and we'll go
ahead and run that and we see 191 and
14. so we didn't really drop anything
but it's always good to check there's
other ways you can also do are there
let's see any n a's you can detect n a's
in here no values infinite values that's
another one you got to watch out for
we're working with data that we're going
to do something with here in a minute so
you got to be a little careful also in
the convergence are you going to convert
something where people typed in weird
characters to describe the data a
certain way so now we've got to this
point where we have all our different
columns we have our different data and
at this point maybe you're asking or
maybe the shareholder the company is
asking hey can we look at the
based on the chip time here's our chip
time can we plot that versus gender how
does gender versus chip time compare and
so we can do that we can take that and
the first thing we look at is we say hey
well chip time came in as a string and
that's going to be an issue now there's
a number of ways you can change this one
of them is we could go all the way back
up here where we created the data and
find a way to tag it and say hey
whenever this cell text maybe instead of
appending this i notice that any time
there's two colons in it that's probably
a time signature and let's uh convert
all the time signatures to date time
filled or whatever a lot of times you
don't get that you don't get that option
and that's always a question in bringing
in data whether you convert the data
coming in at the beginning or do you
wait till you have it open it up and
then convert it when you go to use it
we're going to go ahead and convert it
after we got it into our data frame so
we have our df2 here we've dropped all
of our n a's we dropped our we have a
shape
in fact let's do this since there's no
difference between df and df2 well we'll
just go ahead and use df2 so let's go
ahead and take our df2 and we want to
take those that specific field and
convert it into some kind of numerical
value we can use and let's add another
column a lot of times this is something
you want to do is where you want to go
ahead and keep all the original columns
and just add a new column in there and
this new column is going to be based on
the if remember correctly we had chip
time that's what we're going to look at
okay we want chip time versus gender if
we go into our pandas we find out we
have
pandas 2 delta and this is actually time
delta and then we just want to take our
df2 and we're going to use the chip time
column so this is going to say hey let's
look at let's convert everything in df2
chip time into a time delta format
that's the data type we're going to put
it as let me go ahead and just run this
and if we go in here and we do info df2
and we'll keep our we're going to look
at this particular column but we want to
keep it as a data frame so this is a
list of all the columns we want to look
at we'll just do dot info on here and
run that and we do an info on that you
can see is now a
time delta 64 nanoseconds uh well we
really don't want nanoseconds we
actually probably want to do it in
minutes uh so let's take a look at that
and let's take this whole thing tf2
let's just set the df2 we have our df2
this is a column we're working with here
and we can use the as type property in
pandas and so we can set this equal to
df2 we'll take our same column in here
and we'll set it as type time delta
second so it's still a delta time here
so if i run this you'll see that it
still comes up as where is it hopefully
it turns it into a float so we're now at
a float 64 so it's the number of seconds
in that delta time and then finally we
want to go ahead and turn that from
seconds to say minutes and you know
there's 60 seconds in minutes and so now
we divide by 60 we still have a float we
have our info that shows us it's a float
and then we can go ahead and just do a
print df2 and let's just keep it small
we don't want to look at all our data we
just want to do the head of it and we
run this and you can see right here
where we go to be the last one here's
our chip time in minutes and a lot of
times just to make life easy for viewing
since we're only looking at this
particular element we can do chip time
in minutes and now we just see that oops
we take off we'll go ahead and take off
our info done with that and we'll run
this and you can see we have our minutes
16.8 minutes 17.51 minutes and so on
it's a float number now keep in mind
this is 0.8 that's not 16 minutes and 80
seconds that they can always throw you
if you're going through so many numbers
you forget it's important to remember
that and we're also going to look at the
other one we're looking at is what
gender you want to look at gender to
chip time in minutes and so we can see
here under the head we get male male
male and a number of different setups
and let's switch this to tail real quick
and just look at the end of it and here
we have female female male female female
so we have two different genders and we
have our chip time in minutes and if you
remember we brought in our plt if you
haven't used the plot library the
matplot library you have a drawing place
you're putting stuff on so we have our
plt we're going to do a bar graph and we
just want to simply use our df2 gender
and df2
chip time in minutes so that's going to
plot the two bars and to make it pretty
we'll go ahead and give it the x label
gender the y label chip time minutes and
that simply is remember it always plots
x in the plots y we have our gender our
chip time give it a nice title uh
comparison of average minutes run by
male and female and if we go ahead and
run this with the correct titles in here
and everything matches uh you'll get a
nice graph we can see here the
comparison of average minutes run by
male and female here's our chip time in
minutes the men seem to be slackers in
this particular case and it's actually
uh there's a number of studies that show
that women team tend to have as far as
doing cross-country there's a lot of
women who have a longer endurance than
men so it's not too surprising but we
can see here the average chip time
around 70 and for women over 100 minutes
and then another really cool thing we
can do is we can describe the data so
df2.describe this again is a pandas
function just like info is we're going
to include uh np number the numpy number
and if we run that you'll see here comes
up and says chip time in minutes account
it gives you the average or the mean
standard deviation the minimum the
maximum um all the different descriptive
information you're going to want from
your data set on there and just because
there's all kinds of fun ways let's do a
box plot to display your information uh
we can do a box plot where the column
equals chip time in minutes and let's go
ahead and run that keep mistaking my
chip time in minutes you can see it puts
out a nice box plot showing you the
information we have our different values
and floaters this is always interesting
because this is a nice way of seeing
where we have these uh floaters one up
here and there's two up above and of
course here they're a nice spread on the
box plot and we can also modify this a
little bit and we can add in by equals
gender and then we'll give it we'll just
give it a blank title i don't know why
we're going to get a blank title we'll
just add a y plot y label on there for
run time and if we run this you can see
here box plot grouped by gender chip
time in minutes and now we have our
female and male two different areas and
you can see how they vary you have your
two different your outlier up here and
you can also see how there's such an
overlap between the two different values
so if i was looking at this i'd be like
wow you know i really could not draw a
conclusive thing on this saying that
women's run time was more in general
because they overlapped too much that
would be one of the conclusions i'd have
to come up with then here and then we
get to maybe the partners come in from
the company and say hey we'd like to
know the age versus chip time in minutes
that'd be something worth knowing on the
statistics on this and the first thought
is we can simply plot it and we can do
this we can actually plot the scatter
plot chip time versus df2 of h those are
xy coordinates but if you remember from
df2 when we did the info let's go way
back up here we're looking at a data
object as far as our chip time on this
in our h now we converted the chip time
but we also need to convert the age and
if we do it right here we just plot it
and it'll actually let us plot it it
shouldn't it should give us an error but
it does let us plot it you'll see the
ages come up a mess over here because
they're converting it to weird float
numbers and all kinds of things so what
we want to do is we want to take our age
and we'll just call this ah underscore i
so we're going to take our age and we're
going to create another column for df2
age underscore i and the i is just going
to stand for integers our own choice of
values there is a number of ways to do
this but we're going to do uh pandas 2
merrick is the best way in pandas and
the reason we're doing this is that
numeric creates a float value uh so
right off the bat we want all our stuff
converted it converts it to the least
common denominator so if they're all
integers already you'll get integers as
you can see from here it's doing some
kind of conversion that converts it to a
float value the other thing that numeric
does is if there is a null value or they
put in like a blank line or a dash to
represent no information uh it'll
convert it to a
null uh so it goes from like a string to
a null versus just having some kind of
made up number that python somehow
created for the graph we have below and
then we want to add our df2h because
that's what we're converting to numeric
and then we want to coerce it and
there's a couple different options on
this like you can have it where it just
doesn't process it in pandas but coerce
means that if it gets a weird value that
is a null value now and since we're
dealing with errors this is what happens
when you get an error converting it we
want to coerce it there we go and put
the end bracket on there and then
finally we want to go and round this off
so i'll put brackets around all the way
around it and this rounds off everything
in this series so we've done here is
i've taken df2h which is a d type object
which in this case is mostly strings
with a couple blank ones in there and
we're going to convert it to a numeric
which will automatically go to float and
then we're going to take wherever
there's an error wherever it says hey
this doesn't convert and usually that's
a blank screen like i said i've worked
with so many databases where they
someone puts down none someone types in
space sometimes in a dash to me none and
you get this really weird conversion
coming up this covers all of that in
pandas so it's really a nice way of just
coercing it and saying hey if we don't
have a number in there let's make it a
null value and then we're going to round
it off and then finally let's go ahead
and take our df sign it to
um df2 so df2 now has a rounded out so
it's rounded to the integer we didn't do
any places the age and it's going to be
age i and then we've dropped all our
null values that way we're not going to
get any errors when we try to plot a
null value and it also makes sure that
data by deleting out the rows because
that's what this does it automatically
does axis0 which is your rows axes one
is your column by doing this it
automatically removes all the rows with
null values so it just cleans out the
rows and then when we go ahead and plot
this we see we have a nice clean data
and we have age all the way up to 70. so
we have our chip time set and then our
age going across and it makes a nice
plot that you can easily show for
display and for the the and you can
easily show that to your shareholders or
whatever group you're working on it
makes a really nice and quick easy
display and we're going to cover the
scikit learn tutorial which has a lot of
features and all kinds of api in it to
explore data and do your data science
with effect is probably one of the top
data science packages out there so what
is the site kit learn it's simple and
efficient tool for data mining and data
analysis it's built on numpy scipy and
matplot library so it interfaces very
well with these other modules and it's
an open source commercially usable bsd
license bsd originally stood for
berkeley software distribution license
but it means it's open source with very
few restrictions as far as what you can
do with it another reason to really like
the site kit learn setup so you don't
have to pay for it as a commercial
license versus many other copyrighted
platforms out there what we can achieve
using the site kit learn we use class
the two main things are classification
and regression models classification
identifying which category an object
belongs to for one application very
commonly used is spam detection so is it
a spam or is it not a spam yes no in
banking it might be is this a good loan
bad loan today we'll be looking at wine
is it going to be a good wine or a bad
wine and regression is predicting an
attribute associated with an object one
example is stock prices prediction what
is going to be the next value if the
stock today sold for 23 dollars and five
cents a share what do you think it's
gonna sell for tomorrow and the next day
and the next day so that would be a
regression model same thing with weather
weather forecasting any of these are
regression models where we're looking at
one specific prediction on one attribute
today we'll be doing classification like
i said we're gonna be looking at whether
a wine is good or bad but certainly the
regression model which is in many cases
more useful because you're looking for
an actual value is also a little harder
to follow sometimes so classification is
a really good place to start we can also
do clustering and model selection
clustering is taking an automatic
grouping of similar objects into sets
customer segmentation is an example so
we have these customers like this
they'll probably also like this or if
you like this particular kind of
features on your objects maybe you like
these other objects so it's a referral
is a good one especially on amazon.com
or any of your shopping networks model
selection comparing validating and
choosing parameters and models now this
is actually a little bit deeper as far
as a site kit learn we're looking at
different models for predicting the
right course or the best course or
what's the best solution today like i
said we're looking at wines it's going
to be how do you get the best wine out
of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and fine-tuning
now this is only part one so we're not
gonna do too much tuning on the models
we're looking at but i'll point them out
as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
preprocessing and i'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
i like to use the jupiter notebook and i
use it out of the anaconda navigator so
if you install the anaconda navigator by
default it will come with a jupiter
notebook or you can install the jupyter
notebook by itself this code will work
in any of your python setups i believe
i'm running an environment of 3.7 set up
on there i'd have to go in here in
environments and look it up for the
python setup but was one of the three
x's and uh we go and launch this and
this will open it up in a web browser so
it's kind of nice it keeps everything
separate and in this anaconda you can
actually have different environments
different versions of python different
modules installed in each environment so
it's a very powerful tool if you're
doing a lot of development and jupyter
notebook is just a wonderful visual
display certainly you can use i know
spyder is another one which is installed
with the anaconda i actually use a
simple notepad plus plus when i'm doing
some of my python script any of your
ides will work fine jupyter notebook is
iron python because it's designed for
the interface but it's good to be aware
of these different tools
and when i launch the jupyter notebook
it'll open up like i said a web page in
here and we'll go over here to new and
create a new python setup like i said i
believe this is python 37 but any of the
three this the scikit learn works with
any of the three x's there's even two
seven versions so it's been around a
long time so it's very big on the
development side and then the guys in
the back guys and gals develop they went
ahead and put this together for me and
let's go ahead and import our different
packages now if you've been reading some
of our other tutorials you'll recognize
pandas as pd pandas library is pretty
widely used it's a data frame set up so
it's just like columns and rows in a
spreadsheet with a lot of different
features for looking stuff up seaborne
sits on top of map plot libraries this
is for a graphing we'll see that how
quick it is to throw a graph out there
to view in the jupiter notebook for
demos and showing people what's going on
and then we're going to use the random
forest the svc or support vector
classifier and also the neural network
so we're going to look at this we're
actually going to go through and look at
three different classifiers that are
most common some of the most common
classifiers and let's show how those
work in the scikit-learn setup and how
they're different and then if you're
going to do your setup on here you'll
want to go ahead and import some metrics
so the
sklearn.metrics on here and we'll use
the confusion metrics and the
classification report out of that and
then we're going to use from the sklearn
pre-processing the standard scalar and
label encoder standard scalar is
probably the most commonly used
pre-processing there's a lot of
different pre-processing packages in the
sklearn and then model selection for
splitting our data up it's one of the
many ways we can split data into
different sections and the last line
here is our percentage map plot library
in line some of the seaboard and map
plot library will go ahead and display
perfectly in line without this and some
won't it's good to always include this
when you're in the jupiter notebook this
is jupiter notebook so if you're in ide
when you run this it will actually open
up a new window and display the graphics
that way so you only need this if you're
running it in a editor like this one
with the specifically jupiter notebook
i'm not even familiar with other editors
that are like this but i'm sure they're
out there i'm sure there's a firefox
version or something jupiter notebook
just happens to be the most widely used
out there and we can go ahead and hit
the run button and this now has saved
all this underneath the packages so my
packages are now all loaded i've run
them whether you run it on top we run it
to the left and all the packages are up
there so we now have them all available
to us for our project we're working on
and i'm just gonna make a little side
note on that when you're playing with
these and you delete something out and
add something in even if i went back and
deleted this cell and just hit the
scissors up here these are still loaded
in this kernel so until i go under
kernel and restart or restart and clear
or restart and run all i'll still have
access to pandas
important to know because i've done that
before i've loaded up maybe not a module
here but i've loaded up my own code and
then changed my mind and wondering why
does it keep putting out the wrong
output and then i realize it's still
loaded in the kernel and you have to
restart the kernel just a quick side
note for working with a jupiter notebook
and one of the troubleshooting things
that comes up and we're going to go
ahead and load up our data set we're
using the pandas so if you haven't yet
go look at our pandas tutorial a simple
read the csv with the separation on here
so let me go ahead and run that and
that's now loaded into the variable wine
and let's take a quick look at the
actual file i always like to look at the
actual data i'm working with in this
case we have wine quality dash red i'll
just open that up i have it in my open
office set up separated by semicolons
that's important to notice
and we open that up you'll see we have
go all the way down here it looks like
1600 lines of data minus the first one
so 15
1599 lines and we have a number of
features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it five six seven it's not really i'm
not sure how high of a level it goes but
i don't see anything over a seven so
it's kind of five through seven is what
i see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with i didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates ph density total
sulfur dioxide and so on those are all
the features we're going to be looking
at
and since this is a pandas we'll just do
wine
head and that prints our first five
rolls rows of data that's of course a
pandas command and we can see that looks
very similar to what we're looking at
before we have everything across here
it's automatically assigned an index on
the left that's what pandas does if you
don't give it an index and for the
column names it has assigned the first
row so we have our first row of data
pulled off the our comma separated
variable file in this case semicolon
separated and it shows the different
features going across and we have what
one two three four five six seven eight
nine ten eleven features 12 including
quality but that's the one we want to
work on and understand and then because
we're in uh panda's data frame we can
also do wine.info and let's go ahead and
run that this tells us a lot about our
variables we're working with you'll see
here that there is
1599 that's what i said from the
spreadsheet so that looks correct
non-null float
this is very important information
especially the non-null so there's no
null values in here that can really trip
us up in pre-processing and there's a
number of ways to process non-null
values one is just to delete that data
out of there so if you have enough data
in there you might just delete your
non-null values another one is to fill
that information in with like the
average or the most common values or
other such means but we're not gonna
have to worry about that but we'll look
at another way because we can also do
wine is null and sum it up and this will
give us a similar it won't tell us that
these are float values but it will give
us a summation of there we go let me run
that it'll give us a summation on here
how many null values in each one so if
you wanted to you know from here you
would be able to say okay this is a null
value but it doesn't tell you how many
are null values this one would clearly
tell you that you have maybe five null
values here two null values here and you
might just if you had only seven null
values and all that different data you'd
probably just delete them out where if
ninety percent of the data was null
values you might rethink either a
different data collection setup
or find a different way to deal with the
null values we'll talk about that just a
little bit in the models too because the
models themselves have some built-in
features especially the forest model
which we're going to look at at this
point we need to make a choice and to
keep it simple we're going to do a
little pre-processing of the data and
we're going to create some bins and bins
we're going to do is 2 comma 6.5 comma
8. what this means is that we're going
to take those values if you remember up
here let me scroll back up here we had
our quality the quality comes out
between two and eight basically or one
and eight we have five five five six you
can see just in the in just in the first
five lines of variation in quality
we're going to separate that into just
two
bends of quality and so we've decided to
create two bins and we have bad and good
it's going to be the labels on those two
bins we have a spread of 6.5 and an
exact index of 8. the exact index is
because we're doing 0 to 8 on there the
6.5
we can change we could actually make
this smaller or greater but we're only
looking for the really good wine we're
not looking for the zero one two three
four five
six we're looking for wines with seven
or eight on them so high quality you
know like this is what i want to put on
my dinner table at night
i might taste the good wine not the
semi-good wine or mediocre wine and then
this is a panda so pd remember stands
for pandas pandas cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bins to comma 6.58 so two different bins
and our labels are bad and good and we
can also do
let me just do it this way wine quality
since that's what we're working on and
let's look at unique another pandas
command and we'll run this and i get
this lovely error why did i get an error
well because i replaced wine quality and
i did this cut here which changes things
on here so i literally altered one of
the variables is saved in the memory so
we'll go up here to the kernel restart
and run all starts it from the very
beginning and we can see here that that
fixes the error because i'm not cutting
something that's already been cut we
have our wine quality unique and the
wine quality unique is a bad or good so
we have two qualities objects bad is
less than good meaning bad's going to be
zero and good's going to be one and to
make that happen we need to actually
encode it so we'll use the label quality
equals label encoder and the label
encoder let me just go back there since
this is part of sklearn that was one of
the things we imported was a label
encoder you can see that right here from
the sklearn dot processing import
standard scalar which we're going to use
in a minute and label encoder and that's
what tells it to use that equals 0 and
good equals 1. and we'll go ahead and
run that and then we need to apply it to
the data and when we do that we take our
wine quality that we had before and
we're going to set that equal to label
quality which is our encoder and let's
look at this line right here we have dot
fit transform and you'll see this in the
pre-processing these are the most common
used is fit transform and fit transform
because they're so often that you're
also transforming the data when you fit
it they just combined them into one
command and we're just going to take the
wine quality feed it back into there and
put that back in our wine quality setup
and run that and now when we do
the wine
and the head of the first five values
and we go ahead and run this you can see
right here underneath quality zero zero
zero i have to go down a little further
to look at the better wines
let's see if we have some that are ones
yeah there we go there's some ones down
here so when we look at ten of them you
can see all the way down to zero or one
that's our quality and again we're
looking at high quality we're looking at
the seven and the eights or six point
five and up and uh let's go ahead and
grab our where was it here we go wine
quality let's take a look at what else
more information about the wine quality
itself and we can do a simple pandas
thing value
counts hopefully i type that in there
correctly and we can see that we only
have 217
of our wines which are going to be the
higher quality so 217 and the rest of
them fall into the bad bucket and the
zero which is uh 1382 so again we're
just looking for the top percentage of
these the top what is that it's probably
about a little under 20 percent on there
so we're looking for our top wines our
seven and eights and let's use our let's
plot this on a graph so we take a look
at this and the sns if you remember
correctly that is let me just go back to
the top that's our seaborn seaborn sits
on top of matplot library it has a lot
of added features plus all the features
of the matplot library and also makes it
quick and easy to put out a graph we'll
do a simple bar graph and they actually
call it count plot and then we want to
just do count plot the wine quality so
let's put our wine quality in there and
let's go ahead and run this and see what
that looks like and nice in line
remember this is why we did the inline
so make sure it appears in here and you
can see the blue space or the first
space represents low quality wine and
our second bar is a high quality line
and you can see that we're just looking
at the top quality wine here most of
wine we want to just give it away to the
neighbors no maybe if you don't like
your neighbors maybe give them the good
quality wine and i don't know what to do
with the bad quality wine i guess use it
for cooking there we go but you can see
here it forms a nice little graph for us
with the seaboard on there and you can
see our setup on that so now we've
looked at we've done some pre-processing
we've described our data a little bit we
have a picture of how much of the wine
what we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment if you're in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what uh in this
case quality what we're looking for and
we can take this and go wine that's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality and make sure
we have our axes equals one if you left
it out it would still come out correctly
just because of the way it processes
on the defaults and then our y if we're
going to remove quality for our x that's
just going to be one and it is just the
quality that we're looking at for y so
we put that in there and we'll go ahead
and run this so now we've separated the
features that we want to use to predict
the quality of the wine and the quality
itself the next step is if you're going
to create a data set in a model we got
to know how good our model is so we're
going to split the data train and test
splitting data and this is one of the
packages we imported from sklearn and
the actual package was train test split
and we're going to do x y test size 0.2
random state 42. and this returns four
variables and most common you'll see is
capital x train so we're going to train
our set with capital x test that's the
data we're going to keep on the side to
test it with y train y remember stands
for the quality or the answer we're
looking for so when we train it we're
going to use x train and y train and
then y test to see how good our x test
does and the train test split let me
just go back up to the top that was part
of the sklearn model selection import
train test split there is a lot of ways
to split data up this is when you're
first starting you do your first model
you probably start with the basics on
here you have one test for training one
for test our test size is point two or
twenty percent and random state just
means we just start with a it's like a
random seed number so it's not too
important back there we're randomly
selecting which ones we're going to use
since this is the most common way this
is what we're going to use today there
is and it's not even an sk learned
package yet so someone's still putting
it in there one of the new things they
do is they split the data into thirds
and then they'll run the model on each
of they combine each of those thirds
into two thirds for training and one for
testing and so you actually go through
all the data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you'll have a test size one
test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out it
works great so we have our x train our x
test our y train and our y test and then
we need to go ahead and do the scalar
and let's talk about this because this
is really important some models do not
need to have scaling going on most
models do and so we create our scalar
variable we'll call it sc standard
scalar
and if you remember correctly we
imported that here wrong with the label
encoder the standard scalar setup
so there's our scalar and this is going
to convert the values instead of having
some values that go from zero if you
remember up here we had some values are
54 60 40 59 102. so our total sulfur
dioxide would have these huge values
coming into our model and some models
would look at that and they'd become
very biased to sulfur dioxide it'd have
the hugest impact and then a value that
had
.076.098 or chlorides would have very
little impact because it's such a small
number so we take the scalar we kind of
level the playing field and depending on
our scalar it sets it up between 0 and 1
a lot of times is what it does let's go
ahead and take a look at that and we'll
go ahead and start with our x train and
our x train equals sc fit transform we
talked about that earlier that's an sk
learn setup that's going to both fit and
transform our x train into our x train
variable and if we have an x train we
also need to do that to our test and
this is important because you need to
note that you don't want to refit the
data we want to use the same fit we used
on the training is on the testing
otherwise you get different results and
so we'll do just oops not fit
transform we're only going to transform
the test side of the data so here's our
x test that we want to transform and
let's go ahead and run that and just so
we have an idea let's go ahead and take
and just print out our x train oh let's
do
first 10 variables very similar to the
way you do the head on a data frame you
can see here our variables are now much
more uniform and they've scaled them to
the same scale so they're between
certain numbers and with the basic
scalar you can fine tune it i just let
it do its defaults on this and that's
fine for what we're doing in most cases
you don't really need to mess with it
too much it does look like it goes
between like minus probably minus two to
two or something like that that's just
looking at the train variable i'll go
ahead and cut that one out of there so
before we actually build the models and
start discussing the sk learned models
we're going to use we covered a lot of
ground here most of when you're working
with these models you put a lot of work
into pre-prepping the data so we looked
at the data notice that it's uh
separated loaded it up we went in there
we found out there's no null values
that's hard to say no nodal values we
have there's none there's none nobody i
can't say it
and of course we sum it up if you had a
lot of null values this would be really
important coming in here so is there a
null summary we looked at pre-processing
the data as far as the quality and we're
looking at the bins so this would be
something you might start playing with
maybe you don't want super fine wine you
don't want the seven and eights maybe
you want to split this differently so
certainly you can play with the bins and
get different values and make the bins
smaller or lean more towards the lower
quality so you then have like medium to
high quality and we went ahead and gave
it labels again this is all pandas we're
doing in here setting up with unique
labels and group names bad good badass
lesson good that could be so important
you don't know how many times people go
through these models and they have them
reversed or something and then they go
back they're like why is this data not
looking correct so it's important to
remember what you're doing up here and
double check it and we used our label
encoder so that was um to set that up as
quality zero one good in this case we
have bad good zero one and we just
double check that to make sure that's
what came up in the quality there and
then we threw it into a graph because
people like to see graphs i don't know
about you but you start looking all
these numbers and all this text and you
get down here and you say oh yes you
know here this is how much of the wine
we're going to label as subpar not good
this is how much we're going to label as
good and then we got down here to
finally separating out our data so it's
ready to go into the models and the
models take x and a y in this case x is
all of our features minus the one we're
looking for and then y is the features
we're looking for so in this case we
dropped quality and in the y case we
added quality and then because we need
to have a training set and a test set so
we can see how good our models do we
went ahead and split the models up x
train x test y train y test and that's
using the train test split which is part
of the sk learn package and we did as
far as our testing size point two or
twenty percent the default is twenty
five percent so if you leave that out
it'll do default setup and we did a
random state equals 42. if you leave
that out it'll use a random state i
believe it's default one i'd have to
look that back up and then finally we
scaled the data this is so important to
scale the data going back up to here if
you have something that's coming out as
a hundred is going to really outweigh
something that's 0.071
that's not in all the models different
models handle it differently and as we
look at the different models i'll talk a
little bit about that we're going to
look at three models today three the top
models used for this and see how they
compare and how the numbers come out
between them so we're going to look at
three different setups oh let me change
my cell here to mark down there we go
and we're going to start with the random
forest classifier so the three sets
we're looking at is the random forest
classifier support vector classifier and
then a neural network now we start with
the random forest classifier because it
has the least amount of
parts moving parts to fine tune and
let's go ahead and put this in here so
we're going to call it rfc for random
force classifier and if you remember we
imported that so let me go back up here
to the top real quick and we did an
import of the random fourth classifier
from sk learn ensemble and then we'll
all we also let me just point this out
here's our svm where we imported our
support vector classifier so svm is
support vector model support vector
classifier and then we also have our
neural network and we're going to from
there the
multi-layered perceptron classifier kind
of a mouthful for the p perceptron don't
worry too much about that name it's just
it's a neural network there's a lot of
different options on there and setups
which is where they came up with the
perceptron but so we have our three
different models we're going to go
through one here and then we're going to
weigh them here's our metrics we're
going to use a confusion metrics also
from the sklearn package to see how good
our model does
with our split so let's go back down
there and take a look at that
and we have our rfs equals random forest
classifier and we have n estimators
equals 200. this is the only value you
play with with a random forest
classifier how many forests do you need
or how many trees in the forest so how
many models are in here that makes it
pretty good as a startup model because
we're only playing with one number and
it's pretty clear what it is and you can
lower this number or raise it usually
start up with a higher number and then
bring it down to see if it keeps the
same value so you have less you know the
smaller the
model the better the fit and it's easier
to send out to somebody else if you're
going to distribute it now the random
forest classifier
everything i read says it's used for
kind of a medium-sized data set so you
can run it in on big data you can run it
on smaller data obviously but tends to
work best in the mid-range and we'll go
ahead and take our rfc
and i just copied this from the other
side dot fit x train comma y train so
we're sending it our features and then
the quality in the y train what we want
to predict in there and we just do a
simple fit now remember this is sk learn
so everything is fit or transform
another one is predict which we'll do in
just a second here let's do that now
predict
rfc equals and it's our rfc model
predict and what are we predicting on
well we trained it with our train values
so now we need our test our x test so
this has done it this is going to do
this is the three lines of code we need
to create our random force variable fit
our training data to it so we're
programming it to fit in this case it's
got 200 different trees it's going to
build and then we're going to predict on
here let me go ahead and just run that
and we can actually do something like oh
let's do predict
rfc just real quick we'll look at the
first 20 variables of it let's go ahead
and run that and uh in our first 20
variables we have three wines that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you remember correctly
we'll go ahead and take this out of here
this is based on our test so these are
the first 20 values in our test and this
has as you can see all the different
features listed in there and they've
been skilled so when you look at these
they're a little bit confusing to look
at and hard to read but we have there's
a -01 so this is 0.36 minus 01 so 0.164
minus
0.09 or no it's still -1 so minus 0.9
all between 0 and 1 on here i think i
was confused earlier and i said 0
between 2 negative 2. but between -1 and
1 which is what it should be in the
scale and we'll go ahead and just cut
that out of there run this we have our
setup on here so now we've run the
prediction and we have predicted values
well one you could publish them but what
do we do with them well we want to do
with them is we want to see how where
our model model performed that's the
whole reason for splitting it between a
training and testing model and for that
remember we imported the classification
report
that was again from the sklearn there's
our confusion matrix and classification
report and the classification report
actually sits on the confusion matrix so
it uses that information and our
classification report we want to know
how good are y tests that's the actual
values versus our predicted rfc so we'll
go ahead and print this report out and
let's take a look and we can see here we
have a precision out of the zero we had
about point 92 that were labeled as bad
that were actually bad and out of
precision for the quality wines we're
running about 78 percent so you kind of
give us an overall 90
and you can see our f1 score our support
set up on there our recall you could
also do the confusion matrix on here
which gives you a little bit more
information but for this this is going
to be good enough for right now we're
just going to look at how good this
model was because we want to compare the
random fourth classifier with the other
two models and you know what let's go
ahead and put in the confusion matrix
just so you can see that on there with y
test and prediction rfc so in the
confusion matrix we can see here that we
had
266 correct and seven wrong these are
the missed labels for bad wine and we
had a lot of missed labels for good wine
so our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random force
classifier and let me go ahead upsell
and change cell type to mark down and
run that so we have a nice label let's
look at our svm classifier our support
vector model and this should look
familiar we have our clf we're going to
create what's we'll call it just like we
call this an rfc and then we'll have our
clf dot fit and this should be identical
to up above x train comma y train and
just like we did before let's go ahead
and do the prediction and here is our
clf predict and it's going to equal the
clf dot predict and we want to go ahead
and use x underscore test and right
about now you can realize that you can
create these different models and
actually just create a loop to go
through your different models and put
the data in and that's how they designed
it they designed it to have that ability
let's go ahead and run this and then
let's go ahead and do our classification
report and i'm just going to copy this
right off of here
they say you shouldn't copy and paste
your code and the reason is is when you
go in here and edit it
you unbearably will miss something we
only have two lines so i think i'm safe
to do it today and let's go ahead and
run this
and let's take a look how the svm
classifier came out so up here we had a
90
and down here we're running about an 86
percent so it's not doing as good now
remember we randomly split the data so
if i run this a bunch of times you'll
see some changes down here so these
numbers this size of data if i read it a
hundred times it would probably be
within plus or minus three or four on
here in fact if i ran this 100 times
you'd probably see these come out almost
the same as far as how well they do in
classification and then on the confusion
matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12.
so it's it's doing not quite as good
that shows up here 71 percent versus 78
percent and then if we're going to do a
svm classifier we also want to show you
one more and before i do that kind of
tease you a little bit here before we
jump into neural networks the big save
all deep learning because everything
else must be shallow learning that's a
joke let's just talk a little bit about
the svm versus the
random forest classifier the svm tends
to work better on smaller numbers it
also works really good on a lot of times
you convert things into numbers and bins
and things like that the random forest
tends to do better with those at least
that's my brief experience with it where
if you have just a lot of raw data
coming in the svm is usually the fastest
and easiest to apply model on there so
they each have their own benefits you'll
find that again that when you run these
like 100 times difference between these
two on a data set like this is going to
just go away there's randomness involved
depending on which data we took and how
they classify them the big one is the
neural networks and this is what makes
the neural networks nice is they can do
they can look into huge amounts of data
so for a project like this you probably
don't need a neural network on this but
it's important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series and the order of the
words is really important same thing
with uh starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when i say lots of
data coming in i'm not talking about
just the high lows that you can run an
svm on real easily i'm talking about the
data that comes in where you have maybe
you pulled off the twitter feeds and
have word counts going on and you've
pulled off the uh the different news
feeds that business are looking at and
the different releases when they release
the different reports so you have all
this different data coming in and the
neural network does really good with
that pictures picture processing now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by google it has a neural
network for doing it's kind of goofy but
you can put little star wars androids
dancing around your pictures and things
like that that's all done with the
neural network so it has a lot of
different uses but it's also requires a
lot of data and is a little heavy-handed
for something like this and this should
now look familiar because we've done it
twice before we have our multi-layered
persepotron classifier we'll call it an
mlpc and it's this is what we imported
mlpc classifier there's a lot of
settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then i went
ahead and just did three layers probably
get by with a lot less on this but i
didn't want to sit and play with it all
afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with overfitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500 the default's 200
because i used three layers of 11 each
which is by the way kind of a default i
use i realized that usually you have
about three layers going down and the
number of features going across you'll
see that's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
x train y train and then we're going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict x test let's just put
that down here dot predict test and if i
run that we've now programmed it we now
have our prediction here same as before
and we'll go ahead and do the copy print
again always be careful with the copy
paste now because you always run the
chance of missing one of these variables
so if you're doing a lot of coding you
might want to skip that copy and paste
and just type it in and let's go ahead
and run this and see what that looks
like and we came up with an 88
we're going to compare that with the 86
from our tree our svm classifier and our
90 from the random force classifier and
keep in mind random forest classifiers
they do good on mid-sized data the svm
on smaller amounts of data although to
be honest i don't think that's
necessarily the split between the two
and these things will actually come
together if you random a number of times
and we can see down here the noun of
good wines mislabeled with a
setup on there it's on par with our
random forest so we had 22.25 shouldn't
be a surprise it's identical it just
didn't do as good with the bad wines
labeling what's a bad one and what's not
let's see yeah because they had 266 and
7. we had down here 260 and 13. so
mislabeled a couple of the bad wines as
good wines so we've explored three of
these basic classifiers these are
probably the three most widely used
right now i might even throw in the
random tree
if we open up their website we go under
supervised learning there's a linear
model we didn't do that almost most of
data usually just start with a linear
model because it's going to process the
quickest i mean use the least amount of
resources but you can see they have
linear quadratic they have kernel ridge
there's our support vector stochastic
gradient nearest neighbors nearest
neighbors is another common one that's
used a lot very similar to the svm
gaussian process cross decomposition
naive bayes this is more of an
intellectual one that i don't see used a
lot but it's like the basis of a lot of
other things decision tree there's
another one that's used a lot ensemble
methods not as much multi-class and
multi-label algorithms feature selection
neural networks that's the other one we
use down here and of course the forest
so you can see there's a in sk learn
there are so many different options and
they just developed them over the years
we covered three of the most commonly
used ones in here and went over a little
bit over why they're different neural
network just because it's fun to work in
deep learning and not in shallow
learning as i told you that doesn't mean
that the svm is actually shallow it does
a lot of it covers a lot of things and
same thing with the decision for the
random forest classifier and we notice
that there's a number of other different
classifier options in there these are
just the three most common ones and i'd
probably throw the nearest neighbor in
there and the decision tree which is
usually part of the decision for us
depending on what the back end you're
using and since as human beings if i was
in the shareholders office i wouldn't
want to leave them with a confusion
matrix they need that information for
making decisions but we want to give
them just one particular score and so i
would go ahead and we have our sklearn
metrics we're going to import the
accuracy score and i'm just going to do
this on the
random forest since that was our best
model and we have our cm accuracy score
and i forgot to print it remember in
jupiter notebook we can just do the last
variable we leave out there will print
and so our cm accurate score we get is
90 percent and that matches up here we
should already see that up here in
precision so you can either quote that
but a lot of times people like to see it
highlighted at the very end this is our
precision on this model and then the
final stage is we would like to use this
for future so let's go ahead and take
our wine if you remember correctly we'll
do one head of 10. we'll run that
remember our original data set we've
gone through so many steps now we're
going to go back to the original data
and we can see here we have our top 10
our top 10 on the list only two of them
make it as having high enough quality
wine for us to be interested in them and
then let's go ahead and create some data
here we'll call it x new equals and this
is important this data has to be we just
kind of randomly selected some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our x
new equals
7.3.58 and so on and then it is so
important this is where people forget
this step x new
equals sc remember sc that was our
standard scalar variable we created if
we go right back up here before we did
anything else we created an sc we fit it
and we transformed it and then we need
to do what transform the data we're
going to feed in so we're going to go
back down here and we're going to
transform our x new and then we were
going to go ahead and use the
where are we at here we go our random
forest and if you remember all it is is
our rfc predict model right there let's
go ahead and just grab that down here
and so our y
new equals here's our rfc predict and we
do our x new in and then it's kind of
nice to know what it actually puts out
so according to this it should print out
what our prediction is for this wine and
oh it's a bad wine okay so we didn't
pick out a good wine for our ex new and
that should be expected most of wine if
you remember correctly only a small
percentage of the wine matter quality
requirements so we can look at this and
say oh we'll have to try another wine
out which is fine by me because i like
to try out new wines and i certainly
have a collection of old wine bottles
and very few of them match but you can
see here we've gone through the whole
process just a quick re rehash we had
our imports we touched a lot on the sk
learn our random forest our svm and our
mlp classifier so we had our support
vector classifier we had our random
forests and we have our neural network
three of the top used classifiers in the
sk learn system and we also have our
confusion metric matrix and our
classification report which we used our
standard scalar for scaling it and our
label encoder and of course we needed to
go ahead and split our data up in our
implot line train and we explored the
data in here for null values we set up
our quality into bins
we took a look at the data and what we
actually have and put a nice little plot
to show our quality what we're looking
at and then we went through our three
different models and it's always
interesting because you spend so much
time getting to these models and then
you kind of go through the models and
play with them until you get the best
training on there without becoming
biased that's always a challenge is to
not over train your data to the point
where you're training it to fit the test
value and finally we went ahead and
actually used it and applied it to a new
wine which unfortunately didn't make the
cut it's going to be the one that we
drink a glass out of and save the rest
from cooking
of course that's according to the random
forest on there because we use the best
model that it came up with welcome to
tensorflow 2.0 tutorial
what's in it for you we're going to
cover today deep learning frameworks
what is tensorflow features of
tensorflow tensorflow applications how
tensorflow works
tensorflow 1.0 versus 2.0 tensorflow 2.0
architecture and then we'll go over a
tensorflow demo where we roll up our
sleeves and dive right into the code
so let's start with deep learning
frameworks
to start with this chart doesn't even do
the filled
justice because it's just exploded these
are just some of the major frameworks
out there there's a cross which happens
to sit on tensorflow so they're very
integrated there's tensorflow
pie torches out there cafe piano uh dl4j
and chainer these are just a few of the
deep learning frameworks we're talking
about neural networks if you're just
starting out never seen a neural network
you can go into python in the scikit and
do the neural network in there which is
probably the most simplest version i
know but the most robust version out
there the most top of the ladder as far
as the technology right now is
tensorflow and that of course is
changing from day to day and some of
these
are
better for different purposes
so let's dive into tensorflow let's see
what is tensorflow what is tensorflow
tensorflow is a popular open source
library released in 2015 by google brain
team for building machine learning and
deep learning models
it is based on python programming
language and performs numerical
computations using data flow graphs to
build models so let's take a look at
some of the features of tensorflow it
works efficiently with multi-dimensional
arrays
if you've ever played with any of the
simpler packages of neural networks
you're going to find that you have to
pretty much flatten them and make sure
your your stuff is set in a flat model
tensorflow works really good so we're
talking pictures here where you have
x and y coordinates where the picture is
and then each pixel has three or four
different channels that's a very
complicated array very multi-dimensional
array it provides scalability of
computation across machines and large
data sets
this is so new right now
and you might think that's a minor thing
but when
python is operating on one computer and
it has a float value
and it truncates it differently on each
computer you don't get the same results
and so your training model might work on
one machine and then another it doesn't
this is one of the things that
tensorflow
addresses and does a very good job on
it supports fast debugging and model
building
this is why i love tensorflow
i can go in there and i can build a
model with different layers each layer
might have different properties
they have like the convolutional neural
network which you can then sit on top of
a regular neural network with reverse
propagation there's a lot of tools in
here and a lot of options and each layer
that it goes through can utilize those
different options and stack differently
and it has a large community and
provides tensorboard to visualize the
model tensorboard is pretty uh recent
but it's a really nice tool to have so
you when you're working with other
people or showing your
clients or the
shareholders in the company you can give
them a nice visual model so they know
what's going on what are they paying for
and let's take a glance at some of the
different uses or applications for
tensorflow when we talk about tensorflow
applications
clearly this is data analytics we're
getting into the data science i like to
use data science as probably a better
term this is the programming side
and it's really the sky is a limit um we
can look at face detection language
translation fraud detection video
detection
there are so many different things out
there that tensorflow can be used for
when you think of neural networks
because tensorflow is a neural network
think of complicated chaotic data this
is very different than if you have a set
numbers like you're looking at the stock
market you can use this on the stock
market but if you're doing something
where the numbers are very clear
and not so chaotic as you have in a
picture then you're talking more about
linear regression models
and different regression models when
you're looking at that when you're
talking about these really complicated
data patterns
then you're talking neural networks in
tensorflow and if we're going to talk
about tensorflow we should talk about
what tensors are after all that is what
tensor that's what this is named after
so we talk about tensors in tensorflow
tensorflow is derived from its core
component known as a tensor a tensor is
a vector or a matrix of n dimensions
that represent all types of data and you
can see here we have the scalar which is
just a single
number you have your vector which is two
numbers
might be a number in a direction
you have a simple matrix and then we get
into the tensor i mentioned how a
picture is a very complicated tensor
because it has your x y coordinates and
then each one of those pixels has three
to four channels for your different
colors
and so each image coming in would be its
own tensor
and
in tensorflow tensors are defined by a
unit of dimensionality called as rank
and you can see here we have our
scalar which is a single number that has
a rank of zero because it has no real
dimensions to it other than it's just a
single point
and then you have your vector which
would be a single list of numbers
so it's a rank one
matrix would have rank two and then as
you can see right here as we get into
the full tensor it has a rank three
and so
the next step is to understand how a
tensorflow works and if you haven't
looked at
the basics of a neural network in
reverse propagation that is the basics
of tensorflow and then it goes through a
lot of different options and properties
that you can build into your different
tensors
so a tensorflow performs computations
with the help of data flow graphs it has
nodes that represent the operations in
your model and if you
look at this you should see a
neural network going on here we have our
inputs bc and d
and you might have x equals b plus c y
equals d minus four
a equals x times y and then you have an
output
and so
even though this isn't a neural network
here it's just a simple set of
computations going across
you can see how the more complicated it
gets the more you can actually one of
the tensors is a neural network with
reverse propagation
but it's not limited to that there's so
much more you can do with it and this
here is just a basic uh flow of
computations of the data going across
and you can see we can plug in the
numbers uh b equals four c equals three
d equals six
and you get x equals four plus three
so x equals seven y equals six minus
four so y equals two and finally a
equals seven times two or a equals
fourteen like i said this is a very
simplified version of how tensorflow
works each one of these layers can get
very complicated
but tensorflow does such a nice job that
you can spin different setups up very
easily and test them out so you can test
out these different models to see how
they work now tensorflow has gone
through two major stages
we had the original tensorflow release
of 1.0 and then they came out with the
2.0 version and the 2.0 addressed so
many things out there that the 1.0
really need it so we start talking about
tensorflow 1.0 versus 2.0
i guess you would need to know this for
a legacy programming job if you're
pulling apart somebody else's code the
first thing is that tensorflow 2.0
supports eager execution by default it
allows you to build your models and run
them instantly and you can see here from
tensorflow 1 to tensorflow 2
we have
almost double the code to do the same
thing so if i want to do with tf.session
or tensorflow session as a session the
session run you have your variables your
session run you have your tables
initializer and then you do your model
fit
x train y train and then your validation
data your x value y value and your epics
and your batch size all that goes into
the fit
and you can see here where that was all
just compressed to make it run easier
you can just create a model and do a fit
on it and you only have like that last
set of code on there so it's automatic
that's what they mean by the eager so if
you see the first part you're like what
the heck is all the session thing going
on that's tensorflow 1.0 and then when
you get into 2.0 it's just nice and
clean
if you remember from the beginning i
said cross
on our list up there
and cross is the high level api in
tensorflow 2.0
cross is the official high level api of
tensorflow 2.0 it has incorporated cross
as tf.caras cross provides a number of
model building apis such as sequential
functional and subclassing so you can
choose the right level of abstraction
for your project and
we'll hopefully touch base a little bit
more on this sequential being the most
common uh form that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional
is where you can split the layer so you
might have your input coming in one side
it splits into two completely mod
different models and then they come back
together
and one of them might be doing
classification the other one might be
doing just linear regression kind of
stuff or a neural basic
reverse propagation neural network and
then those all come together into
another layer which is your
neural network reverse propagation setup
subclassing
is the most complicated as you're
building your own models and you can
subclass your own models into cross so
very powerful tools here this is all the
stuff that's been coming out currently
in the tensorflow cross setup a third
big change we're going to look at is it
in tensorflow 1.0 uh in order to use tf
layers as variables you would have to
write tf variable block so you'd have to
pre-define that
in tensorflow 2 you just add your layers
in under the sequential and it
automatically defines them as long as
they're flat layers
of course this changes a little bit as
the more complicated
tensor you have coming in but all of
it's very easy to do and that's what 2.0
does a really good job of and here we
have
a little bit more on the scope of this
and you can see how tensorflow 1 asks
you to do
these different layers and values if you
look at the scope and the default name
you start looking at all the different
code in there to create the variable
scope that's not even necessary in
tensorf 2.0 so you'd have to do one
before you do do what you see the code
in 2.0 in 2.0 you just create your model
it's a sequential model then you can add
all your layers in you don't have to
pre-create the
um variable scope so if you ever see the
variable scope you know that came from
an older version and then we have the
last two which is our api cleanup and
the autograph
in the api cleanup tensorflow 1 you
could build models using tf gans tf app
tf contrib tf flags etc in tensorflow 2
a lot of apis have been removed and this
is just they just cleaned them up
because people weren't using them and
they've simplified them and that's your
tf app your tf flags your tf logging are
all gone
so there's those are three legacy
features that are not in 2.0
and then we have our tf function and
autograph feature
in the old version uh tensorflow 1 0 the
python functions were limited and could
not be compiled or exported re-imported
so you were continually having to redo
your code you couldn't very easily just
put a pointer to it and say hey let's
reuse this
in tensorflow 2 you can write a python
function using the tf function to mark
it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of tf
function helps to write graph code using
natural python syntax
now we just threw in a new word in you
graph a graph is not a picture of a
person
you'll hear graph x and some other
things
graph is what are all those lines that
are connecting different objects so if
you remember from before where we had
the different layers going through
sequentially each one of those white
lined arrows would be a graph x that's
where that computation is taken care of
and that's what they're talking about
and so if you had your own special code
or python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using
in neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensorflow toolkits
in their hierarchy at the high level we
have our object oriented api so this is
what you're working with you have your
tf cross you have your estimators
this sits on top of your tf layers tf
losses tf metrics so you have your
reusable libraries for model building
this is really where tensorflow shines
is between the cross
running your estimators and then being
able to swap in different layers you can
your losses your metrics all of that is
so built into tensorflow makes it really
easy to use and then you can get down to
your low level tf api
you have extensive control over this you
can put your own formulas in there your
own procedures or models in there
you could have it split we talked about
that earlier so with the 2.0 you can now
have it split one direction we do a
linear regression model and then go to
the other where it does a
neural network and maybe each neural
network has a different activation set
on it and then it comes together into
another layer which is another neural
network so you can build these really
complicated models and at the low level
you can put in your own apis you can
move that stuff around and most recently
we have the tf code can run on multiple
platforms
and so you have your cpu
which is
basically like on the computer i'm
running on i have
eight cores and 16 dedicated threads i
hear they now have one out there that
has over 100 cores
so you have your cpu running and then
you have your gpu which is your graphics
card
and most recently they also include the
tpu setup which is specifically for
tensorflow models
neural network kind of setup
so now you can export the tf code and it
can run on all kinds of different
platforms for the most
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture
we have
you can see on the left this is usually
where you start out with and 80 of your
time in data science is spent
pre-processing data making sure it's
loaded correctly and everything looks
right
so the first level in tensorflow is
going to be your read and pre-processed
data your tf data feature columns
this is going to feed into your tf cross
or your pre-made estimators
and kind of you have your tensorflow hub
that sits on top of there so you can see
what's going on uh once you have all
that set up you have your distribution
strategy where are you gonna run it
you're gonna be running it on just your
regular cpu are you gonna be running it
with the gpu added in
like i have a pretty high-end graphics
card so it actually grabs that gpu
processor and uses it or do you have a
specialized tpu setup in there that you
paid extra money for
it could be if you're
in later on when you're distributing the
package you might need to run this on
some really high processors because
you're processing at a server level for
uh let's say net you might be processing
this at a
distribute you're distributing it not
the distribution strategy but you're
distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done
every minute
and so you need that higher speed to
give them a
to give them a recommendation or a
suggestion so they can buy more stuff
off your website or maybe you're looking
for uh data fraud analysis working with
the banks you want to be able to run
this at a high speed so that when you
have hundreds of people sending their
transactions in it says hey this doesn't
look right someone's scamming this
person and probably has their credit
card so when we're talking about all
those fun things we're talking about
saved model this is we were talking
about that earlier where it used to be
when you did one of these models
it wouldn't truncate the float numbers
the same and so a model going from one
you build the model on your
machine in the office and then you need
to distribute it and so we have our
tensorflow serving cloud on premium
that's what i was talking about if
you're like a banking or something like
that
now they have tensorflow lite so you can
actually run a tensorflow on an android
or an ios or raspberry pi a little
breakout board there in fact they just
came out with a new one that has a
built-in this is a little mini tpu with
the camera on it so it can pre-process a
video so you can load your tensorflow
model onto that
talking about an affordable way to beta
test a new product you have the
tensorflow js which is for browser and
node server so you can get that out on
the browser for some simple computations
that don't require a lot of heavy
lifting but you want to distribute to a
lot of endpoints and now they also have
other language bindings so you can now
create your tensorflow backend save it
and have it accessed from c java go
c sharp rust r or from whatever package
you're working on so we kind of have an
overview of the architecture and what's
going on behind the scenes and in this
case what's going on as far as
distributing it let's go ahead and take
a look at
three specific pieces of tensorflow
and those are going to be constants
variables and sessions
so very basic things you need to know
and understand when you're working with
the tensorflow
setup so constants in tensorflow in
tensorflow constants are created using
the function constant in other words
they're going to stay static the whole
time whatever you're working with
the syntax for constant
value d type 9 shape equals none name
constant verify shape equals false
that's kind of the syntax you're looking
at and we'll explore this with our hands
on a little more in depth
and you can see here we do z equals t f
dot constant 5.2 name equals x
d type is a float that means that we're
never going to change that 5.2 it's
going to be a constant value and then we
have our variables in tensorflow
variables in tensorflow are in memory
buffers that store tensors
and so we can declare a two by three
tensor populated by ones you could also
do constants this way by the way so you
can create a an array of ones for your
constants i'm not sure why you'd do that
but you know you might need that for
some reason
in here we have v equals tf.variables
and then in tensorflow you have tf.ones
and you have the shape which is 2 3
which is then going to create a nice 2
by 3
array that's filled with ones and then
of course you can go in there and
they're variables so you can change them
it's a tensor so you have full control
over that and then you of course have
sessions in tensorflow a session in
tensorflow is used to run a
computational graph to evaluate the
nodes
and remember when we're talking a graph
or graph x we're talking about all that
information then goes through all those
arrows and whatever computations they
have that take it to the next node and
you can see down here where we have
import tensorflow as tf if we do x
equals a tf.constant of 10
we do y equals a tf constant of 2.0 or
20.0 and then you can do z equals
tf.variable
and it's a tf dot add x comma y
and then once you have that set up in
there you go ahead and knit your tf
global variables initializer
with tf session as session you can do a
session run init
and then you print the session run y
and so when you run this you're going to
end up with of course the 10 plus 20 is
30. and we'll be looking at this a lot
more closely as we actually roll up our
sleeves and put some code together
so let's go ahead and take a look at
that and for my coding today i'm going
to go ahead and go through anaconda and
then i'll use specifically the jupiter
notebook on there and of course this
code is going to work uh whatever
platform you choose whether you're in a
notebook
the jupiter lab which is just a jupiter
notebook but with tabs for larger
projects we're going to stick with
jupiter notebook
pycharm whatever it is you're going to
use in here you have your spyder and
your qt console for different
programming environments the thing to
note
is kind of hard to see but i have my
main
pi 3 6.
right now when i was writing this
tensorflow works in python version 3 6.
if you have python version 3 7 or 3 8
you're probably going to get some errors
in there might be that they've already
updated it and i don't know it now you
have an older version
but you want to make sure you're in
python version 3.6 in your environment
and of course in anaconda i can easily
set that environment up make sure you go
ahead and and pip in your tensor flow or
if you're in anaconda you can do a conda
install tensorflow to make sure it's in
your package
so let's just go ahead and dive in and
bring that up
this will open up a nice browser window
i just love the fact i can zoom in and
zoom out depending on what i'm working
on making it really easy to just
demo for the right size go under new and
let's go ahead and create a new python
and once we're in our new python window
this is going to leave it untitled
let's go ahead and import import
tensorflow as tf
at this point we'll go ahead and just
run it real quick
no errors yay no errors
i
i do that whenever i do my imports
because i unbearably will have opened up
a new environment and forgotten to
install tensorflow into that environment
uh or something along those lines so
it's always good to double check
uh and if we're gonna double check that
we also it's also good to know uh what
version we're working with and we can do
that simply by
using the version command in tensorflow
which you should know is is probably
intuitively the tf
dot underscore underscore version
underscore underscore
and
you know it always confuses me because
sometimes you do tf.version for one
thing you do tf dot underscore version
underscore for another thing this is a
double underscore in tensorflow for
pulling your version out
and it's good to know what you're
working with we're going to be working
in tensorflow version 2.1.0 and i did
tell you the the um we were going to dig
a little deeper into our constants and
you can do an array of constants and
we'll just create this nice array
a equals tf.constant
and we're just going to put the ray
right in there 4361.
uh we can run this and now that is what
a is equal to and if we want to just
double check that uh remember we're in
jupiter notebook where you can just put
the letter a
and it knows that that's going to be
print
otherwise you'd round you surround it in
print and you can see it's a tf tensor
it has the shape the type and the and
the array on here it's a two by two
array and just like we can create a
constant we can go and create a variable
and this is also going to be a two by
two array and if we go ahead and print
the v out we'll run that
and sure enough there's our tf variable
in here
then we can also let's just go back up
here and add this in here
i could create another tensor and we'll
make it a constant this time
and we're going to put that in over here
we'll have b tf constant
and if we go and print out v and b
we're going to run that
and this is an interesting thing that
always that happens in here you'll see
right here when i print them both out
what happens it only prints the last one
unless you use print commands so
important to remember that in jupyter
notebooks we can easily fix that by go
ahead and print and surround v with
brackets and now we can see with the two
different variables we have
we have the three one five two which is
a variable and this is just a flat
constant so it comes up as a tf tensor
shape two kind of two and that's
interesting to note
that this label is a tf.tensor and this
is a tf variable
so that's how it's looking in the back
end when you're talking about the
difference between a variable and a
constant
the other thing i want you to notice is
that in variable we capitalize the v and
with the constant we have a lowercase c
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work
so those are a couple little things to
note in here and just like any other
array in math
we can do like a concatenate or
concatenate to the different values here
and you can see we can take a b
concatenated you just do a tf.concat
values and there's our a b axes on one
hopefully you're familiar with axes and
how that works when you're dealing with
matrixes and if we go ahead and print
this out
you'll see right here we end up with a
tensor so let's put it in as a constant
not as a variable
and you have your array four three seven
eight and six one four five it's
concatenated the two together and again
i wanna highlight a couple things on
this our axis equals one this means
we're doing the columns
so if you had a longer array like right
now we have an array that is like you
know has a shape one whatever it is two
comma two
axes
zero
is going to be your first one and axes
one is going to be your second one and
it translates as columns and rows
if we had a shape let me just put the
word shape here
um
so you know what i'm talking about it's
very clear and this is i'll tell you
what i spent a lot of time
looking at these shapes and trying to
figure out which direction i'm going in
and whether to flip it or whatever
so you can get lost in which way your
matrix is going which is column which is
rows are you dealing with the third axes
or the second axes
axes one you know zero one two that's
going to be our columns and if you can
do columns then we also can do rows and
that is simply just changing the
concatenate
we'll just grab this one here and copy
it we'll do the whole thing over
ctrl copy
ctrl v and changes from axis one
to axis zero and if we run that
you'll see that now we concatenate by
row as opposed to column
and you have four three six one seven
eight four seven so it just brings it
right down and turns it into rows versus
columns you can see the difference there
your output
this really you want to look at the
output sometimes just to make sure your
eyes are looking at it correctly and
it's in the format i find visually
looking at it is almost more important
than
understanding
what's going on because conceptually
your mind just just too many dimensions
sometimes
the second thing i want you to notice is
this says a numpy array so tensorflow is
utilizing numpy as part of their format
as far as python is concerned
and so you can treat you can treat this
output like a numpy array because it is
just that it's going to be a numpy array
another thing that comes up uh more than
you would think is filling uh one of
these with zeros or ones and so you can
see here we just create a tensor
tf.zeros
and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer
and then if we
print out our tensor again we're in
jupiter so i can just type out tensor
and i run this you can see i have a nice
array of the shape three comma four of
zeros one of the things i want to
highlight here is integer 32 if i go to
the
tensorflow data types i want you to
notice how
we have float 16 float 32 float 64
complex if we scroll down you'll see the
integer down here 32 the reason for this
is that we want to control how many bits
are used in the precision
this is for exporting it to another
platform
uh so what would happen is i might run
it on this computer where python goes
does a float to indefinite however long
it wants to
and then we can take it but we want to
actually say hey we don't want that high
precision we want to be able to run this
on any computer and so we need to
control whether it's a tf float 16 in
this case we did an integer 32
we could also do this as a float
so if i run this as a float 32
that means this has a 32-bit precision
you'll see zero point whatever and then
to go with zeros
we have ones if we're going from the
opposite side and so we can easily just
create a tensorflow with ones and you
might ask yourself why would i want
zeros and ones and your first thought
might be to initiate a new tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
a uniform
set of ones or zeros you're dealing with
a lot of bias so be very careful about
starting a neural network
for one of your rows or something like
that with ones and zeros
on the other hand
i use this for masking you can do a lot
of work with masking you can also have
it might be that
one tensor row is masked
you know zero is is false one is true or
whatever you want to do it um
and so in that case you do want to use
the zeros and ones and there are cases
where you do want to initialize it with
all zeros or all ones and then swap in
different numbers as
the tensor learns so it's another form
of control
but in general
you see zeros and ones you usually are
talking about a mask over another array
and just like in numpy you can also
do reshapes so if we take our remember
this is shaped three comma four maybe we
wanna swap that to four comma three
and if we print this out
you will see let me just go and do that
ctrl v
let me run that
and you'll see that the the order of
these is now switched instead of four
across now we have three across and four
down
and just for fun let's go back up here
where we did the ones
and i'm going to change the ones to
tf.random
uniform
and we'll go ahead and just take off
well we'll go and leave that we'll go
and run this
and you'll see now we have 0.0441
and this way you can actually see how
the reshape looks a lot different
0.041.15.71 and then instead of having
this one it rolls down here to the 0.14
and this is what i was talking about
sometimes you fill a lot of times you
fill these with random numbers and so
this is the random.uniform is one of the
ways to do that now i just talked a
little bit about this float 32 and all
these data types
one of the things that comes up of
course is
recasting your data
so if we have a d type float 32 we might
want to convert these to integers
because of the project we're working on
i know one of the projects i've worked
on
ended up wanting to do a lot of round
off so that it would take a dollar
amount or a float value and then have to
rent it off to a dollar amount so we
only wanted two decimal points um in
which case you have a lot of different
options you can multiply by 100 and then
round it off or whatever you want to do
there's a lot of or then converted to an
integer was one way to round it off
kind of
a cheap and dirty trick
uh so we can take this and we can take
this same
tensor and we'll go ahead and create a
um as an integer and so we're going to
take this tensor we're going to tf.cast
it
and if we print
tensor
and then we're going to go ahead and
print
our
tensor
let me just do a quick copy and paste
and when i'm actually programming i
usually type out a lot of my stuff just
to double check it
in doing a demo
copy and paste works fine but sometimes
be aware that copy and paste can copy
the wrong code over
personal choice depends on what i'm
working on
and you can see here we took a float 32
4.6 4.2 and so on and it just converts
it right down to a integer value
that's our integer 32 set up and
remember we talked about
a little bit about reshape
as far as flipping it and i just did
four comma three on the reshape up here
and we talked about axes zero axis one
uh one of the things that is important
to be able to do is to take one of these
variables we'll just take this last one
tensor as integer
and i want to go ahead and transpose it
and so i can do
we'll do a
equals tf.transpose
and we'll do our tensor integer in there
and then if i print the a out and we run
this
you'll see that's the same array but
we've flipped it so that our columns and
rows are flipped this is
the same as reshaping uh so when you
transpose you're just doing a reshape
what's nice about this is if you look at
the numbers the columns
when we went up here and we did the
reshape they kind of rolled down to the
next row so you're not maintaining the
structure of your matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a transform
uh so we come down here it's all done
for you and so there are so many times
you have to transpose
your digits that this is important to
know that you can just do that you can
flip your rows and columns rather
quickly here and just like numpy you can
also do multiple your different math
functions we'll look at multiplication
and so we're going to take matrix
multiplication of tensors
we'll go ahead and create a
as a constant 5839
and we'll put in a vector v 4 comma 2.
and we could have done this where they
matched where this was a 2 by 2 array
but instead
we're going to do just a 2 by 1 array
and the code for that is your tf.mat
mole
so matrix multiplier and we have a times
v and if we go ahead and run this up
let's
make sure we print out our av on there
and if we go ahead and run this
you'll see that we end up with 36 by 30.
and if it's been a while since you've
seen the matrix math
this is
5 times 4 plus 8 times 2
um three times four plus nine times two
and that's where we get the 36 and 30.
now i know we're covering a lot really
quickly as far as the basic
functionality
so the matrix or your matrix multiplier
is a very commonly used back-end tool as
far as computing
different models or linear regression
stuff like that
one of the things
is to note is that just like in
numpy you have all of your different
math so we have our tf math
and if we go in here we have
functions we have our cosines absolute
angle all of that's in here so all of
these
are available for you to use in the
tensorflow model
and if we go back to our example and
let's go ahead and pull
oh let's do some multiplication that's
always good we'll stick with our
av
our
constant a and our vector v
and we'll go ahead and do some bit wise
multiplication and we'll create an av
which is a times b let's go and print
that out
and you can see coming across here
we have the 4 2 and the 5 8 3 9 and it
produces 20 32 6 18.
and that's pretty straight forward if
you look at you have 4 times
5 is 20 4 times 8 is
32 that's where those numbers come from
now we can also quickly create an
identity matrix
which is basically
your main values on the diagonal being
ones and zeros across the other side
let's go ahead and take a look and see
what that
looks like and we can do let's do this
ah
so we're going to get the shape this is
a simple way very similar to your numpy
you can do a dot shape and it's going to
return a tuple in this case our rows and
columns and so we can do a quick print
we'll do rows
and we'll do columns
and if we run this
you can see we have three rows
two columns
and then if we go ahead and create an
identity matrix
the script for that
got a wrong button there the script for
that looks like this
where we have the number of rows equals
rows the number of columns equals
columns and d type is a 32 and then if
we go ahead and just print out our
identity
you can see we have a nice identity
column with our ones going across here
now clearly we're not going to go
through every math module
available but we do want to start
looking at this as a prediction model
and seeing how it functions
so we're going to move on to a more of a
direct setup we can actually see the
full tensorflow in use for that let's go
back and create a
new setup
and we'll go in here new python 3 module
there we go
bring this out so it takes up the whole
window because i like to do that
hopefully you made it through that first
part and you have a basic understanding
of tensorflow as far as being a series
of numpy arrays you've got your math
equations and different things that go
into them
we're going to start building a full
setup as far as the numpy so you can see
how
kara sits on top of it and the different
aspects of how it works
the first thing we want to do is we're
going to go ahead and do a lot of
imports
date times warning scipy scipy is your
um math so the back end scientific math
warnings because
whenever we do a lot of this you have
older versions newer versions
and so sometimes when you get warnings
you want to go ahead and just suppress
them we'll talk about that if it comes
up on this particular setup and of
course date time
pandas again is your data frame think
rows and columns we import it as pd
numpy is your numbers array
which of course tensorflow is integrated
heavily with
seaborne for our graphics and the
seaborn as sns is going to be set on top
of our map plot library which we import
as mpl and then of course we're going to
import our matplot library pi plot as
plt and right off the bat we're going to
set some graphic colors um patch force
edge color equals true
the style we're going to use the 538
style you can look this all up there's
when you get into matplot library into
seaborn there are so many options in
here it's just kind of nice to make it
look pretty when we start the um
when we start up that way we don't have
to think about it later on
uh and then we're going to take we have
our uh mplrc we're going to put a patch
add color dim gray line width again this
is all part of our graphics here in our
setup uh we'll go ahead and do an
interactive shell node interactivity
equals last expression
here we are pd for pandas options
display max columns so we don't want to
display more than 50.
and then our matplot library is going to
be inline this is a jupiter notebook
thing the matplot library in line then
warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
don't have to worry about them
not really what you want to do when
you're working on a major project you
want to make sure you know those
warnings and then
filter them out and ignore them later on
and if we run this it's just going to be
loading all that into the background
so that's a little back end kind of
stuff then we want to go ahead and do is
we want to go ahead and import
our specific packages
that we're going to be working with
which is under keras now remember cross
kind of sits on tensorflow so when we're
importing cross and the sequential model
we are in effect importing
tensorflow underneath of it
we just brought in the math probably
should have put that up above
and then we have our cross models we're
going to import sequential now if you
remember from our
slide there was three different options
let me just flip back over there so we
can have a quick uh recall on that and
so in cross uh we have sequential
functional and subclassing so remember
those three different setups in here we
talked about earlier and if you remember
from here we have a sequential where
it's going
one tensor flow layer at a time you go
kind of look at think of it as going
from left to right or top to bottom or
whatever direction it's going in but it
goes in one direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate
tensors and then it comes back together
into another tensor
all those kinds of things and then
subclassing is really the really
complicated one where now you're adding
your own subclasses into the tensor to
do external computations right in the
middle of like a huge flow of data
but we're going to stick with sequential
it's not a big jump to go from
sequential to functional but we're
running a sequential tensorflow and
that's what this first import is here we
want to bring in our sequential and then
we have our layers and let's talk a
little bit about these layers this is
where cross and tensorflow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built
so from cross we have layers import
dense
from cross layers import lstm
when we talk about these layers
cross has so many built-in layers you
can do your own layers
the dense layer is your standard neural
network
by default it uses relu for its
activation
and then the lstm is a long short term
memory layer since we're going to be
looking probably at sequential data
we want to go ahead and do the lstm and
if we go into um cross and we look at
their layers this is across website you
can see as we scroll down for the cross
layers that are built in
we can get down here and we can look at
let's see here we have our layer
activation our base layers
activation layer weight layer waste
there's a lot of stuff in here we have
the relu which is the basic activation
that was listed up here for layer
activations you can change those and
here we have our core layers
and are dense layers you have an input
layer a dense layer
and then we've added a more customized
one with the long term short term memory
layer and of course you can even do your
own custom layers in cross there's a
whole functionality in there if you're
doing your own thing what's really nice
about this is it's all built in even the
convolutional layers this is for
processing graphics there's a lot of
cool things in here you can do
this is why cross is so popular it's
open source and you have all these tools
right at your fingertips so from cross
we're just going to import a couple
layers the dense layer
and the long short term memory layer
and then of course from
sk learn our scikit
we want to go ahead and do our min max
scalar standard scalar for pre editing
our
data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in
is going to be air quality.csv
and let's go ahead and take a quick look
at that this is in openoffice it's just
a standard you know like you can do
excel whatever you're using for your
spreadsheet and you can see here we have
a number of columns uh number of rows it
actually goes down to like 8 000.
the first thing we want to notice is
that the first row
is kind of just a random number put in
going down
probably not something we're going to
work with
the second row
is bandung
i'm guessing that's a reference for the
profile
if we scroll to the bottom which i'm not
going to do because it takes forever to
get back up
they're all the same
the same thing with the status the
status is the same
we have a date so we have a sequential
order here
here is the jam which i'm going to guess
is the time stamp on there so we have a
date and time
we have our o3 co no2 reading so2 no co2
voc
and then some other numbers here pm1 pm
2.5 pm 4 pm 10
10
without actually
looking through the data i mean some of
this i can guess is like temperature
humidity i'm not sure what the pms are
but we have a whole slew of data here so
we're looking at air quality as far as
an area in a region and what's going on
with our date time stamps on there and
so code wise we're going to read this
into a pandas data frame so our data
frame df is a nice abbreviation commonly
used for data frames equals pd.read csv
and then our the path to it just happens
to be on my d drive uh separated by
spaces and so if we go ahead and run
this
we'll print out the head of our data and
again this looks very similar to what we
were just looking at
being in jupiter i can take this and go
the other way
make it real small so you can see all
the columns going across and we get a
full view of it
or we can bring it back up in size
that's pretty small on there overshot
but you can see it's the same data we
were just looking at we're looking at
the number we're looking at the profile
which is the bandung the
date we have a timestamp
our 03 count co and so forth on here
and this is just your basic
pandas printing out the top five rows we
could easily have done
three rows
five rows ten whatever you want to put
in there by default that's five for
pandas now i talk about this all the
time so i know i've already said it at
least once or twice during this video
most of our work is in pre-formatting
data what are we looking at how do we
bring it together
so we want to go ahead and start with
our
date time it's come in in two columns
we have our date here and we have our
time
and we want to go ahead and combine that
and then we have this is just a simple
script in there that says combine date
time that's our formula we're building
our we're going to submit our
pandas data frame
and the tab name
when we go ahead and do this
that's all of our information that we
want to go ahead and create
and then goes for i in range df
shape 0.
so we're going to go through
the whole setup and we're going to list
tab append df location i
and here is our
date going in there and then return the
numpy array list tab d types date time
64. that's all we're doing we're just
switching this to a date time stamp and
if we go ahead and do df date time
equals combined date time
and then i always like to
print we'll do df head
just so we can see what that looks like
and so when we come out of this
we now have our setup on here and of
course it's edited on to the far right
here's our date time
you can see the format's changed
so there's our we've added in the date
time column and we've brought the date
over and we've taken this format here
and it's an actual variable with a 0 0 0
on here well that doesn't look good so
we need to also include the time part of
this we want to convert it into hourly
data
so let's go ahead and do that
to do that to finish combining our date
time let's go ahead and create a
little
script here to combine the time in there
same thing we just did we're just
creating a numpy array returning a numpy
array and cr forcing this into a date
time format and we can actually spend
hours just going through uh these
conversions how do you
pull it from the pandas data frame how
do you set it up um so i'm kind of
skipping through it a little fast
because i want to stay focused on
tensorflow and cross
keep in mind this is like 80 of your
coding when you're doing a lot of this
stuff is going to be reformatting these
things resetting them back up
so that it looks right on here and you
know just takes time to get through all
that but that is usually what the
companies are paying you for that's what
the big
bucks are for
and we want to go ahead and a couple
things going on here is we're going to
go ahead and do our date time we're
going to reorganize some of our setup in
here convert into hourly data we just
put a pause in there
now remember we can select from df are
different columns we're going to be
working with and you're going to see
that we actually dropped
a couple of the columns those ones i
showed you earlier they're just
repetitive data so there's nothing in
there that exciting
and then we want to go ahead and we'll
create a second
data frame here let me just get rid of
the df head
and df2 is we're going to group by date
time and we're looking at the mean value
and then we'll print that out so you can
see we're talking about
we have now reorganized this so we put
in date time 03 co
so now this is in the same order
as it was before
and you'll see the date time now has our
zero zero same date 1 2 3 and so on so
let's group the data together
so there's a lot more manageable and in
the format we want and in the right
sequential order
and if we go back to
there we go our air quality
you can see right here we're looking at
um
these columns going across we really
don't need since we're going to create
our own
date time column we can get rid of those
these are the different columns of
information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data
grouped it together by date and then you
can see the different data coming out
set up on there and then as a data
scientist
first thing i want to do is get a
description what am i looking at uh and
so we can go ahead and do the df2
describe and this gives us our you know
describe gives us our basic uh data
analytics information we might be
looking for like what is the mean
standard deviation
uh minimum amount maximum amount we have
our first quarter second quarter and
third quarter
um
numbers also in there uh so you can get
a quick look at a glance describing the
data or descriptive analysis
and even though we have our quantile
information in here we're going to dig a
little deeper into that
we're going to calculate the quantile
for each variable uh we're going to look
at a number of things for each variable
and we'll see right here q1 we can
simply do the quantile 0.25 percent
which should match
our 25 percent up here and we'll be
looking at the min the max
and we're just going to do this is
basically we're breaking this down
for each uh different
variable in there
one of the things that's kind of fun to
do
we're going to look at that in just a
second let me get put the next piece of
code in here um clean out some of our um
we're going to drop a couple thing our
last rows and first row because those
have
usually have a lot of null values in the
first row is just our titles so that's
important it's important to drop those
rows in here and so this right here as
we look at our different quantiles
again it's it's the same you know we're
still looking at the 25
quantile here
we're going to do a little bit more with
this
so now that we've cleared off our first
and last rows
we're going to go ahead and go through
all of our columns and this way we can
look at each
column individually and so we'll
just create a q1 q3 min max min iqr max
iqr
and calculate the quantile of i of df2
we're basically doing
the math that they did up here but we're
splitting it apart that's all this is
and this happens a lot because you might
want to look at individual if this was
my own project i would probably spend
days and days going through
what these different values mean
one of the biggest
data science uh
things we can look at that's important
is uh use your use your common sense
you know if you're looking at this data
and it doesn't make sense and you go
back in there and you're like wait a
minute what the heck did i just do at
that point you probably should go back
and double check what you have going on
now
we're looking at this and you can see
right here here's our attribute for our
o3 so we've broken it down
we have our q1 5.88 q3 10.37 if we go
back up here here's our 5.8 we've
rounded it off
10.37 is in there
so we've basically done the same math
just split it up we have our minimum and
our max iqr and that's computed let's
see where is it here we go uh q1 minus
1.5 times iqr and the iqr is your q3
minus q1 so that's the difference
between our two different quarters this
is all
data science
as far as the hard math
we're really not we're actually trying
to focus on cross and tensorflow you
still got to go through all this stuff i
told you 80 percent of your programming
is going through and understanding what
the heck
happened here
what's going on what does this data mean
and so when we're looking in that we're
going to go ahead and say hey
we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your minimum iqr
uh that means something's going wrong
there and they usually in this case is
going to show us an outlier so we want
to go ahead and find the minimum value
if it's less than the min minimum iqr
it's an outlier and if the max value is
greater than the
max iqr we have an outlier and that's
all this is doing low outliers found
minimum value high outliers found
really important actually outliers are
almost everything in data sometimes
sometimes you do this project just to
find the outliers because you want to
know
crime detection what are we looking for
we're looking for the outliers what
doesn't fit a normal business deal and
then we'll go ahead and throw in um just
threw in a lot of code oh my goodness uh
so we have if your max is greater than
iqr print outlier is found what we want
to do is we want to start cleaning up
these outliers and so we want to convert
we'll do create a convert nand
x max iqr equals max
underscore iqr min iqr equals mini qr so
this is just saying this is the data
we're going to send that's all that is
in python and if x is greater than the
max iqr and x is less than the min iqr x
equals uh null we're going to set it to
null why because we want to clear these
outliers out of the data now again if
you're doing fraud detection you would
do the opposite you would be cleaning
everything else that's not in that
series so that you can look at just the
outlier and then we're going to convert
the nand hum again we have x
max iqr is 100 percent min iqr is min
iqr
if x is greater than max iqr and x is
less than min iqr again we're going to
return a null value otherwise it's going
to remain the same value x x equals x
and you can see as we go through the
code if i equals
our hqm
then we go ahead and do that's the
that's a column specific to humidity
that's your hum column
uh then we're going to go ahead and
convert do the
run a map on there and convert the non
e2m
uh you can see here it's this is just
cleanup uh we run we found out that
humidity probably has some weird values
in it
we have our outliers
that's all this is
and so when we go ahead and finish this
and we take a look at our outliers
and we run this code here
we have a low outlier 2.04 we have a
high outlier
99.06
outliers have been interpolated
that means we've given them a new value
chances are these days when you're
looking at something like
these sensors coming in
they probably have a failed sensor in
there something went wrong
that's the kind of thing that you really
don't want to do your data analysis on
uh so that's what we're doing is we're
pulling that out and then uh converting
it over and setting it up uh method
linear
so we interpolate left linear as it's
going to fill that data in based on a
linear regression model of similar data
same thing with this up here with the
df2y interpolate that's what we're doing
again this is all data prep we're not
actually talking about tensorflow we're
just trying to get all our data
set up correctly so that when we run it
it's not going to cause problems or have
a huge bias
so we've dealt with outliers
specifically in
humidity and again this is one of these
things where when we start running
we run through this you can see down
here that we have our
outliers found
high low outliers
migrated them in
we also know there's other issues going
on with this data
how do we know that
some of it's just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and use a logarithmic function on
the temp value
and
it's interesting because it's like how
do you how do you heck do you even know
to use logarithmic on the temp value
that's domain specific
we're talking about being an expert in
air care i'm not an expert in air care
um you know it's not
what i go look at i don't look at air
care data in fact this is probably the
first air care data set up i've looked
at but the experts come in there and
they come to you and say hey in data
science this is a exponentially variable
variable on here so we need to go ahead
and do
transform it
and use a logarithmic scale on that
so at that point that would be coming
from your
data here we go data science programmer
overview does a lot of stuff connecting
the database and connecting in with the
experts
data analytics a lot of times you're
talking about somebody who is a data
analysis might be all the way usually a
phd level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it
so when we're looking at this we're
looking at something they've sent to me
and they said hey
domain air care
this needs to be this is a skew because
the data just goes up exponentially and
affects everything else and we'll go
ahead and take that data let me just go
ahead and run this
just for another quick look at it
we have our
uh we'll do a distribution df
we'll create another data frame from the
temp values and then from a data set
from the
log temp so we can put them side by side
and we'll just go ahead and do a quick
histogram and this is kind of nice plot
of figure figure size here's our plt
from matplot library
and then we'll just do a distribution
underscore df there's our data frames
this is nice because it just integrates
the histogram right into pandas love
pandas
and this is the chart you would send
back to your data analysis and say hey
is this what you wanted this is how the
data is converting on here as a data
scientist scientist the first thing i
note is we've gone from a 10
20 30 scales a 2.5 3.0 3.5 scale
and the data itself has kind of been
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our
cross on here
we'll go ahead and split our data up
and this of course is any good data
scientists
you want to have a training set and a
test set
and we'll go ahead and do the train size
we're going to use 0.75 percent of the
data make sure it's an integer we don't
want to take a slice as a float value
give you a nice error
and we'll have our train size of 75
percent and the test size is going to be
of course the train size minus the
length of the data set and then we can
simply do train comma test
here's our data set
which is going to be the train size the
test size uh and then if we go and print
this let me just go ahead and run this
we can see how these values
split it's a nice split of 1298 and then
433 points of value
they're going to be for our
setup on here and if you remember we're
specifically looking at the data set
where did we create that data set from
um that was from up here that's what we
called the
logarithmic
value of the temp
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do
convert an array of values into a data
set matrix we're going to create a
little
setup in here we'll create our data set
our data set's going to come in we're
going to do a look back of one so we're
going to look back one piece of data
going backward
and we have our data x and our data y
for i and range length of data set look
back -1
this is creating let me just go ahead
and run this actually the best way to do
this
is to go ahead and create this data
and take a look at the shape of it let
me go ahead and just put that code in
here
so we're going to do a look back one
here's our train x our train y
and it's going to be adding the data on
there and then when we come up here
and we take a look at the shape
there we go
and we run this piece of code here
we look at the shape on this and we have
a new slightly different change on here
but we have a shape of x 1296 comma 1
shape of y train y test x text y
and so what we're looking at is that
the x comes in
and we're only having a single value out
we want to predict what the next one is
that's what this little piece of code is
here for what are we looking for well we
want to look back one that's the um what
we're going to train the data with is
yesterday's data yesterday says hey the
humidity was at
97
what should today's humidity be at if
it's 97 yesterday is it going to go up
or is it going to go down today if 97
does it go up to 100 what's going on
there uh and so our we're looking
forward to the next piece of data which
says hey tomorrow's is going to you know
today's humidity is this this is what
tomorrow's humidity is going to be
that's all that is all that is is
stacking our data so that
our y is basically
x plus 1 or x could be y minus 1.
and then a couple things to note is our
x data
we're only dealing with the one column
but you need to have it in a shape that
has it by the columns so you have the
two different numbers and since we're
doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here
now this is going to run into a problem
and the reason is is that we have what
they call a time step
and the time step is that long-term
short-term memory layer
uh so we're going to add another reshape
on here let me just go down here and put
it into the next cell
and so we want to reshape the input
array in the form of sample time step
features
we're only looking at one feature
and i mean this is one of those things
when you're playing with this you're
like why am i getting an error in the
numpy array why is this giving me
something weird going on
so we're going to do is we're going to
add one more
level on here instead of being 1299.1 we
want to go one more
and when they put the code together in
the back you can see we kept the same
shape the 1299
we added the one dimension and then we
have our train x shape one
and this could have depends again on how
far back in the long short term memory
you want to go
that is what that piece of code is for
and that reshape is and you can see the
new shape is now 1
12 99 1 1
versus the 1299 one and then the other
part of the shape 432 1 1
again this is our tr our x in and of
course our test x and then our y is just
a single column because we're just doing
one output that we're looking for
so now we've done our eighty percent um
you know that's all the the
writing all the code reformatting our
data
um
bringing it in now we want to go ahead
and do the fun part which is we're going
to go ahead and create and fit
the lstm neural network
and if we're going to do that the first
thing we need is we're going to need to
go ahead and create a model and we'll do
this sequential model
and if you remember sequential means it
just goes in order that means we have if
you have two layers the layers go from
layer one to layer two or layer zero to
layer one
this is different than functional
functional allows you to split the data
and run two completely separate models
and then bring them back together
we're doing just sequential on here and
then we decided to do the long short
term memory uh and we have our input
shape which it comes in again this is
what all this switching was we could
have easily made this one two three or
four going back as far as the end number
on there we just stuck to going back one
and it's always a good idea when you get
to this point where the heck is this
model coming from
what kind of models do we have available
and there's let me go and put the next
model in there
because we're going to do two models and
the next model is going to go ahead and
we're going to do dents so we have model
equal sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our code
where we did the imports oops there we
go our cross
this is it right here here's our
importing a dense model and here's our
importing an lstm now just about every
tensorflow model uses dents
your dense model is your basic
forward propagation reverse propagation
error
or it does reverse propagation to
program the model
so any of your neural networks you've
already looked at that
luxon says here's the error and sends
the error backwards that's what this is
the long short-term memory is a little
different the real question that we want
to look at right now is where do you
find these models what kind of models do
you have available and so for that let's
go to the cross website
which is the cross dot io
if you go under api
layers and i always have to do a search
just search for cross api layers it'll
open up and you can see we have
your base layers right here class
trainable weights all kinds of stuff
like there your activation
so a lot of your layers you can switch
how it activates
relu which is like your smaller arrays
or if you're doing convolutional neural
networks the convolution usually uses a
relu
your sigmoid all the way up to soft mac
soft plus all these different choices as
far as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer
so you have an input object your dense
layer your activation layer embedding
layer
this is your your kind of your one set
up on there that's most common
uh convolutional neural networks or
convolutional layers these are like for
doing image categorizing so trying to
find objects in a picture that kind of
thing
we have pooling layers so as you have
the layers come together
usually you bring them down into
a single layer although you can still do
like global max pulling 3d and there is
just i mean this list just goes on and
on there's all kinds of different things
hidden in here as far as what you can do
and it changes you know you go in here
and you just have to do a search for
what you're looking for
and figure out what's going to work best
for you
as far as which project you're working
on
long short-term memory is a big one
because this is when we start talking
about text
what if someone says the what comes
after the
the cat and the hat a little kid's book
there
it starts programming it and so you
really want to know not only
what's going on but it's going to be
something that has a history the history
behind it tells you what the next one
coming up is
now once we've built all our different
you know we built our model we've added
our different layers we went in there
play with it remember if you're in
functional you can actually link these
layers together and they branch out and
come back together if you do a
the sub
setup then you can create your own
different model you can embed a model in
there that might be coming linear
regression you can embed a linear
regression model
as part of your functional split and
then have that come back together with
other things
so we're going to go ahead and compile
your model this brings everything
together we're going to put in what the
loss is which will use the mean squared
error
and we'll go ahead and use the atom
optimizer clearly there's a lot of
choices on here depending on what you're
doing and just like any of these
different prediction models if you've
been doing any
scikit from python
you'll recognize that we have to then
fit the model
so what are we doing in here we're going
to send in our train x our train y
um we're going to decide how many epochs
we're going to run it through
500 is probably a lot for this i'm
guessing it'd probably be about 200 or
300 probably do just fine
our batch size
so how many different uh when you
process it this is the math behind it
if you're in data analytics
you might try know what this number is
as a data scientist where i haven't had
the phd level math
that says this is why you want to use
this particular batch size you kind of
play with this number a little bit
you can dig deeper into the math
see how it affects the results depending
what you're doing
and there's a number of other settings
on here we did verbose 2. i'd have to
actually look that up to tell you what
verbose means i think that's actually
the default on there if i remember
correctly
there's a lot of different settings when
you go to fit it
the big ones are your epic and your
batch size those are what we're looking
for and so we're going to go ahead and
run this
and this is going to take a few minutes
to run because it's going through
500 times
through all the data so if you have a
huge data set this is the point where
you're kind of wondering oh my gosh is
this going to finish tomorrow
if i'm running this on a single machine
and i have a tera terabyte of data
going into it
if this is my personal computer and i'm
running a terabyte of data into this um
you know this is running rather quickly
through all 500 iterations uh but yeah a
terabyte of data we're talking something
closer to days week
you know even with a
3.5 gigahertz machine and in eight cores
it's still going to take a long time to
go through a full terabyte of data
and then we want to start looking at
putting it into some other framework
like spark or something that will probe
the process on there more across
multiple um processors and multiple
computers
and if we scroll all the way down to the
bottom you're going to see here's our
square mean error of 0.0088
if we scroll way up you'll see it kind
of oscillates between 0.088 and 08089
it's right around 2
250 where you start seeing that
oscillation where it's really not going
anywhere so we really didn't need to go
through a full 500 epics
you know if you're retraining the stuff
over and over again it's kind of good to
know where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model
we want to go ahead and run a prediction
on it
so let's go ahead and make our
prediction remember we have our training
test set
and our test set or the
we have the
train x and the train y for training it
or train predict and then we have our
test x and our test y going in there
so we can test to see how good it did
and we come in here we have
you'll see right here we go ahead and do
our train predict equals
model predict train x
and test predict model predict test x
why would we want to run the prediction
on train x well it's not 100 on its
prediction we know it has a certain
amount of error and we want to compare
the error we have on what we programmed
it with with the error we get when we
run it on new data that's never seen the
model's never seen before and one of the
things we can do we go ahead and invert
the predictions this helps us
level it off a little bit more
get rid of some of our bias we have
train predict equals and np
exponential m1 the train predict
and then train y equals the exponential
m1 for train y and then we do the again
that with train test predict and test y
um
again reformatting the data so that we
can it all matches and then we want to
go ahead and calculate the root mean
square error
so we have our train score
which is your math square root times the
mean square root error train
y and train predict and again we're just
this is just feeding the data through so
we can compare it and the same thing
with the test
and let's take a look at that because
really
the code makes sense if you're going
through it line by line you can see
we're doing but the answer really helps
to zoom in
so we have a train score which is 2.40
of our root mean square error
and we have a test score of 3.16 of the
root mean square error
if these were reversed if our test score
is better than our training score than
we've over trained something's really
wrong at that point you got to go back
and figure out what you did wrong
because you should never have a better
result on your test data than you do
when you're training data and that's why
we put them both through that's why we
look at the error for both the training
and the testing
when you're going out and quoting you're
publishing this and you're saying hey
how good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is and a lot of times you
actually want to put together like a
little formal code
where we actually want to print that out
and if we print that out you can see
down here
test prediction standard deviation of
data set 3.16 is less than 4.40
i'd have to go back
and we're
up here if you remember we did the
square means error this is standard
deviation that's why these numbers are
different
it's saying the same thing that we just
talked about
uh
3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
and along with
putting a formal print out of there um
we want to go ahead and plot what's
going on
uh and this we just want to pretty graft
here so that people can see what's going
on when i walk into a meeting and i'm
dealing with a number of people
they really don't want these numbers
they don't want to say hey what's i mean
standard deviation unless you know what
statistics are
you might be dealing with a number of
different departments head of cells
might not work with standard deviation
or have any idea what that really means
number wise and so at this point we
really want to put it in a graph so we
have something to display and with
displaying you remember that we're
looking at the data
today going into it and what's going to
happen tomorrow
so let's take a quick look at this we're
going to go ahead and shift the train
predictions for plotting we have our
train predict plot
np empty like data set train predict
plot
set it up with null values
you know it's just kind of it's kind of
a weird thing where we're creating the
um
the
data groups as we like them
and then putting the data in there is
what's going on here
so we have our train predict plot
uh which is going to be our look back
our length
plus look back
we're just is going to equal train uh
train predict so we're creating this
basically we're taking this and we're
dumping the train predict into it so now
we have our nice train predict plot
and then we have the shift test
predictions for the plotting we're going
to continue more of that oops looks like
i put it in here double no it's just
yeah they put it in here double
um didn't mean to do that
we really only need to do it once oh
here we go
um this is where the problem was is
because
this is the test predict
so we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
data sets
test
predict plot length prediction
setup on there
and then we're going to go through the
plotting the original data set and the
predictions so we have a time axis
always nice to have your time set on
there
set that to the time array
time axes lap
all this is setting up the time variable
for the bottom and then we have a lot of
stuff going on here as far as setting up
our figure
let's go ahead and run that and then
we'll break it down
we have on here
uh our main plot we have two different
plots going on here the ispu going up
and the data and the ispu here with all
these different settings on it
and so we look at this we have our ax1
that's the main plot i mean our ax
that's the main plot and we have our ax1
which is the secondary plot over here so
we're doing a figure plt
or plt.figure and we're going to dump
those two graphs on there
and so we take
and if you
go through the code piece by piece
uh which we're not going to do we're
going to do the um the
data set here
exponential reverse exponential so it
looks correctly we're going to label it
the original data set
we're going to plot the train predict
plot that's what we just created
we're going to make that orange and
we'll label it train prediction
test predicts plot we're going to make
that red and label it test prediction
and so forth
set our ticks up this actually just put
ticks time axes gets its ticks
the little little marks there going
along the axes that kind of thing and
let's take a look and see what these
graphs look like
and these are just kind of fun you know
when you show up into a meeting and this
is the final output you say hey this is
what we're looking at
here's our original data in blue
here's our training prediction
you can see that it trains pretty close
to what the data is up there
i would also probably put a
like a little little time stamp and do
just right before and right after where
we go from train to test prediction
and you can see with the test prediction
the data comes in in red
and then you can also see what the
original data set look like behind it
and how it differs
and then we can just isolate that here
on the right that's all this is
is just the
test prediction on the right uh and it's
you know there's you'll see with the
original data set there's a lot of peaks
were missing and a lot of lows were
missing but as far as the actual test
prediction it's pretty does pretty good
it's pretty right on you can get a good
idea what to expect for your ispu
and so from this we would probably
publish it and say hey this is
what you expect and this is our area of
this is a range of error that's the kind
of thing i put out on a daily basis
maybe we predict the cells are going to
be this or maybe a weekly
so you kind of get a nice you kind of
flatten the
data coming out and you say hey this is
what we're looking at the big takeaway
from this is that we're working with
let me go back up here oops oh too far
there we go
is this model here this is what this is
all about we worked through all of those
pieces all the tensorflows and that is
to build this sequential model and we're
only putting in the two layers
this can get pretty complicated if you
get too complicated it never
it never verges into a usable model
so if you have like 30 different layers
in here there's a good chance you might
crash it kind of thing
so don't go too haywire on that and that
you kind of learn as you go again it's
domain knowledge
and also starting to understand all
these different layers and what they
mean
the data
analytics behind those layers
is something that your data analysis
professional will come in and say this
is what we want to try
but i tell you as a data scientist
a lot of these basic setups are common
and i don't know how many times uh
working with somebody and they're like
oh my gosh if i only did a tangent h
instead of a relu activation
i worked for two weeks to figure that
out well the data science i can run it
through the model in you know five
minutes instead of spending two weeks
doing the the math behind it um so
that's one of the advantages of data
scientists is we do it from programming
side and a data analytics is going to
look for it how does it work in math and
this is really the core right here of
tensorflow and cross is being able to
build your data model quickly and
efficiently and of course uh with any
data science putting out a pretty graph
so that your shareholders again we want
to take and
reduce the information down to something
people can look at and say oh that's
what's going on they can see stuff
what's going on as far as the dates and
the change in the ispu
and with that
we have come to the end of this video
tutorial on python libraries for data
science i hope it was useful and
interesting if you enjoyed watching the
video then please feel free to like and
share it
don't forget to subscribe to the simply
learn channel thank you for watching and
keep learning
[Music]
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here
you
