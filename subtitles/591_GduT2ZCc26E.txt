hey guys it's Greg welcome back today we
have an awesome video pretty much every
feature engineering technique for
training machine learning models I'll
explain how to do that in Python code we
are doing this in a co-op notebook so
take a look in the video description if
you want to follow along with that but
either way we've got one dimensionality
reduction using PCA two pre-processing
and scaling methods three categorical
encoding so dummy and one hot variables
four we've got bidding grouping
aggregating features together five
clustering so here's the map of
California for example we can cluster
the latitude and longitude and get this
kind of pretty picture out of it it
doesn't have to be that geographical
data but that makes the most sense
visually and finally we've got feature
selection combining all of this
different stuff together and only taking
what you want to okay if you only want
to learn one or a particular subset of
these ideas I do have marked in the
chapters Down Below on all these
different uh topics so just go and learn
what it is that you want to learn or if
you want to follow everything along with
me I've got it here at the start with
initial setup but before you go off and
do your own thing with this video I want
to point out that we are using the
California Housing data set that is in
the link in the description down below
so that's the particular Kangle data set
whether you're doing it like this or you
click that link and download it it's the
same data set but the point is we're not
using this one that is in Sample data
California Housing over here we actually
have to download it straight from kagle
like this okay so with that being said
um I put a lot of effort into this video
to code everything up and explain it so
if you don't mind dropping a like in
this video video I'd really appreciate
it I know this is going to help a lot of
different people and that would be great
if you could support me and others that
way H cuz we're all learning together
and if you're not subscribed to the
channel this is a really good time to do
that but with that being said let's get
started with our initial setup so
firstly we have to grab that California
Housing data set there's two ways to do
it you go to the video description click
that link download the zip folder and
then you can actually go ahead and
upload that zip folder up here or what I
do is I use my kagle API and so I go to
my kle profile generate API token it
downloads this k. Json thing and then
you can upload that or put it in the
working directory of all the stuff that
you're doing whatever environment that
you are using and then you can just run
this block of code here in collab and
it'll do all of the downloading and UNS
it for you so I am going to do that it
complains at me because I've already
done it before but I can go over here
and click yes just to kind of redo this
stuff it inflates or whatever housing.
CSV so the important part is we've got
this California housing data set right
there now now that we've got that we can
readen the CSV into a pandas data frame
so import pandas aspd we'll make this
variable DF equal to the read pandas
read CSV of housing. CSV we're going to
immediately drop any rows that have any
null values that's what this part does
and then we can just call DF to Output
that we see we've got
20433 rows by 10 columns just to explain
the data set if you don't AR aren't
already familiar with it if you are skip
over over the section for sure but
basically each of these rows are a
particular area in California and we
have a bunch of different information
for each of those areas so for each of
these different areas we've got the
longitude the latitude the housing
median age so of all the houses in that
area we can uh get all their ages get
the median of that and so that happens
to be 41 for this one uh we can sum up
the total number of rooms so 880 total
bedrooms is 129 population household the
income uh the median income in that area
and then finally up and two we have
median house value which is generally
what this data set is used to predict we
generally use a bunch of these different
variables to try and learn a function
that Maps these variables into the
predicted median house value okay so
we'll do that for everything that we do
in this video we're also going to use
this is why we got the extra data set
the different data set here is it has
this ocean proximity this categorical
variable here which we will do in the uh
the categorical encoding section okay so
we have our DF here the first thing that
we're going to do since we're training a
machine learning model or many of them
that are going to learn uh basically a
transformation of these features into
trying to learn the median house value
from these features well we're going to
split into a train and test data frame
so that we can train our machine
learning models on that dat on that data
frame and then we can test in uh our
models on our test data frame as well
okay so with this what this does here DF
is equal to DF do sample set Frack equal
to one what this does here is basically
it shuffles the rows here and we set
random state to two that's uh that's if
you want to reproduce the same results
you should use the same uh random seed
value here or if you want a different
results you could change the value there
okay and then what I do is just make
train and test the first uh train is
going to be the first 177,000 rows
because remember it has a the the data
frame itself has 20,00 433 rows so I can
do a decent split of roughly like 80 80%
in the train and maybe 20% or so in the
test data frame I get the first 17,000
in train I have to do this reset index
thing don't worry too much about that it
just kind of it just makes it so that
this index is still correct and then
what we do is get test DF is 17,000
onward and then we can reset that's
index as well so we got our train data
frame is all the columns and then the
first 177,000 rows and then test data
frame is all of this stuff and the the
randomization randomly split um the the
train and test information okay so now
what we're going to do all our machine
models are going to be about using the
median house value to predict that and
so the training the training output
basically we say we use y for output y
train is just going to be that
particular column so we get the column
this actually returns a panda series and
so we can do train DF sub the median
house value we convert that to a numpy
array which is just going to be a
flattened all 177,000 values and we do
that for both y train and Y test and we
can output their numpy shapes okay so
these we are going to use very heavily
these are all of our observed or values
or outputs we have 177,000 uh observed
outputs for the train and 3,433 for the
test so the first thing we'll do here is
create something called a baseline model
and we're going to do that by making it
just the average median house value in
in the train data frame okay so the
point of what we're generally trying to
do here is learn just some sort of
mapping between these inputs and this uh
this output over here the median house
value we're trying to predict that and
so usually we'll try and learn from
these features and use those but we
don't necessarily have to we can
actually just make a baseline model so a
very simple model for terms of
comparison uh where we just calculate
the average of everything so this is the
train data frame if we calculate the
average of all of these median house
values will be left with some number and
it's it's not a very good prediction
because obviously we should be using
some of the features that we have
available to us like longitude and
magnitude but we don't have to so we'll
make our first Baseline model which is
simply the average median house value in
the train data frame and for all of our
models we're going to be evaluating them
based off of the error metric the mean
absolute error and we'll import that
from sklearn or pyit learnmetrics
okay so we get the average median house
value in Train by this is a panda Series
so we take the train data frame sub
median house value and that has a panda
series have an object called or a
function called mean and so we can call
that to get some value here and then we
can get the test predictions which is
just all of our predictions for the test
set and that's what we're always trying
to do here when we we train models off
of the train data frame that's what we
did by taking the average and then we
compare our prediction one by one to all
these different actual test values okay
so the first prediction we are trying to
guess this that was our best possible uh
but what we instead predicted is
whatever this average actually is and
just to show what this average is uh
very quickly we can do that with this so
every time we're predicting simply
20,000 uh sorry
27,1 189 as is the house value and we
can see that's wrong in many places it's
actually pretty close to this value but
it's far off the majority for sure but
what we do is we we make the list we
just get the list of that average value
and then we multiply that by the length
of the test data frame so that we're
going to have and we know that's sorry
about that we know that it's going to
have 3,433 predictions and so it's
really just the average value uh 3,433
times and so that'll be our predictions
for everything we get the mean absolute
error of those test predictions right
right there and Y test so we're
comparing one by one the difference
between the Baseline model which is just
one value over and over again versus the
actual y test which is again just these
values here in the test data frame and
we come up with some number which as we
can see is not defined clearly I'm not
running this stuff as much as I have to
run that run that and we get
90,9 52 it means on average we off by
the absolute value of $90,000 which is
not terrible I mean it's definitely not
a very good model uh but we want to do a
lot of different methods to improve upon
that we're going to start with
dimensionality reduction in PCA and by
the way the point of this video is not
necessarily to actually get uh better
values than this although we definitely
will I'm not trying to show you how to
make the best machine learning model on
this particular data set I'm showing you
all the different techniques so that you
can do what you think is right and you
know all the different techniques
available so you can try and get the
best model for whatever your problem is
okay so firstly we're going to start
with dimensionality reduction with PCA
observe the correlation between total
rooms total bedrooms and households if
we get to the train DF all of these
columns so total rooms total bedrooms
and households were left with just a
smaller data frame and then we can get
the correlation between those where the
correlation is always going to be a
value between negative one and one these
are actually all positives and this
makes sense because if it is a positive
value that means for example as the
number of total rooms increases the
number of total bedrooms tends to
increase as well so if we look at total
rooms with total bedrooms we get 9
31023 and that value is the same as that
one over here because it's total
bedrooms and total rooms total bedrooms
and total rooms it's the same thing so
what we're seeing here is there's a very
high positive correlation it's almost
it's very close to this perfect linear
pattern of when the total bedroom is
going to increase the total rooms
increases as well why am I telling you
this well that's because maybe we don't
actually need all of these different
variables maybe we don't need total
rooms and total bedrooms and households
to try and predict uh the the median
house value because there's so much
relationship and information captured in
them already so one option here and of
course you could just simply use one of
these different features um you could
just only use total rooms for example
but something that's most likely a
little bit better is to do something
called PCA which we'll look at very
shortly so what we're going to do first
here is set up our Matrix to make a
machine learning model of our input with
just total rooms total bedrooms and
households as inputs so without PCA at
all for now we import numpy as NP and
we'll make xtrain 3 equal to that uh
that same data frame there and the the
train so that's just those three columns
and then we convert that to numpy and we
what we're left over with is a 17 ,000
row by three column nump array that
makes sense now we do that exact same
thing uh for X tests and I just put a
three on it to remind you that it's with
the with these three variables and so we
get our test information as well now we
have an input Matrix where it's the
number of examples by the number of
columns the number of examples by the
number of columns so 177,000 in the
train and 3,433 in the test what we're
going to do now is get an error of a
random forest model and it doesn't
matter if you know much about ROM force
or not uh but on just XT train 3 y train
and then testing that on X test 3 and Y
test okay so that's a lot of words but
basically we're going to uh import the
rain and Forest regression model it's
going to we're going to be doing
regression that's a it's it's predicting
the median house value so it's
predicting continuous value that's what
regression means we get the forest base
I'm just writing Bas as in a base model
is equal to the random Forest regressor
where we'll just make these variabl for
fun the N estimators is equal to 50 Max
deps is five don't worry about don't
worry about that stuff if you don't
understand it and then we fit it with
xtrain 3 so we feed it in our input
Matrix uh for Te for train sorry we
train it uh we feed it in the
corresponding inputs in the training and
then we can then now that we have a
model okay so we have our forest base as
a model trained one and we can get
Forest based test prediction so the same
thing we are making for the average
model where we just getting
uh instead of the the average just every
single time we're actually going to be
using a random Forest to make a
prediction now and so we feeded in our X
test information we feed it in our input
and then we can get the mean absolute V
value or mean absolute error on y test
comparing that to the forest models
based predictions okay so if we run that
we're going to see it's not defined I
keep forgetting to run stuff X test and
then X train 3 is now to find I guess
I'll run that
one funny okay so we run that model and
then we get
81,9 196 so on average our our model was
off by about
$882,000 instead of $90,000 like our
average Baseline model was so clearly we
learned 91,000 actually where that was
off okay so our random Forest did a lot
much better obviously but now let's
actually do a PCA and see if this helps
for this remember for this particular
data set and this particular model that
we happen to use does it help well we
can train PCA which is just a linear uh
dimensionality reduction technique you
can also do something called
autoencoders and some other weird ones
if you want to look at them uh so PCA is
a linear dimensionality reduction
technique from SK learn. decomposition
we'll import PCA and we'll train a PCA
where we set it's number of components
equal to two so that means uh whatever
you feed me in here I'm I'm feeding you
in uh three different columns that's why
I call it xtrain three here and we're
getting back just two columns which is
supposedly just better information more
concise information into two columns we
then we don't need these three columns
we can just get it in two two better
ones so we can use PCA to transform
xtrain 3 and X test 3 and xtrain PCA and
X test PCA and so as we see if we call
the pca. transform on those inputs we
will be left over with 17,000 X3 but
17,2 and 3,433 by 2 instead let's see if
that does any better this random Forest
is doing the exact same thing as before
we'll get a random forest and then we'll
fit it with extrem PCA instead on the
PCA data feed in that y train Forest PCA
test predictions is equal to Forest pca.
predict we feed it in that X test PCA
and back we get we happen to have and I
always forget what I'm actually running
here but we get the absolute error on
this model is 79 something okay so the
other one was 82 for this particular
model and maybe it would change things
if we change this value or this to a
linear regression or whatever but the
PCA did happen to help for this model so
that is one particular way of doing uh
feature engineering techniques is with
dimensionality reduction that's how to
do it with PCA in particular okay I
quickly added in xra three here to show
you what happens if we pre-process or
scale it beforehand from SK to
pre-processing will import standard
scaler normalizer and minmax scaler
there're three different tools to do
that and we'll also get our graphing
Library here if we make a scaler equal
to for example the standard scaler and
we fit that with xtrain 3 we we fit it
feed it in this we'll get xtrain 3
scaled by transforming using that scaler
to transform xtrain 3 we transform this
thing and then we get this thing out
called Xtreme 3 scaled and it's it looks
exactly the same except just these the
values look different and in particular
we see two things well one they they can
be negative now and that's just that
that happens to happen from standard
scale or these ones might not do that um
but in general they are all every single
value here is pretty much on a closer
range to each other and what I mean by
that is these are all in the thousands
as we see and these are all in the
hundreds as we see that is a big problem
for many machine learning models not all
of them some SK learned stuff might not
even care but any deep learning
algorithm is going to really care that
these values are just so much bigger
than these ones and that's because you
know if we change from 2,700 to 1200
that's a 1500 value drop that's huge
compared to 574 versus 214 we only drop
this by 3 350 or so 360 so why do we
care that this change is so much bigger
than that one well that just says that
this feature this this column is more
important than this other column and if
we really wanted to do that maybe we
could keep it but in general we assume
in setup that all features are pretty
much equally important and so to do that
we have to do for example this standard
scaler thing where now all these all
these values here are between -3 and 3
or so these are all between 3 and 3 or
so why am I saying three and not like 7
and negative. 7 that's because we are
doing what standard scaler is is uh
performing a standardization thing which
means we are subtracting we calculate
the mean of this column and we calculate
standard deviation of this column we
take this value and we subtract it by
its mean divide by standard deviation we
take this value we subtract it by its
mean standard de divide by standard
deviation and we do that for all of the
values in that column we do it for this
column where we take this we calculate
the mean for the column and then we
subtract by the standard deviation of
the column we do that for all of those
values and then they will all be roughly
in if we were to graph it this is just
the First Column after the
transformation these are between as we
can see -3 and three or so almost
everything is that and if we were to do
say look at the first uh second column
actually they again they moved a little
bit more to the negatives but most of
them are between -3 and 3 not everything
but most are and so that makes it a lot
easier for these machine learning
algorithms to figure out what the heck
is going on it treats these treats these
Colum equally basically okay so we can
use standard scaler uh that that does
this particular transformation we don't
technically need between ne3 and three
or so but we could do normalizer instead
which for this particular data set uh
we'll see this is just looking at the
First Column uh this makes the values
between zero and one we might get
different Behavior if our values were
allowed to be negative in the first
place because note that all these are
positive uh or we could do a mmax scaler
and I'm not really going to show you
uh the different ones like all the
differences between these uh that's what
SK learn is for and so I have this over
here if you want to read the
documentation on why you might want to
use one over the other and what exactly
they're doing uh then you can read about
it there or I might make a future video
on that um but the idea is that most
values you are going to be uh
standardizing and uh if you happen to
know that there's already a maximum out
there so for pictur
for example for picture data all the
values are going to be between 0 and 255
and so your pre-processing could just be
divide everything by
255 note that in that particular example
I just said uh all the values are
between 0 and 255 they're all already on
the same sort of range why are we
dividing by anything why aren't we just
happy with the values in general nural
networks are still and many machine
learning Al algorithms are still happier
with values between zero and one or so
and negative3 and three or so or
something small like that it's just
easier for a lot of things okay so those
are our different options uh each of
these produce a different uh algorithm
and you can produce a different output
you can look here I'll include this in
the do in the description down below if
you want to see which one you want to
use but that's the idea you just kind of
try them out and see what works best
overall okay it's important to note that
I did actually rerun The Notebook above
so if you see slightly different numbers
that would be why um but anyway for now
note that we were doing the exact same
transformation at least the same
Transformer object on every column that
we were using we picked standard scaler
and here we fed in the whole xtrain 3
here there is times when you might want
to pick um you know maybe du only the
First Column a standard scal on that one
maybe a normalizer on the second column
you can feel free to play around with
that it takes a little bit of
manipulation and numpy to to kind of get
those arrays separated and then joined
back again later I just wanted to let
you know that's an option that you can
do and sometimes we we apply different
transformations to different columns
like that but for now we're going to do
a standard scaler on xtrain 3 and we do
that with we'll get X test 3 scaled as
well so that we have X train down here
we have XT train scaled and X test
scaled I don't know why I didn't include
the three on that but I guess that's
okay and then we make a random forest
with the scaled information to be that
same random Forest we fit in XT train 3
scaled uh and Y train we get the test
predictions for that model with that
random Forest there do predict with the
X test 3 scaled and then we check the
absolute error comparative y test and
those predictions which gives us
82270 uh for this example okay and so if
we look up above and compare this to
exactly the same model but without any
St scaling being done this is 824 that
one the other one is uh 82 to okay so
they're very very similar it turns out
that just this random forest model
doesn't really care too much about the
scaling but I promise you neural
networks care a lot so we might want to
do that for those and some other machine
learning models as well now it turns out
that that PCA thing we did earlier
actually requires uh at least it prefers
some sort of standard scale or some sort
of scaling operation like that before
the data is fed in before we just fed it
in the Raw values uh now we want to
actually scale the data then do PCA then
train a random Forest on that and now
that we have three steps here it's
probably the best uh best choice to
actually use this psychic learn pipeline
object from SK learn. pipeline import
pipeline where we'll make a scale then
PCA then pipe uh then Forest pipeline so
we do standardization then PCA then
random Forest to be the pipeline with
standard scaler and then PCA with number
of components of two and then a random
forest regressor with that same thing as
before note how we're passing in these
objects which is just like bracket
bracket here we're not fitting we're not
passing any sort of inputs this is just
defining the pipeline of what can be
trained and then and then predicted with
later so we can fit it so we get this
scale PCA pipe forest and we call fit on
that with it still before X train 3 and
Y train okay so we feed in those RW
inputs what it does is it learns this
standard scaler okay it knows what to do
uh to it it learned its parameters so
that it can perform that transformation
then we did a PCA so it can perform that
PCA transformation with that data
actually being scaled because when we
call fit what it does is it fits this
object and then it passes it in to this
object which it then fits that and then
it passes into this object which then
fits that so it fits all of these in the
full pipeline so we call fit and then we
can get the predictions with it but now
that we fit with just that thing.
predict with the XS 3 that input matrix
it's going to do the all those
Transformations so on predict it's going
to scale that data it's going to apply
PCA to that data to make it two columns
and then we're going to actually apply
this previously learned random Forest to
that to get the mean absolute error
which totals uh that is not defined
because I haven't run that up there and
that is going to get an error of 80
8,224 which as we look above is uh not
very different from the other ones still
what is going on here uh that is just
because standard scaler may not be the
best possible object you might want to
use normalizer instead so let's try
normalizer and see if we get a better
result we do this and although this
comment is technically wrong right now
it doesn't really matter now we get 77
okay that is that is better than all the
other ones so it turns out that
normalizer happens to be a better uh
transformation
to perform on this data okay so I'll
just change the comment again to that
normalization and above to normalization
as well okay now we're on to something
called categorical encoding which
generally we want to convert this into
dummy or one hot variables dummy is for
the pandas version and one hot is for
the numpy version although it's doing
the same thing we'll look at that
shortly there is other ways of encoding
categorical variables what this assumes
is that each different category oral
variable for example red green and blue
are treated entirely differently and
that actually makes sense for RGB that's
why we make colors out of RGB because
the there are three primary colors but
then instead think about more colors if
we had yellow and green well they
actually have similarities to red and
blue and and the primary colors so what
I'm trying to say here is that we might
not necessarily want to do this dumy
encoding for everything for example for
words words have similarities to each
other we use something called embeddings
for words but we're going to use dummy
encoding uh for for this because it does
make sense we have low dimensionality
meaning we have uh what we're looking at
is if we get the train data frame sub
ocean proximity that's that categorical
column uh from way above and we look at
this column uh it's just it's just that
column there and then hopefully I can
find this again very quickly here we
have uh we pd. getet dummies on that
column and that returns a data frame
when we get the head of that that
returns this object right here where it
is going to have since we called that on
train DF it'll be 177,000 rows and for
each of these different columns it has
uh well it has it now has one column for
each of these different variables that
were in that list so all those different
columns are we have one for near Bay we
have one for Island and all of the
different options we have right there
okay so why do we want this well now we
can say hey this row is it's one of
these one of these possible things and
the one that is it's a near Bay this row
is one of these many possible things the
one that it is is an inland so we call
or Inland whatever um so we call this
either dummies because uh actually I
don't know the term but I think it's a
statistical thing and uh one hot when
we're when we are to convert this thing
to a numpy array really this is just one
Vector here which is a this is one hot
this is hot as in it's the thing that's
not cold this is cold this is cold this
is cold this is cold and this is hot and
so it's one hot because this is this is
a one okay so that's the idea it's just
a way of decoding to this row that this
is this is this type of thing and this
row is this type of thing and this row
is this type of thing now importantly uh
how many different values do we have for
each of these different things well
firstly before we look at this code If
You observe the frequency of categories
we can look at the value counts of train
DF Sub sub ocean proximity that's just
that categorical variable of above we
look at the value counts which shows how
many this this total value should be the
number of rows that we have in the data
frame and so we have
7,522 of this one which is an hour less
than the ocean or like an hours drive to
the ocean or something like that we have
5,48 inlands we have 2,172 NE I
1,895 near Bas we only have three
islands so what we're actually going to
do here after we just do a simple
concatenate to append these new columns
to our train data frame so we train DF
PD duck and cat train DF train dummies
across the uh the column AIS AIS is one
we combine all of those to get uh
basically just the dummies or one hot
from this this variable here so we
append that and then since we don't have
many isets here we're going to draw drop
Island train DF drop Island in place is
true meaning we don't have to do uh we
don't have to do train DF is equal to
that thing if we do uh in place is true
we do it on AIS equals 1 and we train DF
do head we output that and we can see it
is all of these columns except uh we
removed Island okay so this uh this data
frame isn't overly useful as it is uh
we'll change that very shortly for now
we're just going to do exactly the same
thing on the test data set so we'll get
the test dummies which is the data frame
of that gets the pd. get dummies of the
test DF sub ocean proximity test
dummies. head gets that data frame there
and then we append the dummies and drop
Island on the test data frame so test
dummies is H that same thing I don't
know why I would need that I definitely
don't need that so test DF is equal to
PD do and Cat of test DF test dummies
across axis equals one test do DF drop
Island in placees true accesses one test
DF head I haven't ran a few of these
things for a while so I'm actually just
going to go back from the start and
start running these
operations and now that we have um so
now that we've concatenated that we have
those columns on the train test uh train
and test data frames so uh we can get
this we're just going to try and make a
machine learning model based off of only
those columns of course in the end we
will use some other columns as well but
for now we're just going to make a model
that only looks at these different
categories and see how that does so we
get X train dummies which is just that
train DF to numpy this part's a little
bit weird so we get all rows and then we
want -4 up until the end well if we
count back here we get uh min-1 -2 -3 -4
we get this column so it starts at Min
-4 that column and then we want to go to
the end so we say all rows which is all
of this information but then all only
columns we want is just this chunk here
so it's just this chunk here uh except
this is the head so it's all that
information and so if we were to look at
the shape of this thing it is 17,000 by4
so we did that for the training
information we can do that for the test
and so we do test DF that's Den by all
the rows ne4 up to the end and that's
the test dummies. shape we will make a
linear regression model just for fun uh
we don't have to do that but I'm
choosing to do a linear regression on
the X train dummies and Y train we make
that our linear dummy model we get the
linear dummy test predictions which is
that model predict with the X test
dummies and we get the mean absolute
error with Y test and linear dummy test
predictions to show that it is in total
I don't know why I'm not just running
the stuff as I go um 77,000 195 okay so
that's actually a pretty good model like
that's actually a really good model that
only uses uh the a one hot encoding or
dummy encoding of these variables okay
so it just use the ocean proximity in
its fullest capacity to return a pretty
good model next up is something called
binning and I'm going to use other terms
for it like grouping and aggregating
really just referring to the same thing
which is if we're to look at train DFS
up housing median age for example not
the median value this is one of our
inputs we range from about 1 three to
like 55 years or so okay so why is this
important well there's no reason that we
have to use this column as it is as just
its values or even just scaling of it uh
what we can do is maybe sort of move
things into bins and so if I was to draw
a line right down the middle which is uh
say 30 or so okay so if I draw a line
here what I can do is basically shove
everything in here into one big bin and
say that it's less than 30 or it's
bigger than 30 or equivalently bigger
than or equal to 30 so we get two big
bins out of this which is everything is
either less than 30 or it is greater
than 30 if I do train sub train DF sub
median age less than 30 this is adding a
new column into the data frame we make
that equal to train DF sub housing
median age is less than 30 so it applies
that operation uh to every single every
single Row in there and then it returns
a Boolean okay so what this thing what
this thing is here is a panda series of
booleans where it's either this thing
this first one is happens to be less
than 30 the second one happens to be
less than 30 third one is bigger than 30
okay so we get this Boolean series out
of it and then we convert that into an
INT and so that'll make the falses zero
and the trues uh will be one and then
train DF sub uh uh train DF do head
after that we now included this column
here so we basically just bin uh so
there's no reason we could have just
made this a binary variable we could
have of course uh made this multiple
variables like say between zero and 15
15 and 30 and 30 and so on then we'd
have to do that one hot encoding thing
again um that's that's the main idea
though is basically we just kind of
group these things into different bins
and treat them as different ideas now
housing median age is bigger than 30 is
kind of something I made up uh but for
pretend for now that that's actually
makes sense that uh some realtor went
over some really uh famous real realtor
said something stupid like uh any
anything less than anything older than
30 is just completely useless like it's
just so much worse um and so that would
actually have a strong effect and so
this is something reasonable to do to
give uh the Zero versus the one value
very uh different feelings and it's not
just about this kind of this gradual
increase it's more about the fact that
it's all either less than 30 or it's all
bigger than 30 that's why we're doing
this it's probably not going to work
super well but it would better for other
examples but that's the idea so again uh
we could of course combine this with
other features but for now we're going
to make a model just out of that
variable and see how that does we'll get
the X train with just median AG that
should be really median age less than 30
but that's fine uh so we get that we
convert that to numpy and for these SK
learn models they always have to have
this kind of other column shape like
number of columns so if if we just
grabbed this technically it would have
been 177,000 flat we reshape it into
this is the same as doing uh 17,000 by 1
but I'm going to leave it as negative 1
in case the number of values uh happens
to change over time so we do that we
reshape it and we get 70,000 by 1 which
is all either zeros uh sorry I must have
forgotten to do this and then we get
this which is all either zeros or ones
that same thing as we had before we you
can do the exact same thing with test
which uh not really going to show how to
do that but it's the exact same
operation but on the test
information and then we train a model
with just that information I'm going to
start to gloss over what I'm doing here
because you've seen this a bunch of
times now I'm just whatever training and
testing uh data set I'm setting up here
uh that's the one that I'm using for the
model okay so we do that and then we
make a model out of that we're going to
do a linear regression just for fun and
we can see we got
90,000 uh
90.7 which if we compare that to the
Baseline model at the very top that
model so it's 90.7 compared to 90.9 okay
so it's ever so slightly a better guess
than the average so it's clearly it's
it's means something to to do that so
that is the idea of uh bidding and
grouping you could do it into more more
variables you could of course combine
this with other variables uh but that's
that's the idea now we're moving on to
clustering where first we're going to
plot a map of California uh I'm very
certain we are that is definitely doing
nothing so I'm going to kill that and
we'll do a scatter plot of we'll do the
longitude on the X and the latitude on
the Y and this is California right there
it's shaped like that so there's are all
the points we can make a k means model
from longitude and latitude and get the
Clusters so what are clusters well
they're just groups and so clusters it's
really like a label it's saying No this
group is different than that group and
it's different than that group if we
from K learn. cluster import K means
that's just one algorithm for clustering
and it's important for this because it
actually has a predict method you can
learn a k means and then you can predict
that uh on a different data set than you
learned from a lot of algorithms uh
clustering algorithms can't do that um
but we get XT Trin lat long which is
just train DF subat longitude latitude
that's numpy we make a k means to be a k
means with the number clusters you know
I put seven but there's no reason I
couldn't do three and then we we fit
that with uh the input just the latitude
and the longitude and we can get the
labels K means. labels uncore and so
this is saying the first one belong to
this group and the second one belong to
this group and the third one belong to
this group the numbers have no meaning
other than they are different groups
than each other there's it's not like uh
three group three is closer to group two
uh than it is to group zero like it's
just different values are different
groups entirely now let's plot a colored
map of California we'll do uh P plot.
Express is PX my favorite Library px.
scatter with the x is train DF sub
longitude Y is train DF sub latitude and
the color so this is so much easier to
do this in plotly rather than map plot
lip we make color equal to K means.
labels under score so what that does is
it colors here each of the different
groups so here these are all the fours
these are all the ones these are all the
sixes these are all the the twos these
are the fives okay so it makes this it
makes this uh this grouping here and I
forgot I hadn't run that again because
we actually only made three groups and
so here this is the these are the twos
these are the ones these are zeros and
so it grouped them according to these
values here which is really really cool
so you can do this uh clustering idea
with you know other variables as well
which won't produce as as cool
visualizations as something like a
latitude and a longitude uh but I just
wanted to show you this so it really
made sense what we were doing um and
often by the way you might also want to
do uh the same sort of scaling stuff
before running a k means or some other
clustering algorithms as well okay so
now we're going to make X train
clustering uh which uses just the one
hot encoding from the cluster viel so
how do we take this and make a model out
of this well really these are
categorical variables this is a two
these are all twos these are all ones
and these are all zeros so what we're
going to do is that same sort of dummies
thing uh where we don't have that many
for this we have three columns and so
all of these this is if it's zero this
is if it's one this is if it's two these
are the zeros these are the ones these
are the twos so we do that same thing
extreme clustering is pd. getet dummies
on the that a series so that's just a
way of transforming that into a column
not even sure if I need that but um so
we we get a a pandas data frame out of
those labels and then we convert that to
numpy so that this is that one hot
encoding thing and I definitely call
that one hot here when I'm looking at it
at in a numpy format like this so that
is how we make uh the Clusters turned
into at least one way there's others um
but one way to get uh turn clusters into
meaningful
descriptions so we'll now predict the
clusters of the test data and create X
test clustering using one hot encoding
so we get X test latitude longitude or
test DF which is test DF sub longitude
latitude I don't know why it's kind of
confusing how I said lat long and then
long lat but um we get that to numpy
which is just the longitude and the
latitudes we get the clustering uh which
is pd. get dummies of the series I
should have organized this organized
this a little better but basically we do
cing St predict on the X test last long
and then we do the series of that and
then we do the get dummies on that so
what gets the labels it turns it into a
series it converts that into a data
frame which is one hot and then we do
tumpi to get the full shape is going to
be that by three instead okay so now
we'll check the error of a linear model
that uses only the cluster one hot
encodings how does a linear model do
with just knowing that it's kind of
organized like this is this even related
to the median house value uh let's find
out so we make a a model and it's got 80
88 so it's not bad uh but what if we
were to increase the the cluster the
amount of clusters what if we actually
got more information out of saying uh
maybe these ones versus these ones
versus these ones so if we were to go
back to that seven if we were to do
seven
here uh so train it plot it on the train
and
then uh make the matrices out of it and
then we were to make another model we
got way down to 75 okay so that's how
important this information is is we can
make this is just a linear model out of
all just out of these different clusters
of information we' learned so much about
the median house value based off of this
these areas and feel free to play around
with that value uh of number of clusters
if you want to so that really cool you
of course you don't have to do
geographical data U that's just a
usually you won't uh but if you do have
geographical data that can be uh a
useful way to get better predictions out
of it uh or at least different
predictions there's it it we didn't even
make a model that just used the vitude
and longitude as is you could do that
and maybe it did better I don't know
this is just a cool clustering technique
that will have its applications for many
other variables okay so finally we're
going to do feature selection which is
just a combination of features so we'll
observe the shapes of XT train
clustering XT train scaled and dummy X
train so all this is if if you didn't
watch this part basically this is uh
data that's from clustering this is data
that's uh some variables scaled and this
is uh the dummies from that'll actually
be the dummies from the geographical
information okay so what we can see here
is clustering has the seven columns the
scaled has those three columns and the
dummies those are those four different
uh categorical
variables so we conat concatenate the
training arrays side by side to make one
big xra full input Matrix so we do the
numpy do concatenate of X train
clustering xra 3 scaled and ex string
dummies across axis one to get this full
thing so we can see these are 70,000
rows 70k rows 17K rows so is that and
then we just kind of concatenate uh we
just put these seven and then we put
these three and then we put these four
so you can see that the shape is like
this so it's this is a First Column
second column third column up until the
first seven are this the next three are
this and the next last four are these
ones here okay so that is our full
information here this is just one way
why why am I doing this well this is
just one way that we can combine
multiple features uh and to make a model
okay we could of course drop one of
these columns at a different one I just
picked some random stuff because that's
what feature selection or combining
features is about just play around with
stuff figure out what works well and
only use what you need to use okay so we
get our full Matrix there I'm going to
make sure yes that's already run we can
look at the shapes of the testing
information so it's the same thing but
the tests and then the test as well this
full testing array we check the eror of
a random Forest this is the combination
of these features we'll make this random
Forest we'll fit it with that train full
and compare that to White train we'll do
test predictions where we predict the uh
random Forest clustering that's not the
right model guys so uh okay you're going
to watch me fix this which is basically
making a a random Forest we'll call that
full this is going to be full based off
of random Forest full and this is going
to be full test
predictions and yeah so it didn't
actually matter what the name was but
that was definitely not that was
definitely not the right name and in
total here we get a model that seems to
be uh at 67k okay so that is easily
better than anything we looked at so far
uh it's better than our Baseline
coverage it's better than uh just the
clustering just the dummies uh so we
combine all of this stuff together to
make that and just for fun here let's
even uh try to make an even better model
you know what let's not use these
features let's try and let's try and
figure out uh some new ones so I'll do
maybe just an initial one can we do
better than
67,000 uh I bet you we can if we try and
change those parameters and we get bum
bum bum 60k okay so immediately uh you
know this these are things you always
have to do which is train the parameters
of a model we could fit that a lot
better as well but that's the idea so uh
if you aren't subscribed I think you
probably figured out right now that
that's a good idea drop a like if you
did get value from this I'd really
appreciate it and yeah let me know what
you want to see in the future I hope
this one was a good one CU a lot of
people are asking for it and so I I put
in a lot of work to make sure that I got
that out and I look forward to uploading
this I I'm really happy if you to see it
I hope it does well and yeah I'll see
you later guys
