hello on my name is Krishna and welcome
to my youtube channel so I guys I think
you like this new setup and probably
I'll be recording all my videos like
this and upcoming videos itself and in
this particular video we are going to
implement the sequence to sequence
learning with the help of machine
learning language translation and I'll
be showing you how we can take up the
data set how we can do all the things as
we go ahead and for this please remember
guys you need to be very very good at
the basics that is you need to know RN n
you need to know unless key M RN n you
need to know bi-directional RNA and this
particular video is more to show you or
make you understand how a code is
actually implemented to do language
translation so I am going to refer this
particular block guys amazing blog it
has been written and it is provided to
everyone you can just read and
understand this whole thing if you have
a basics right ok and again as I said
that we are doing machine learning the
language translation so far first of all
from where we are getting the data set
just click on this ok in this particular
scenario again this is blog you know
they have actually done English to
French translation so English to French
data set is already present over here
why they have actually taken that
because then there is a huge data set
for this you know they are more than 100
K records with respect to different
different English and French words so we
are going to use this and probably I'll
also run this whole thing and show it to
you now let me just go to the
architecture make you understand what
are things we have to actually do for
that I'm just going to quickly rub this
you know so wrap this all so that I'll
be able to able to explain you
everything now fine let's go ahead and
try to understand this remember guys if
you have seen my encoders and decoders
the video or sequence to sequence
learning their two main important
components one is the encoder so this is
basically my encoder ok the another one
is basically called a decoder okay now
we have seen that in encoder we just
have to provide our words
the processing will actually happen
there will be a context vector that will
be W that will be created once this
context vector is actually created then
it is passed to my decoder layer and
with the help of that we are actually
creating our output one important thing
to load over here is that in our
encoders we do not consider the output
over here
we skip those output we don't require
this output right because we are
actually more focused on getting the
context vector okay so what we are going
to do is that I am going to make you
understand how that code is actually
implemented if you just copy and paste
that code it will work but we still need
to understand how we can design this
encoder to decoders right now one more
thing that you need to consider over
here is that this as you know that we
are passing character like a B C right
you are passing all this particular
characters and this is specifically
character to character translation here
now when I pass this particular
character we also need to take care that
how it is actually passed it should be
passed in the form of vectors and in
this particular example we are going to
convert that into or actually send these
characters in the terms of 100
presentation one art representation
basically means one suppose you have 73
characters
okay and in English specifically in this
particular example in English there are
73 unique characters now if I want to
represent a with some vector
representation I will basically have 73
features and based on that I suppose
that a wherever it at whichever location
it is present I'll keep it as one
remaining all the characters will be 0
that is how is basically represented so
similarly we are going to do in this
particular way and remember guys please
make sure that you have your basic
concepts of LS T M R and N and you know
how to implement all those things so it
will be pretty much important that will
be also easy for me for me to make you
understand ok now what we are going to
do first of all okay remember we are
going to make sentences into 3 lumpiness
one is the character input encoder input
data decoder input data and decoder
target data now what does this basically
mean that is we obviously need to
provide this input data and as you know
that in L esteem we need to provide the
input it has to be provided in the three
right so this is basically my decoder
input right decoder input and this is
all my decoded target data decoder input
is basically nothing but the vectors or
the context patterns that is been
generated by the encoders so we are
going to use that okay then we are going
to the next step over here you can see
that they have given on what 3d array of
shape basically number of tails is like
how many number of sentences that I have
what is the maximum sentence length and
what is the number of English characters
so if you really want to give in the
form of one order presentation we
basically have to consider this this is
basically that the total number of
characters that is actually present
because we are going to provide this
into a one heart of acceleration of the
English sentence so let me just go back
and this is how I did a set looks like
that is you can see this is my English
word this is my French word this is my
English word this is my French word this
is my English word this is my furniture
so like this you have huge amount of
data set we need to remove this this
this is not required so for that we will
be doing some kind of processing Oh
currently in my system guys we I have
tensorflow 2.0 so whenever you have
tensorflow 2.0 that basically is chaos
and integrated within them
if you don't have tensorflow greater
than 2.0 at that time you just have to
remove this particular code that is
tensorflow from tensorflow just remove
it and you can just write from chaos
drop models import model from Cara's not
layer input input a list intensity now
we are going to initialize some of the
variables like bath size we are going to
initialize epochs we are going to
initialize some latent dimension this is
basically the feature dimensions or
dimension it is case that we are going
to consider how the number of samples
should really not be able to take it as
ten thousand okay here we are just
specifying some values itself so that we
will be training on that bunch of data
now the first step after this here I
have actually provided my dataset path
in this particular step what we are
doing is that we are reading the input
and the target data input data is
basically all my English sentence target
data is basically all my output sentence
and then with respect to each and every
input sentence we are we are we are
reading all the characters all the
unique
characters that is what we are doing so
here you can see there is something like
input characters and target characters
so input target input characters and
target characters will give you all the
English sentence so suck up all the
English words so if I go and rent out
input characters so these are all my
English words you can see over here okay
guys I think this is a simple for loop I
I think you will be able to understand
okay so input characters and target
characters before input and target
characters you can see that first of all
you have opened that particular file
then you are traversing to each and
every lines you are doing the split with
respect to data input text and target
text input text if you want to really
see this input text so here is all my
English input text you can see over here
similarly if you want to go and see the
target text you'll be able to see all
the targets test and this remember the
/n is put because we need to indicate
that which is the end of that specific
sentence because then only we'll be able
to understand that how we would be
training our neural networks how we will
be training our ell STM lens you know
how we will come to know that okay this
is the end of the sentence with respect
to that so this target text input text
we have got and similarly we have got
input characters and concrete characters
target characters if you see guys this
basically indicates that all the French
characters that I have in my dataset
okay so target characters you can see
these are all the French characters that
are present okay and if you really want
to see the length it will be probably
somewhere around I guess 90 I just seen
this okay 90 okay guys so next step is
actually to take all this particular
input characters target characters and
this included tokens like how many the
length of the input characters you can
see we are printing it you know I'm
actually finding out what is the maximum
sequence length and what is the decoder
sequences all these rings are there so
we have the number of samples basically
say that how many number of sentences I
have number of unique input tokens
basically indicates that how many unique
characters I have in my English language
over there based on the data set that I
have
93
basically the target characters the
length of the target characters what is
the maximum length for input centered so
basically I have all my English
sentences from that the maximum length
of the English sentence is somewhere
around 16 words
similarly the maximum sequence length
for the output which is basically my
French the words is basically 59 okay
now the next step I'm assigning each and
every token so token basically means
like 0 1 2 3 those kind of indexes to
each and every characters based on the
sorted list and this is what that
specific code will do okay so if I
execute this and if I show you what is
the input token index so input token
index what it will do how many English
characters are present based on that in
the sorted order it will start putting
up indexes so here you can see that for
the space it has put 0 for this it has
put 1 for dollar it has put 2 3 4 like
this so total number of 70s and last
character it is given as 17 decks or
total number of characters over here is
basically 71 similarly if I go and see
for the target target token index you
will be able to see that yes I'm having
somewhere around 92 okay now the next
step is basically to show you how we can
do one Hart representation by using
numba initially for the encoder input
data I am going to use NP dot 0 so I am
going to put all zeroes values based on
the dimensions that I have given over
here so remember the first dimension
again if I go to go in this particular
block the first dimension it says I have
to put number of pairs number of times
basically means how many total number of
sentences are there I am going to put
that the second parameter is basically
what is the maximum English seat
sentence length that also I know that
right then what is the number of English
characters that also I know that right
so here I am going to put as Mac Mac's
include a sequence length the total
length of the input sentences and total
number of encoded tokens so that will
actually give me like how many number of
English sentence words are actually
present right so here is those values
this is basically my sequence length so
you know you can see that my sequence
length max equals length for English is
somewhere on 16
the unique input tokens that is 71 that
is why abused number of input tokens
over here right similarly for the
decoder input also have to do in that
particular way and for the target data
also I have to do in that particular way
always remember this will be same in
decoder input data instead of using
English I will be using French over here
so max decoder sequence length max
number of decoder tokens everything over
here will be basically taken as the
French directors so what will be the
French character length if I go and see
over here right so this is the decoder
sequence length this is basically of the
French that we are trying to see it is
somewhere around 59
right how many number of unique tokens
are actually present in the French
characters they are 93 so I am going to
use that similarly in this decoder
target it also I am going to use the
information of the decoder or the French
character itself
now this particular code will actually
help us to do the whatnot representation
it is simple gas suppose I have the
words let me just quickly show show it
in front of you okay
suppose I am just going to wrap this
okay
suppose suppose I have some words okay I
have some words like the blue o D right
suppose I just have these four words
now suppose are in my first sentence I
have my blue word so W will be given the
value as 1 and remaining all will be
given as the value is 0 this is what is
one hot representation basically says
and that is what I am going to do over
here right but remember you're the
target length is completely different
from English it is somewhere around or
how much we saw if I go and see in the
top for English it is somewhere around
71 unique tokens are there but the
sentence length is somewhere around 16
right
so considering the 16 length and
considering the 71 unique tokens
wherever that particular token is
present that will become 1 and remaining
all will become 0 so that is what this
whole code is doing if that is what this
whole code is doing actually you have to
understand this particular code guys
just try to free that you know so here
you can see for T comma characters in
enumerate input underscore text what is
input and that's good text okay I'll
just show you what is input underscore
next you can see that okay first of all
it is traversing inside this in so input
underscore text is one one sentences
like this okay one one syntax is like
this now when I see this this is
basically replacing wherever that
particular character is there that is
getting replaced by one okay guys I'll
not deep dive more into the code just
try to understand this okay just try to
go line by line and try to understand it
it will be always beneficial to you okay
so that is how in and but remember this
particular whole code is actually being
the one hut representation okay so after
you do this what we are going to do is
that then we will start creating our
lsdm layer now lsdm layer initially will
ask for the encoder input shape so the
encoder input shape I am giving it as
number of included tokens then in lsdm
are taken what is my little late and I
mention I had initialized over here the
leading dimension is 256 this is just
like our timestamps that we are going to
consider and then we are going to take a
return state as true one very very
important thing is happening over here
just understand if I go to my WordPad
remember I am NOT taking all these
particular outputs okay I I don't want
this output because that is not how
encoder and decoder actually work
right this is not how it actually works
so what we I am doing actually me see
you me so first of all I will go and
initialize this I have initialized my
HTM layer now this encoder when it is
taking the encoder input it gives us
three values one is the encoder output
which we have to skip we don't require
it what is the state underscore H which
is basically Magadan sense and one is
this cell state okay now if I go back
over here guys I will be requiring the
students and information whatever the
output I am getting it over here with
respect to the complete time stamps as I
am going hi okay so I don't what I don't
want this out
I don't require any of this output so
what I am doing over here you can see
that I am just taking this state H and
state C and I am putting inside my
encoder state variable okay encoder
state beautiful now similarly what I do
with respect to the decoder inputs now
in decoder in course I know what is my
decoder number of decoded tokens I
absolutely know that again I have
created a latent dimension created all
the lists iam like that okay but here
now we will be much more concerned in
getting the output itself I don't have
to worry about the other things okay so
now here I am focusing on getting the
decoder outputs okay now once I get the
decoder outputs the decoder output
basically means this information this
information I want to get this
information and this is the layer of
that I am actually creating in my second
step right in the first layer I took
this okay
I want it this value this value and this
output value I have taken that I have
stored it in a variable and now from the
decoder listing I am just interested in
the decoder output right then I'm using
a dense layer and finally I am actually
getting all the outputs with respect to
that okay now this is what is with
respect to my encoder inputs and my
decoder inputs and my decoder outputs
now I will be taking all this particular
value and considering in creating our
model so if I give this values inside my
list you can see my encoder inputs and
decoder inputs this both will be almost
same but just with timestamp the +1 and
then finally I am getting my decoder
after that I am compiling I am fitting
the data with some validation split with
some hundred each box you can see that
and probably this will be able to give
you around now
87 percent accuracy remember guys this
will also work in your local laptop and
it will work in proper like within 15
minutes it will be able to do the
hundred imposter since this is a
character to character implementation
okay now this was how my model got
trained now one assignment I really want
to give it to you guys is that please
try to understand this code you know
where I am actually generating the
sentences okay you can see that this is
basically generating the sentence
so just try to understand this and
probably I know it will be little bit
difficult but I want you to read the
blocks and understand this how we are
doing the sampling and how we are
generating okay
I have explained you till here like how
the model is actually generated this is
pretty much simple refer the blocks in
the reason why I am Telling You is that
understand and probably if you don't
know about lsdm don't go over here first
of all then only you will not be able to
understand because in the next model
which I am going to discuss which is
about attention models there again we
need to make some architecture changes
you know instead of using this encoder
over here right I will be using this
will be getting changed to
bi-directional LSD okay so yes guys this
was all about this particular video used
to let me know whether you have any
queries but I will be very very happy to
help you out but please give a try give
a try to understand this code which is
actually changing your ten sentences or
which is running your course this is
what I want you want to explore this is
hardly around 15 to 20 lines of code but
I really want to make make you
understand a thing if you have
understood this the other part will be
pretty much easy so yes guys this is all
about this particular video in my next
video deep learning will be coming with
attention models and we'll discuss about
attention mall as or how it actually was
so yes guys this was all about this
particularly de sel in the next video I
have a great day thank you wonder bye
