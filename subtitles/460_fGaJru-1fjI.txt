[Music]
hello everyone i welcome you all to the
live session on data warehouse by
intellipaat but before we begin the
session make sure to subscribe to our
channel and also hit on the bell icon so
that you will never miss an update from
us now let's see the agenda of the day
firstly we will begin with introduction
to data warehouse
in that we will be explaining you what
is data warehouse what are the types of
data warehouse and also tell you what
are the popular data warehouse tools
for that we will be doing a hands-on
demo to show you how data flows in data
warehouse
then we will move on to introduction to
data mart followed by data modeling then
finally we have covered data warehouse
interview questions and answers
so guys this is the agenda now let's
begin the session what is the need of a
data warehouse
so the goal of every business is to make
better decisions and this is where
business intelligence comes in
business intelligence turns the massive
amount of data from operational systems
into a proper format that is easy to
understand so that this data could be
analyzed timely and accurately to obtain
actionable insights of business and also
to bring an effectiveness in our
decision-making for instance in the
e-commerce industry we maintain data
about products customer login
credentials check out details and other
technical and non-technical information
now all of this information would be
available in different servers so we
need a system which can combine all this
information so that companies can
extract and analyze this data in an
integrated manner and that's exactly
what the requirement of data warehouse
comes into picture now let's understand
what exactly is a data warehouse so the
concept of data warehousing is pretty
simple
data is extracted on a periodic basis
from source systems which could be
applications such as erp crm sales
finance and so on
now data from these systems is moved to
a dedicated server that contains a data
warehouse so when the data is moved it
is cleansed formatted summarized and
supplemented with data from many other
sources and this resulting data
warehouse will become the main source of
information for report generation and
analysis via reporting tools that can be
used for things such as ad hoc queries
and dashboards
now sometimes it has been noticed that
many companies say they have a data
warehouse but they don't really have a
true data warehouse rather it is
actually a dumping ground for tables
that are copied from source systems with
little modification so for a company to
be successful in future they must make
good decisions and to make good
decisions they require all relevant data
to be taken into consideration and the
best source for that data is a
well-defined data warehouse so a data
warehouse is specially designed to
perform business intelligence activities
and enables professionals and employees
to comprehend and improve the
organization's overall performance and
the two major functions of a data
warehouse are to maintain past and
present records and also help
organizations to take effective business
decisions with precise data analysis
additionally the data warehouse
environment also supports etl data
mining capabilities statistical analysis
reporting and olep tools which help in
interactive and efficient data analysis
in a multi-faceted view
now we'll look at some of the key
features of data warehousing so number
one in the list is subject oriented so
data warehousing gives you an option to
analyze a particular subject area thus
subject matter expert can answer
relevant questions from the data for
example if that particular subject is
sales then a sales executive for an
online website can develop a
subject-oriented database including the
data fields he wants to query so the
salesperson can extract data writing
different queries like how many
customers purchase the database
architect course today
and next in the list is integrated so a
data warehouse supports consistency by
arranging data from diverse sources in a
uniform and rational format so it should
not allow any conflicts in the field
names and other units of measure having
accomplished the studiness we can refer
it to be an integrated warehouse
the third key feature is non-volatile so
as the name suggests non-volatile data
warehouse refers to no data change once
created it is an important and relevant
feature since the aim of developing a
warehouse is to evaluate what has
occurred until then and the fourth key
feature is time variant so the data
collected in a data warehouse is
identified with a particular time period
so this data provides information from
the historical point of view for example
one can retrieve data from the past 3
months 6 months 12 months or even all
the data from a data warehouse
so now look at some of the most
frequently used terms in data
warehousing
we'll start with metadata in simple
terms metadata provides the answers to
all your data related questions in the
data warehouse it is referred as data
about data
so it gives users a detailed explanation
of the syntax and semantics and
describing all relevant attributes of
the data in the data warehouse for
instance the number of tables in a
database can be referred as metadata
now the metadata is also commonly known
as table of contents for the data data
catalog data directory data warehouse
roadmap and the nerve center so with the
help of metadata developers and database
administrators can create their own ad
hoc reports which is of prime
significance in this era of big data
in fact without meta data the data
warehouse is considered futile it would
just be a big box without any valuable
and meaningful information then we have
olap which expands to online analytical
processing as the name suggests olap is
computer processing which allows users
to interactively analyze
multi-dimensional data from multiple
prospects the various techniques of
multi-dimensional model of olap
constitute and encapsulate large volumes
of data for rapid evaluation using
online analysis tools
in a multi-dimensional environment each
attribute of data is considered a
separate dimension
and olap can establish an intersection
between these dimensions
so the olap cube is typically similar to
the shape of a cube and it has the
multi-dimensional representation of data
which is optimized for data warehouse it
is an array or metrics of facts and
dimensions wherein a fact table contains
facts or measurements and the dimension
table contains attributes or dimensions
of the fact it is useful when businesses
require presenting a spreadsheet
containing two or three dimensions for
instance in this image the data is
summarized with respect to time
department and store value
then we have dimension and dimensional
model
as we already know a dimension refers to
single attribute of the same data type
for instance year month minute and
second are all divisions of time
attribute thus data warehouse supports
dimensional model that enables users to
store and analyze information on each
dimension
unlike entity relationship model
dimensional model does not involve a
relational database every time this type
of modeling technique is useful for end
user queries in data warehouse
then we have extract transform and load
which refers to movement of data from
one location to another
next we have drill down drill up drill
across and drill through
so drill across allows data analysis
across multiple dimensions
while drill up allows data analysis up
to the parent attribute and drill down
allows data analysis down to the child
attribute and finally drill through
helps in analyzing data from an olap
cube to the relational database
then we have data marts so our data mart
is a subsection of data warehouse which
is aligned to a specific business
perspective
while a data warehouse stores detailed
information of multiple subjects
the latter focuses on storing data about
only one subject
and the data mart concentrates on
storing more summarized form than
complete data and integrating
information within specific set of
source systems
now let's move further and discuss some
of the major differences between olap
and oltp
generally people get confused with olap
and oltp they are two different types of
data analysis techniques and are
distinct from each other in many ways so
here on the screen we see the
collaboration between oltp and olap in
this workflow architecture we can depict
the importance of each system-based
application and how they are linked with
each other
the first difference is the name itself
oltp stands for online transaction
processing while olap stands for online
analytical processing the main
difference between them is that oltp
focuses on small transactions and faster
query process
olap on the other hand often involves
complex queries involving aggregations
another difference between them is that
the data for oltp is mostly operational
while for olap it has consolidated data
which comes from various oltp databases
and oltp system shows an image of all
the activities going on in a business
environment
while olap system shows different views
of individual events
now that we've understood the concept of
data warehouse it's time to gain some
insights into the architecture of data
warehouse
so there are two main components to
building a data warehouse
an interface designed from operational
systems
and the individual data warehouse design
thus the construction of data warehouse
depends on the business requirements
where one development stage depends on
the results of the previously developed
fees
the structure of a data warehouse can be
understood better through its layered
model which lists all the main
components of the data whereas
architecture
so here is a typical architecture of
data warehouse consisting of different
important components
you can see that it is nothing but the
movement of data from source to the
staging area and then finally to the
confirmed data merged through etl
the first layer is the data source layer
which refers to various data stores in
multiple formats like relational
database excel file and others these
stores can consist of different types of
data such as business data web server
logs and internet research data the next
step is extract where the data from data
sources is extracted and put into the
warehouse staging area
this extracted data is minimally cleaned
with no major transformations
then comes the staging area which is
divided into two parts data cleaning and
data ordering so as the name suggests
this layer takes care of data processing
methods thus removing data redundancy
filtering bad data and proper
integration of data
overall the stage allows application of
business intelligent logic to transform
transactional data into analytical data
it is indeed the most time consuming
phase in the whole data warehouse
architecture and as the chief process
between data source and presentation
layer of data warehouse
finally we have the data presentation
layer which has the target data
warehouse the place where the
successfully cleaned integrated and
transformed data is stored in a
multi-dimensional environment
now the data is available for analysis
and query purposes the information is
also available to end users in the form
of data mods this data can then be
accessed by various business
intelligence tools like tableau and
presented in multiple formats like
tables graphs and reports
it is important to note that the data
warehouse supports and holds both the
persistent data that is stored for
longer time and also temporary data the
major purpose of a data warehouse is the
attainment of cleansed integrated and
properly aligned data so that it is easy
to analyze and present to clients and
customers in several businesses
now we'll understand for what purpose
data warehouse is used in various
sectors
in the airline industry it is used for
operation purpose like crew assignment
analysis of route profitability and
frequent flyer program promotions
in the banking sector it is widely used
to manage the resources available on
desk few banks also use data warehousing
for market research performance analysis
of the product and operations
healthcare also uses data warehouse to
strategize and predict outcomes generate
patients treatment reports and medical
aid services
in the public sector data warehouse is
used for intelligence gathering it helps
government agencies to maintain and
analyze tax records and health policy
records for every individual
in investment and insurance sector the
warehouses are primarily used to analyze
data patterns customer trends and to
track market movements in retail chains
data warehouse is widely used for
distribution and marketing
and telecommunication data warehouse is
used for product promotions sales
decisions and to make distribution
decisions the hospitality industry
utilizes warehouse services to design as
well as estimate their advertising and
promotion campaigns where they want to
target clients based on their feedback
and travel patterns
now there are many data warehousing
tools available in the market let's have
a look at some of the prominent ones
so oracle is the industry leading
database it offers a wide range of data
warehouse solutions for both on-premise
and cloud it helps to optimize customer
experience by increasing operational
efficiency
amazon redshift is another data
warehouse tool it is a simple and
cost-effective tool to analyze all types
of data using standard sql and existing
bi tools it also allows running complex
queries against beta bytes of structured
data using the technique of query
optimization
mark logic is another useful data
warehousing solution which makes data
integration easier and faster using an
array of enterprise features
this tool helps to perform very complex
search operations and helps in querying
different types of data like documents
relationships and metadata
now there are a lot of opportunities in
the market to build a career in data
warehousing
so candidates who are interested to
become data warehouse professionals can
look for job opportunities in sectors
like information technology financial
services health care and retail
according to the salary estimates of
natural salary data which is released by
b scale the annual compensation of a
data warehouse professional ranges from
68 thousand dollars to 149 000
with the median salary being about
hundred thousand dollars per annum we'll
start with the data warehouse
let's say
for example
there is an atm machine
okay
so here we have one database
okay so this atm machine is connected to
the database
not only atm machine so if you have an
account with the bank
okay so we can do online transactions
right so we can log into our internet
banking and then we can also do online
transactions so through atm machine or
any front-end application or banking
application if we do any transactions
all the transaction data is going to be
inserted into a database
okay so whether it is in my ms sql or
any other database so there are
different types of databases in market
right so ms sql oracle sybase db2
teradata like that we have different
types of databases so here let's say we
have a database so in this database we
will have the tables okay
we'll have the schemas inside the
schemas we will have the tables like
this
so whatever transactions made the user
all the transaction data first it is
going to be inserted into a database
okay let's say an user he has made the
first transaction
so whatever first transaction he has
made
the same first transaction data is going
to be inserted into a database table
right transaction means withdrawing or
depositing the money through atm machine
so the first transaction data is going
to be inserted into the database so
after one hour let's say he has made the
second transaction
the same second transaction data also is
going to be inserted into the database
after one hour he has made the third
transaction the same third transaction
data also going to be inserted into the
database so like that let's say in user
he has made around
1000 transactions in a day
so if he has made 1000 transactions all
the 1000 transaction data should be
there in the database right okay so in
user through atm machine or any
front-end application or internet
through internet banking so he has made
around 1000 transactions in a day all
the transaction data should be there in
the database okay
end of the day he has selected mini
statement option
so as soon as we select that mini
statement option so this mini statement
we have in atm machine as well as in
internet banking also we have a mini
statement option as soon as we select
that option it is going to generate one
mini statement so that will have some
transaction data okay so many statement
will have few transactions or recent
transactions will have the recent means
last five transactions or last 10
transactions will be there
okay so it will not display all the
transactions or it will not display the
older transactions mini statement
contains only recent transactions okay
so why it contains a recent transactions
why not older transactions okay
because you know one of the reasons
we have so whichever the database here
connected from atm machine or a
front-end application
this database itself is designed to
maintain limited number of transactions
okay so this database itself is designed
to maintain limited number of
transactions limited means let's say if
this database is designed to maintain
last 10 transactions
okay as soon as user made the 11th
transaction
before inserting the 11th record in the
database we'll delete the first record
from this database table and then we
will insert the 11th record as soon as
user made the 12th transaction before
inserting the 12th record in the
database table we'll delete the second
record and then we'll insert the 12th
record as soon as user made the 13th
transaction before inserting the 13th
record we'll delete the third record and
then we'll insert the 13th record
like that if the database is designed
even though user has made 1000
transactions in a day we will have total
how many transactions in the database
will have only lost the 10 transactions
not all the transactions
so why the database is designed like
that
why can't we maintain all the
transaction data in this database
means
suppose let's say the same user from
last one year he has made one million
transactions
so if he has made one million
transactions assume we have inserted all
the one million transaction database in
the database
so an user he has made one million
transactions and all the 1 million
transactions we have inserted in the
database
okay so inside the database will have
the tables let's say we have a
transaction table inside the transaction
table will have all the transaction data
for one user
today he has selected balancing query
option
so we know there is a balancing query
option right so in atm machine or in
front-end application also in internet
banking also we have a balancing query
option
so as soon as we click on that option
what it will do at the back it will
connect to the database on top of the
database some query will execute right
so this option
at the back it will connect to the
database on top of the database some
select query will execute okay now for
one user we have one million transaction
data since we have one million
transaction data for one user assume to
display the balance the select query is
going to execute on top of 1 million
transaction records okay so
user selected the balancing query option
at the back it has connected to the
database on top of the database select
query is going to execute to display the
balance now for one user we have 1
million transaction records so assume
the select query is going to execute on
top of 1 million records to display the
balance
so if the query is going to execute on
top of huge records like this will it
take time or not
definitely it will take time
so assume it took five minutes of time
so user selected balancing query option
and it took five minutes of time
okay so why it took five minutes because
of huge transaction data we have in the
database huge transaction with millions
of transaction records we have in the
database so that is the reason query
took a long time to display the balance
okay
like that so here in the database if you
maintain all the transaction data or
huge transaction data queries will take
long time to execute and it will affect
it to the customer
so that is the reason we are not going
to insert all the transaction data in
this database
okay so instead of maintaining all the
millions of records if this database is
designed to maintain only last 10
regards in those last 10 records only if
you have all the user transaction
details
today if you select the balancing query
option and if you have only last 10
records query is going to execute on top
of only last 10 records and within
fraction of seconds it is going to
display the result right so whereas if
you maintain huge transaction data then
definitely queries will take a long time
to execute
there it will affect you to the customer
so that is the reason we should not
maintain all the transaction data we
should have to maintain only a recent
data or latest transactions in this
database
okay so latest or recent transaction
means last five transactions are only
last 10 transactions
okay
now we are clear that so this database
contains only recent data
okay so recent data or latest data or we
also called it as a current data so why
we maintain only recenter data why can't
we maintain all the transaction data
means we have seen if you maintain all
the transaction data then queries will
take long time to execute and it will
affect to the customer
okay so that is the reason this database
will have the latest data okay fine now
let's say user requested for last one
month bank statement our last two months
bank statement our last one year bank
statement
do we have last one year data here to
generate last one year bank statement
no right we don't have last one year
data to generate last one year bank
statement
then from where we will get lost one
year data means parallelly to this
database we will have one more database
as soon as user made the first
transaction through atm machine or any
front-end application first transaction
data first it will be inserted into the
first database the same first record
also going to insert into the second
database
as soon as user made the second
transaction second transaction record
first will insert into the first
database the same second transaction
record we are also going to insert into
the second database
as soon as user made the third
transaction third transaction record
first it will be inserted into the first
database the same third record we are
also going to insert into the second
database
like that so if the user has made 11th
transaction we know first database is
designed to maintain only last 10
records
as soon as user made the 11th
transaction before inserting the 11th
record we delete the first record and
then we'll insert the 11th regard
right whereas here in the second
database we are not going to delete
anything we will be keep on inserting
the records like this
okay so if the user has made one million
transactions first database contains
only a recent transactions or last 10
transactions whereas the second database
contains all the 1 million transactions
so here we'll have all the transaction
data from day 1 to till date
so all the millions of transactions will
have in the second database
so whatever the data here we have in the
second database so this data we called
it as a historical data
historical data means from day one to
till date if we insert or if you load
all the transactions then we can call
that as a historical transactions okay
so can we call the first database as a
historical transaction database no
because in the first database we have
only recent transactions so here in the
second database we have all the
historical transactions historical means
from day one to till date all the
transactions we will be keep on
inserting the data
we will not delete the data from this
database we will not update the data in
the second database if you delete the
data or if you update the data then we
can lose the
historical data right so we will not
have the historical data so if you
delete some of the records here then how
come we'll have the historical data we
will not have the historical data in the
second database okay so in the second
database here we'll have all the
historical data so whichever the
database is having the historical data
like this
that database we call it as a data
warehouse database okay so in ms sql
also you know we will have tables right
so in that database we'll have the
tables in those tables you know if you
insert all the records from day one to
till date then that database also we
called it as a historical database okay
so now however the first database the
first database contains the recent data
this database we call it as a vltp
database so oltp means online
transaction process so oeltp database
contains the recenter data whereas the
data warehouse contains the historical
data what is mean by historical data all
the transactions if we insert from day
one to till date we can call that as a
historical data right data warehouse
contains the historical data so
historical data means all the
transaction should be there
from a data warehouse like from day one
to till date all the transaction day
will be there okay so we'll see what is
the use of storing the historical data
in data warehouse okay so suppose let's
say
here we have one more atm machine
belongs to same bank and this is another
atm machine belongs to same bank okay so
let's say this atm machine we have in uh
hyd location and this is in
bangalore location this is in chennai
location
so like this there are three atm
machines belongs to same bank in three
different locations okay so all of these
three first they have connected to oltp
database
from oltp so we have connected to the
data warehouse database
so all of these three databases we have
connected to oltp database from oltp we
have connected to the data warehouse
database so which means in data
warehouse we have historical data of all
these three atm machines right
okay let's say for example we have the
historical data like this
so in the year of 2001 from hyd location
machine we have total 120 transactions
okay so from blr location assume here we
have around 80 transactions
from chennai location let's say we have
around 40 transactions
like this we have whole year transaction
data belongs to these three atm machines
we have in data warehouse database
okay so let's say in the next year from
this location we have around 90
transactions from this location we have
around
you know 100 transactions from this
location we have 30 transactions
in the same way we have one more year
transaction data
from this location we have around 150
transactions here we have some 90
transactions here we have 40
transactions
like this we have three years of
transaction data belongs to these three
atm machines in data warehouse database
we know data warehouse contains years of
data right okay so there will have the
huge transaction data our years of data
will be there in the data warehouse okay
next
since all of these three belongs to the
same man let's say they have some cut
off transactions if the transactions are
greater than 50 that is in profits if it
is in less than 50 that is in loss so
like this let's say they have some cut
off transactions
okay so here chennai location this is
you can see this is in loss right so if
you keep on maintaining like that
already it is in loss right
now what you did here you have some data
by seeing this data you have done some
analysis based on your analysis you have
taken the decision on the business to
keep the business in profits right okay
so whatever the data here we have
so we can call this as a report by using
which data we have generated this report
by using the data warehouse data right
so by using the data warehouse data we
can generate the report these reports
are helpful to the business people to do
analysis on their business and to take
the decision on the business and to keep
the business in profits
so if you don't have such type of report
how would we know that which is in
profit and which is in loss
we don't know right
and also is it possible to generate this
type of report based on oltp data
no right so we cannot generate because
oltp database contains the latest data
or current data you know we cannot take
the decision or we cannot generate a
report based on oltp data so oltp
database contains the recent data
so just by seeing the recenter data like
one day or two days of data we cannot
take the decision on the business right
so to take the decision on the business
we need to have the historical data so
by using the historical data we can
generate the reports the reports are
helpful to the business people to do
analysis on their business and to take
the decision on the business
so this is the overview of data
warehouse okay
now so first all the transactions are
going to be inserted in oltp database
right the same transactions here we
should have to insert okay into the data
warehouse it's whose responsibility to
extract the data from oltp database
transform and load into the data
warehouse so who will take care to
extract the data from oltp to data
warehouse
means there are different process to
maintain the data or to process the data
or to load the data from oltp to data
warehouse
what process we have so that process we
call it as a edl process
so etl team so etl means extract
transform and load so etl by using the
etl tools different types of edl tools
are there in market
like informatica data stage of initial
pentaho
right warehouse builder
and also like through pl sql also we can
write the code okay so like that so
there will be separate teams so they
will be taking care to maintain the data
between these oltp and data warehouse so
they will be extracting the data from
oltp database transform and load into
the data warehouse
okay once the data loaded into the data
warehouse on top of the data warehouse
reports will be generated so here mainly
etl developers will be taking care to
maintain the data between oltp and data
warehouse okay so while loading the data
from one database to another database
there are different scenarios
okay for example let's say in oldp we
have a table
like this employee table we have this is
a source table employee number employee
name salary
okay so here we have the data like this
100
to be
200
and 3c 300
again 100
to be 200 now like this we have a
employee table okay so if you have
employee table like this and if you
observe here there are duplicate rows
so in a table in oldp database we have a
table in the table we have the duplicate
rows these two are the duplicate rows
okay
now let's say we have to load that data
from you know
here oltp to data warehouse this is a
target table
so while loading the data from oltp to
data warehouse you know
here in oltp we have duplicate records
let's say we have processed all the
duplicate records also
into the data warehouse table
so if you process all the duplicate
records also into the data warehouse
stable
you know on top the data warehouse will
have the duplicate data or incorrect
data or invalid data we will have in the
data warehouse
okay so if you have the incorrect data
or invalid data in the data warehouse
then on top of the data warehouse we
know reports will be generated right
okay so if you have the incorrect data
how the reports are going to be
generated
is it going to be valid report or
invalid report definitely they are going
to be in valid report so that is the
reason here generally
if there are incorrect records so the
first here the incorrect records to be
fixed
and then valid records only we have to
extract transform and load into that
data warehouse tables
okay so next so as i told you we will be
loading the data from oltp to data
warehouse right so here this is a oltp
database and the data warehouse database
this is oltp which will have the latest
data and this data warehouse will have
the historical data okay now here to
this yltp database so let's say these
are from different locations
so atm machines are connected in a
country millions of customers will be
there to the bank and they will be
accessing their account details or they
will be doing the transactions from
different different locations
okay let's say all these are atm
machines they are there from different
different location first they have
connected to oltp database okay and also
users they will be doing online
transactions let's say these are the
internet banking applications
okay so like that we have different
internet banking applications so users
can log into their account and they can
do online transactions okay
so we know that millions of customers
will be there and they will be accessing
their account details or doing the
transactions now there is only one
database here which is oltp database
oltp means online transaction process
right
so which will have the recenter data so
if millions of customers are accessing
the oltp database data at a time from
different different locations then
definitely there may be chance to go the
database performance down right
so that is the reason instead of
connecting all these applications to a
single database you know here we will
have another database
this is called ods database
ods means operational database
okay so in ms sql only we will have
schemas okay inside the database we will
have schemas
so this is one schema oltp is one schema
or a database and this is another schema
this is another schema like that in the
same database only we will be creating
different schemas to maintain the data
in different different layers
okay so if this database performance is
down then definitely it will affect it
to the customer so that is the reason
instead of connecting all these
applications to a single database you
know some of the applications we will be
connecting to the oltp database some of
the applications are connecting to ods
database
okay so here let's say if all the atm
machines are connected to oltp database
you know these front-end applications
are banking applications we will be
connecting to the ods database
ods is a replica of oltp database
so whatever tables present in ods
database the same tables also present in
old bp
whatever tables present in oltp the same
will present in ods
not only tables or data also it will be
in a sync between these two databases
you know the data will be in a sync okay
so any transaction happen
from when like inside the oldp database
immediately that transaction also is
going to be updated or inserted or
deleted in ods database
any transaction happen through this
front-end application or internet
banking which will be directly inserting
into ods database immediately that
transaction also going to be inserted in
oltp database like that these two
databases will be in a sync
okay
so to maintain the data in a sync
between these two databases jobs will be
created mssqlr plsql
or
you know etl jobs will be created to
maintain the data in a sync between
these two databases
again once the data is available in ods
database so to maintain the data in a
sync between these two databases again
etl jobs will be created
here etl team they will be loading the
data into the data warehouse
okay so like that the data flow so why
we have the audience database pms since
oltp is a sensitive database we cannot
insert all the transaction data in only
yltp database or all the millions of
customers should not access the same
database data at a time so that is the
reason you know we have here ods
database since oltp is a sensitive
database so if we insert all the
transaction data it will affect it to
the customer so that is the reason we
have created the replica so in many of
the companies this replica of the
database will be the audio so from there
again the data will be loaded into the
data warehouse
next so another data flow will see in
data warehouse so this is yltp database
right so let's say this is ods database
operational data source
and then
all the data so whatever data present in
oltp database the same data also present
in ods so immediately
there will be some jobs at the back they
will be taking care to maintain the data
in a sync between these two databases
oldp and ods database
they will be in a sync so both the
databases will be in a sync
okay
so if there are 100 tables in oltp the
same 100 tables also present in otp and
ods all the applications are not
connected to both the databases some of
the applications are connected to oltp
database some of the applications are
connected to ods database okay and the
same data will present in the data
warehouse also right so suppose let's
say here in oltp database if there are
100 tables
instead of 100 tables we'll talk about
one table let's say in oltp we have
employee table so in this table if there
are 100 records same employee table will
also have in ods database here also we
will insert all the 100 records and the
same employee table will have in the
data warehouse there also will insert
all the 100 records
so across these databases we will load
the data first data transfer will happen
from old to ods once we have the data in
ods database so there will be some other
jobs they will be extracting the data
from ods transform and load into the
data warehouse okay we'll see this data
flow here in this scenario
now let's say this is oltp database
and here we have podius database
okay
so as i told you these two will be in a
sync
okay let's say in oltp there are four
tables the same four tables also present
in ods database also
these are the tables
inside the ods
okay so this is t1 table
t2 table
t3 table
t4 table
so like that there are four tables in
ods okay
now as i told you here we'll have the
data warehouse
so let's say this is data warehouse
database
okay so inside the data warehouse we
will have dimension table and fact table
so this is dimension
dimension and this is fact table
okay this is data warehouse
okay and then
in between ods and data warehouse we
will have another database called
staging area
this is called
staging area
or stage database
if you see staging area and stage data
is also replication of ods database
so whatever data present in
ods table perceives tables also present
in staging database so first here there
will be some etl mappings to extract the
data from ods t1 table transform and
load into the stage t1 table
in the same way there will be another
etl mappings to extract the data from
odsd odsd2 table
transform and loading to the stage d2
table so here also there will be some
etl mapping to load the data from t3
table extract the data from t3 table
transform and load into the stage d3
table in the same way t4 to t4 like that
the data will flow from ods to staging
database
and then once this data is available
here in the staging database again etl
will happen to extract the data from
these transform and load into the data
warehouse so like that the data will
flow to the data warehouse
okay
just a quick info guys intellipaat
provides business intelligence masters
program in partnership with microsoft
the course link of which is given in the
description below
now let's continue with the session
so here from ods to data warehouse we
should not load the data directly
okay for example let's say there is a
table in ods right t1 table so we this
is a ods d1 table
right so here we have some columns like
this employee number employee name
salary created
update date
so like this we have an ods
table
okay so here we have some records like
this
create data on date
so let's say we have employee number 10
a 100 some created created let's take
today's date so today
is 8 24
okay 20
21
okay
same run date also will have today's
date
next second record 20b
200
and here we have this one 30 c 300
and here we have
like this
fourth record
40 d 400
and here we have like this 50
e 500
so like that there are five rows in ods
d1 table
so for one of the row let's say created
by mistake someone has inserted like
this
so through some other application or
let's say we have got like this okay
instead of created so we will take you
know salary so let's say here salary for
one of the record we have the null value
in ods d1 table we have the salary as
null value for one of the record okay
now
here in this case you know if you create
here if there is a etl which will
extract the data from ods transform and
load into the data warehouse
that etl what it will do it will extract
these five records and it will insert
all the five records into the data
warehouse
right so data warehouse there we will
not have the constraints
so constraint means primary key foreign
key check constraint default you know
not null constraint so these constraints
we will not set in the data warehouse
because if you have the constraints on
the data warehouse
so the load will happen i mean like it
will take long time to execute because
queries also will take long time
and the insert also will take a long
time if there are millions of records so
generally we will not have any
constraints
we know we have to store historical data
in data warehouse whatever the records
coming from the source all the records
should insert inside the data warehouse
okay so that is the reason here we will
not have any constraints whatever data
coming from the source same as it is we
will insert into the data warehouse
okay so now in data warehouse if you
insert all these five records
there for one of the row we have salary
as null so which is incorrect record so
there should not be salary as null so
someone by mistake they have inserted
the salary as null for one of the
employee in the source table
okay so if the etl process will extract
these five records and if it is going to
insert all the five records into the
data warehouse data warehouse will have
the incorrect data on top of the data
warehouse if the reports are going to be
generated incorrect or invalid reports
are going to be generated
so that is the reason before loading the
data into the data warehouse first here
in the stage database or staging area so
there will have all the constraints in
stage tables okay so inside the stage
table will have all the constraints date
should be 1 to 31 month should be 1 to
12
okay salary should not be null ok
employee number should not be duplicate
so primary key like that all the
constraints at a database or table level
we will have in the staging database
in staging also now let's say we have
the table so stage t1 table we have so
if you have the stage d1 table like this
so in ods we can have the validation but
you know sometimes it may happen to get
incorrect data or duplicate data even
though you have the validation here so i
will tell you another example there even
though you have the validation sometime
you may get incorrect data so what type
of incorrect data will get even though
if there is a validation that we will
see
so first here
in staging table let's say
stage
t1 table you have here
employee number employee name salary
create
date
and then run date something like this we
have so these are the columns we have
now in on stage t1 table we have not
null constraint
okay let's say here in ods let's say
they have not created the not null
constraint so because of that here it
allowed to insert the null value okay
somehow there is no constraint on salary
column so that's the reason
here it allowed to insert the null value
otherwise if there is a normal
constraint here itself
okay so here we will not get the null
value so i am just giving the example of
error record how
like what is the purpose of staging area
here okay and why can't we load the data
directly from the ods to data warehouse
that i am explaining currently
okay so now if you have normal
constraint in this case what will happen
so if the etl process happens to extract
the data from ods t1 table transform and
load into the stage table
okay so since we have the normal
constraint what will happen to that etl
process so it will fail the execution it
will not be allowed to insert these
records now let's say it has failed the
execution so after failing the execution
then etl team they will check
why you know there is a failure because
of the incorrect cut there is a failure
then uh we will check in ods so ods
source side here again there will be
some other team technical team will be
there
they will be maintaining the data
between these two databases mainly
vltp and ods there will be some other
technical team they will be maintaining
the data between these two databases to
maintain in a sync
okay so once we have the data in ods so
etl team will extract transform and
loading to the staging of stage 2 data
warehouse
so now while loading the data here it
has failed because of the null value so
here etl team they will inform to the
ods team or source a team saying that so
for one of the record there's the
incorrect value so you have to fix it
okay so after fixing again so suppose
let's say ods team they have updated
some salary here so again if the etl
process runs you know that this time we
will have all the valid data that valid
data it will etl process will extract
transform and load into the staging
database and from stage again it will
extract transform and load into the data
warehouse now this time data warehouse
will have all the valid data
okay so we'll talk about another
scenario so in another case we have some
salary here
okay and by mistake someone has inserted
so here created
something like this 8 24 20 55
okay created 820 for 2055
okay since we have the future date here
now
in this ods t1 table we have the
validation let's say we have set on
employee number there is a primary key
salary we have normal constraint so date
also we are you know checking like
whether it is a 1 to 31
our month is 1 to 12 like that it will
allow but we do not have validation at a
table level whether it is a future date
or less than or equal to system date so
like that such type of validation we do
not have
right so to check whether the date is
less than or equal to system data on the
future date so that is the reason for
the year we do not have any validation
so it allows someone by mistake they
have inserted the future date record
here
so it is not possible to have the future
data record
so in this case what will happen so this
record future all the five records if
you execute when like if the etl process
is executed so from audience to stage it
will extract all these five records and
it will insert all the five records into
the stage d1 table
again so from stage to data warehouse
again these five records will be
extracted it will be loaded into the
data warehouse
right so in data warehouse also let's
say we have these five records
we know on top of the data warehouse
reports will be generated let's say here
we have
there is separate team reporting team
will there so that team they will
extract the data from the data warehouse
and they will generate a report
so in that report they have written a
logic to extract the records from data
warehouse where created less than or
equal to system data
so like that they have written the logic
to extract the data from data warehouse
where created less than or equal to
system data so in this case it shows
only four employees information right
so here we will get only four employees
information
because for one of the required there is
a created future date
so we have all the validations but still
so this incorrect record is present in
ods stage and the data warehouse
okay
now what business people will do
business people as soon as they receive
the report so they are going to do the
analysis on their report so during their
analysis they found that only four
employees information they are expecting
five employees information but here we
have only four employees information now
what business people will do they will
raise a ticket or an incident
so to the reporting team first so
then what reporting team will do they
will do the analysis during their
analysis they found that in a data
warehouse table there is an incorrect
record
future data record is there so because
of that reason you know here in the
report it has extracted only four
employees information
now that ticket will move to the data
warehouse team
so data warehouse team they only loaded
the data from stage to data warehouse
right so as soon as that ticket data
warehouse team receives what data
warehouse team will do they will write a
query and see whether that record in
character card is present in the data
warehouse or not
so after executing the query so they
found that that incorrect record present
in data warehouse yes for one of the
required there is a future date
now again from where that incorrect
record has loaded from staging so like
that now they are going to do the back
track
now first client side technical team
will be there our source team our ods
team so they should have to fix that
record first in ods database
okay so let's say if you are working in
the team so you have to write a update
statement or through front-end
application to you have to fix that
record
okay so here you will be uh let's say
so they have done the analysis and
so they have sent to ms sqlr database
team or dba team so they have sent that
request whoever is responsible to
correct the data in the database they
should have to take the responsibility
and they have to
fix that regard so first they have to
fix that record by writing the update
statement let's say odst markline team
our client that they have confirmed that
this is not it should not be 255 it
should be 251 so like that they have
confirmed so if they have confirmed to
correct this date instead of
2055 it should be 2021
so if you have got the request you have
to write a update statement to correct
that record in ods database
so update created is equals to 824 2021
where employee number is equals to 50.
so like that we have to correct that
record that is called data fixing in the
same way the same i know we also have to
execute the update statement in staging
database in stage t1 table also there
also that update statement should
execute
right so like that we have to correct
the data so to correct the data we have
tried to update statement so this time
here in this scenario we have only one
incorrect record okay so in real time
there may be chance to have you know
many records also incorrect sometimes we
may get 10 records or 100 records or
1000 records or 1 million records also
we will get such type of incorrect
records because of you know sometimes
because of suddenly the services failure
our server went down so by mistake
something if happens there may be chance
to get the incorrect records or
duplicate records or error records like
this in the source database when the
data is flowing to the data warehouse so
there may be chance to load such type of
incorrect records across all these
databases so here during the analysis
sometimes you know instead of now this
time here a client has found that one
employee record is missing in the report
sometimes here data analysis team also
will be there so if you are in data
analysis team
so you will be writing the queries to
find the incorrect records in those
database tables okay so they will be
giving the some business criteria's okay
so if this doesn't satisfy then
construct that as a error record or
check are there any duplicate records or
check are there any future date records
or check are there any null values in
this column so like that you will also
be writing the queries and doing the
analysis you know and finding for the
error records or incorrect records so
after
finding the error records then we should
have to get the confirmation with the
client okay so these are the records we
have found as in correct record so in
these rows we have the invalid
information
please let us know what data we have to
update here so once you get the
confirmation from the client then you
have to write an insert statement or
update statement or delete statement
depends on our requirement so how we
want to fix so if there are duplicate
records right so like that we'll be
fixing those records here in ods days
and data warehouse
once the data loaded into the data
warehouse on top of the data warehouse
reports will be generated and those
reports will send it to the business
people
so like that the data will flow from one
database to another database
right
next
so this is standard data flow like the
different databases will be there
okay and then the inside those will have
the tables so while loading the data you
know if there are any incorrect records
so we should also have to fix those in
correct regards and load only valid data
into the data warehouse so in some of
the companies again you know instead of
directly generating the reports on top
of the data warehouse here next to the
data warehouse we will have
data mart
so inside ms sql also we can create data
right so what is data mart data mart
again inside the
msa scale is a software type that's a
type of database software right
different databases we can create inside
the databases we have the schemas inside
the schemas will have the tables right
so like that ms sql itself we can have
this data warehouse ods stage okay all
these databases we can have and also in
the same way we'll also have the data
marked
so what is this data mart so why we have
this data mart means
so we know data warehouse contains the
historical data years of transaction
data will be there let's say here we
have 10 years of transaction data
so if they are extracting the report on
top of 10 years of data definitely
queries will take time to execute so
they don't want to display all the 10
years of data in the report they just
want to display last three years of data
so if they want to display only last
three years of data in the report
and
so if that three years of report query
is going to execute on top of 10 years
of data then definitely queries again
will take a long time to
extract that report information last
three years of report information
so that is the reason you know here
we'll have the data mods data mart also
this also contains the historical data
but this historical data is specific to
the report
okay so now business people let's say
they want only three years of data in
that report
okay so what here data mod here in the
table we will maintain only three years
of data
okay so again atl process will happen
here also
to extract the data from the data
warehouse last three years of data
transform and load into the data mods
so here instead of selecting all the 10
years of data so whichever the data is
required to generate a report only that
data will be loaded into the data mods
suppose now let's say business people
says that i want only last six months
data for my report or last one year data
so then in that table from data
warehouse you select the recent last one
year data or last two years data and
maintain only tasks that last one or two
years of data into the data mods
okay so on top of the data marks if the
report is going to be extracted then it
will not take much time
so very quickly the reports are going to
be generated
right compared with the data warehouse
on top of the data mods if the reports
are going to be generated it will be
very easily accessible
okay so this is the data flow inside
that any type of database software will
have these databases again data will
flow like this so finally it will be
loading into the data warehouse we know
data warehouse contains the historical
data as i told you on top of the
historical like historical means from
last 10 years or 20 years
okay so we'll have
years of transaction data in the data
warehouse if you execute small query
also it will take a long time to execute
or to written the result
so that is the reason in instead of
extracting all the
reports on top of the data warehouse
so instead of extracting all the data
all the reports on top of the data
warehouse so as i told you so the from
data warehouse again in data marts will
have different different tables so
each and every table you know so if the
report is going to be extracted on top
of that table so that report if you want
only three years of data also from data
warehouse we in that data mod table will
maintain only three years of data
okay
so let's say in this data warehouse we
will store the data at enterprise level
so enterprise level means let's say in a
company if there are 10 departments are
there
okay so data warehouse will store all
the tech departments data but while
generating the report so for each and
every department we need to have the
separate report okay for department one
we need to have the separate report for
department 2 we need to have the
separate report for another report we
need to have another data for another
department we need to have the separate
report so for that you know instead of
maintaining all the 10 departments data
in a single database which is a data
warehouse so again from data warehouse
we can connect to each data market like
this
so again adl process will happen so the
data will warehouse to different data
mods if there are 10 departments we can
have 10 data mods and on each and every
data mark corresponding to that
department report will be generated okay
so we can create
the etl process so whatever the process
here will load the data extract
transform and load etl process will
taken care to maintain the data between
these databases we can also call it as a
data transformation or we called it as a
etl
so there are different atl tools in the
market like informatica power center
data stage abner show pentaho
okay warehouse builder in sql uh when
like in uh we can also have the pl sql
packages
to transform the data from one database
to another database but finally you know
so very rare will be generating directly
on top of the data warehouse so again
from the data warehouse data will be
transferred into the data mods
so all these types of like otp ods will
have you know
will be there in ms sql so
these are the naming conventions of
again the databases which will be
creating inside that ms sql only the
data warehouse will have the dimensional
fact table
so all the other like wheel dp ods
staging and data mods or sometimes in
data marts we also called it as no
reporting database so this is also we
call it as in
in some of the companies they will call
this as a data mark or in some companies
also called it as a reporting database
or some companies also called it as a
metalized views
so metallized views also will be created
uh you know because you know in data
warehouse let's say there is a table
in that table if there are 100 columns
okay
and there are 1 million records in the
table but for the report they don't want
all the 100 columns
they just want only some 40 columns of
the data in the report
okay and also they don't want all the 1
million records so in the table let's
say there are active and inactive
records are there so they want to
extract only active records and 40
columns of data they need
so in that case you know if the
metallized view is created on top of the
data warehouse stable
okay to have only 40 columns and only
active records from that table the
metallized view physically it will be
created in a database and that view
contains only active records and with 40
columns of data and on top of that as a
source so materialize view as a source
to generate a report
so like that in some of the companies
you know they will be creating the
metallized views also after a data
warehouse in some companies data mods
will be created in some other companies
also called it as a reporting database
so all these things will be there in a
same you know
ms sql or oracle
or cybase db2 teradata depends on
company
so they will be creating such type of
databases
okay so we should have to be aware of
these things other data flow and
what type of databases will be there
so
now we'll talk about what is dimension
and what is fact table
dimension
table or we call it as a dim table
or factor table
so dimension table
contains
primary key
and master data
so if there is a table
which we are calling as a dimension
table
we can say that it contains the primary
key and master data
okay so master data also we called it as
a non-measurable data
okay so whereas fact contains
primary keys of the dimensions
okay or we also called it as a foreign
case
and
measurable data or measurable columns
columns or data will have okay so this
is the definition of dimension table and
fact table so by loading the data from
relational table to a dimension fact
table so all the master data which is
non-measurable data we load into a
dimension table and measurable columns
or measurable data will have in a fact
table so i have one dimensional fact
model here in the data warehouse we will
have the schemas
like star schema and snowflake schema
okay so we have these two types of
schemas in data warehouse
okay so in star schema
i mean like here you can see we have a
dimension table and fact table
okay so dimension table
here we have item dimension branch
dimension and then
you know time dimension
location dimension like that we have
different types of dimensions in data
warehouse
okay so if you see in this dimension
table we have a primary key item key is
a primary key column right so apart from
the primary key column the other columns
are item name brand name
sold by category all these are non
measurable
in the same way here we have branch
dimension so branch dimension also here
we have branch id which is one one
primary key column the other column
balance name owner name you know branch
and owner name are non-measurable
columns in the same way here time id
it's a key column another day month
quarter year are you know
non-measurable columns
okay so location
here we have location id name state pin
code so here also location id is a key
column the other columns are
non-measurable columns in dimensions you
will not find the measurable columns
that's how it is designed so generally
this and all architect level architects
they will design the data models and
they will decide which column should be
there in a dimension and which column
should be there in effect
okay but we just have to know what is
dimension and
what type of data will be there in a
dimension table and what type of data
will be there in a fact table
now if you see fact so here we have
foreign keys foreign keys means like
primary keys of the dimensions item key
which is in a dimension table also
present in a fact table
branch id here we have a dimension table
same present in a
fact table in the same way time id also
present in a
whatever is present we have in a time
dimension the same time id and a branch
id from the branch dimension location id
from the location dimension like this
fact table contains primary keys of the
dimensions or foreign keys like this and
also other columns are measurable
columns quantity sold so quantity is a
number type of data so which is a
measurable column amount sold amount
again it's a number type of data average
sales it's a number type of data right
so all these columns are you know
measurable columns like this you know
dimension contains the key column apart
from the key column the other columns
are non-measurable columns fact table
contains primary keys of the dimensions
which are form case and the other
columns are measurable columns
okay so like that we will have the
schema so inside the schema or we call
it as a data model so in a data model or
its schema in data warehouse you know we
will have dimensions and facts dimension
contains the primary key and
non-measurable columns whereas fact
contains primary keys of the dimensions
in the measurable columns this is how
the dimension and fat will be
differentiated and as i told you it's
architect responsibility to design the
data model like this
based on this they will be creating you
know dbs will be creating the tables and
so if you store the data like this it
would be easy to generate a report so to
for easy access report so this dimension
fact models are designed
and compared with the relational so here
you know if it is designed based on the
data warehouse model reports would be
easy to access so that is the reason
this dimensional fact model we have here
next
so we have two types of schemas in data
warehouse the first one is star schema
the second one is snowflake
so what is star schema model in data
warehouse means so you can see this is a
star schema model why we call this as a
star schema model means in a schema or a
data model if fact is connected with all
the dimensions
then we can call this as a star schema
model connected means all the dimension
keys should present in the fact
now here there are four dimensions so
that is the reason we call this as a
star schema model if at least one of the
dimensions suppose let's say this branch
ide is not present in this fact table
then we cannot call this as a star
schema model
okay so in star schema all the dimension
keys should present in the fact then
only we can call that as a star schema
model
we have another schema here which is
called snowflake okay
so in this snowflake schema model we
have one fact and left side to the fact
we have four dimension tables right side
to this fact we have to have five
dimension tables
so in this schema we have total nine
dimensions are there even though we have
nine dimensions we have only three keys
present like
date id store id product id so like that
we have only three dimension keys are
present in this fact table
so like this in a schema or a data model
if only few of the dimension keys are
present in the fact not all the
dimension keys
only few of the dimension keys are
present in the fact then this model we
called it as a snowflake schema model
okay or so at least one of the dimension
key is not present us say you have total
nine here you have eight dimension keys
in this fact but the one dimension key
is not present that means one dimension
is not connected to that fact
okay so in that case also we can call
this as a snowflake schema model so fact
table contains free of the dimension
keys and measurable columns in snowflake
schema model whereas star schema model
fact should have all the dimension keys
again so
it's on all architect level they will
design uh the data model schemas like
this and they will decide which column
should they're in a dimension and which
column should be there in effect so we
just have to know what is star schema
model and what is snowflake schema model
so in data warehouse we have different
types of dimensions we have four types
of dimensions here one is confirmed
dimension
or reusable
okay the fourth one is
slowly changing dimension or scd
so in slowly changing dimension again we
have three types
okay so scd type 1
scd type 2
scd type 3
so acd type 1 dimension contains the
latest data or current data
whereas s3 type 2 dimension contains you
know historical data
so whereas acidic type 3 dimension
contains partial history
okay so confirm
our reusable dimension means if any
dimension can be reused across multiple
projects
same dimension table so if you use
across multiple project then that is
called conform dimension for example
let's say there is a employee dimension
so
employee dimension is there here we have
employee number employee name salary
something like this we have
so you are working for let's say project
called ge
so you are working for a ge project so
in this ge project there is a employee
dimension so this employee dimension
contain you know ge employees
information so let's say he is a ge
employee okay so in the same company
there is another project
called ae aero electronics like that you
have another you know project in the
same company okay so
here also we have to create an employee
dimension so here there is a employee
table created okay now here also we have
to create an employee table so can we
reuse whatever the employee dimension we
have created no right it's not possible
we cannot do like that because this
employee dimension contains ge employees
information we cannot reuse this
dimension into another accounts or
another projects
okay so this is not valid so this is not
a confirm dimension okay so instead of
employee dimension let's say we have
another dimension in this ge called
calendar
so in this calendar dimension we have
all the calendar information
okay
so
the date and time and uh the calendar
dimension generally it can take the
holidays okay so public holidays and all
so
those things will be there in the
calendar dimension so we are sure that
calendar data remains same across
multiple projects so that calendar
information is not going to change
okay so confirm dimension contains you
know
the data which can be reused across
multiple projects the same data again we
should not change anything
such type of dimensions we can call it
as a conform dimension we just have to
know the definitions
and then junk dimension so junk
dimension means
if any dimension data every second or
every minute if it is getting keep on
updated the latest data if you are
getting for example it says stock market
information if you store in any
dimension so that data every second or
every minute we will be getting the
latest updates right so such type of
information if you store any any
dimension then that dimension we called
it as a junk dimension then degenerated
dimension so degenerated dimension means
now you know here we have a
pin code let's say in a location
dimension here we have a pin code so pin
code contains characters sometimes we'll
have numbers also right so let's say
that one we have stored in a dimension
pin code column currently we have in a
dimension
okay very rare we will also store such
type of columns in fact also
which is a non-measurable pin code pin
codes we cannot measure right but that
will have the number type of data also
okay so such type of columns if you
store in a fact
okay so if any dimension is sharing its
non-measurable column with a fact then
such dimension we call it as a
degenerated dimension here for example
location dimension we have in location
dimension we have a pin code column so
that pin code column we have in a
location dimension as well as in factor
table so that pin code column is coming
from you know location dimension so in
this case so whichever the dimension is
sharing its non-measurable column with
the fact in such type of dimension we
call it as a degenerated dimension
next slowly changing dimension in slowly
changing dimension we have three types
scd type 1 sad type 2 scd type 3
so type 1 data means let's say in a
source table you have a table like this
emp underscore sad type 1 underscore
source here you have employee number
employee name address
okay and here you have 10 a hyd 20 b blr
30 cc like this you know we have a
source table and then
so let's say here we have a target table
this is target
now by loading the data from source to
target table how the data will be
maintaining this table for a cd type one
means okay along with this we also have
the create it updated
okay so how we will maintain the data
means for the first time we will check
whether the source record present or not
in the target table if the source record
does not exist so it should be insert
right okay so here we will be inserting
like this
and a
hydrated should be a system
so it should be 8 24
20 2021 and update it also will have
system date
okay and then
for the second record also source record
does not exist so second record also is
going to be insert
like this third record also does not
exist this record also going to be
insert
like this
so for the first time none of the
records present in the target table so
all the records is going to be inserted
this is how data transformation will
happen in a serie type one okay second
time let's say this employee has more
hype to usa
so here we have got an update as usa
so let's say we have got this update on
8 25 on august 25th he has moved to usa
so if you have got an update here uh to
move to usa so this time if you execute
this film let's say we are executing
this mapping on next day tomorrow we are
executing
this data transformation so what will
happen it will check whether the source
employee number exists or not in the
target table
okay
so now in this case what will happen
so we have to update here only
specific columns we have to update right
so we have if the source employee number
already present in the target table we
should have to update the record in the
target table so
hyd with usa and also we also have to
update the update date
so like that it will happen
in type 1
so this is going to be 8 25
20 21
so we are not going to update the
employee name and also create it create
date is the date which actually the
record got created
okay so we no need to do anything for
the second and third record because
there is no change okay so no insert no
update we just have to leave that record
as it is okay next third time third time
again let's say this employee has more
usb to uk
on august 30th
okay let's say on august 30th we have
got an update here in this table so this
time in serie type one what we have to
do again if we are running this uh
transformation data transformation
between uh source to target on 8th 30th
it will check whether the source
employee number present or not so this
source employee number does not it's
already present and there is a change
between source and target record so in
this case again it should have to update
a usa with uk
so here we have to update with uk
and then update date also it should be
updated to 825 to 830
like this it should be updated
okay so for the second and third record
no change so if we got the new record
here in the source table that new record
should be inserted inside the target
table
so like that it should happen so a
employee number 10 he was there in hyd
and then he moved to usa and currently
is in uk so we have whatever data
present in the source same as it is we
have present in the target table we
don't have older transactions here
right so whatever data present in the
source same as it is if you maintain in
the target table that is called you know
latest data
it will not update automatically so
there will be some atl process
okay so etl jobs will be there
informatica team or atl team so our pl
sql team project depends on the client
they will decide what how they want to
process the data
okay so it will not be updated
automatically so there will be some
process to transform the data from
source to target right so that process
will take care to maintain the data like
this in the target table
okay
so as soon as that process is executed
that will take the source employee
number it will check in the target table
whether it is present or not so if it is
not present then it will insert if it is
already present then it will check is
there any change between source and
target if there is a change then uh you
know it will update only the specific
columns in the target okay so like that
it will happen so this is in srd type
one next serie type two how it will
happen in a city type two
so employee number employee name address
so here for the first time we have like
this
in the source table
so now let's say here
this is the source and this is type 2
target
so here we will have another key column
should be there employee id
on top of this will have the primary key
constraint
and then here we will have
effective
start to date
and effective end date also will be
there
okay so here also what we have to do for
the first time we should have to check
with the source and buy number present
or not in the target
so here we have to insert the record
while inserting in this eid primary key
column so identity should be defined on
this eid column or
we have to generate a auto increment
sequence number so we have to set on top
of this column so that will take care to
generate a sequence value
1 10 10
hyd will come from the source effective
started it should be a system date
8 24 20 21 end date always it should be
1 1 double 9 double 9 future day
okay so like this so start to date
column here we should have to load the
system date and here it should be always
future date in type 2 next for the
second record so employee number 20 also
does not exist this also we have to
insert 2 20
b blr and this one so like that we have
to insert for employee number 30 also
does not exist 3 30 c cni and this one
like this
for the first time none of the record
present in the target table
so all the records that etl process
you know will extract transform and load
into the target
so this is a data warehouse table sc
type 2
target is the data warehouse table so
here we are seeing like how the data
will be maintain the historical data
second time let's say this employee has
more hvac to usa on august 25th
okay so this record we have got on
august 25th
so tomorrow if you are executing this
mapping to load the data
from source to target
so what will happen
it will take the source employee number
check in the target whether that is
present or not so this time already
present so if it is already present is
there any change between source and
target yes there is a change source we
have latest address usa present target
table we have previous or old address
instead of updating the address what we
have to do is so we have to update the
end date with the system date
to make that record as inactive
okay so here we should have to update
the end date with the system date first
that etl process will take care all of
these things
we just have to know the concept how it
is going to maintain the history
so it will be updating with the system
dates this date is 8 25 let's say since
we are executing this mapping on next
day so it will be updated with the
system date
and then
the latest updated record for the same
employee a new record will be created
this is a sequence value here it will be
generated
employee number 10 a usa and 825
and this is one one double nine double
nine so like that we should have to
insert
okay
next let's say again
uh this employee has more usa to uk
okay so we have got an update here
uk so
this he has moved to uk on august 30th
so we have got an update on august 30th
this time what do we have to do so we
should have to check in the source and
target so do we have source empire
number present or not this time for
employee number 10 how many rows are
present in the target there are two rows
in these two rows to make the recorders
inactive so we have to update the fourth
record which is the active record this
is we call it the active record the
first row is inactive record for that
employee so active record end date
should be updated with the system date
so this one it should have to update
just a quick info guys
intellipaat provides business
intelligence masters program in
partnership with microsoft
the course link of which is given in the
description below
now let's continue with the session
with system date this date is 8 30 so if
you are running this
load on 8 30 so it is going to be 8 30
and then the new record is going to be
inserted for the same employee
5
10
a
ok 8 30
and then here
1 1 double line double 9 like this it is
going to be in center whenever we insert
this should be a system date and it
should be a future date
okay next second and third record there
is no change no insert no update
so again fourth time if any new record
comes here
this employee number 40 record needs to
be inserted so fourth time if you if
this load happens so the data is going
to have like this in the table so you
can see
440 del only this record not for the
hair six sequence number for the next
row six and here the system let's say if
it is 8 30 so it is going to be inserted
like this this is how the historical
data will be maintained in the data
warehouse
so next we have a serie type 3 partial
history so we'll see how to maintain the
partial history here
emp scd3 underscore yes source table
here we'll have employee number employee
name original address and current
address let's say here we have
addressing two different columns
okay
so we have address in two different
columns
and then uh
currently we have one a hyd current
order is also hvad for the first time we
have the same
or what we will do is here we will have
only one column for the address
target will take two columns so like
that here we have the source 10
hydr and r
30 cc like this we have a source table
so whereas in target table
for type 3
we'll have like this
so here
uh original address
and current address
okay so
original address and current address and
if you want you can also have created
updated columns okay
so now for the first time none of the
record present in the target table all
the three records needs to be inserted
10 ea original address also hrd current
order is also hyd and also we'll have
the create date update date columns so
created today's e24 update it also we'll
have today's date only e24
next for the second record 20 bblr and
this is blr and this is 824
824
okay next 30 c cmi and this is also cli
and 824 824
so like that you know for the first time
one of the records will present on all
the records are going to be inserted
inside the target table in a city type 3
so next time let's see if we have got an
update here
okay so here
this employee has more hype to usa
so we have got an update here
so this update we have got on august
25th
okay so this employee moved to usa and
25th so this time let's say we are
executing data transformation happening
on 25th
so in that case this date will be 25th
so it will take the source employee
number checking the target table whether
it is present or not since it is present
now what it should have to do it will
check whether is there any change
between source and current address so we
here we should have to compare between
original source address in the current
address
these two columns we should have to
compare so if there is a change yes then
what we have to do we have to update the
current address and the updated these
two columns we have to update here
original address remain same so current
address only we will be updating to usa
and then here the update it will be uh
like system date 825
for the second and third record there
will not be any change no insert no
update so it will just leave that as it
is
okay next again let's say again we have
got an update this employee again he has
more usa to uk so in the source table we
have got an update here so let's say
here we have got an uk and this update
we have got on august 30th we have to
compare so we will check whether the
source employee number exist or not if
it is already present check is there any
change between source and target while
comparing we have to compare with the
current address in source we have uk
whereas in target we have usa
now in this case what we have to do we
have to take the source address and
update that source address in the target
table
here in the current address column
right so we will be updating uk and also
will be updating the update date
so updated with a system date says date
is 8 25 so like that it will be 8 30 so
it will be updated
okay so for second and third record
there is no change
right so like that it will be updated
now in this case you can see here so he
was there in hyd from basically from hyd
and then he moved to usa
okay and then he moved to uk we don't
have in between transaction we have
original transaction
and also we have the latest transaction
in between transaction data we don't
have usa record we don't have so
original transaction we have and also we
have the current transaction this is
called a cd type three okay and if you
have any other columns here suppose
let's say contract number so you're
saying contract number so contract
number some numbers you'll have like
this
something like this if you have here
also will have only one column so
basically here in serie type three we
should have to
decide on based on which column we have
to maintain the partial history
so here we are maintaining the partial
history based on the address column
so based on personally we are deciding
if there is a change between source and
target
okay so type 1 and type 3 difference if
you see here in type 1 finally we have
got an update as uk right the same uk
present but here he was there in hyd he
was then in usa original transaction
also we have here whereas here in type 1
we do not have original transaction we
have only reset a transaction here
that is the difference between type 1
and type 3 so type 1 will have only a
recent transaction whereas in type 3
will have both like original and current
a transaction in between transaction we
do not have
so these are the three types of you know
mainly the slowly changing dimensions
we have in data warehouse
next
we'll have types of facts
facts also we have four types of facts
one is
additive fact
the second one is semi semi-additive
fact
third one is
non-additive fact
and fourth one is fatless fat
sodium fat contains primary keys of the
dimensions or we also called it as a
foreign keys
and measurable data
whereas this one contains
a semi-artifact
few
of the
dimension primary case
and measurable columns
measurable data
and
non-additive fact contains no
primary use
of
dimensions
and it contains only measurable data
so fact less fact it contains only
primary keys of dimensions
and no measurable data
so the fact which we have in a star
schema model this fact we called it as a
additive fact because it should have
primary keys of the dimensions and
measurable data
all the primary keys of the dimension
should be there inside this fact
okay whereas
semi additive effect contains only few
of the dimensions so the fact which we
have in a snowflake schema
is called semi additive fact so semi
additive fact contains only few of the
dimension keys and measurable data not
all the dimensions
okay and then non-additive fact no
primary case of the dimensions are only
measurable data which means that fact
will not have any of the dimension keys
so that will not be connected with any
of the dimensions
which contains only measurable columns
there will not be any dimension keys so
if any fact contains only measurable
data no primary case of the dimensions
then we can call that as a non-additive
fact
okay and then factless fact so factless
fact we have only primary keys of the
dimensions and no measurable data
so it should have
all the dimension keys but this column
should not be there
no measurable data such type of fact we
called it as
factless fact so non-additive fact so in
data warehouse schema so any table which
contains only measurable data and no uh
primary case of the dimensions or no
foreign is nothing then that is called
non-additive fact so that will be there
in a data warehouse that's why we call
that as a fact
next so generally data marked contains a
specific department history whereas data
warehouse contains enterprise level
history so enterprise level means all
the departments historical data if you
store in any data warehouse
like if we store all the departments
historical data then that is called data
warehouse so if you store any specific
department history or
report based history if you store in any
database then that is called data marts
so we can have n number of data marks
but data venous can be only one so where
we'll have all the departments
historical data
which will be for enterprise level
history will be there in data warehouse
present data mods will have only
specific department level history
okay so we have finally in data
warehouse we have a
top down
and bottom top approach is there
so top down approach means so from oltp
to this is yltp this is ods this is
staging and then this is data warehouse
okay so from data warehouse if you load
the data into the different data mods
this is called top down approach
okay so this is oltp database and this
is ods database and this is a staging
database
and this is the data warehouse database
and these are the data mods
data mart one
data mod 2
data mod 3
okay so this is called top down approach
so on top of the data mods reports if
they are going to generate a report in
data warehouse we call this approach as
a top down approach whereas bottom top
approach means so instead of
loading the data first into the data
warehouse
so first
we load into the data mods from staging
we can also load the data into the data
mods
and then
from data mod again instead of
generating the report on top of the data
mods so we can load the data into the
data warehouse and then the reports will
be generated this is very rare
most of the times top down approach only
will be there so here this is a data
warehouse so from data marts again if
you load the data into the data
warehouse on top of the data warehouse
if the reports are going to be generated
then this is called
bottom top approach
okay but
generally we'll have top down approach
so these are the concepts we have in
data warehouse
data mart is nothing but
a subset of our data values
so consider this is my
uh the transactional database which we
are having okay in real time
so this is operational systems
so
we are not going to
load
in the single stage
to the data barriers we do have so many
stages will be available okay so
from the oltp to oil ap tran transfer
rate data transfer so definitely we do
have multiple stages available in the
real time
so
like staging
staging area is one of the
stages in the etl process
like we will land all the data from
different different oil system to oltp
system to staging from the staging too
we will move the data to data barriers
so from the data values a subset of data
values we will create
either logically or separate data much
will be created again the data marks
also a database only
but it is a logically individ
how we do how one terabyte of hard disk
have been divided like c type d drive
and e drive the same way so here also
we will logically divide
the data values or one particular
database into
multiple data mart the data mart will
have a subject oriented informations
based on their business
so consider this is the two
this is the difference between data
values and data mart
the data values
it is it will have all the enterprise
data
okay so it will have all the enterprise
data
but data mart will have a particular
subject a department by data
so here we do have multiple subject
areas are
available in this particular data values
but here it will be a single subject
area each data mod will have single
subject area so multiple data sources
definitely will have multiple from the
multiple data sources we
uh we will accumulate all the data we
will integrate the data into paragraphs
but here it will be limited data source
so might be this might be the source for
data mart
again we need to do the etl process to
fetch the data from it database goes to
data mart here we used to do one etl
process
again here etl tool will be uh
introduced
so here
it will occupy a large memory okay the
data values will occupy large memory but
the data mark will occupy limited memory
so
so this will take lot of lot of time to
implement this database but the data
march will
take some shorter time to implement it
so this is the major difference between
data match and
data values
so we do have different types of data
march
in the real time
so for example three different types of
data mods are available
so the first types of data mark is
independent
data mart
okay so the dependent data mart
and independent data mart
and hybrid data mart
so what is meant by independent data
mart
so here we do have oltp systems
so from the oltp we are moving the data
to data variables so from the vargos we
are
we are fetching the data for our data
mode
so this is called dependent data match
the reactor mark is dependent on the
data values
okay so from the variables we are
pulling the data so the data is first
extracted from the oltp system
and then populated to the
data mart the central data bar goes to
data marked so from the variables the
data travels to the data mod but here
independent data merge here the data
various will not be available okay
so directly the data marked will be
loaded from
oltp source so this is called
independent data merge
and then we do have hybrid data marked
with a hybrid data match the data mark
will get the data from
either from the oltp system or from the
database system
so this is the three different data part
available in the
data values so what is the data modeling
so data modeling is nothing but
so
for example if the business users
so will give us the business related
terms
okay business related
uh terminologies
he knows or she knows the business
and she wants to convert the business
into a model
okay data model so how to convert that
so we do have a different methodology
to draw this okay to create that data
model so we do have three different
method
so one is called conceptual model
and logical model
and physical data model
so what it does mean so what is
conceptual model so how to create this
conceptual model and what is logical
model what is physical model we will see
one by one you assume that so we are
going to create the data model for one
of the
particular a particular online portal or
something okay so online
portal
the online product uh selling portal we
are going to create it
this online portal right definitely they
do have uh
they need to have so these are all the
the tables we should have
so in order to
have our customers informations we need
to have the customer table
in order to maintain our employees
details we will have the employees
tables
in order to maintain the customers
accounts we will have the accounts table
in order to maintain
the product information we have the
product now the region table so to have
the different regions on the sales to
analyze which region is performing well
to analyze which product is performing
well so which account they have they
have
transferring the money or doing the
transaction the money which employee is
doing well okay so
how is the performance on the customers
okay so like like this we do have a lot
of tables are involved in the
data modeling
so if you consider these tables so
countries table regions table
accounts table merchant table so a lot
of tables are available okay
so in this customer table so each
customer will be identified by customer
id
so this id we will call it as a primary
key in a database
okay so then we will have all other
information of our customer
so for example customer name
so customer name might be
a duplicator okay so two customers may
have same name but the customer id will
be different
the customer mobile number okay we can
get the customer mobile number to give
them the update
the mobile number
we can get the data but of the customer
to identify to analyze the age of the
customers
so which type of customers which age
group of the customers is ordering the
product okay more
so in order to get that we need to have
the data birth
so we will get the address of the
customers which location their customers
are
ordering more
and on the city of the customer
and
country of the customer from working at
different countries
so these are all called the different
dimensions
okay different dimensions of the
customers
so if you consider customer is one of
the table then this is the customer id
is the primary key
so primary key in the sense it will not
allow duplicate to the columns
so primary key
and customer name
the mobile number the data but is the
different dimensions of the customers
this is the dimension table we do have
an employee table the same way employee
id
employee name mobile number employee
skill set employees of experience and
employee city address all those we can
get it the account
the account number will be the primary
key
and what is the type of account what is
the active account or inactive account
so
different different details we can get
the
accounts so these are all the tables are
diamonds and tables
okay so different dimensions of the data
and
by using these dimensions if i'm going
to create a sales
okay
sales
this table is called as pack table the
sense fact table will have the
each
key values here
which customer has ordered the product
okay which employees have helped
to purchase the product employee id
okay then which product he has
purchased product id
then
which account account number okay which
account number he has purchased
which region is purchased the product so
at the end of the day i want to analyze
the summarized data on the regions which
region is performing well
okay which merchant is performing well
i'll have the merchant id this is the
way i will get all the
key values from the table
okay key values from the table
so these key values are called as
foreign keys
okay the integrity to maintain the
integrity will have the foreign keys so
normalization we will see more on the
primary and foreign key but as of now
you unders
understand uh this
foreign key primary key relationship so
each column is called as foreign key
that is the the reference is coming from
the base table
the dimension table
so this is like
the the diamond the fact table we have
all the keys we have and then we do have
the quantity how much quantity he has
purchased the product okay for example
two qualities he has purchased
what is the one quantity unit price
one quantity unit price
and what is the
total quantity yes persist
or total price
so if one quantity two quantity is
purchased one quantity is 100
then the total price will be 200
okay so these three
are
call it as the measures the measurable
values these three
are called as the measures
why because these are the measurable
values
and
this this
column is called as
the dimensions so these are like the
foreign keys will be maintained in the
fact table this is a fact table f
underscore sales is the fact table
the fact table is derived from the
dimension table to have all the
the customers
the customers information the employee
we will not keep all the customers
information here just we will keep the
customer id which customer he has
purchased
which employee has purchased help to
purchase which product has been
purchased on which account number
from which account he has purchased
which region id and which merchandise
all those
in order to
after that if i want to get this
particular
total sales for the day so even i will
use the date key
okay date id update key
to have this date
so date dimensions i will get the date
so which date so after that so i will
combine i will sum up this total price
for a particular day
and then i will get the merchant
on the ranking and i will go for the
merchant id
join with this merchant table and then
get which merchant
is
having the highest
sales today
so this is what the fact and dimension
tables are related so again the data
modeling is having the three different
types one is conceptual model and
logical model and physical model
so what is that conceptual model so you
have concept okay you have business
concept okay so i want to create
a data model for this particular
business
okay you take the online food delivering
uh
the business okay online food delivery
business
so this is the one of the con concept i
have as of now so for this i want to
draw a model data model
so what are the tables needed for this
food delivering
particular
business
so i will have the time okay we have the
date and time dimensions
i will have the product okay different
product i will have
i will have a different store to
purchase the products all those so
whatever the diamonds and tables you
needed for
all the
for this particular business you will
draw all the dimension tables
like time dimensions product
store and regions and
the merchant account whatever the
dimensions you want to have you have to
draw all the dimensions
so with
so with that only name okay only name no
need to mention any detailed
informations okay
so these these are called its entities
entities is nothing but a tables a
technical word is stable
but in the conceptual data model we will
call it as entity
so what are the entities we are going to
have all the entities we are going to
draw
okay so includes the important entities
and the relationship among them okay so
what is the relationship we are going to
create
so the tables are
like divided into two parts
one is the upper the path is having the
primary keys
so whatever the keys you want to have
for example this product i product table
might have the product id the store
table will have the store 80 the sales
table will have all the foreign keys
here
so this is what the conceptual diagram
we will write so here we will not write
any attributes attributes is nothing but
column names
since it is a conceptual data model we
are not going to write any primary keys
foreign keys all those
we are just mentioning the entity names
and how the entities are related to each
other
so this is called conceptual model so
you can have a sing a simple paper
and paint pen to draw this
or you can have this paint
okay so just you can drag and drop all
those and then you can have this
conceptual data model in this
conceptual data model it will be highly
abstract okay so it will be highly
abstract
so you cannot have any uh column names
you cannot have any column names it will
be easily understood
okay it will be easily understood uh
each and every model so this conceptual
data model will have this highly
abstract
okay it will not have any uh
attributes names attributes name in the
sense the field name the column names it
will be easily understand you can easily
enhance so you can add
more
entities to the particular model
okay so only the entities are visible so
you should not write any uh
so it's like a first step of the
data model so we will have all the
entities so only entities we will
drop that
all the
relationship between the tables
entities so no software tool is required
to define a conceptual data model since
it is only the concept
we will no need to have any software to
to draw this
so this is one of the
the conceptual data model so time
product sales store regions account
merchant so n number of
tables can be connected with uh the fact
tables the fact tables might be
connected with different dimension
tables as well
so this is called a concept conceptual
data model then we have the logical data
model
in this logical meta model it has been
enhanced from the conceptual data model
so the first step is we will write the
we will draw this on the conceptual data
model after that
in this conceptual data model
we will have this what is the primary
key we will use okay so definitely each
entity we need to have a primary key for
this
time dimensions we will have the date
primary key date key
in product it will have obviously a
product id
so it is like a description of the
column not the column name
not the exact column name we will write
so simply we will write product id okay
with space also
if it is a column name so definitely if
it is attribute name definitely we
should write with underscore
so since it is a description so you can
write
with this
okay so date descriptions
month descriptions
and year
go all those so whatever the
the attributes you want to have you can
have all the attributes here
in the product table
in the products entity you can have
product id product description category
and category descriptions unit price
created time on updated time you can
have all those
and then the sales the store right the
store id store descriptions which region
is from the store the region name
created date updated all the all those
you can have it and then the store that
i as i told the
the upper part of the
fact table will have all the foreign
keys
so from the store table you'll have the
store id
from the product table you'll have a
product id from the date from the time
table will have the date key
after that item sold item amount i told
right it will be the measures only the
message the foreign keys and messages
will be there on the fact table
so item sold is the quantity again the
sales amount is the price so these two
are called as measures
okay foreign keys and measures will be
maintained on the fact table
so this is what we will maintain in the
logical data model
we will not write exact column names we
will not write any primary keys and all
those we will simply mention that the
column description
it includes all the entities and
relationship among them
all the attributes attributes of the
cells the column names all the
attributes for each entity are specified
the primary key is for each entity is
specified
the foreign key is also identified and
specified
with this fact table
and normalization occurs at this level
normalization in the sense
okay so for example i do have store
table
i will i'm
assuming that in the store table i have
the region and region name
okay so i can split this store table
into one more
table okay consider
this is called normalization anyhow we
are going to see tomorrow
so consider this is one more table
as the region table we are creating
so
the region table we are creating here
okay
on this region table so we we are going
to delete these two fields from this
table
and they
here we have only have the teacher id
okay region id column so this region id
is connected with this region id
in this table
so this is region id we have
register id so and then region name
so
a single dimension table is again
splitted into other dimension table to
have
to avoid the data redundancy
so how we will avoid the data redundancy
so i because if i have a particular
store
okay a particular store
in a different different regions okay so
i do have different different regions
and
in a single region i have multiple store
100 stores are there in the single
regions so the 100 regions informations
will have 100 times the region names
will be repeated
to avoid this
we will have only the region id here and
we will take that region name only one
time here region id and region
so that the data redundancy will be
avoid by using the normalization so this
is called normalization
so normalization occurs this level so
if any possibility of the normalization
on the tables we will split up this
table into multiple tables this is
called logical data model
so presence of attributes so definitely
you will have the attributes present in
each entity
and the key attributes the primary and
foreign key
and the non-key attributes the
description you will have right the
non-key attributes you have
these are all the non-key attributes
this date is the key attribute here the
product id the key attributes
then primary key foreign key
relationship definitely will have the
primary and foreign key relationship
and user friendly attribute names you
can have only the descriptions no need
to have
the
the primary can foreign key
sorry no need to have that entire column
name you can simply you can have these
descriptions
and you can it will be more detailed
than the conceptual model so you'll have
all the column names right
so edwin is the data modeling tool our
power designer okay can be used to
create logical data models so this data
model we are going to create
and then this can be automatically
converted to a physical data model with
the help of this tool
so
if you create the logical data model so
only this model then the ervin tool will
convert this model into a physical data
model
okay so it will create the table names
all those it will create the with the
columns
so this is the physical data model see
here so each and every table will have
the
column attributes as well as
the data type of the attributes and the
data length of the
attributes okay for example if you
consider this is one of the uh table the
time table okay so here that the table
name is uh mentioned as dim underscore
time so dim is nothing but so dim is
nothing but dimensions
so dim underscore time
dem underscore product
fact fact underscore sales i as i told
uh the sales table will be a fact table
so this is the actual
format we will give okay so deem
underscore store is a dimension table
the dimension tables are connected with
uh the fact table here
so see here here we have the
all the column names underscore with
underscore exact column name should be
there
okay so what is the data type or for
example it is an integer
or it is a vector so how much is the
data length that is also will be
provided here
so detailed the logical uh model from
the logical model the physical model
will be automatically drawn by using any
other
data modeling tool so so we can have the
admin tool
so if you if you give this for the
airplane tool if you give this this
model so automatically it will convert
that into this model
okay this model of data
so fact underscore sales
so definitely it will have uh
the fact tables so all the foreign keys
again it is also integer integer integer
and integer and float
so mostly it will have the id columns
the key columns
and these are dimensions so see here
the the specification of all tables and
columns will be maintained
in this physical data model
the foreign keys are used to identify
the relationship between the tables
and the normalization may be occur okay
denormalization is in the sense
we will again
we will we have splitted the tables
right two tables here
might be merge these two tables into a
single table
so if necessary we will do this
denomination based on the
requirement so physical consideration
may cause the physical data model okay
to be quite different from the logical
data model so the physical data model
again it will create okay uh it will it
will differ from
each database
if it is teradata
might be the backer it will be okay
the integer big integer okay but if it
is that oracle the integer might be
number the worker might be webcast 2
okay so different different databases so
whenever we are having that airbender
tool if you create
the logical data model we should select
what is the database we are going to
apply this
so if you select oracle then
based on this particular database
it will convert that logical into a
physical data model so this is the
physical data model it will create
uh then
so entities referred as a tables here so
again we will call it as since it is a
physical table
so here we are going to
make
the terminologies as table here okay we
will not call this as entity we will
call it a table attributes will be
referred as column here
so database compatible table uh database
again compatible the table names
database compatible column names we need
to provide and database specific data
types also mentioned
so difficult for user to understand
so those who do not have any technical
knowledge
so
so they were somewhat difficult to
understand but if they are seeing that
they can able to understand what is this
column names so overall they can able to
understand
but in detail level they cannot they
could not able to understand
again we can create the index constraint
triggers all the db objects with this
physical data model so the create table
structure has been created automatically
for this period
physical data model okay by using any
other tool and this is what
this is what the tool will create the
automated automatically logical data
model to physical data model of the
different versions different database
servers
okay so this is the the concept of
three different
data model
so let us get to the interview questions
first question that we have is compare a
database with a data warehouse now there
are obvious differences between the two
for instance the type of data which is
used in databases include relational or
object oriented data while in data
warehouse since we are getting data from
multiple sources we have a large volume
of data as well as multiple data types
for the data operations aspect databases
deal with transactional processing while
data warehouses deal with data modeling
and analysis the dimension of data and
databases is two dimensional because we
are dealing with tables which are
essentially 2d arrays while in data
warehouses we can have multi-dimensional
data they could be 3d 4d data design
databases have er based and application
oriented database design while data
warehouses have star snowflake schema
and subject oriented database design the
size of data and databases traditional
databases not big data databases is
small usually
in the maximum they are in the gigabytes
while in data warehouses they are in the
terabytes functionality for databases
it's high availability and performance
with data warehouse it's high
flexibility and user autonomy because we
are going to be performing a lot of
analysis with the data warehouse moving
on to question number two what is the
purpose of cluster analysis and data
warehousing now one of the purposes of
cluster analysis is to achieve
scalability so regardless of the
quantity of data we are able to analyze
it ability to deal with different kinds
of attributes so no matter the data type
of the attributes present in the data
set we are able to deal with it
discovery of clusters with attribute
shape high dimensionality which means we
can have multiple dimensions
more than 2d to be precise ability to
deal with noise so any
inconsistencies in the data we are able
to deal with that and interpretability
moving on to question number three what
is the difference between agglomerative
and divisive hierarchical clustering so
agglomerative hierarchical constraining
method allows clusters to be read from
bottom to top so that the program always
reads from the sub component first and
then moves to the parent in an upward
direction whereas divisive hierarchical
clustering uses top to bottom approach
in which the parent is visited first and
then the child and
agglomerative hierarchical method it
consists of objects in which each object
creates its own clusters and these
clusters are grouped together to form a
larger cluster it is also the process of
continuous merging until all the single
clusters are merged together into a
complete big cluster that will consist
of the objects of the chart clusters
however in divisive clustering the
parent cluster is divided into smaller
clusters and it keeps on dividing until
each cluster has a singular object to
represent moving on to question number
four explain the chameleon method used
in data warehousing so chameleon is a
methodology which is a hierarchical
clustering algorithm that overcomes the
limitations of the existing models and
methods in data warehousing this method
operates on the sparse graph having
nodes that represent data items and
edges which represent the weights of the
data items this representation allows
large data sets to be created and
operated successfully the method finds
the clusters that are used in the data
set using the two-phase algorithm the
first phase consists of the graph
partitioning that allows the clustering
of the data items into larger number of
subclusters the second phase on the
other hand uses an agglomerative
hierarchical clustering algorithm to
search for the clusters that are genuine
and can be combined together with the
sub clusters that are produced moving on
to question number five what is virtual
data warehousing now a virtual data
warehouse provides a collective view of
the completed data in that warehouse a
virtual data warehouse has no historic
data it can be considered as a logical
data model of the given metadata the
virtual data warehousing is the de facto
information system strategy for
supporting analytical decision making it
is one of the best ways for translating
raw data and presenting it in the form
that can be used by decision makers it
provides a semantic map which allows the
end user as well
for viewing as the data is virtualized
moving on to question number six now we
need to understand what active data
warehousing is an active data warehouse
represents a single state of a business
active data warehousing considers the
analytical perspectives of customers and
suppliers just a quick info guys
intellipaat provides business
intelligence masters program in
partnership with microsoft
the course link of which is given in the
description below
now let's continue with the session it
helps deliver the updated data through
reports
now this is the most common form of data
warehousing which is used for businesses
large businesses and specific which deal
in the industry of e-commerce or
commerce
a form of repository of captured
transactional data is known as the
active data warehousing using this
concept trends and patterns are found to
be used for future decision making so
based on the analytical results that you
get from the data warehouse you can
perform further business decisions
active data warehouse as a feature which
can integrate the changes of data while
scheduled cycles refresh enterprises
utilize an active data warehouse and
drawing the company's image in a very
statistical manner so everything
essentially you get a combination of all
the data that is present in various data
sources you combine it all together and
then you perform some analytics on it to
get insights for further business
decisions
what is a snapshot with reference to
data warehouse now snapshots are pretty
common in software especially in
databases so essentially it is what the
name suggests snapshot refers to the
complete visualization of data at the
time of extraction it occupies less
space and can be used to backup and
restore data quickly so you essentially
snapshot a data warehouse when you want
to create a backup of it so using the
data warehouse catalog you're creating a
report and the report will be generated
as shown as soon as you disconnect from
the data warehouse
number eight what is xmla
xmla is xml for analysis which can be
used and considered as a standard for
accessing data in the olap method data
mining or data sources on the internet
it is the simple object access protocol
xmla uses discover and execute methods
discover fetch's information from the
internet while the execute allows the
application to execute against the data
sources that are present xmla is an
industry standard for accessing data in
analytical systems such as olap it is
based on xml soap and http xmla
specifies mdxml as a query language in
xmla 1.1 version the only construct is
the md xml in an mdx statement enclosed
in the tag moving on to question number
nine what is ods now ods is the database
which is designed to integrate data from
multiple sources for additional
operations of the data the full form of
ods is operational data source so unlike
the master data source that you have the
data is not sent back to the operational
systems it may be
passed for further operations and to the
data warehouse for reporting in ods data
can be scrubbed resolved for redundancy
and checked for compliance with the
corresponding business rules so whatever
data you have in order to filter it out
basically to see if there is some data
redundancy in the data it is checked and
it also sees whether the data is
compliant with the organization's
business rules this data can be used for
integrating disparate data from multiple
sources so that business operations
analysis and reporting can be carried
out this is the place where most of the
data used in the current operation is
housed before it's transferred to the
data warehouse for longer term and for
storage and archiving and ods is
designed for relatively simple queries
on small amounts of data such as finding
the status of a customer order so you
have a data warehouse which is present
to you say for instance for a particular
customer ticket you need to find
information with regards to a customer
then you can use the ods rather than the
complex queries on large amounts of data
typical of the data warehouse so you're
not going to be running it on the
entirety of the data which will take a
lot of time to find a single customer's
details you would rather use ods for
that matter and ods is similar to the
short-term memory where it only stores
very recent information on the contrary
the data warehouse is more like a
long-term memory storing relatively
permanent information because you're
creating a data warehouse for a
permanent basis what is the level of
granularity of a fact table a fact table
is usually designed at a low level of
granularity this means that we need to
find the lowest of information that can
be stored in a fact table for example
employee performance is a very high
level of granularity while employee
performance daily and employee
performance weekly can be considered as
low levels of granularity because they
are much more frequently recorded data
the granularity is the lowest level of
information stored in the fact table the
depth of the data level is known as
granularity in date dimension the level
could be year month quarter period week
and the day of granularity so the day
being the lowest level the year being
the highest level the process consists
of the following two steps determining
the dimensions that are to be included
and determining the location to find the
hierarchy of each dimension of that
information the above factors of
determination will be resent as per the
requirements moving on to question
number 11 what is the difference between
view and materialized view because the
name suggests that now view simply means
the raid table data representation that
is provided with a view to access the
data from its table that does not occupy
space and changes get affected in the
corresponding tables while in
materialized view pre-calculated data
persists it has physical data space
occupation in the memory and changes
will not get affected in the
corresponding tables
what is junk dimension in scenarios
where data may not be appropriate to
store in the schema the data or
attributes can be stored in a junk
dimension the nature of the junk in this
particular dimension is usually boolean
or flag values a single dimension is
formed by lumping a small number of
dimensions this is called a junk
dimension adjunct dimension has
unrelated attributes the process of
grouping these random flags and text
attributes in a dimension by
transmitting them to a distinguished sub
dimension is related to junk dimension
so essentially any data that need not be
stored in the data warehouse because it
is unnecessary is stored in the junk
dimension moving on to question number
13 what are the different types of scds
used in data warehousing scds are also
short for slowly changing dimensions
they are the dimensions in which the
data changes slowly rather than changing
regularly on a timely basis there are
three types of stds the first is scd1 it
is a record that is used to replace the
original record even when there is only
one record existing in the database the
current data will be replaced and the
new data will take its place scd2 it is
the new record file that is added to the
dimension table this record exists in
the database with the current data and
the previous data that is stored in the
history scd3 this uses the original data
that is modified to the new data this
consists of two records one record that
exists in the database and the another
record which will replace the old
database record with this new
information moving on to question number
14 which one is faster multi-dimensional
olap or relational olap now
multi-dimensional olap also known as
molap is faster than relational olap
because of the following reasons in
molap the data is stored in a
multi-dimensional queue the storage is
not in the relational database but in
proprietary formats one for example is
power olap which is the of the
extension.olp molab products are
compatible with excel which make data
interactions easy to use even for
non-technical members of a team enroll
our products access a relational
database by using sql which is the
standard language that is used to define
and manipulate data in an rdbms so since
rollapp is not accessible by simple
programs like excel you need to know sql
in order to be able to query that data
that is why it's not as compatible with
people who don't have that much
technical knowledge subsequent
processing may occur in the rdbms or
within a mid-tier server which accepts
requests from clients translates them
into sql statements and passes them to
the rdbms so due to multi-dimensional
olap using a proprietary file format it
is faster than relational olap moving on
to question number 15 what is hybrid scd
now we discussed uh std one two and
three earlier on hybrid stds are
combinations of both scd1 and scd2 uh it
may happen that in a table some columns
are important and we need to track
changes for them that is capture the
historical data for them whereas in some
columns even if the data changes we do
not need to bother for such tables we
implement hybrid scds wherein some
columns are of type 1 and some are of
type 2. so basically we are not applying
a blanket rule on the entire table we
can customize on which particular
columns we want to apply what rule
moving on to question number 16 why do
we overwrite the execute method in
struts so as part of the start framework
we can develop the action servlets and
the action form servlets and other
servlet classes in the action form class
we can develop validate method this
method can return action errors object
and in this method we can write the
validation code as well if this method
returns null or action errors with the
size of 0 the web container will call
execute as part of the action class if
it returns size greater than 0 it will
call the execute method it will rather
execute the jsp servlet or the html file
as the value for the input attribute is
part of the attribute in the
strutsconfig.xml file question number 17
what is vldb so vldb stands for very
large database and it is a database that
contains an extremely large number of
tuples or rows or occupies an extremely
large physical file system storage one
terabyte database would normally be
considered as vldb question number 18
how do you load the time dimension so
time dimensions are usually loaded by a
program that loops through all possible
dates appearing in the data
it is not unusual for 100 years to be
represented in a time dimension with one
row per day
question number 19 what are conformed
dimensions so conform dimensions are the
dimensions which can be used across
multiple data marts in combination with
multiple fact tables a conform dimension
is a dimension that has exactly the same
meaning and contents when being referred
from different fact tables it can refer
to multiple tables in multiple data
marts within the same organization
itself
question number 20. what is the main
difference between nmon and kimbel
philosophies of data warehousing so
these are two philosophies that we have
in data warehousing so in the gimbal
philosophy data warehousing is viewed as
a constituency of data mods so data mods
are focused on delivering business
objectives for departments in an
organization and the data warehouse is a
confirmed dimension of the data marts
hence a unified view of the enterprise
can be obtained from the dimension
modeling on a local departmental level
and in the inman philosophy
we can create a data warehouse on a
subject by subject area basis hence the
development of the data warehouse can
start with the data from the online
store other subject areas can be added
to the data warehouse as their need
arises point of sale or pos data can be
added later if management decides that
it is required and
if we can look at it on a sort of
algorithmic basis in the gimbal
philosophy we first go with data marts
and then we combine it and we get our
data warehouse while with inman
philosophy we first create our data
warehouse and then we create our data
marts question number 21 what is the
difference between a data warehouse and
a data mart so we understood both of the
philosophies in the previous slide let
us now understand the actual difference
between a data warehouse and a data mart
a data warehouse is a set of data
isolated from operational systems so it
is basically away from the database
itself it is a view of the database this
helps an organization deal with its
decision making process a data mart is a
subset of a data warehouse that is
geared to a particular business line
data mods provide the stock of condensed
data collected in the organization for
research on a particular field or entity
so this is basically stating that the
data warehouse contains a whole variety
of information while a data mart is just
a subset of that information based on a
particular business line or model so
basically say you have a data warehouse
that is based on customer and sales data
you can create a data mart that is
purely based on transactional data with
the combination of those two data lines
instead of getting the entirety of the
customer and sales data you're just
getting the data that is relevant to
your transactional data set the second
difference is a data warehouse typically
has a size greater than 100 gigabytes
while the size of a data want is less
than 100 gigabytes due to the disparity
in scope the design and utility of data
marts are comparatively simpler so
naturally since data marts are subsets
of data warehouses their size is less as
well
question number 22 explain the etl
cycles three layer architecture now as
we all know etl stands for extraction
transformation and loading so there are
three phases involved in it so the first
is the staging layer and then we have
the data integration layer and then we
have the access layer so these are the
three layers that are involved for the
three specific phases in the atl cycle
so in the staging layer it is used for
the data extraction from various data
structures of the source in the data
integration layer data from the staging
layer is transformed and transferred to
the database using the integration layer
the data is arranged in hierarchical
groups often referred to as dimensions
facts or aggregates in a data
warehousing system the combination of
facts and dimension tables is called a
schema so basically once you have the
data loaded once you have the data
extracted in the staging layer you're
basically transforming it in the data
integration layer and finally you have
the access layer where the data is
accessed and can be loaded for further
analytics question number 23 what does
data purging mean so data purging the
name is quite straightforward it is the
process involving methods that can erase
data permanently from the storage
several techniques and strategies can be
used for data purging the process of
data forging often contrasts with data
deletion so they are not the same the
leading data is more on a temporary
basis while data purging permanently
removes the data this in turn frees up
more storage and memory space which can
be utilized for other purposes the
purging process allows us to archive
data even if it is permanently removed
from the main source giving us an option
to recover that data in case we purge it
the deleting process also permanently
removes the data but does not
necessarily involve keeping a backup it
generally involves
insignificant amounts of data question
number 24 what are the five main testing
phases of a project so the etl test is
performed in five stages which are the
following the identification of data
sources and requirements first you will
identify which data sources you want for
your data warehouse and what is the
requirement of the data warehouse and
the analytical requirements that your
organization needs the acquisition of
data naturally after identifying the
data source you will acquire that data
implementing business logic and
dimensional modeling on that data
building and publishing that data and
the reports that you'll create out of
the analytics that you perform
and finally question number 25 what do
you mean by the slice action how many
slice operated dimensions are used so a
slice operation is the filtration
process in a data warehouse it selects a
specific dimension from a given cube and
provides a new sub cube in the slice
operation only a single dimension is
used so basically out of a
multi-dimensional data warehouse if you
need a very specific dimension that you
need for further analytics or processing
then you will use the slice operation in
that data warehouse just a quick info
guys
intellipaat provides business
intelligence masters program in
partnership with microsoft
the course link of which is given in the
description below
you
