in this lesson you will be learning the
concept of ensemble learning
by the end of this lesson you will be
able to
explain ensemble learning evaluate the
performance of boosting models
overview ensemble technique combines
individual models together to improve
the stability and predictive power of
the model
this approach permits higher predictive
performance compared to a single model
ensemble finds ways to combine multiple
machine learning models into one
predictive model in order to decrease
variance using bagging decrease bias
using boosting or improve predictions
using stacking let's understand the
ideology behind ensembl learning as you
have seen before certain models do well
in modeling one aspect of the data while
others do well in modeling another
instead of learning a single complex
model learn several simple models and
combine their output to produce the
final decision
in ensemble learning the combined
strength of the models offsets
individual model variances and biases
ensemble learning will provide a
composite prediction where the final
accuracy is better than the accuracy of
individual models
ensemble methods can be divided into two
groups sequential ensemble methods and
parallel ensemble methods in sequential
ensemble methods base learners are
generated consecutively that is add a
boost
the basic motivation of sequential
methods is to use the dependence between
the base learners
by weighing previously mislabeled
examples with higher weight
the overall performance of a model can
be boosted
parallel ensemble methods are applied
wherever the base learners are generated
in parallel for example random forest
since the errors are often reduced
dramatically by averaging the basic
motivation of parallel methods is to use
independence between the base learners
here in the given diagram we are using
parallel ensemble method where the
different models work best in one or
other aspect of data
they are combined in the ensemble model
where the best aspect of each of the
models is considered upon the input data
and then ensemble learning model comes
up with the predictions
ensemble model is the application of
multiple models in order to obtain
better performances from a single model
performance depends on two major factors
robustness and accuracy ensemble models
incorporate the predictions from all the
base learners this provides robustness
ensemble models deliver accurate
predictions and have improved
performances
ensemble learning methods part a
you can create an ensemble by combining
all weak learners once weak models are
properly combined we can obtain more
accurate and or robust models
you can also create an ensemble of
well-chosen strong and diverse models
by combining data from numerous modeling
approaches ensemble models gain more
accuracy and robustness than a
fine-tuned single model can gain
model averaging is an approach to
ensemble learning where each ensemble
member contributes an equal amount to
the final prediction
in the case of regression the ensemble
prediction is calculated as the average
of the member predictions
in the case of predicting a class label
the prediction is calculated as the mode
of the member predictions
in the case of predicting class
probability the prediction can be
calculated as the arg max of the summed
probabilities for each class label
a limitation of this approach is that
each model has an equal contribution to
the final prediction made by the
ensemble
there is a requirement that all ensemble
members have skills compared to random
chance although some models are known to
perform much better or much worse than
other models
a weighted average ensemble is an
extension of a model averaging ensemble
where the contribution of each member to
the final prediction
is weighted by the performance of the
model
the model weights are small positive
values and the sum of all weights equals
one allowing the weights to indicate the
percentage of trust or expected
performance from each model
now let's take a look at what bagging is
the idea behind bagging is to combine
the results of multiple models to get a
generalized result from a single model
bagging or bootstrap aggregation reduces
variance of an estimate by taking the
mean of multiple estimates
there are three steps to perform bagging
step one is to create randomly sampled
data sets of the original training data
bootstrapping
step two is to build and fit several
classifiers to each of these diverse
copies
step three is to take the average of the
predictions to make a final overall
prediction
ensemble learning methods part b
let's understand how random forest is an
ensemble learning model
random forest is a good example of
ensemble machine learning method random
forests as one can guess combine various
decision trees to produce a more
generalized model by reducing the
notorious overfitting tendency of
decision trees
random forests are utilized to produce
decorated decision trees
it creates random subsets of the
features
smaller trees are built using these
subsets creating tree diversity
to overcome overfitting diverse sets of
decision trees are required
boosting is a sequential process where
the errors of the previous model is
corrected by each subsequent model
let's assume an example of different
types of body pain and their diagnosis
headache stomach pain and leg pain will
have a different reason
let's assume that x causes headache then
x will not cause leg pain because why
causes leg pain
these two variables are not the reason
for stomach pain stomach pain will be
caused by variable z depending on the
results of weighted opinions the doctor
will suggest the diagnosis sequentially
let's understand the way boosting works
in the steps flow step one is to train a
classifier h1 that best classifies the
data with respect to accuracy step two
is to identify regions where h1 produces
errors and weighs to them and produces a
h2 classifier step three is to aggregate
it those samples for which h1 gives a
different result from h2 and produces h3
classifier
repeat step 2 for a new classifier
let's understand what adaboost is and
how it works let's first understand
boosting boosting is a technique of
changing weak learners into strong
learners in boosting each new tree as a
fit on a modified version of the
original data set
adaboost is one of the first boosting
algorithms to be adapted in solving
practices adaboost helps you mix
multiple weak classifiers into one
strong classifier
let's understand this through a scenario
consider a scenario where there are plus
and minus symbols indicating the data
points we will try to understand
adaboost workflow
working of add-a-boost
so the first step is to assign equal
weights to each data point and apply a
decision stump to classify them as a
plus or minus
the most suited and most common
algorithm used with adaboost is a
decision tree with one level these trees
are short and solely contain one
decision for classification they are
often called decision stumps
as you can see in the given graph
decision stump d1 has generated a
vertical plane at the left side to
classify the plus
a decision stump is a decision tree it
uses only a single attribute for
splitting for distinct attributes this
typically means that the tree consists
only of a single interior node
now apply higher weights to the
incorrectly predicted three plus as
shown in the given graph and add another
decision stump
you can see that the size of three
incorrectly predicted plus is bigger
than the rest of the data points
in the second step the second decision
stump d2 will try to predict them
correctly so as you can see d2 has
classified three misclassified plus
correctly
d2 has also caused misclassification
error to three minus in the next step d3
adds higher weights to 3 minus a
horizontal line is generated to classify
plus and minus based on higher weight of
the misclassified observation
in the final step d1 d2 and d3 are
combined to form a strong prediction
that has complex rules as compared to
the individual weak learners
at a boost algorithm and flowchart
now since you know the workflow of
adaboost classifier let's understand the
attaboost algorithm a weak classifier is
prepared on the training data using the
weighted samples
only binary classification problems are
supported
so every decision stump makes one
decision on one input variable and
outputs a plus
1.0 or minus 1.0 value for the first or
second class value
the misclassification rate is calculated
as error equals
correct minus n
divided by n
in the given equation error is the
misclassification rate correct is the
number of training instances predicted
by the model and n is the total number
of training instances so let's
understand the steps involved in this
algorithm
step one initially data points is
weighted equally with weight w i equals
1 over n where n is the number of
samples step 2 a classifier h1 picked up
that classifies the data with minimal
error rate
step 3 the weighted factor alpha is
dependent on errors e t
step four weight after time t is given
as this formula
where z is the normalizing factor
h1x
yx is the sign of the current output
let's understand the flow of adaboost
algorithm it follows the following steps
one initially adaboost selects a
training subset randomly
two it iteratively trains the adaboost
machine learning model by selecting the
training set based on the accurate
prediction of the last training
three it assigns the higher weight to
wrongly classified observations so that
in the next iteration these observations
will get a high probability for
classification
four
also it assigns weight to the trained
classifier in each iteration according
to the accuracy of the classifier
the more accurate classifier will get
higher weight
five this process iterates until the
complete training data fits without any
error or until it reaches the specified
maximum number of estimators
gradient boosting
now let's understand gradient boosting
gradient boosting trains several models
in a very gradual additive and
sequential manner the major difference
between adaboost and a gradient boosting
algorithm is how the two algorithms
identify the shortcomings of weak
learners that is decision trees gbm
minimizes the loss function mse of a
model by adding weak learners using a
gradient descent procedure
gradient boosting involves three
elements
one a loss function to be optimized two
a weak learner to make predictions
three an additive model to add weak
learners to minimize the loss function
let's understand the gbm mechanism
gradient boosting trains many models
sequentially each new model gradually
minimizes the loss function of the whole
system using the gradient descent method
so in y equals ax plus b plus e e needs
special attention because it is an error
term
the learning procedure consecutively
fits new models to produce an additional
accurate estimate of the response
variable the principal idea behind this
algorithm is to construct new base
learners that might be maximally
correlated with negative gradient of the
loss function associated with the whole
ensemble
so let's understand the working gradient
boosting
gbm predicts the residuals or errors of
the prior models and then sums them to
make the final prediction
step 2 one week learner is added at a
time and existing weak learners in the
model are left unchanged step 3 gbm
repetitively leverages the patterns in
residuals and strengthens a model with
weak predictions
step four modeling is stopped when they
do not have any pattern that can be
modeled
let's look at the steps involved in gbm
algorithm
step one fit a simple regression or
classification model
step 2 calculate error residuals that is
actual value minus predicted value
step 3 fit a new model on error
residuals as target variable with the
same input variables step 4 add the
predicted residuals to the previous
predictions
step 5 fit another model on residuals
and repeat steps 2 and 5 until the model
is over fit or the sum of the residuals
become constant
xgboost
let's understand extreme gradient
boosting it is a library for developing
quick and superior gradient boosting
tree models xg boost is one of the
implementations of the gradient boosting
concept but what makes it unique is that
it uses a more regularized model
formulation to control overfitting which
gives it better performance thus it
helps to reduce overfitting
extreme gradient boosting is a custom
tree based algorithm used for
classification regression and ranking
with custom loss function
it has interfaces for python and r that
can be executed on yarn xg boost is
extensively used in ml competitions as
it is almost 10 times faster than other
gradient boosting techniques
let's understand the xgboost library
features xgboost library tools are built
for the sole purpose of model
performance and computational speed
let's look at system features followed
by algorithm features we will finally
take a look at the model features of
xgboost
system features are
parallelization
parallelization it produces tree
construction using all cpu cores while
training distributed computing it uses a
cluster of machines in order to train
very large models cache optimization
data structure involved in xg boost
makes the best use of hardware
algorithm features are
sparse aware
algorithm behind xg boost provides
automatic handling of missing data
values
block structure it supports the
parallelization of tree construction
continued training
its algorithm provides continued
training in order to boost an already
fitted model on new data
model features are gradient boosting it
uses gradient boosting machine algorithm
including learning rate
stochastic grading boosting
it does sub sampling at the row column
and column per split levels
regularized gradient boosting it does
regularization using both l1 and l2
regularization
xg boost parameters part a
let's understand the xg boost parameters
general parameters consist of a number
of threats that help to prepare data
task parameters correspond to the
objective that helps to build a model
evaluation metrics help to predict and
visualize the predicted values booster
parameters consist of a step size that
evaluates the prediction performance
metrics and roc curve and regularization
that compares the prediction performance
against the logistic regression model
let's understand the general parameters
of xgboost general parameters guide the
overall functioning of xg boost so let's
understand what these general parameters
are
n thread is used to define the number of
parallel threads we are going to use if
no value is entered the algorithm
automatically detects the number of
cores and runs a thread on all the cores
booster parameters guide the individual
tree or regression at each step there
are two types of boosters which booster
can you use gb tree uses tree based
model
gp linear uses linear function
usually a tree booster outperforms the
linear booster
silent default equals zero is used to
print the running messages if the silent
parameter is set to one no running
messages will be printed hence keep it
zero as the messages might help in
understanding the model
let's look at the booster parameters
booster parameters give the individual
booster tree regression at each step
let's look at the parameters for tree
booster first eta defines the step size
shrinkage used in update to prevent
overfitting
after each boosting step you can
directly get the weights of new features
and eta shrinks the feature weights to
make the boosting process more
conservative
eta range is between 0 and 1 and the
default value is
0.3 gamma defines the minimal loss
reduction required to make a further
partition on a leaf node of the tree
the algorithm will be more conservative
when the gamma is larger
its value ranges from zero to infinity
and the default value is zero
max depth is used to define the maximum
depth of a tree
its value ranges from one to infinity
and the default value is six
min child weight is used to define the
minimum sum of instance weight needed in
a child if the tree partition step
results in a leaf node with the sum of
instance weight less than min child
weight then the building process will
give up further partitioning
in linear regression tasks this simply
corresponds to the minimum number of
instances needed in each node
the algorithm will be more conservative
if the min child weight is larger it
ranges from zero to infinity and the
default value is one
xg boost parameters part b
max delta step is used to set the
maximum delta step allowed in each
tree's weight estimation if the value is
set to zero it means there is no
constraint
if it is set to a positive value it can
help make the update step more
conservative
usually this parameter is not needed but
it might help in logistic regression
when the class is extremely imbalanced
its value ranges from zero to infinity
and the default value is zero
subsample is used to calculate the
subsample ratio of the training instance
its value ranges between zero and one
and the default value is one call sample
by tree is the subsample of ratio of
columns when constructing each tree
sub sampling occurs once for every tree
constructed its value ranges between
zero and one and the default value is
one
now let's understand the parameters for
linear booster
lambda defines the ridge regression
regularization term on weights
increasing this value will make the
model more conservative the default
value is zero
alpha defines the lasso regression
regularization term on weights
increasing this value will make the
model more conservative
the default value is zero
lambda bias defines the ridge regression
regularization term on bias
increasing this value will make the
model more conservative the default
value is zero
now let's look at the task parameters
task parameters guide the optimization
objective to be calculated at each step
the objective options are binary
logistic defines logistic regression for
binary classification whose output is
probability not class
multi-soft max defines the multi-class
classification using softmax objective
evaluation metrics for validation data
a default metric will be assigned
according to the objective rmse for
regression error for classification and
mean average precision for ranking the
evaluation metric options are rmse
log loss error auc
mirror
mlo gloss
demo pima indians diabetes
problem statement the objective of the
data set is to diagnostically predict
whether or not a patient has diabetes
based on certain diagnostic measurements
including in the data set
all patients are females of at least 21
years of age and of pima indian heritage
the data set consists of several medical
predictor variables and one target
variable outcome
predictor variables include the number
of pregnancies the patients have had
their bmi
insulin level age and so on
let us import the required libraries
including pandas and model selection
from sklearn also we will import
adaboost classifier from
sklearn.ensemble to further optimize our
model
import pandas import sklearn import
model underscore selection
from
sklearn.ensemble import add a boost
classifier
now load the data set with a pandas data
frame
extract the values from the columns in
the form of an array
set the random seed value as seven and
number of trees as thirty
we will build classifiers using adaboost
and xg boost let's create the adaboost
model using
scikit learn
attaboost uses decision tree classifier
as the default classifier
pass the model within the cross
validation score function to evaluate
the results using the cross validation
technique
construct the model now by splitting the
train test indices into 10 consecutive
folds
again evaluate the model such that each
fold gets used once as a validation
while the remaining nine folds form the
training set
print the results
adaboost gives an accuracy of 76
similarly we apply the xgboost algorithm
import the respective modules namely svm
and xgb classifier from sklearn import
svm
from xgboost import xgb classifier
initialize the xgb classifier under the
name clf
set the seed value a 7 and number of
trees is 30.
construct the xgb classifier using the
k-folds technique
such that number of folds equals 10.
evaluate the model using the cross
validation score similar to the way we
did for add a boost classifier
again the accuracy is around 77 percent
we have shown only two classifiers you
can use different classifiers and
compare the results
also check results after applying
ten-fold cross-validation
great we have seen how to apply boosting
algorithms to classification let's
quickly recap the steps we covered one
build models using adaboost and xg boost
to compare the accuracy of classifiers
model selection
when do you need to evaluate the model
performance
models can be evaluated based on
measures of performance model evaluation
aims to estimate the generalization
accuracy of a model on future data that
is unseen out of sample data
so model evaluation techniques are used
to evaluate the model's performance when
there is a chance of evaluating new data
that is unseen or out of sample
let's now look at how to access model
performance train test split or cross
validation split are the techniques used
to evaluate the model performance train
test split divides the training data set
into two sets and uses the first
training set to train the model and
tests the model using the second set
cross validation split splits the data
set in two sets namely train and test
and then calculates the accuracy of each
set and finds the average of the results
let's look at how train test split works
so we have a given data sets that we
need to split into training data and
testing data
split the data randomly using negative
and positive symbol
now when you have training data randomly
selected from the given data set the
remaining data falls under the test data
move this data into the hypothesis space
make sure that your test set meets the
two subsequent conditions given
the test set must be large enough to
yield statistically meaningful results
the test set must have different
characteristics than the training set
now verify the result by matching the
train data with the test data
now test the training data individually
with respect to the test data present in
the hypothesis space
wherever the data that has the same
symbol matches it provides accurate
result prediction given here is 9 13
which shows the matching prediction
common splitting strategies
so let's understand what percentage of
data should be used for training and for
testing so we can use either of the two
common splitting strategies to split the
data
k-fold cross-validation or leave one out
let's first understand how k-fold
cross-validation works the procedure
contains a single parameter referred to
as k and points to the number of groups
that a given data sample is to be split
into as such the procedure is usually
referred to as k-fold cross validation
when a particular value for k is chosen
it's going to be employed in place of k
within the relevancy of the model
so how does the leave one out technique
work
this approach leaves one data point out
of the training data that is if there
are n data points in the original sample
then n1 samples are used to train the
model and one point is used as the
validation set
all the combinations are created in this
way the original sample can be separated
and the error is averaged for all trials
in order to provide overall
effectiveness
now let's understand the difference
between train test split and cross
validation advantages of cross
validation one it is a better estimator
of out of sample accuracy two it
provides a more efficient use of data
since each data is used for training and
testing
advantages of test train split
one runs k times faster than k fold
two simpler than k fold so it would be
easier to analyze the testing errors
demo cross validation
problem statement a few learners have
implemented random forest classifier on
the iris data but better accuracy can be
achieved by using the cross validation
sampling technique
in this analysis we will generate the
random forest using the cross validation
splitting technique
let us import the inbuilt iris data set
from the
sklearn.datasets library
from
sklearn.dataset import load underscore
iris
load the dataset with a data frame
extract the input data
similarly extract the target values
k-fold cross-validation
use the k-fold cross-validation
technique to split the data set into k
consecutive folds without shuffling by
default here we will split the data set
into five consecutive groups from
sklearn.model underscore selection
import k-fold
using a for loop we will now iterate the
individual train test sets within the
input data
let's now split the data set into 10
consecutive folds
using a for loop we will now iterate the
individual train test sets within the
output data
let's initialize the random forest
classifier from
sklearn.ensemble import random forest
classifier
import the cross validation score
library from sklearn and get the model
evaluated for accuracy by using the
cross validation score parameter
from sklearn.model underscore selection
import cross underscore val underscore
score
the accuracy of the classifier is ninety
five
great we have seen how to apply cross
validation to split the data set let's
quickly recap the steps we covered one
shuffle and split the data set randomly
into k groups
two summarize the skill of the model
using model evaluation scores
this brings us to the end of ensemble
learning you are now able to explain
ensemble learning evaluate the
performance of boosting models
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here
