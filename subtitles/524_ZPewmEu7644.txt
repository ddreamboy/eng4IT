hi this is Jeff Dean welcome to
applications of deep neural networks of
Washington University in this video
we're going to look at how we can use
ganz to generate additional training
data for the latest on my a I course and
projects click subscribe in the bell
next to it to be notified of every new
video Dan's have a wide array of uses
beyond just the face generation that you
often see them use for they can
definitely generate other types of
images but they can also work on tabular
data and really any sort of data where
you are attempting to have a neural
network that is generating data that
should be real or should or could be
classified as fake the key element to
having something as again is having that
discriminator that tells the difference
in the generator that actually generates
the data another area that we are seeing
ganz use for a great deal is in the area
of semi supervised training so let's
first talk about what semi-supervised
training actually is and see how again
can be used to implement this first
let's talk about supervised training and
unsupervised training which you've
probably seen in previous machine
learning literature but just in case you
haven't supervised training is what
we've been doing up to this point I
would say probably the vast majority of
this class is in the area of supervised
learning this is where you have multiple
axes in the case of tabular data or
grids and other things in the case of
image data but you have some sort of
input coming in which is the X and you
know what the correct Y's are you are
going to train the model to produce
these Y's when you have these X's
because later on you're going to have
X's coming in where you don't know what
the Y is and that's where you want the
neural network or other model to be able
to give you some estimate as far as what
the Y value is going to actually be
unsupervised training is where we have
the X's it could look just like this it
would work with image data tabular or
really just about anything
but there is no y we're letting the
neural network or whatever
model it is and you know typically by
the way use neural networks for
unsupervised training this is usually
the area of things like k-means
clustering and other things
your classic unsupervised training is
just going to take the inputs and
cluster them in such a way so that
similar ones are together these could be
similar images these could be similar
inputs in tabular data a variety of
things semi supervised training it's
actually much closer to supervised
training I would say than unsupervised
and this is where gams really shine and
semi-supervised training you have X's
just like you have in these others but
you don't have a label or a Y for every
single one of them you might have a
small number of them by oh no means have
the complete data set label
traditionally what would be done is
these values that were not labeled would
be left out because they there was no
way to feed them into traditional
supervised learning or you would train
it on the ones that you did have Y's for
with classic back propagation or however
you were training that particular model
then you would create predictions Y
predictions for all the missing values
and then retrain the whole thing on the
predictive values with the others in
practice I never had a great deal of
success with that technique but there is
some theoretical basis for it with semi
supervised training and Ganz will see
that there's a way that we are able to
actually make use of these now
semi-supervised training this does make
sense from a biological standpoint if
you think about a child who is seeing
all sorts of vehicles as they go about
their daily lives with their parents or
whoever they're with and they're seeing
all these vehicles as they pass on the
street and they're not labeled nobody is
telling them even though that's a
vehicle seeing just a barrage of images
as they as they grow up they learn edges
they learn other sorts of things they
learn how to classify if something is on
top of something else just by observing
there's no particular labels then
eventually somebody says hey that's a
that's a bus that's that's a train
that's a bicycle using that that small
handful of labels that they're given
when somebody actually tells them what
they're looking at or they verify it
independently
that is semi-supervised training because
it is building on those years and years
of having unlabeled data that they they
didn't know what they were looking at
but they knew they were looking at
something and it just it gives them
additional reference that's exactly the
same thing with supervised training
these values even though we don't have
wise
they're still valuable for the neural
network to be learning structure in this
data as it is learning to predict the
ones that we do actually in fact have
the wise heart so let's look at the
structure for this this is the structure
of a normal image generating gang
baseline so to speak where they research
started we saw this before but just to
quickly review we have actual images
they go into a discriminator and we have
the generated images that the generator
so the cyan pieces those are the two
neural networks random seed values are
causing that generator to generate
images the discriminator is learning to
better and better discriminate between
actual and generated the generator is
learning to create better and better
images that fool the discriminate now
once this is all done you keep the
generator because it generates images
for you and you likely throw away the
discriminator it was just there for the
generator to practice against will see
that this flips for semi-supervised
learning and semi-supervised learning we
care about the discriminator and not so
much the generator we typically throw
the generator away this is how you would
train a semi supervised classification
neural network it's very very similar to
the diagram that we just looked at in
this case we're looking at how we would
train it on tabular data it's a medical
record the discriminator would learn to
tell the difference between a fake
medical record or whatever the generator
is generating this parts all the same as
the previous one as is as is this part
the difference is we're training it now
to tell not just the difference between
fake and real these are the real
and this is this is fake we're teaching
it to learn classes so there's four
different classes of say medical record
that we're looking at maybe four
different health levels we're teaching
it as a classification neural network to
classify between five things the four
classes that were actually interested in
and is it a fake once we're done
training the whole thing we now have
this discriminator that can tell the
difference between fake and what the
what the classes are we also have the
generator that is able to generate these
fake medical records but we can then
throw away the generator and we'll use
the discriminator really truly as our
actual neural network now for the
medical records where we don't have the
Y so we're missing this we still feed
those in it's just now we're evaluating
it not based on if it classified it
correctly but just if it knew the
difference between fake and real the
street house view data set is a image
data set that is often used to
demonstrate semi-supervised game
learning and I have a link to a curious
example external to this class that
demonstrates this if you're interested
in this sort of technology but what this
does is you have data on these addresses
from images that were taken on the sides
of buildings and not all of those are
labeled or you simulate them not all
being labeled and you see that the Gann
is capable of learning to classify these
10-digit types even though it doesn't
have labels on each of those now if you
want to do the same thing for regression
it becomes very similar you have two
outputs so you have a multi output
neural network one is the actual
regression value that you're trying to
train it on and the other is the
probability that it's a fake record
being generated now these I'm doing
tabular as just the example again these
could be medical records and perhaps the
regression output would be a health
level or maybe a guess at how old the
patient is or some other value perhaps
if they have a current disease or not a
prediction so it's it's doing the same
two things when you feed an medical
records where we don't know the Y output
then we want to see that this regression
on the fake record when we're feeding in
values where we have the medical record
where you don't have the Y we just want
to make sure that the probability that
the fake record is fairly high and
that's built into the training we don't
so much care about what its regressing
on or what the Russian output is for
ones where we do have it we're
penalizing it based on how close or how
far away it was from the expected Y from
this and just like the classification
one when we're all done with this we
throw away the generator in the
discriminator becomes the
semi-supervised neural network that was
trained on this now if you want to go
further with this semi-supervised
learning technique i've given you a
couple of lengths of articles that i
found useful for this there is a link to
the actual house data set that's a
pretty interesting data set to look at
it has all those house numbers above you
can deal with in several ways you can
deal with classifying the individual
digits they give you the bounding
rectangles around the digits they also
give you just the bounding rectangle of
the entire set of digits if you want to
so you can be classifying digits or you
can be classifying the entire address it
just depends on how you want to set up
the problem the examples that I give you
here we're using individual digits this
is the original paper that first started
looking at this unsupervised
representation learning with deep
convolutional generative Gass general
generative adversarial Network I have a
link to this paper in the module thank
you for watching this video in the next
video we're going to take a look at some
of the most cutting-edge and current
research into Ganz it's a very active
area of research this content change is
often so subscribe to the channel to
stay akha date on this course and other
topics and artificial intelligence
