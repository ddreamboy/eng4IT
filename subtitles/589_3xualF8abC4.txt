SHILPA KANCHARLA: Hi.
My name is Shilpa Kancharla.
And I'm an engineer on
the TensorFlow team.
Today, we're going to
learn how to pre-process
and load video data
into a TensorFlow model.
You'll be able to use
the skills you learn here
to develop video classification
models like this one.
This model uses the UDF101 data
set and predicts the action
the person is taking.
For example, on the left,
you'd classify this video
as band marching.
And on the right, you'd
classify it as apply eye makeup.
I'll outline the data set
you'll use, how to create frames
from videos, how to
visualize those frames,
and how to configure the
data set for performance.
You can find complete
details on all these steps
in the associated tutorial.
Here, I'll cover the
key idea for you.
OK.
Let's get started.
Today, we'll be using the UCF101
action recognition data set.
This data set contains
101 different actions
from diverse categories.
Each category has
about 25 examples
of that action which are split
into four to seven videos
for each example.
To keep things simple
for this tutorial,
we'll be using the first
10 classes listed here
to keep the data set
size small, so our model
can be trained quickly.
Let's take a look at the
shape of the video data
that we're going to pass
into the model as well.
Loading video data into
a deep learning model
is similar to how you
would load an image data,
but with an extra dimension.
In a later video,
I'll explain how
to set up 3D CNN
classifier which accepts
data of the following shape--
batch size times the
number of frames times
height by width by channels.
The number of frames
dimension can also
be synonymous with time.
Videos can be split up
into a series of frames
which are essentially images.
We use the OpenCV library
to help us with this task.
When we call this
function, we specify
the number of frames we
want, the output size
of the frame we want to
produce, and the number
of steps between each frame.
These are user
defined parameters
that are heavily
dependent on the data set.
If the video isn't long enough
for us to collect frames from,
we start collecting
them from the beginning.
Otherwise, we can pick a
random time in the video
to start getting frames from.
We apply a couple
of transformations
using TensorFlow functions
to get our frames
to be the size we specified.
Finally, we return a NumPy
array of these frames.
Use the to_gif function to
create a visualization of some
of the frames you've generated.
Call the frames from
video file function first.
Take the output of that, and
place it to_gif function.
The FrameGenerator class
uses a generator function
that will yield the frame array
and an encoded label associated
with that set of frames.
Use this class to create a
TensorFlow data input pipeline.
It allows you to feed
in data into your model.
Consider using
buffered prefetching,
so you can yield
data from the disk
without having an
input-output bottleneck.
Prefetching allows
you to grab the data
a step ahead of when it's
inputted into the model,
therefore reducing
the amount of time
between consuming a data
point and producing another.
Caching allows you to store
data in memory or local storage.
Therefore, saving
some operations,
like file opening, from
being performed every epoch.
Overall, these additional
preprocessing steps
allow for work on the CPU
and GPU to run in parallel.
So your processor
can prepare data
while your GPU is classifying.
EfficientNet is a convolutional
neural network architecture
method that uniformly scales all
dimensions of your input data.
We can treat it as the
Hello World example
of using video data for now.
Let's try putting
our training data
set into a pre-trained
EfficientNet model, which
trains our data to a high
accuracy in just a few minutes.
You can also take a look
at an additional model
of video classification I
wrote in the description.
You can try applying what
you've learned in this video
and tutorial to other
kinds of video data.
See if you can use a generator
class we've created here
in your own machine learning
pipelines for video.
Moreover, video data has
an extra time dimension
unlike image data.
Volumetric data,
such as MRI scans,
also have an extra
dimension of volume
unlike image data as well.
So you could attempt to
use the code shown here
for volumetric data.
If possible, I'd
like to invite you
to share links of your
open source projects
down in the comments as well.
Thanks very much for watching.
[AUDIO LOGO]
