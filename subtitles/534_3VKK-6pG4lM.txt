if you're new to this channel you may
consider subscribing and hit the Bell
icon so that you continue to receive the
updates also after watching this video
you may want to refer to some of the
playlists that we have created for
people who are interested in in-depth
knowledge these are the videos in the
right sequence which will give you
thorough knowledge of the subject please
share it with all others who might
benefit let's get started so finally
we've made it to the last leg of
ensembling techniques and we're going to
talk about the composite techniques in
this video it has two broad categories
blending and stacking and they have
certain common characteristics so we'll
talk about these two together and then
I'll specifically talk about the
difference between these two techniques
it's going to be a lot interesting
needless to emphasize one more sign that
this is something that you need to know
if you are a data science practitioner
let's Deep dive so talking about the
models that we use for blending and
stocking these are going to be different
types of Base models where else we
talked about different types of Base
models we talked about it in case of
voting but what makes blending and
stacking different from voting is the
way we deal with the data in this
situation so we use a combination of
sequential and parallel approaches so we
use certain base models in parallel the
predictions of these base models are
then passed on to a meta model which
generates the final predictions and this
will become clearer as I continue to
explain this to you so just bear with me
let's start with some data and let's
zoom in to what does do this data
contain so this is the same example that
we've been talking about we have a
credit score we have the average account
balance in thousands of dollars we have
the work experience of the individual
and we are checking for the loan amount
eligibility it could be a personal loan
so this is how our data would be you've
just taken one sample record you can
imagine all the rows in the data could
be hundreds of thousands of rows would
be like this now in order to be able to
predict the load amount given the
factors like trade score average balance
and work experience we would need
different models let's say the first
model that we call is a linear
regression model the second model that
we call is a decision free regressor and
the third model is a KNN regressor all
these are called base models and please
note these models are all learning in
parallel which means they are not
influenced by each other's outcomes
linear regression has no influence of
the other two models whatsoever and any
model is not influenced by the other
models so this is the parallel learning
piece that we are talking about now when
each record is passed to these models
each of these models is capable of
generating a prediction so let's say
given the credit score average balance
and work experience we populate a table
which captures the predictions made by
each of these models for example a
linear regression when given these
inputs generates a prediction which is
let's say 28,000 decision tree regressor
generates a prediction which is
34,000 and Cann generates a prediction
which is 31,000 please note at all times
we always have the target column
available because we are talking about
supervised learning so at this stage
what you're seeing is basically how the
data is being generated for The Meta
model we had the original data we gave
that as an input to the base models and
base models generated the predictions so
for the meta model now we don't need the
original data anymore we can simply
ignore it and we have to concentrate
entirely on the data so generated out of
these predictions so this is what we're
going to focus on now the problem
statement for The Meta model reduces to
what is the loan amount eligibility
given that we have predictions from
these three base models so these
predictions from the base models are
acting as features for the meta model so
just to sum it up one more time we had a
data and this data got passed to three
different models which learn in parallel
these models generated some prediction
and that's another data just like we saw
there were three features containing
predictions from each of these models
this data is given as an input to a meta
model this model could be a different
model we've let's say taken a support
Vector regressor in this case now if you
look at this while the base models learn
in parallel The Meta model is actually
being given the input in a sequential
way so this is why we call it a
composite technique because it's
learning both parall and sequentially
Now we move to the final stage this meta
model is going to generate the
prediction which would determine the
loan amount eligibility the amount that
the bank is willing to approve this meta
models prediction is taken as the final
result but generally when you do these
things practically you understand that
we always divide the data into train
test and we try to learn on the train we
try to make our predictions on the test
how does that work in case of blending
and stacking let's understand that so
we'll take an example of blending for
first and and then I'll just tell you
what is it that's different in case of
stacking let's start with the data so we
have the data first of all we divide
this data into three parts train
validation and test generally as a
convention we always keep a larger
portion of the data as the train data
now this train data will be given as
input to each of these base models which
are parallel models so base models learn
using the train set now once the base
models have been trained
they generate predictions on the
validation set remember this data that's
being generated in the form of
predictions is the data or the input for
The Meta model so the outputs from the
validation set are being supplied to The
Meta model and finally when the meta
model has been trained The Meta model
would generate predictions for the test
set so this is how it overall Works in
terms of train validation and test this
approach where we part the portion of
the train data for validation purposes
is also known as the hold out approach
and the approach that you just saw is
called
blending then what's different between
blending and stacking there's one major
difference and that is in case of
stacking instead of just taking one
portion as validation you do something
called as cross validation now we've
covered cross validation as a separate
tutorial from a general perspective of
supervised learning you may refer to it
but stacking instead of just taking one
cut of the train and parking it as
validation does a cross validation
essentially it kind of repeats the
process multiple times it could be kfold
cross validation where the value of K
could be five or 10 commonly as we go
about there's one more difference in
stacking and blending that in case of
stacking we once again train the base
models on the overall data so we do the
same cycle we come up to the stage of a
meta mod and we once again train the
base models on the entire data so that's
one more difference but if that gets too
complicated for you to follow you may
just keep this in mind that this is what
by enlarge blending and stacking to the
basic structure is that we have a set of
Base models which learn in parallel and
then we have a meta model now there
could also be variations of how you want
to do this you may create another layer
of Base models so you can do multiple
layers of Base models and then finally
do a meta model choice of these models
that we mentioned here is not again a
mandate we could try a different mix of
different types of models and look at
which combination gives us the best
results while this might sound like a
relatively heavy concept to understand
in terms of coding it's very very
simplified so when we do the Hands-On
exercise you will see it's pretty
straightforward that's it about blending
and stacking and this is the last video
in the sequence of ensembling technique
videos
