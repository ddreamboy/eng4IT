hello and welcome to the tutorial on
data science in this tutorial we are
going to see what is data science and
who is a data scientist what are the
skills of a data scientist and what does
the data scientist do a day in the life
of a data scientist what is the
methodology used for data science like
data acquisition preparation
mining and model building and testing
and then maintenance and then towards
the end we will also see an example
program as well i'll take you through a
quick code where we have done some data
science activity and then we will
conclude so let's get started so what is
data science as the name suggests it is
nothing but study of using data and
trying to find out some insights or
extracting some insights or knowledge
using all the data that is at your
disposal so that's pretty much what data
science is all about so you take the
data and apply certain methodologies
certain algorithms and
your business domain knowledge as well
and of course a certain amount of
creativity to extract some insights very
often they are also known as actionable
insights because once you have the
insights you should be in a position to
take some action to let's say solve a
problem or improve a situation there are
a lot of areas where data science can be
used one of the very common one is fraud
detection or fraud prevention there are
a lot of fraudulent activities or
transactions primarily on the internet
it's very easy to commit fraud and
therefore we can use data science to
either prevent or detect fraud there are
certain algorithms machine learning
algorithms that can be used like for
example some outlier techniques
clustering techniques that can be used
to detect fraud and prevent fraud as
well so who is a data scientist rather
it is actually a very generic role that
defines somebody who is working with
data is known as a data scientist but
there can be very specific activities
and the roles can be actually much more
specific what exactly a person does
within the area of data science can be
much more specific but broadly anybody
working in the area of data science is
known as a data scientist so what does a
data scientist do these are some of the
activities data acquisition data
preparation data mining data modeling
and then model maintenance we will talk
about each of these in a great detail
but at a very high level the first step
obviously is to get the raw data which
is known as data acquisition it can be
in all kinds of format and it could be
multiple sources but obviously that raw
data cannot be used as it is for
performing data mining activities or
data modeling activities so the data has
to be claimed and prepared for using in
the data models or in the data mining
activity so that is the data preparation
then we actually do the data mining
which can also include some exploratory
activities and then if we have to do
stuff like machine learning then you
need to build a machine learning model
and test the model get insights out of
it and then if
the model is fine you deploy it and then
you need to maintain the model because
over a period of time it is possible
that you need to tweak the model because
of change in the process or changing the
data and so on so that all comes under
the model maintenance so let's take
deeper look at each of these activities
let's start with data acquisition so the
stage of data acquisition basically the
data scientist will collect raw data
from all possible sources so this could
be typically an rdbms which is a
relational database or it can also be a
non-rdbms or it could be flat files or
unstructured data and so on so we need
to bring all that data from different
sources if required we need to do some
kind of homogeneous formatting so that
it all fits into a in a looks at least
format from a format perspective it
looks homogeneous so that may be
requiring some kind of transformation
very often this is loaded into what is
known as data warehouse so this can also
be sometimes referred to as etl or
extract transform and load so a data
warehouse is like a common place where
data from different sources is brought
together so that people can perform data
science activities like reporting or
data mining or statistical analysis and
so on so data from various sources is
put in a centralized place which is
known as a data warehouse so that is
also known as etl and in order to do
this there can be a data scientist can
take help of some etl tools there are
some existing tools that a data
scientist can take help of like for
example data stage or talent or
informatica these are pretty good tools
for performing these etl activities and
getting the data the next stage now that
you have the raw data into a data
warehouse you still probably are not in
a position to straight away use this
data for performing the data mining
activities so that is where data
preparation comes into play and there
are multiple reasons for that one of
them could be the data is dirty there
are some missing values and so on and so
forth so a lot of time is actually spent
in this particular stage so a data
scientist spends a lot of time almost 60
to 70 percent of the time in
this part of the project or the process
which is data preparation so there are
again within this there can be multiple
sub activities starting from let's say
data cleaning you will probably have
missing values the data there is some
columns the values are missing or the
values are incorrect there are null
values and so on and so forth so that is
basically the data cleaning part of it
then you need to perform certain
transformations like for example
normalizing the data and so on right so
you could probably have to modify a
categorical values into numerical values
and so on and so forth so these are
transformational activities then we may
have to handle outliers so the data
could be such that there are a few
values which are way beyond the normal
behavior of the data for whatever reason
either people have keyed in wrong values
or for some reason some of the values
are completely out of range so those are
known as outliers so there are certain
ways of handling these outliers and
detecting and handling these outliers so
this is a part of what is known as
exploratory analysis so you quickly
explore the data to find out other so
and you can use visual tools like plots
and identify what are the outliers and
see how we can get rid of the outliers
and so on then the next part could be
data integrity data integrity is to
validate for example if there are some
primary keys that all the primary keys
are populated there are some foreign
keys then at least most of the foreign
keys should be populated and otherwise
when we are trying to query the data you
may get wrong values and so on so that
is the data integrity part of it and
then we have what is known as data
reduction sometimes we may have
duplicate values we may have columns
that may be
duplicated because they are coming from
different sources the same values are
there and so on so a lot of this can be
done using what is known as data
reduction and thereby you can reduce the
size of the data drastically because
very often this could be written in data
which can be removed and so on so let's
take a look at what are the various
techniques that are used for data
cleaning so we need to ensure that the
data is valid and it is consistent and
uniform and accurate so these are the
various parameters that we need to
ensure as a part of the data cleaning
process now what are the techniques that
that are used for data cleaning or so we
will see what each of these are in this
particular case and so what is the data
set that we have we have data about a
bank and its customer details so let's
take an example and see how we go about
cleaning the data and in this particular
example we are assuming we are using
python so let's assume we loaded this
data which is the raw file.csv this is
how the customer data looks like and
we will see for example we take a closer
look at the geography column we will see
that there are quite a few blank spaces
so how do we go about when we have some
blank spaces or if it is a string value
then we put an empty string here or we
just use a space or empty string if
there are numerical values then we need
to come up with a strategy
for example
we put the mean value so wherever it is
missing we find the mean for that
particular column so in this case let's
assume we have credit score and we see
that quite a few of these values are
missing so what do we do here we find
the mean for this column for all the
existing values and we found that the
mean is equal to
638.6 so we kind of write a piece of
code to replace wherever there are blank
values nan is basically like null and
we just go ahead and say fill it with
the mean value so this is the piece of
code we are writing to fill it so all
the blanks or all the null values get
replaced with the mean value now one of
the reasons for doing this is that very
often if you have some such situation
many of your statistical functions may
not even work so that's the reason you
need to fill up these values or either
get rid of these records or fill up
these values with something meaningful
so this is one mechanism which is
basically using a mean there are few
others as we move forward we can see
what are the other ways for example we
can also say that any missing value in a
particular row if even one column the
value is missing you just drop that
particular row or delete all rows where
even a single column has missing values
so that is one way of dealing now the
problem here can be that if a lot of
data has let's say one or two columns
missing and
we dropped many such rows then overall
you may lose out on let's say 60 of the
data has some value or the other missing
60 of the rows then it may not be a good
idea to delete all the rows like in that
manner because then you're losing pretty
much 60 of your data therefore your
analysis won't be accurate but if it is
only 5 or 10 percent then this will work
another way is only to drop values where
or rather dropped rows where all the
columns are empty which makes sense
because that means that record is of
really no use because it has no
information in it so there can be some
situations like that so we can provide a
condition saying that drop the records
where all the columns are blank or not
applicable we can also specify some kind
of a threshold let's say you have 10 or
20 columns in a row you can specify that
maybe five columns are blank or null
then you drop that record so again we
need to take care that such a condition
such a situation the amount of data that
has been removed or excluded is not
large if it is like maybe five percent
maximum ten percent then it's okay but
by doing this if you're losing out on a
large chunk of data then it may not be a
good idea you need to come up with
something better what else we need to do
next is so the data preparation part is
done so now we get into the data mining
part so what exactly we do in data
mining primarily we come up with ways to
take meaningful decisions so data mining
will give us insights into the data what
is existing there and then we can do
additional stuff like maybe machine
learning and so on to get perform
advanced analytics and so on so one of
the first steps we do is what is known
as data discovery and
which is basically like exploratory
analysis so we can use tools like
tableau for doing some of this so let's
just take a quick look at how we go
about that so tableau is excellent data
mining or actually more of a reporting
or a bi tool
and you can download a trial version of
tableau at tableau.com or there is also
tableau public which is free and you can
actually use and play around however if
you want to use it for enterprise
purpose then it is a commercial software
so you need to purchase license and you
can then run some of the data mining
activities say your data source your
data is in some excel sheet so you can
select the source as microsoft excel or
any other format and the data will be
brought into the tableau environment and
then it will show you what is known as
dimensions and measures so dimensions
are all the descriptive columns so and
tableau is intelligent enough to
actually identify these dimensions and
measure so measures are the numerical
values so as you can see here customer
id gender geography these are all
dimensions non-numerical values whereas
age balance credit score and so on are
numeric values so they come under
measures so you've got your data into
tableau and then you want to let's say
build a small model and you want to
let's say solve a particular problem so
what is the problem statement all right
let's say we want to analyze why
customers are
leaving the bank which is known as exit
and we want to analyze and see what are
some of the factors for exiting the bank
and we want to let's assume consider
these three of them like let's say
gender credit card and geography these
as a criteria and analyze if these are
in any way impacting or have some
bearing on the customer exiting or the
customer exit behavior okay so let's um
use tableau and very quickly we will be
able to find out how these parameters
are affecting all right so let's see so
this is our customer data so from our
excel sheet we have data set about let's
say 10 000 rows and we want to find out
what is the criteria let's start with
gender let's say we want to first use
gender as a criteria so tableau really
offers an easy drag and drop kind of a
mechanism so that makes it really really
easy to perform this kind of analysis so
what we need to do is exited says
whether the customer has exited or not
so it has a value of 0 and 1 and then of
course you have gender and so on so we
will take these two and simply drag and
drop okay so exit it and then we will
put gender and if we drag and drop into
the analysis side of tableau all right
so here what we are doing is we are
showing male female as two different
columns here and zero for people who did
not exist and one for people who exited
and that is color coded so the blue
color means people who did not exist and
this yellow color means people who
detects it all right so now if we pull
the data here create like bar graphs
this is how it would look so what is
yellow let's go back so yellow is uh who
exited and
for the male only
16.45 percent have exited and we can
also draw a reference line that will
help us or even provide aliases so these
are a lot of fancy stuff that is
provided by tableau you can create
aliases and so that it looks good rather
than basic labels and you can also add a
reference line so you add a reference
line something like this from here we
can make out that on an average female
customers exit more than the male
customers right so that is what we are
seeing here on an average so we have
analyzed based on gender we do see that
there is some difference in the male and
female behavior now let's take the next
criteria which is the credit card so
let's see if having a credit card has
any impact on the customer exit behavior
so just like before we drag and drop the
credit card has credit card a column if
we drag and drop here and then we will
see that there is pretty much no
difference between people having credit
card and not having credit card
20.81 percent of people who have no
credit card have exited and similarly
20.18 of people who have credit card
have also exited so the credit card is
not having
much of an impact that's what this piece
of analysis shows last we will basically
go and check how the geography is
impacting so once again we can drag and
drop geography column onto this side and
if we see here there are geographies
like i think there are about three
geographies like france germany and
spain and
we see that there is some kind of
impact with the geography as well okay
so what we derive from this is that the
credit card is really we can ignore the
credit card variable or feature from our
analysis because that doesn't have any
impact but gender and geography we can
keep and do further analysis okay all
right so what are some of the advantages
of data mining bit more detailed
analysis can help us in predicting the
future trends and it also helps in
identifying customer behavior patterns
okay so you can take informed decisions
because the data is telling you or
providing you with some insights and
then you take a decision based on that
if there is any fraudulent activity data
mining will help in quickly identifying
such a fraud as well and of course it
will also help us in identifying the
right algorithm for performing more
advanced data mining activities like
machine learning and so on all right so
the next activity now that we have the
data we have prepared the data and
performed some data mining activity the
next step is model building let's take a
look at model building so what is model
building if we want to perform a more
detailed data mining activity like maybe
perform some machine learning then
you need to build a model and how do you
build a model first thing is you need to
select which algorithm you want to use
to solve the problem on hand and also
what kind of data that is available and
so on and so forth so you need to make a
choice of the algorithm and based on
that you go ahead and create a model
train the model and so on now machine
learning is kind of at a very high level
classified into supervised and
unsupervised so if we want to predict a
continuous value could be a price or a
temperature or a height or a length or
things like that so those are continuous
values and if you want to find some of
those then you use techniques like
regression linear regression simple
linear regression multiple linear
regression and so on so these are the
algorithms on the other hand there will
be situations or there may be situations
where you need to perform unsupervised
learning case of unsupervised learning
you don't have any historical
labeled data so to learn from so that is
when you use unsupervised learning and
some of the algorithms in unsupervised
learning are clustering k means
clustering is the most common algorithm
used in unsupervised learning and
similarly in supervised learning if you
want to perform some activity on
categorical values like for example it
is not measured but it is counted like
you want to classify whether this image
is a cat or a dog whether you want to
classify whether this customer will buy
the product or not or you want to
classify whether this email is spam or
not spam so these are examples of
categorical values and
these are examples of classification
then you have algorithms like logistic
regression k nearest neighbor or k nn
and support vector machine so these are
some of the algorithms that are used in
this case and similarly in case of
unsupervised learning if you need to
perform on categorical values you have
some algorithms like association
analysis and hidden markov model okay so
in order to understand this better let's
take an example and
take you through the whole process and
then we will also see how the code can
be written to perform this now let's
take our example here where we want to
perform supervised learning which is
basically we want to do a multi-linear
regression which means there are
multiple independent variables and then
you want to perform a linear regression
to predict certain value so in this
particular example we have world
happiness data so this is the data about
the happiness quotient of people from
various countries and we are trying to
predict and see whether or how our model
will perform so what is the question
that we need to ask first of all how to
describe the data and then can we make a
predictive model to calculate the
happiness score right so based on this
we can then decide on what algorithm to
use and what model to use and so on so
variables that are available or used in
this model this is a list of variables
that are available there is a happiness
rank i'll load the data and i'll show
you the data in a little bit so it
becomes clear what are these so there is
what is known as the happiness rank
happiness score which is happiness score
is more uh like absolute value whereas
rank is what is the ranking and then
which country we are talking about and
within that country which region and
what kind of economy and whether the
family which family and health details
and freedom trust generosity and so on
and so forth so there are multiple
variables that are available to us and
the specific details probably are not
required and there can be
in another example the variables can be
completely different so we don't have to
go into the details of what exactly
these variables are but just enough to
understand that we have a bunch of these
variables and now we need to use either
all or some of these variables and then
which we also sometimes refer to as
features and then we need to build our
model and train our model all right so
let's assume we will use python in order
to perform this analysis or perform this
machine learning activity and i will
actually show you in our lab in in a
little bit this whole thing we will run
the live code but quickly i will run you
through the slides and then we will go
into the lab so what are we doing here
first thing we need to do is import a
bunch of libraries in python which are
required to perform our analysis most of
these are for manipulating the data
preparing the data and then scikit-learn
or sk learn is the library which you
will use actually for this particular
machine learning activity which is
linear regression so we have numpy we
have pandas and so on and so forth so
all these libraries are imported and
then we load our data and the data is in
the form of a csv file and there are
different files for each year so we have
data for 2015 16 and 17. and so we will
load this data and then combine them
concatenate them to prepare a single
data frame and here we are making an
assumption that you are familiar with
python so it becomes easier if you are
familiar with python programming
language or at least some programming
language so that you can at least
understand by looking at the code so we
are reading the file each of these files
for each year and this is basically we
are creating a list of all the names of
the columns we will be using later on
you will see in the code so we have
loaded 2015 then 2016
and then also 2017. so we have created
three data frames and then we
concatenate all these three data frames
this is what we are doing here then we
identify which of these columns are
required which for example some of the
categorical values do we really need we
probably don't then we drop those
columns so that we don't unnecessarily
use all the columns and make the
computation complicated we can then
create some plots using plotly library
and it has some powerful features
including creation or creation of maps
and so on just to understand the pattern
the happiness quotient or how the
happiness is across all the countries so
it's a nice visualization we can see
each of these countries how they are in
terms of their happiness score this is
the legend here so the lighter colored
countries have lower ranking and so
these are the lower ranking ones and
these are higher ranking which means
that the ones with these dark colors are
the happiest ones so as you can see here
australia and maybe this side us and so
on are the happiest ones
okay the other thing that we need to do
is the correlation between the happiness
score and happiness rank we can find a
correlation using a scatter plot and we
find that yes they are kind of inversely
proportioned which is obvious so if the
score is high happiness score is high
then they are ranked number one for
example highest is scored as number one
so that's the idea behind this so the
happiness score given here and the
happiness rank is actually given here so
they are inversely proportional because
the higher the score then the absolute
value of the rank will be lower right so
number one has the highest value of the
score and so on so they are inversely
correlated but there is a strong what
this graph shows is that there is a
strong correlation between happiness
rank and happiness score and then we do
some more plots to visualize this we
determine that probably rank and score
are pretty much conveying the same
message so we don't need both of them so
we will kind of drop one of them and
that is what we are doing here so we
drop the happiness rank and similarly so
this is one example of how we can remove
some columns which are not adding value
so we will see in the code as well how
that works moving on this is a
correlation between pretty much each of
the columns with the other columns so
this is a correlation you can plot using
plot function and we will see here that
for example happiness score and
happiness score are correlated strongest
correlation right because every variable
will be highly correlated to itself so
that's the reason so the darker the
color is the higher the correlation and
as so the and correlation in numerical
terms goes from zero to one so one is
the highest value and it can only be
between zero and one correlation between
two variables can be only have a value
between zero and one so the numerical
value can go from zero to one and one
here is dark color and
zero is uh kind of dark but it is blue
color from red it goes down the dark
blue color indicates pretty much no
correlation so from this heat map we see
that happiness and economy and family
are probably also health probably are
the most correlated and then it keeps
decreasing after freedom kind of keeps
decreasing and coming to pretty much
zero all right so that is a correlation
graph and then we can probably use this
to find out which are the columns that
need to be dropped which do not have
very high correlation and
we take only those columns that we will
need so this is the code for dropping
some of the columns once we have
prepared the data when we have the
required columns then we use scikit
learn to actually split the data first
of all this is a normal machine learning
process you need to split the data into
training and test data set in this case
we are splitting into 80 20 so 80 is the
training data set and 20 is the test
data set so that's what we are doing
here so we use train test split method
or function so you have all your
training data
in x underscore train the labels in y
underscore train similarly x underscore
test has the test data the inputs
whereas the labels are in y underscore
test so that's how and this value
whether it is 80 20 or 50 50 that is all
individual preference so in our case we
are using 80 20. all right and then the
next is to create a linear regression
instance so this is what we are doing we
are creating an instance of linear
regression and then we train the model
using the fit function and we are
passing x and y which is the x value and
the label data regular input and the
label data label information then we do
the test we run the or we perform the
evaluation on the test data set so this
is what we are doing with the test data
set and then we will evaluate how
accurate the model is and using the
scikit-learn functionality itself we can
also see what are the various parameters
and what are the various coefficients
because in linear regression you will
get like a equation of like a straight
line y is equal to beta 0 plus beta 1 x
1 plus beta 2 x 2 so those beta 1 beta 2
beta 3 are known as the coefficients and
beta0 is the intercept after the
training you can actually get these
information of the model what is the
intercept value what are the
coefficients and so on by using these
functions so let's take quickly go into
the lab and take a look at our code okay
so this is
my
lab this is my jupiter notebook where
the code i have the actual code and i
will take you through this code to run
this linear regression on the world
happiness data so we will import a bunch
of libraries numpy pandas plot plotly
and so on also yeah scikit learn that's
also very important
so
that's the first step then i will import
my data and the data is in three parts
there are three files one for each year
2015 2016 and 2017 and it is a csv file
so i've imported my data let's take a
look at the data quickly glance at data
so this is how it looks we have the
country region happiness rank and then
happiness score there are some standard
errors and then what is the per capita
family and so on so then we will keep
going we will create a list of all these
column names we will be using later so
for now just i will run this code no
need of major explanation at this point
we know that some of these columns
probably are not required so you can use
this drop functionality to remove some
of the columns which we don't need like
for example region and standard error
will not be contributing to our model so
we will basically drop those values out
here so we use the drop and then we
created a vector with these names column
names that's what we are passing here
instead of giving the names of the
columns here we can pass a vector so
that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also 2017
and then we will concatenate this data
so let's do that so we have now data
frame called happiness which is a
concatenation of both all the three
files let's take a quick look at the
data now so most of the unwanted columns
have been removed and you have all the
data in one place for all the three
years and this is how the data looks and
if you want to take a
look at the summary
of the columns you can say describe and
you will get this information for
example for each of the columns what is
the count what what is the mean value
standard deviation especially the
numeric values okay not the categorical
values so this is a quick way to see how
the data is and
initial little bit of exploratory
analysis can be done here so what is the
maximum value what's the minimum value
and so on for each of the columns all
right so then we go ahead and create
some visualizations using plotly so let
us go and
build a plot so if we see here now this
is the relation correlation between
happiness rank and happiness score this
is what we have seen in the slides as
well we can see that there is a tight
correlation between them only thing is
it is inverse correlation but otherwise
they are very tightly correlated which
also says that they both probably
provide the same information so there is
no not much of value add so we will go
ahead and drop the happiness rank as
well from our columns so that's what
we're doing here and now we can
do the creation of the correlation heat
map let us plot the correlation heat map
to see how each of these columns is
correlated to the others and we as we
have seen in the slides this is how it
looks so happiness score is very highly
correlated so this is the legend we have
seen in the slide as well so blue color
indicates pretty much zero or very low
correlation deep red color indicates
very high correlation and the value
correlation is a numeric value and the
value goes from zero to one if the two
items or two features or columns are
highly correlated then there will be as
close to one as possible and two columns
that are not at all correlated will be
as close to zero as possible so that's
how it is for example here happiness
score and happiness score every column
or every feature will be highly
correlated to itself so it is like
between them there will be correlation
value will be one so that's why we see
deep red color but then others are for
example with higher values are economy
and then health and then maybe family
and freedom so these are generosity and
trust are not very highly correlated to
happiness score so that is
one quick exploratory analysis we can do
and therefore we can drop the country
and happiness rank because they also
again don't have any major impact on the
analysis on our analysis so now we have
prepared our data there was no need to
clean the data because the data was
clean but if there were some missing
values and so on as we have discussed in
the slides we would have had to perform
some of the data cleaning activities as
well but in this case the data was clean
all we needed to do was just the
preparation part so we removed some
unwanted columns and we did some
exploratory data analysis now we are
ready to perform the machine learning
activity so we use scikit-learn for
doing the machine learning circuit learn
is python library that is available for
performing our machine learning once
again we will import some of these
libraries like pandas and numpy and
also psychic learn first step we will do
is split the data in 2080 format so you
have all the test data which is 20 of
the data is test data and 80 percent is
your training data so this test size
indicates how much of it is the what is
the size of the test data the remaining
which is a point here we are saying
point two therefore that means training
is point eight so training data is
eighty percent all right so we have
executed that split the data and now we
create an instance of the linear
regression model so lm is our linear
regression model and we pass x and y the
training data set and call the function
fit so that the model gets trained so
now once that is done training is done
training is completed and now what we
have to do is we need to predict the
values for the test data so the next
step is using so you see your fit will
basically run the training method
predict will actually predict the values
so we are passing the input values which
is the independent variables and we are
asking for the values of the dependent
variable which is which we are capturing
in y underscore track and we use the
predict method here lm.predict so this
will give us all the predicted y values
and remember we already have y
underscore test has the actual values
which are the labels so that we can use
these two to compare and find out how
much of it is error so that's what we
are doing here we are trying to find the
difference between the predicted value
and the actual value why underscore test
is the actual value for the test data
and why underscore predict is the
predicted value we just found out the
predictive line so we will run that and
we can do a quick check as to how the
data looks how is the difference so in
some cases it is positive some cases it
is negative but in most of the cases i
think the difference is very small this
is exponential to the power of 0 minus
04 and so on so looks like our model has
performed reasonably well we can now
check some of the parameters of our
model like the intercept and the
coefficients so that's what we are doing
here so these are the coefficients of
the various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variables so
the coefficients against the
corresponding independent variable so
1.0051 is the coefficient for economy
0.99983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of scikit-learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that the root mean square error
is pretty low which is a good sign and
which is one of the measures of
how well our model is performing we can
do one more quick plot to just see how
the actual values and the predicted
values are looking and once again you
can see that as we have seen from the
root mean square error root mean square
error is very very low that means that
the actual values and the predicted
values are pretty much matching up
almost matching up and this plot also
shows the same so this line is going
through the predicted values and the
actual values and the differences very
very low so again this is actual data
this is one example where the accuracy
is high and the predicted values are
pretty much matching with the actual
values but in real life you may find
that these values are slightly more
scattered and you may get the error
value can be relatively on the higher
side the root mean squared okay so this
was a good
quick example of the code to perform
data science activity or a machine
learning or data mining activity in this
case we did what is known as linear
regression so let's go back to our
slides and see what else is there so we
saw this these are the coefficients of
each of the features in our code and
we have seen the root mean square error
as well and we can take a few hundred
countries
certain values and actually predict to
see if how the model is performing and i
think we have done this as well and in
this case as we have seen pretty much
the predicted values and the actual
values are pretty much matching which
means our model is almost hundred
percent accurate as i mentioned in real
life it may not be the case but in this
particular case we have got a pretty
good model which is very good also
subsequently we can assume that this is
how the equation in linear regression
the model is nothing but an equation
like y is equal to beta 0 plus beta 1 x
1 plus beta 2 x 2 plus beta 3 x 3 and so
on so this is what we are showing here
so this is our intercept which is beta0
and then we have beta1 into economy
value beta2 into the family value beta3
into health value and so on so that is
what is shown here okay so i think the
next step once we have the results from
the data mining or machine learning
activity the next step is to communicate
these results to the appropriate
stakeholders so that is what we will see
here now so how do we communicate
usually you take these results and then
either prepare a presentation or put it
in a document and then show them these
actionable results or reasonable
insights and you need to find out who
are your target audience and put all the
results in context and maybe if there
was a problem statement you need to put
this results in the context of the
problem statement what was our initial
goal that we wanted to achieve so that
we need to communicate here based on you
remember we started off with what is the
question and what is the data and so on
and then what is the answer so we we
need to put the results and then what is
the methodology that we have used all
that has to be put and clearly
communicated in business terms so that
the people understand very well from a
business perspective so once the model
building is done once the results are
published and communicated the last part
is maintenance of this model now very
often what can happen is the model may
have to be subsequently updated or
modified because of multiple reasons
either the the data has changed the way
the data comes has changed or the
process has changed or for whatever
reason the accuracy may keep changing
once you have a trained model the for
example we got a very high accuracy but
then over a period of time there can be
various factors which can cause that so
from time to time we need to check
whether the model is performing well or
not the accuracy needs to be tested once
in a while and if required you may have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model with the latest data
that you have and then you deploy it to
you build the model train it and then
you deploy it so that is like the
maintenance cycle that you may have to
take the model through all right so we
are pretty much at the end of our
tutorial so what did we learn in this
tutorial we talked about what is data
science and
who is a data scientist then we talked
about what a data scientist performs or
does a day in the life of a data
scientist some of the activities or the
methodologies like the processes rather
data acquisition data preparation
mining and model building and so on and
then last but not least the model
maintenance and in the process we have
also taken a look at the python code
that was used for performing a linear
regression model creating a linear
regression model training it and testing
it and checking the various parameters
of that model with that we come to the
end of this tutorial i hope you enjoyed
it if you have any questions or comments
put it below the video in the comment
section we will be more than happy to
respond especially if you leave your
email address i would like to thank you
once again and have a great day thank
you and bye
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here
