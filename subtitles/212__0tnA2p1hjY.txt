[Music]
there are variety of devops rules that
are commonly used to automate and
streamline different aspects of the
software development and deployment
process in order to improve the speed
and quality of the software releases
hello everyone and welcome to this
session you are currently watching an
edureka devops tools full course
by the end of this video you will have
an idea of the multiple devops tools
that are currently in demand now if you
love watching videos like these then
subscribe to edunaker's YouTube channel
and click the Bell button to never miss
out any updates from us also if you want
to learn more about devops after
watching this session and wish to obtain
a Eureka's devop certification course
then please see the link in the
description below now let's begin with
our agenda where we'll have a brief
overview of what we will cover in this
devops tools full course video
we will start with the introduction to
devops where we will learn what devops
is and why should we learn it
after which we will see the devops life
cycle now it's time to get our hands
dirty with some popular devops tools we
will start with Git followed by Jenkins
and Docker after this we will learn
about kubernetes puppet and ansible once
this is done we will then move ahead
with terraform Prometheus and gravna we
will also see selenium and egg years now
after all this we will head over to
Azure devops and AWS demons we will then
compare Azure devops to AWS devops
we hope that this session assists you in
getting jobs in the industry in order to
accomplish this we will look at how our
career intervals work with some
essential devops interview questions
with answers
so stick to lead now let's get started
with our first topic that is what is
devops foreign
[Music]
what is devops
it's basically a combination of Tours
development and operations so what do we
get when we combine these two
[Music]
devops is a culture that implements
technology in order to promote
collaboration between development and
operations team to deploy code to
production faster in an automated and
repeatable way the goal of devops is to
increase an organization's speed when it
comes to delivering applications and
services many companies have
successfully implemented devops to
enhance their user experience let's say
for example Facebook's mobile app which
is updated every two weeks effectively
telling users you can have what you want
and you can have it now ever wondered
how Facebook was able to do this so
smoothly it's the divorce philosophy
that helps Facebook ensure that its apps
aren't outdated and that its users get
the best experience Facebook
accomplishes this through a code
ownership model that makes its
developers responsible that includes
testing and supporting through
production and Delivery for each kernel
of code they write and update
extra policies like this that Facebook
has developed a divorce culture and has
successfully accelerated its development
life cycle Industries have started to
cure up for the digital transformation
by shifting their needs to weeks and
months instead of yours while
maintaining high quality as a result we
will soon see that devops Engineers have
more access and control of the end user
than any other person in the Enterprise
[Music]
let's understand one major concept of
devops that is the devops life cycle so
if you are into devops from quite some
time now you must have heard at least
one of these devops faces that is
continuous development continuous
integration continuous testing
continuous deployment and continuous
monitoring so all of these phases that I
just mentioned make up the devops life
cycle the first phase is the continuous
development phase so this is the phase
that involves planning and coding of the
software so the vision of the project is
decided during the planning phase and
the developers begin developing the code
for the application there are no devops
tools that are required for planning but
there are a number of tools for
maintaining the code the code can be
written in any language but it is mainly
maintained by using Version Control
tools so here maintaining the code is
referred to a source code management
so some of the most popular version
control tools are git SVN Mercurial CVS
Etc also tools like ant Maven Gradle can
be used in this phase for building or
packaging the code into an executable
file that can eventually be forwarded to
any of the next phases
the next phase in the devops life cycle
is continuous testing so this is the
stage where the develop software is
continuously tested for bugs for
continuous testing automation testing
tools like selenium test NG junit Etc
are used so basically the tools that I
just mentioned right now I love quality
assurances to test multiple code bases
thoroughly in parallel to ensure there
are no flaws in the functionality so
what automation testing does is it saves
a lot of time effort and labor for
executing the test instead of doing this
manually right also in this phase Docker
containers can be used for simulating
the test environment selenium here does
the automation testing and the reports
are generated by test NG so this entire
testing phase can be automated with the
help of a continuous integration tool
called Jenkins we'll talk about Jenkins
later on in the session
so now let me explain you this with an
example suppose you've written a
selenium code in Java to test your
application now what you can do is you
can build this code using Anto Maven and
once the code is built it is tested for
user acceptance testing so this entire
process can be automated using Jenkins
besides that the report generation is a
very big plus point the task of
evaluating the test cases that failed in
a test Suite gets simpler we can also
schedule the execution of test cases at
predefined times once the entire testing
is done the code is continuously
integrated with the existing code the
third phase of this life cycle is
continuous integration so now this stage
is the heart of the entire devops life
cycle it is a software development
practice in which the developers require
to commit changes to the source code
more frequently this may be on a daily
or a weekly basis so every comment is
then built and this allows early
detection of problems if there present
building code does not involve
compilation but it also includes code
review unit testing integration testing
and packaging the code that is
supporting new functionality is
continuously integrated with the
existing code since there is continuous
development of software the updated code
needs to be integrated continuously as
well as smoothly with the systems to
reflect changes to the end users as I've
already said in the previous slide
Jenkins is a very popular tool used in
this phase so whenever there is a change
in the git repository Jenkins simply
fetches the updated code and It prepares
a build of that code which is in an
executable file so once this entire
build is performed it is then forwarded
to the test server or the production
server now let's move on to the fourth
phase of the cycle that is the
continuous deployment phase so this is
the stage where the code is deployed to
the production servers it is very
important to ensure that the code is
correctly deployed on all the servers
now before moving on let us understand a
few things about configuration
management and containerization tools
now these set of tools help in achieving
continuous deployment firstly I will
talk about configuration management it
is basically the act of establishing and
maintaining consistency in an
application's functional requirements
and performance so let me put this in
simple words it is the act of releasing
deployments to servers scheduling
updates and most importantly keeping the
configurations consistent across all the
servers since the new code is deployed
on a continuous basis configuration
management tools play an extremely
important role in executing tasks
quickly and frequently now some popular
tools that are used here are puppet chef
solstack and ansible
containerization tools also play an
equally important role in the deployment
stage Docker is one of the most popular
tool used for this purpose basically
these tools help produce consistency
across development test staging and
production environments besides this
they also help in scaling up and scaling
down of instances very swiftly basically
containerization tools help in
maintaining consistency across the
environments where the application is
deployed developed and tested using
these tools there is no scope of errors
or failures in the production
environment as they package and
replicate the same dependencies and
packages used in the development or
testing or staging environment basically
it makes your application easy to run on
different computers
the next stage is the continuous
monitoring stage this is the last stage
in the devops life cycle this is a very
crucial stage where you continuously
monitor the performance of your
application so here Vital Information
about the use of the software is
recorded this information is processed
to recognize the proper functionality of
the application the system errors such
as low memory server not reachable Etc
are resolved in this phase the root
cause of any issue is determined in this
phase it also maintains security and
availability of the services
also if there are any network issues
they are resolved in this phase that is
it helps us automatically fix the
problem as soon as they're detected
so now this practice involves the
participation of the operations team who
will monitor the user activity for bugs
or any improper behavior of the system
the most popular tools used for this
phase is Splunk El key stack negios New
Relic and sensual so what these tools do
is they help you monitor the
application's performance and the
servers very closely and they also
eventually enable you to check the
health of the system proactively now
using these tools they can also improve
productivity and increase the
reliability of the systems which in turn
reduces it support costs any major
issues if found are reported to the
development team so that it can be fixed
in the continuous development phase this
also leads to a faster resolution of the
problems the stages are carried out on
Loop continuously till you achieve the
desired product quality therefore almost
all of the major it companies have
shifted to devops for building their
products
moving on if you want to become a devops
engineer or be a part of this ideology
it is extremely important for you to be
familiar with some devops related tools
today I will be talking about seven
important devops tools now before we
understand the tools devops Engineers
use it is important to understand
containers so what are containers
containers allow you to package your
application and its dependencies
together into one manifest that can be
version controlled allowing for easy
replication of your application across
developers on your team and machines in
your cluster this is exactly why
container-based microservices
architectures have profoundly changed
the way development and operation teams
test and deploy software containers
really help companies modernize by
making it easier to scale and deploy
applications but they have also
introduced new challenges in complexity
by creating an entirely new
infrastructure ecosystem
it companies are now deploying thousands
of container instances daily and that's
a complexity of scale they have to
manage how do they do it here is where
kubernetes comes to the rescue
kubernetes is a container management
technology developed in Google to manage
containerized applications in different
kinds of environments like physical
virtual Cloud infrastructure Etc so it
really is just an open source system
which helps in creating and managing
containerization of applications its
container management responsibilities
include container deployment scaling and
descaling of containers and container
load balancing it is extremely important
to remember that kubernetes is not a
containerization platform it is a
multi-container management solution now
let's move on and discuss some of the
features of kubernetes the first one is
automated scheduling so kubernetes
provides an advanced scheduler to launch
containers on cluster nodes based on the
resource requirements and other
constraints while not sacrificing
availability the second feature is
self-healing capabilities so kubernetes
allows you to replace and reschedule
containers when nodes type it also kills
containers that do not respond to
user-defined health checks and does not
advertise them to clients until they are
ready to serve
the third feature is automated rollouts
and rollback so what kubernetes does is
it rolls out changes to the application
or its configuration while monitoring
your application Health to ensure it
does not kill all your instances at the
same time so if something goes wrong
with kubernetes you can simply roll back
the change the fourth feature is
horizontal scaling and load balancing
kubernetes can scale up and scale down
the application as per the requirements
with a simple command using a user
interface or automatically based on CPU
usage now let us understand the
architecture the kubernetes architecture
has two main components the first one is
the masternode the second one is the
worker node so the master node is
responsible for the management of
kubernetes cluster it is mainly the
entry point for all administrative tasks
they can always be more than one Master
node in the cluster to check for fault
tolerance
so there are few components in the
master node that I'd like to discuss the
first one is the API server so here the
API server is the entry point for all
the resd commands used to control the
cluster the second one is the controller
manager it is a Daemon that regulates
the kubernetes cluster and manages
different non-terminating control loops
the third one is the scheduler so the
scheduler schedules the task to worker
nodes so what it really does is it
stores the resource usage information
for each worker node the fourth one is
the etcd so the etcd is simple
distributed consistent key value store
so what it mainly is used for is to
share configuration and service
discovery now that we know how the
masternodes work let's move on to the
worker nodes
so these worker nodes contain all the
necessary services to manage the
networking between the containers
communicate with the masternode and
assign resources to the scheduled
containers
so the first component that we'll be
talking about in this worker node is the
docker container so Docker runs on each
of the worker nodes and runs the
configured pods the second one is the
cubelet the cubelet gets the
configuration of a pod from the API
server and ensures that the Discraft
containers are up and running the third
one is Cube proxy so Q proxy acts as a
network proxy and a load balancer for a
service on a single Walker node the
fourth one is pods so a pod is one or
more containers that logically run
together on nodes
with this we come to the end of the
First Tool in devops tool let's move on
to the second tool that is Docker so
what is a Docker Docker is simply a
containerization platform that packages
your application and all its
dependencies together in the form of
containers to ensure that your
application works seamlessly in any
environment so each application here
will run on a separate container and
will have its own set of libraries and
dependencies it must be really
convenient to move around these
containers right
so containers also ensures that there is
a process level isolation meaning each
application is independent of other
applications giving developers surety
that they can build applications that
will not interfere with one another now
the quality assurance team need not
install all the dependent software and
applications to test the code and this
helps them save lots and lots of time
and energy this also ensures that the
working environment is consistent across
all the individuals involved in the
process starting from development to
deployment so here the number of systems
can be scaled up easily and the code can
be deployed on them effortlessly there
are some basic concepts of Docker you
must be aware of that is Doc file Docker
image and Docker container so a Docker
image is created by the sequence of
commands written in a file called Docker
file when this Docker file is executed
using a Docker command it results into a
Docker image with a name
so finally when this image is executed
by Docker run command it will by itself
start whatever application or service it
must start on its execution
so I hope Docker file Docker images and
Docker containers are clear we'll
understand what is Docker Hub now so
Docker Hub is basically a cloud registry
where you can find Docker images
uploaded by different communities also
you can develop your own image and
upload on Docker Hub But first you need
to create an account to Docker hub
so let's move on and understand what a
Docker engine is so the docker engine is
the heart of the system it is simply the
application that is installed on your
host machine basically it works like a
client server application it uses a
server which is a type of a long running
program called a Daemon process it also
uses a command line interface client and
finally it uses an rest API which is
used for communication between the CLI
client and Docker daemin as per the
image in a Linux operating system there
is a client which can be accessed from
the terminal and a host which runs the
daemin we build our images and run
containers by passing commands from the
CLI client to the daemin however in the
case of Windows or Mac there is an
additional toolbox component inside the
docker host so this Docker toolbox is an
installer to quickly and easily install
and set up a Docker environment on your
windows or your Macbook so this toolbox
installs Docker client machine compose
which is only present in mac and
virtualbox with this we come to the end
of Docker we'll move on to the third
tool of devops tools that is git so git
is an extremely important tool for
software developers it is a free open
source distributed Version Control
System designed to handle everything
from small to very large projects with
speed and efficiency it was created by
lennis travels in 2005 to develop Linux
kernel git has the functionality
performance security and flexibility
that most teams and individual
developers need it also serves as an
important distributed Version Control
devops tool so git is primarily used to
manage your project comprising a set of
code or text files that may change over
time
now before we go further let us take a
step back to learn about Version Control
Systems and how git came into existence
so Version Control is the management of
changes to documents computer programs
large websites and other collection of
information there are basically two
types of Version Control Systems the
first one being centralized Version
Control System the second one is
distributed version control system so
git is a distributed Version Control
System where every contributor that is
the user has a local copy or clone of
the main repository what is the
repository a repository is simply a
storage space for your project files
here everyone maintains a local
repository of their own which contains
all the files and metadata present in
the main repository so the local
repository is usually on the local
machine of the user so every programmer
maintains a local repository which is
actually the copy or clone of the
central repository on the hard drive the
central repository may be present on a
web hosting platform like GitHub they
can commit and update the local
repository without any interference they
can also update their local repositories
with new data from the central server by
an operation called pull and effect
changes to the main repository by an
operation called push from the local
repository so I hope everyone understood
how git works now moving on let us
understand the role of get in devops so
give plays a very vital role when it
comes to managing the code that the
collaborators contribute to the shared
repository so the score is then
extracted for performing continuous
integration to create a build and test
it on the test server and eventually
deploy it to the production tools like
get enable communication between the
development and the operations team when
you're developing a large project with a
huge number of collaborators it is
extremely important to have
communication between the collaborators
while making changes within the project
commit messages and get paid a very
important role in communicating among
the team the bits and pieces that we all
deploy lies in the version control
system like get to succeed in devops you
need to have all of the communication in
Version Control hence git plays a very
vital role in succeeding at devops now
let's move on to the next tool that is
selenium selenium is one of the most
widely used open source web user
interface automation testing Suite it
was originally developed by Jason
Hawkins in 2004 as an internal tool at
thoughtworks so selenium supports
automation across different browsers
platforms and programming languages so
this tool can be easily deployed on
platforms such as Windows Linux Solaris
and Macintosh moreover it supports
operating systems for mobile
applications like Windows and Android
selenium supports a wide variety of
programming languages through the use of
drivers specific to each language
language is supported by selenium
include c-sharp Java Perl PHP Python and
Ruby currently selenium Webdriver is the
most popular with Java and c-sharp
selenium test strips can be coded in any
of the supported programming languages
and can be run directly in most modern
web browsers browsers supported by
selenium include Internet Explorer
Mozilla Firefox Google Chrome and Safari
so basically it can be used to automate
functional tests and can be integrated
with automation test tools such as Maven
Jenkins and Docker to achieve continuous
testing it can be also integrated with
tools such as test NG and junit for
managing test cases and generating
reports so now moving on what are the
advantages of selenium why should we
choose selenium since selenium is open
source there is no licensing cost
involved which is a major advantage over
other testing tools there are many other
reasons behind selenium's ever-growing
popularity the first one is that test
scripts can be written in any of the
programming languages that is Java
python C sharp PHP Ruby pearl.net Etc
desk can also be carried out in any of
the operating systems that is Windows
Mac or Linux test can also be carried
out using any browser that is Mozilla
Firefox Internet Explorer Google Chrome
Safari Opera we've all discussed this in
the previous slide
and the last use is that it can be
integrated with Maven Jenkins and Docker
to achieve continuous testing
so in short selenium can be used for
automated testing also because it has
cross browsers compatibility it is more
preferred and it also increases test
coverage and reduces test execution time
now moving on let us understand an
important Concept in selenium that is
the selenium grid selenium grid was
developed by Patrick lightbody and
initially called it hosted QA it was
used in a combination with remote
control to run tests on remote machines
in fact with grid multiple test scripts
can be executed at the same time on
multiple machines parallel execution is
achieved with the help of Hub node
architecture one machine will assume the
role of Hub and the others will be the
nodes Hub controls the test scripts
running on various browsers inside
various operating systems test scripts
being executed on different nodes can be
written in different programming
languages grid is still in use and works
with both Webdriver and remote control
so we'll be learning about the Webdriver
in the next slide however maintaining a
grid with all required browsers and
operating systems is a very huge
challenge for this there are multiple
online platforms that provide an online
selenium grid that lets you access to
run your selenium automation scripts for
example you can use Lambda test it has
more than 2000 browser environments over
which you can run your test and truly
automate cross browser testing now let's
understand what is selenium Webdriver
selenium Webdriver is a webis automation
testing framework that can test web
pages initiated on various web browsers
and various operating systems in fact
you also have the freedom to write test
scripts in different programming
languages like Java Perl python
Etc it is important to remember that
Mozilla Firefox selenium webdriver's
default browser now some benefits of
selenium Webdriver are that they support
seven programming languages they test on
various browsers and they work on
different operating systems also it
overcomes limitations of selenium
version 1 like file upload download
pop-ups and dialogues period now let's
move on to the next most important tool
that is Jenkins
continuous integration is the most
important part of devops that is used to
integrate various devops stages
Jenkins is the most popular continuous
integration tool it is an open source
automation tool written in Java with
plugins built for continuous integration
purposes
Jenkins is basically used to build and
test your software projects continuously
making it easier for developers to
integrate changes to the project and
making it easier for users to obtain a
fresh build it also allows you to
continuously deliver your software by
integrating with a large number of
testing and deployment Technologies
so basically with Jenkins organizations
can accelerate the software development
process through automation Jenkins
integrates development lifecycle
processes of all kinds including build
document test package stage deploy
static analysis and much more so this
tool also achieves continuous
integration with the help of plugins
plugins are extremely important it
allows the integration of various devops
stages if you want to integrate a
particular tool you need to install that
particular plugin for that tool for
example git Maven to project Amazon ec2
HTML publisher Etc there are many
advantages of Jenkins and I'd want to
discuss each one of them in detail so
the first one is it is an open source
tool with a great Community Support the
second one is it is extremely easy to
install
the next one is it has more than
thousand plugins to ease your work now
if a plugin does not exist you can
simply code it and share it with the
community
the fourth Advantage is it is free of
cost the last one is it is built with
Java now if you have been coding for
some time now you must be knowing that
Java is portable to all the major
platforms so this gives an added
advantage to Jenkins
so what makes Jenkins different from the
other continuous integration tools there
are two reasons the first one is
adoption the second one is plugins so
Jenkins is widespread with more than one
lakh 47 000 active installations and
over 1 million users around the world
the adoption of Jenkins is extremely
high the second one is that Jenkins is
interconnected with well over thousand
plugins that allow it to integrate with
most of the development testing and
deployment tools so these two features
that is adoption and plugins make it one
of the best continuous integration tools
currently available in the market
now let's move on to the next tool of
devops tools that is puppet today the
most mature tool for configuration
management is puppet but I know you must
be wondering why puppet is so popular
and what makes it unique when compared
to other configuration management tools
puppet is a configuration management
tool that is used for deploying
configuring and managing servers so now
let's understand the functions of puppet
so the first one is it helps in defining
distinct configurations for each and
every host and it also continuously
checks and confirms whether the required
configuration is in place and is not
altered on the host the second function
is dynamic scaling up and scaling down
of machines the last one is it provides
control over all your configured
machines so a centralized change gets
propagated to all automatically puppet
basically uses a master sleeve
architecture in which the master and
slave communicate through a secure
encrypted Channel with the help of SSL
now let's see this architecture in
detail so as you can see the diagram on
the screen the puppet agent sends the
fax to the Puppet Master facts are
basically key value data pair that
represents some aspect of the slave
state such as its IP address uptime
operating system or whether it's a
virtual machine
Puppet Master uses the facts to compile
a catalog that defines how the slave
should be configured catalog is a
document that describes the desired
state for each resource that Puppet
Master manages on a Slave
puppet slave reports back to the master
indicating that the configuration is
finally complete which is visible in the
puppet dashboard
now with this we come to the last tool
of devops nag iOS
knock iOS monitors your entire it
infrastructure to ensure systems
applications servers and business
processes are functioning properly it is
an integral to the devops lifecycle so
it is basically used for continuously
monitoring of systems application
servers and business processors in a
devops culture so in the event of a
failure nag iOS can alert technical
staff of the problem allowing them to
begin remediation processes before
outages affect business processes end
users or customers with this tool you
don't really have to explain why an
unseen infrastructure outage affects
your organization's bottom line
now the iOS runs on a server usually as
a Daemon or a service it periodically
runs plugins residing on the same server
the contact hosts or servers on your
network or on the Internet one can view
the status information using the web
interface you can also receive email or
SMS notification if something happens
the NOG iOS Daemon behaves like a
scheduler that runs certain scripts at
certain moments it stores the results of
those scripts and will run other scripts
if these result changes
Naga iOS also use plugins so these are
compiled executables scripts that can be
run from a command line to check the
status or a host or a service nag iOS
also uses the results from the plugins
to determine the current status of the
hosts and services on your network
now the iOS is built on a server or
agent's architecture usually on a
network a nag iOS server is running on a
host and plugins interact with local and
all the remote hosts that need to be
monitored so these plugins will send
information to the scheduler which
displays that in a graphical user
interface with this we come to the end
of devops tools let's move on to the
next part of the session that is devops
automation
automation is the crucial need for
devops practices and automating
everything is the fundamental principle
of devops we discussed this initially in
the session
automation kickstarts from the code
generation on the developers machine
until the code is pushed to the code and
after that to monitor the application
and system in the production
automating infrastructure setup and
configurations software deployment is
the key highlight of devops practice
devops practice is dependent on
automation to make deliveries over a few
hours and make frequent deliveries
across platforms
Automation and devops boosts speed
consistency higher accuracy reliability
and increases the number of deliveries
Automation and devops encapsulates
everything right from the building
deploying and monitoring in large devops
team that meet an extensive massive I.T
infrastructure the tools can be
classified into six categories that is
infrastructure automation configuration
management deployment automation
Performance Management log management
and monitoring so for infrastructure
automation a great example is the Amazon
web services being a cloud service you
don't really need to be physically
present in the data center they're easy
to scale on demand and there are no
upfront Hardware costs it can also be
configured to provide more servers based
on traffic automatically for the second
one Chef is a great example Chef is a
handy devops tool for achieving speed
scale and consistency it can be used
these are complex tasks and perform
configuration management with the help
of this tool the devops team can avoid
making changes across 10 000 servers
rather they need to make changes in one
place which is automatically reflected
in other servers
for deployment automation Jenkins is a
great example as we have already
discussed before it helps facilitate
continuous integration and testing it
integrates project changes more
efficiently by quickly finding issues as
soon as bills are deployed
for the fourth one that is Performance
Management application Dynamic is a
great example it offers real-time
performance monitoring the data that is
collected by this tool help developers
to debug whenever an issue occurs
the fifth one that is for log management
Splunk is a great example this devops
tool solves issues such as storing
aggregating and analyzing all logs in
one place
so for the last one that is monitoring
now iOS is a great example it notifies
people when infrastructure and related
Services go down it is a tool for this
purpose which helps the devops team to
find and correct problems immediately
[Music]
let's start out with what's Version
Control so to understand what is Version
Control System we need to understand
first why we need a version control
system so we'll see why do we exactly
need a version control system so for
that let's take a scenario where there
are three developers working remotely on
a web application or a mobile
application now for simplicity's sake
let's just assume that it is a streaming
application and one of them is working
on a streaming page let's call them
developer one developer 2 is working on
a user information page and developer 3
is creating the payment portal now all
three of them are done with their
respective Pages at 10 o'clock 11
o'clock and 12 o'clock respectively now
thing is all three developers are
working in isolation there may be their
own changes there may be changes in
somebody's else's page they're all kind
of adding some new files or modifying
older files changing the source code or
something like that throughout the
course of this project
but how exactly are they going to
collaborate considering that they are
working remotely the solution to this is
a version control system with this
system I'll tell you what happens at 10
o'clock developer 1 uploads all the
files regarding the streaming page with
the Version Control System this
modification is recorded and updated to
one Central directory or folder so every
time you're modifying an older file or
adding a new file it creates a snapshot
of the latest version or the latest
update of the changes that you've made
or all the files that you've uploaded so
the snapshot of your update at 10
o'clock is saved and then when developer
2 adds their user info page this
modification is also saved as a snapshot
now there are two snapshots saved and
finally the payment portal page is added
and this is the last modification done
at 12 o'clock and this is the most
recent snapshot saved along with other
snapshots of other updates and
modifications done throughout the course
of this project every change in your
project a snapshot is created and your
entire project is saved that way and
these snapshots are actually known as
different versions which are basically
the state of your project at a
particular time or at the current time
it means that it will contain the kind
of files your project is storing at one
particular time and what kind of changes
you have made so far between the
previous version and this particular
version and this is exactly how a
version control system works so Version
Control System basically is a system
that records changes to a file or a set
of files over time so that you can
record specific versions later these
versions are recorded in a repository
and they can be called any time from
these repositories during the course of
your project
so Version Control Systems essentially
are of three types now the local version
control system is one of the simplest
forms of VCS and has a database that
keeps all the changes of the files under
revision control so all the files are
saved by this one user and they manage
all of the files but it's actually
really hard to maintain the changes of
the file and you can accidentally
replace one or the other file that you
need it also needs something known as
RCS or revision control system which
keeps patch sets or the differences
between updates in a special format on
your disk all of this your local Version
Control Systems maintains all the track
of all the files within your local
system as a user you can track
specifically our versions later and it
basically works as an independent
Standalone system for an application so
your applications like spreadsheets and
word processors have this control
mechanism so that's one type apart from
this you also have your centralized
version control system which basically
uses a central server to store all the
files and enables team collaboration so
it's not one local system but multiple
systems which work on a single
repository to which users can directly
access as a central server so the
repository is a central server that
could be local or remote which is
directly connected to each of the
programmers workstation and the Third
Kind of VCS is a distributed version
control system now these systems do not
necessarily rely on a central server to
store all the versions of a project file
in a distributed Version Control System
every contributor has a local copy or a
clone of the main repository so everyone
maintains their own local repository
which contains all the files and the
metadata present in the main repository
and they can push or come commit or
update the main repository if and when
required now we are going to cover more
on the latter two Version Control
Systems further in the session
so now let's try and understand in depth
what's the difference between a
centralized version control system and a
distributed version control system as I
had mentioned before in a centralized
Version Control System each programmer
working on a branch on a project has
absolute access to the central server so
the centralized version control system
uses a central server to store all the
files and everybody on the team can
directly access the central server as it
is one single repository so if you have
three workstations workstation one
workstation 2 workstation 3 all of them
can directly commit to the repository
and get updates from the repository
which is your central server the
repository here indicates a server that
could be local or remote directly
connected to each programmer's
workstation every programmer can extract
or update their workstations with the
data present in the report repository or
can make changes or updates or commit in
the Repository
and every operation is performed
directly on this repository even though
this seems pretty convenient when we are
just talking about three people to
maintain a single repository like this
has some major drawbacks one of them is
that it's not locally available meaning
every change that you have to make you
will have to be connected to a network
to perform any action also in any case
the central server getting crashed or
corrupted will result in losing the
entire data of the project also another
small thing which is revertible but is
if one person accidentally makes some
unwanted updates instead of his or her
local system they will be making that
changes directly in the central server
so recovering that part back will be
again a time consuming process this is
where distributed Version Control
Systems come to the rescue now these
systems do not necessarily rely on a
central server to store all the versions
of a project file in the distributed
Version Control System each and every
contributor has a local copy or a local
workstation clone of the main repository
that is everybody maintains a local
repository of their own which contains
all the files and metadata present in
the main repository you will understand
it better if you take a look at the
diagram as you can see every programmer
maintains a local repository or a local
copy on its own which is actually the
copy or clone of the central repository
on their local hard drive they can
commit and update their local repository
without any interference from one
another and then they can update their
local repositories with new data from
the central server by by an operation
called pull once they are done with
their bit of the code they can affect
the changes of the main repository by an
operation called push from their local
Repository
now this system or this act of cloning
an entire repository to your workstation
and making all the changes on a local
repository gives you a couple of
advantages over a centralized Version
Control System first of all all the
operations except push and pull are very
fast because the tool only needs access
to the hard drive and not a remote
server hence you do not always need to
be connected via a network connection or
you always do not need an internet
connection committing new change sets
can be done locally without manipulating
the data on the main repository once you
have a group of change sets ready you
can push them all at once so you
complete your set of code on your local
system and then you push all the files
once and for all into the main
Repository third since every contributor
has a full copy of the project
repository they can share changes with
one another on wanting to get some
feedback before affecting the changes in
the main repository so this enhances the
essence of collaboration amongst a
development team and finally if the
server gets crashed at any point in time
the loss data can be easily recovered
from one of the contributors local
repository and that is why distributed
version control system has been picking
up popularity over the past decade so
why version controller is important
first of all it helps collaboration it
allows remote development shared
workspace and real-time updates
all versions of your code are preserved
hence it helps you manage versions of
the same code easy rollback from current
version suppose a part of your project
has been headed to a wrong direction you
can easily roll back from the current
version to the last version which was
stable now because you can reverse
faulty updates you can save time and
hence it reduces downtime of your
project development and finally it
improves visibility you can analyze and
compare different versions and that
gives you like a 50 foot top view of
your entire project which just
accelerates your product delivery so now
that you have a bare sense of what is
Version Control and why do you need
Version Control let's understand one of
the most popular version control systems
that exist today and that is git
so git is a distributed Version Control
tool that supports distributed
non-linear workflows by providing data
Assurance from developing quality
software so basically what it does is
that it lets you and your team of
developers work together on the same
project remotely from anywhere across
the world your team members can work
efficiently on files and easily merge
their changes into one source without
the fear of losing or deleting anything
of importance due to the easy rollback
feature it is primarily used to manage a
project comprising of a set of code or
text files that you may want to change
from time to time now git is an integral
part of devops devops as most of you
might have the idea of is the practice
of bringing agility to the process of
development and operations a couple
years ago it was this entirely
revolutionary ideology with swept the
organizations worldwide boosting project
life cycles and in turn increasing
profits devops promoted communication
between development engineers and
operations participating together in the
entire life cycle from design through
development process to production
support now in the entire life cycle of
devops starting from planning of the
projects to its deployment and
monitoring git plays a vital role when
it comes to managing the code that
collaborators contribute to the shared
repository this code is extracted for
performing continuous integration to
create a build and test it on the test
server and eventually deploy it to
production tools like git enable
communication between the development
and operation in Steam when you are
developing a large project with a huge
number of collaborators it is extremely
important to have communication between
the collaborators between the developers
while making changes in the project
project comment messages in git play a
very important role in this particular
collaboration or communication amongst
the team the bits and pieces that we
deploy or update lies in this version
control system such as git to succeed in
devops you need to have all the
communication in Version Control
hence git plays an extremely vital role
in succeeding ant devops and due to this
very reason git has earned way more
popularity compared to other version
control tools available in the market
such as Apache subversion concurrent
version systems and Mercurial if you
compare the interest of git buy time
with other Version Control Systems you
shall realize that larger companies
products are generally developed by
developers using git all around the
globe and some famous names out of them
are Facebook Yahoo Zynga quora Twitter
eBay Salesforce Microsoft and many many
more
now lately all of Microsoft's new
development work has been in git
features Microsoft migrating.net and
many of its open source projects on
GitHub which are managed by git one of
such projects is the light GBM it's a
fast distributed high performance
gradient boosting framework based on the
decision tree algorithms which is used
for ranking classification and many
other machine learning tasks here git
plays an important role in managing this
distributed version of light GBM by
providing speed and accuracy so
basically to enable Version Control git
is your go to solution it's fast and
suitable for handling massive code bases
scattered across multiple developers
which makes it the most popular tool
used today now that you know about git a
terminology we'll be using a lot while
talking about git and GitHub is a
repository now repository or repo as its
most commonly known as is a directory or
a storage space where your projects can
basically live it can be local to a
folder or your computer or it can be a
stored space in another online host such
as your GitHub and in this particular
space in this particular directory you
can keep your code files text files
images you name it all of it inside a
repository to the course of this we
shall also be talking about something
known as GitHub which is nothing but a
central repository the kind that we
spoke about when we discussed
centralized versus distributed Version
Control Systems there's a central
repository and that is where all of your
code will live
you have your local repository where
you'll make the changes and there you
have the central repository to which you
will push all of the changes yeah and
the central repository is something that
all of the developers involved in this
particular project in a certain project
have access to
all right moving on let's discuss a few
git features which make it so popular
amongst organizations as well as
individuals first of all it's economical
it's free and open source git is
released under GPS general public
licenses open source license so you
don't need to purchase Git it is
absolutely free and since it's open
source you can modify the source code as
per your requirement second is its speed
now since you do not have to connect to
any network for performing all of the
operations it completes all the tasks
really fast performance tests done by
Mozilla showed it was an order of
magnitude faster than other Version
Control Systems fetching version history
from locally sold repository can be 100
times faster than fetching it from the
remote server so the core part of git is
written in C which avoids runtime
overheads associated with other
high-level languages which makes it
extremely fast compared to other Version
Control Systems next is that git
supports non-linear development it
supports rapid branching and merging and
includes specific tools for
visualization and navigation of a
non-linear development history a
co-assumption in git is that a change
will be merged more often than it's
written as it is passed on various
reviewers hence branches in git are very
very lightweight and is only a reference
to a single Comet with a parental comets
the full branch structure can be
constructed next let's talk about the
robustness
nearly every single task in git is
undoable git gives each developer a
local copy of the entire development
history and the changes copied from one
repository to another these changes are
imported as additional development
branches and can be merged and deleted
and recovered the same way as a locally
developed Branch next is the snapshots
which are recorded changes made to a
file rather than the file itself
followed by which you have integrity
which means no changes can be made
without git recording it since every
contributor has their own local
repository on the events of a system
crash the loss data can be recovered
from any of the local repositories you
will always have a backup of all of your
files and none of the changes made to
the central repository will go unlocked
by git git uses the sha1 or the secure
hash function to name and identify
objects within its repository and every
file every comment is check summed and
retrieved by its checksum at the time of
the checkout the git history is stored
in such a way that the ID of a
particular version a commit in gets
terms depends upon the complete
development history leading up to that
Comet once it's published it's not
possible to change the old versions
without it being recorded then it's a
distributed system which means every
user has their own copy of their
repository and the data is stored
locally git gives each developer a local
copy of the entire development history
and the changes are copied from one such
repository to another these changes are
imported as additional development
branches and can be merged in the same
way as a locally developed branch and
finally it's easy branching now Branch
management with Git is very simple it
only takes a few seconds to create
delete and merge branches feature
branches provide an isolated environment
for each change to your code base so
when a developer wants to start working
on something no matter how big or small
they can create a fresh new branch which
is not interfering with the main branch
which ensures that the master branch of
the main branch always contains
production quality code so any little
experimentation that you want to do or
little things that you want to try out
can always be on a separate feature
Branch instead of the changes made to
your master Branch because ultimately
your master Branch should contain your
publishable product so now that you know
the basic features of git let's try and
understand what the basic workflow of
git is or how does git work so the basic
overview of how git Works goes like this
you basically create a repository or a
project with a git hosting tool like git
orbit bucket you copy or clone the
repository to your own local machine or
your working directory you add a file to
your local repository and commit the
changes which basically means you save
the changes then you push the changes to
your master Branch then you make changes
to your file and commit then you pull
the changes to your local machine you
create a branch or a version make a
change commit the change same process
open a pull request and merge your
branch to the master Branch now this is
a little complicated listening to it
like this but once we get on to our
demonstration I think things will be way
more clear you can still take a good
look at this diagram
but if you don't get it don't worry I
will be explaining these operations one
by one when we get to the demonstration
part of this git tutorial moving on
let's get to the good part the Hands-On
section
starting with the installation and setup
of git now there are two ways in which
this goes first of all for Windows users
you need to install git bash for Windows
and for Linux users you'll have to start
by updating the package index and then
install git using the terminal
so first of all for our Learners using
Windows you'll have to log on to get
scm.com
downloads and then click on downloads
for Windows button and then run the
executable file and follow through till
you finally install git Bash
I'm not going to install it completely
as I already have git bash downloaded
but I'm still going to open the page to
show you guys all right so the latest
Source release is get
2.30.1 on the 8th of Feb and you can
just click on this button the moment you
click on this button it's going to start
downloading it to you and once you have
the executable file all you have to do
is open it or run it in your own system
right
next for people using Linux this is an
example considering you are using Ubuntu
if you're using Centos a lot of the
commands don't change but instead of Apt
install you will be using yum install
for sentos so for that now I'm going to
open my terminal on a virtual machine so
here I have running an Ubuntu machine
using the oracle virtualbox
so I'm gonna start by
running
an update
all right this is going to take a while
kindly be patient
so here I have my Ubuntu running on a
virtual machine
so to install git on your Ubuntu machine
you have to start by updating so this
will take some time
if you're going to use sudo of Apt
update if you're using Centos you will
be using yum instead of Apt
let's scale that out and
then we're going to install git
and then you can look for the Git
Version
yeah all right now that we have our git
installed let's go ahead and look at the
operations and commands that we are
going to take a look at today you have
your repository set up which include the
commands init clone config and Alias
then you have your save changes
operations which have your add commit
diff and stash all of these commands we
are going to go on and off and look at
collectively mostly then we are going to
look at the inspect operations your
inspect repository operations of which
the git status command is something we
are also going to look at throughout
this demonstration right from the
beginning so apart from that you have
your kit log git tag and get blame
commands so you have your merge rebase
command
so let's move on to our first set of get
operations first of all you have your
git init command which basically creates
a new empty git repository now this can
be used to convert an existing inversion
project to a kit repository or initiate
an absolute fresh git repository now
most of the git commands are not
available outside of the initialized
repository so this is usually the first
command that you will run in a new
project that is if you're using git Bash
so when I run git in it it
re-initializes an existing git
repository which I already have here now
what this does is it
re-initialized my existing git
repository in this particular directory
in this dot get directory yeah what git
init does is that it creates an empty
git repository or re-initializes an
existing one like it did for me right
now it basically creates a DOT get
directory with subdirectories and
template files and when you run the git
init command in an existing repository
it will not overwrite the things that
are already there it'll rather pick up
the newly added templates
so that's the repository initialized you
can go ahead and create some files in
the directory or repository whatever you
call it now that is about initialization
now what about if there was an existing
repository on git that you had to act
upon that you had to make updates to
that you had to add files to things like
that
that's where you use the git clone
utility yeah now you have the git clone
utility which you use to Target an
existing repository and create a clone
or copy of this particular repository on
your local machine now this is one of
the features provided to you by a
distributed version control system as we
had discussed before so this is a blank
file called edureka rep with just the
readme text file let me just go ahead
and
copy this
and then I can just
get clone and paste its
https URL
and as you can see it's cloning this
into a Eureka rep
and now if I change directory
to edit a wrap
you can see the readme file is right
there yeah
so we're basically going to work off of
this particular repository today at
Eureka rep yeah so now that we created a
clone moving on we have in the git
config command which basically is a
convenience function that is used to set
up your git configuration values on your
Global or local project level this
command is basically used to modify your
configuration text file
so if I typed get config
globaluser.email and here
by type
so there I given my email ID as the
global user email ID so if I ask for
what the global user email ID is it will
return my ID back to me yeah
basically these configuration levels
like a username and your user ID
correspond to your dot get config text
files now executing this basically will
help you modify the configuration text
file git uses this series of
configuration files to determine
non-default behavior that you may want
yeah so the first place git looks for
these values is in the system-wide
configuration file which contains
settings that are applied to every user
on this system so basically if you pass
the option to your git config it will
read and write from this file
specifically and the next place the git
looks is the config slash git config
file which is specific to each user now
you can make git read or write to this
file by passing your Global option which
is what I did
and the final command in the setup
operations that we are going to look at
is an alias now Alias as the name
suggests is basically used to create
shorter commands that map to longer
commands they enable more efficient
workflows when you obviously have to use
lesser keystrokes to execute a command
there are these lengthy commands which
obviously kill a lot of your time so
these aliases you can use to shorten
those commands and these are also
something that you will create via the
git config command there isn't really a
direct way or a direct command of
creating aliases yeah this is just
something I wanted to add in here along
with the git config command like a very
common Alias created is co for checkout
or anything else EK for checkout so yeah
this is something that you can do using
the git config command just creating
aliases moving on let's look at
operations that you use to add changes
and save changes to your git Repository
now the first two commands here that
we're going to look at are git add and
git commit now this combination is
basically used to create these snapshots
that I was talking about initially in
this session right so basically what you
do is first you use the git add command
to add a change in your current
directory or your working directory and
you add that to the staging area it
tells git that you want to include a
particular set of updates or changes in
a particular file in the next comment
then what you do is you move on to git
commit after making the changes to your
working tree then you use the git commit
command which captures a snapshot of
your Project's current state
so basically you use add to update the
index using the current content found in
your working tree and then you basically
prepare the content in the staging area
for the next commit then you run the
commit command to take that snapshot of
your Project's current status now
remember this is very important and you
have to do it each time you must use the
add command to add any new modified
files to the index
you can't directly commit anything
without adding it to the stage area
first
now what I did was I quickly went ahead
and added three files to my repository
at Eureka rep so
if I go into this repository you can see
there are three files at Eureka file one
file two file three now let's see if
these files are in my index or not using
the command get status now git status
basically displays the state of the
working directory and the staging area
it will list all the modified files
which are ready to be added to the local
repository it lets you see which changes
have been staged and which haven't which
files are being tracked by get and not
so if I typed get status
you can see on your main branch
you have three untracked files which are
the three files that I just added and
during a file one two and three
these are not added to the index yet
this means I cannot commit these changes
unless I added them explicitly to the
index
so now we are going to add these three
files this command will update the index
using the current content which is found
in your working tree as you can see in
front of you and then it will prepare
the content in the staging area for the
next comment
so either I can just get add and put in
the directory name or I can just use git
add with an extension of a which means
all and this will add all of these files
to the index which are in the directory
but not updated in the index yet yeah
now I've done that let me go ahead and
use git status again and you can see all
of these file names are written in green
and you can see changes to be committed
written here which means all of these
changes are ready to be committed they
are added to the index now these files
are added to the index you are ready to
commit them
now as I had mentioned before we're
going to commit them and committing
refers to recording the snapshots of the
repository at any given time so your
committed snapshots will never change
unless it's done explicitly so I'm going
to clear this and we're going to come at
this by typing git Comet and then adding
a tag or a message called
committing
three files
let's try this out as you can see three
files change three insertions you can
see our add Eureka files one two three
have been committed the git comment
command has committed the changes in the
three files in your local repository now
if you want to commit a snapshot of all
of the changes in your working directory
at once you can always go with commit
hyphen a now let me add a couple more
files to my directory just to show you
the difference between your files added
to the index versus the ones that are
not
if I type LS again you can see there are
two more files over here
as you can see I have two other files
I'm just going to add
the edureka file 4 and not the file 5
and then I'm just gonna get commit
commenting files
and as you can see only one file changed
only one insertion happened and that
only happened because
edureka file 5 is not added to the index
and even if I use the extension a I
cannot add edureka file 5 because that
has not been added to your index
now if I just go ahead and add I didn't
make a file 5
voila
the change has occurred in our git
Repository now if I typed git status
there are no more files to add or commit
our branch is ahead of the origin or
main branch by three commits and you can
use git push to publish your local
commits now what is git push this is
something we will cover later in this
session
Now understand that before you affect
the changes made to the central
repository you should always pull
changes from the central repository to
your local repository just to get the
most updated version of your branch
you'll get all the work of your
collaborators that have been
contributing in the central repository
for that you use the pull command now as
you can see there's nothing to commit
and you're working very slim
there are other commands like the git
status command which you could use for
similar purposes for example you have
the git log command which only operates
on committed history it basically will
display to you the committed snapshots
and will let you list the project
history filtrate and search for specific
changes so if you are typed git log it's
going to show you all the comments that
have happened since we started with this
git file okay here you can see at the
bottom your original main there is your
initial commit right then our first
comment where we committed three files
then we committed the fourth file and
finally when we committed file number
five
right these are all the comments
including the initial commit the first
one yeah along with the author and the
date and time remember when I told you
about everything is recorded when you're
using git this is what I meant you will
can always access who made comments at
what time and what comments were made
when you use a Version Control System
such as git now at a point like this I
think it's crucial to mention uh one
more command which is the git tag and
it's used to basically capture a tag in
history or a point in history that is
used for a marked version release right
so it's basically uh like a branch that
doesn't change and after being created
they have no further history of comets
yeah so that was one and another one is
git blame now the get blame function
displays of the author's metadata
attached to a specific committed line in
a file it's used to examine specific
points in a files history and get
context as to who the last author was
that modified that line so if I went
back and typed get blame
[Music]
a Eureka file 5 dot txt
it's going to show who the person is
that is to blame who committed
this particular file or made this
particular change right it'll also if
you paid attention give you the commit
number as you can see this E9
d63ef0 comment number
this also has this E9
d63ef0 which is the first few characters
of this particular comment number yeah
so now that you know how to check the
status of the changes and see who made
the changes it's time that you
understand how you fetch changes from
your remote repository to a local
repository and how you push comments
from your local repository to your
remote repository so here we are going
to look at two commands called pull and
push
now first of all you have the git pull
command which fetches changes from your
remote repository to a local repository
basically it merges Upstream changes in
your local repository which is a common
task in gitbase collaborations
but at first you need to set your
central repository as the origin using
the command
git remote add origin and I'm going to
add the link to my central repository
which is this
and as it says the remote origin already
exists
now that your origin is set and now
we're going to pull our main branch
so I'm just gonna type git pull origin
Main
and it says we are already up to date
now certain number of tutorials or
documents that you might be following
might even give you the command git pull
origin master in which case you're
working on the master Branch now to know
which branch you're at you can just use
a command git branch
will you be yeah I'll tell you which
branch are you working on yeah
so yeah we are working on the main
branch and we are already up to date
with the branch since my local
repository was already updated with
files from the main branch this is the
message of an up-to-date branch
now you can also like use git pull
origin
and your whatever Branch name here I'm
using Main
that's a mistake on my part maybe I was
trying to clear the screen all right so
here even if I pull from the branch
using get pull origin and the branch
name to my local get repository you can
see it's already up to date so now my
local git repository is now all updated
with the recent changes
so now it's time to make the changes to
your central repository by using the
push command
so we're going to use the push command
this command transfers commits from your
local repository to your remote
repository and it is basically the
opposite of your poll operations pooling
will import commits to your local
repositories whereas pushing will export
commits to your remote repositories
it's going to ask you for your
username
and password before it lets you push
into your main branch there so I've put
in the username and password for my
GitHub account and with that this has
pushed the changes from my local
repository to the remote repository
along with all the necessary commits and
internal objects now this has created a
local branch in your destination
repository the use of git push is to
publish your local changes to a central
depository after you've accumulated
several local comments and are ready to
share them with the rest of the team you
can then push them to your central
repository by using the get push command
now the files which have already been
committed previously in the comment
section and they were all push ready
you will use git push origin Master to
reflect these files in the master branch
of your Repository
all right
so let's go ahead to our repository and
see
if changes have been made and yes you
can see all of our files on our GitHub
repository here when you break our file
one two three four and five all the five
files that we had created on our local
repository yeah you have all these five
files and the readme file here here also
you have the readme file and all the
five files that we had created
now at this point in this git tutorial I
hope you have understood the basic
commands of git you have pulling pushing
adding committing things like that now
let's take a step further and understand
the three basic commands of parallel
development here I'm going to be talking
about branching merging and rebasing so
first of all let's talk about merging
now merging is a way to combine the work
of different branches together this will
basically allow you to Branch off
develop a new feature and then combine
it back in as you can see in the diagram
it shows you two basically different
branches new branch and master now when
we merge the work of the new Branch into
the master it creates a new Comet which
contains all the work of the master and
New Branch
now if we merge the two branches with
the merge command
as I had mentioned before git allows you
for easy branching which means you have
parallel branches with your master
branch that help you carry on your
individual work while not interrupting
your main product
which is your master Branch or your main
branch so branches in git are nothing
but pointers to specific comments yeah
so git generally prefers to keep its
branches as lightweight as possible now
there are basically two kinds of
branches we are local and remote
tracking branches a local branch is just
not the path of your working tree on the
other hand the remote tracking branches
have special purposes
some of them are linking your work from
your local repository to your central
repository and automatically detecting
which remote branches to get changes
from when you do a git pull so you can
check what your local branch is by using
get
branch
so you know you are at your main branch
and the one thing you should understand
one thing is like a main Mantra of
people who use git are Branch early and
Branch often yes so to create a new
Branch you're going to use a command
get
branch and your branch name and this
case I'm just going to use a branch name
at Eureka images all right and here you
can see this diagram shows the workflow
of a new branch that is created when we
create a new Branch it originates from
the master Branch itself so since there
is no storage or memory overhead with
making many branches it's usually easier
to logically divide your work up than
having big chunky branches hence the
branch often part
now let's see how to commit using
branches now branching includes the work
of a particular commit along with all
parent comments right so as you see the
new branch has detached itself from the
master or the main branch and hence
it'll create a different path
so we are going to
get
check out the branch name which in our
case is edureka images
and then we are going to get commit
so you have on track files and Eureka
images and nothing added
so you made that branch
now
merging is a way to combine the work of
different branches together so basically
what this does is now that we've
branched off to something known as
edureka images you can go ahead add
files to it develop a new feature and
then combine it back into your main
branch here you can see two different
branches you have your new branch and
your main branch and when we merge the
work of a new Branch into the main
branch it creates a new comment which
contains all the work of the main branch
and the new Branch so now if we merge
this Branch as you can see it's already
up to date it's important to know that
the branch name here should be the
branch that you want to merge into the
branch that you are currently checking
out so make sure that you are checked
out of this destination Branch so let's
just merge all the work of the branch at
Eureka images into your main branch for
that I will first check out my main
branch
and then merge at Eureka images with kit
merge
edureka images
and as you can see all the data from the
branch at Eureka images has merged into
the main branch
now notice that merging in Gate creates
a special Comet that has two unique
patterns now this is one way of merging
into the main branch there's also
another way of combining the work
between two branches which is called
rebasing now rebasing takes a set of
comets copies them and stores them
outside your repository the advantage of
rebasing is that it can also be used to
make linear sequence comets the comet
log or the history of the repository
stays clean every basing is done now our
work from the new branch is placed right
after master and we have this nice
linear sequence of comments this command
doesn't copy but moves all our work from
the current Branch to the master or the
main branch they look like they've been
developed sequentially instead of having
developed parallely
so if I just did another
branch called git Branch at Eureka
images 2. and then maybe get checkout
okay then we get Commit This
and then I could just get rebase min and
your current Branch will be up to date
this command basically moves all of your
work from your current Branch to your
master branch and it'll basically look
like it has been developed sequentially
instead of having developed parallelly
now apart from these there are certain
operations that you ought to know
firstly archiving your repository you
can use this command it stores all of
your files and data in a zip file rather
than a DOT get directory this creates
only a single snapshot omitting Version
Control completely though this comes in
handy when you want to send these files
to a client for review who doesn't have
git installed in their computers apart
from this you can also bundle your
repository which basically turns your
entire repository into a single file
this pushes the master Branch or the
main branch to a remote Branch only
contained in a file instead of a
repository you could alternately change
the repository clone the repo bundle
create a copy and use git log but that
is like five commands instead of one why
not just use one command and finally
stashing uncommitted changes when you
want to undo adding a feature or any
kind of added data temporarily we can
always stash them temporarily you can
use git status stash status and then you
want to reapply the changes you stashed
using git stash apply having said that
let's move on to another very important
topic in this git tutorial let's go
through a brief introduction to SSH now
what is ssh what is an SSH key now an
SSH key is an access credential for
secure shell Network protocol it's
basically an authenticated and encrypted
secure network protocol and is used for
remote Communications between machines
on an unsecured open network
for remote file transfer Network
management and remote operating system
access
so basically it's typically used to log
into a remote machine and execute
commands in it but this also supports
tunneling forwarding TCP ports and X11
conditions it can transfer files using
Associated SSH file transfer or secure
copy protocols
it provides you several alternative
options for strong authentication and
protects the communication security and
integrity with very strong encryption
it's a secure alternative to the
non-protected login protocols and
insecure file transfer methods such as
FTP so basically how do you create SSH
Keys you start by using the command SSH
hyphen Keygen
and then you'll be prompted to enter a
file in which to save the key
so home at Eureka
then we're going to enter
a passphrase I'm just going to leave it
empty again
and you can see our identification has
been saved in the file that we specified
and we have something known as a key
fingerprint
and finally what we have to do is add
the new SSH key to the SSH agent
and then you will see identity added
and that's it you have created an SSH
key
[Music]
the need for GitHub it is extremely
important for software developers to
work on a web-based platform to share
their projects and collaborate with
other developers this platform must be a
version control system that is it must
enable multiple people to simultaneously
work on a single project each person
edits his or her own copy of the files
and chooses when to share those changes
with the rest of the team this
application must also be capable of
Hosting millions of programmers and
hobbyists that download and evaluate
each other's work GitHub is one such
platform of choice for developers that
can host multiple programmers and review
their code
GitHub has several competitors for
instance git lab gitlab is an open
source web interface and Source control
platform based on git whereas Microsoft
team Foundation server is an Enterprise
grade server for teams to share code
track work and ship software for any
language all in a single package
bitbucket on the other hand stores all
of your git and Mercurial source code in
one place with unlimited private
repositories
so what really makes GitHub so powerful
and popular among Developers
GitHub is an open source platform and
the community is really what fuels it
moreover GitHub is the platform of
choice for developers from various large
corporations too Microsoft is the number
one contributor to the system but there
are also Google sap Airbnb IBM PayPal
and many others exposure and insight
that you can get on GitHub are simply
unmatched by any other platform here you
can discover code written by others
learn from it and even use it for your
own projects
versions control on GitHub works very
much like Microsoft Office or Google
drive it simply tracks all the changes
made to your code and who makes them you
can always review the detailed change
log that neatly hosts all of the
relevant information
using GitHub eliminates the need for
complex corporate security solution
because everything is on cloud the
platforms protects code branches
verifies commit signing and controls
access
now that we know why we need GitHub let
us understand what is GitHub GitHub is a
git repository hosting service that
provides a web-based graphical interface
with many features a repository is
usually used to organize a single
project repositories can contain folders
files images videos spreadsheets
anything your project needs let's say
for example a team wants to work on a
particular project here they can
simultaneously write and update the code
to a central repository which is present
on GitHub so GitHub is a highly used
software that is typically used for
Version Control it is helpful when more
than just see one person is working on a
project for example a software
development team wants to build a
website and everyone has to update their
codes simultaneously while working on
this project in this case GitHub helps
them to build a centralized repository
where everyone can upload edit and
manage the code files
most software projects have a bug
tracker of some kind github's tracker is
called issues and has its very own
section in every repository issues
basically are a great way to keep track
of tasks enhancements and bugs for your
project
moving on people often get confused
between the terms get and GitHub now let
me clearly explain the difference
between them
jit is simply a version control system
that lets you manage and track changes
within your project whereas GitHub is a
cloud-based service that lets you manage
git repositories so basically git is the
tool and GitHub is the service
now that we know the difference between
git and GitHub let us move on and
understand how these two work hand in
hand
we already know that git is a Version
Control tool that will allow you to
perform all kinds of operations to fetch
data from the central server or push
data to it whereas GitHub is a code
hosting platform for Version Control
collaboration
GitHub is basically a company that
allows you to host a central repository
in a remote server now without any
further Ado let's get started with the
demonstration on how to use GitHub so
for this demonstration we're working on
the website version of GitHub there's
another version of GitHub that is the
desktop version which you can download
it to your personal computer
so we're simply going to search for
GitHub in our search engine
the first link will lead you to the
official website of GitHub so I'm going
to click on that
so this will redirect me to the main
homepage of GitHub as you can see there
is a search GitHub option there are also
two buttons that says sign in and sign
up if you're new to GitHub you can
simply enter in your credentials that is
a username email password and sign up
for GitHub but if you already have an
account like I do I'm simply going to
click on the sign in button and it'll
redirect me to a page where I have to
enter the credentials that is my email
address and password I'm going to do
that now and I'm going to click on the
sign in button now this is the main page
of my account as you can see I have no
repositories it's all new it's all fresh
but if you're not new to GitHub you can
view all of your repositories on the
left hand corner now before we move on
I'm going to explain you all the
features that are present within GitHub
so you can see or search bar here so the
search bar will allow you to look for
profiles certain keywords look for
different kinds of projects that are
available on GitHub all of those can be
done using this bar here and you can see
four options next to the bar that says
pull request issues Marketplace explore
pull requests will learn later on in the
session but the issues in Marketplace we
won't be discussing in this video for
now the explore button on the other hand
is an extremely important and
interesting button so once I click on
that it'll redirect me to a page with
some activities that are going on around
in GitHub you can see here their
trending repositories they're attending
developers basically this is a fee that
will allow you to interact with
developers and other people
collaborators from all around the world
basically in Instagram too you have an
explore button which will allow you to
interact with different people from all
around the world so the same concept is
implied in the GitHub explore button too
so you can explore top picks you can
explore tending repositories developers
basically it's an interaction with other
people from different parts of the world
so I hope that's clear now the most
important part of the session are the
three buttons that are available in the
right hand corner of the navigation bar
so you can see there's a bell icon
there's a plus icon there's a pixelated
icon on the right hand corner so the
Bell icon allows you to read
notifications of your activities that
occur in GitHub so that's what it really
is you can see the inbox will allow you
to view all of the notifications you can
also view the Android notifications by
clicking on this unread button as of now
I don't have any notifications so
there's nothing available you can also
group these notifications by the date or
repository by clicking on this group by
button here you can also view your save
notifications by clicking on here and
the done button on the other hand will
let you mark all of your notifications
that you're done with your previous
notifications so these are the three
important buttons you have to know in
this Bell icon and the filters are not
necessary as of now so I'm not going to
discuss that this button on the other
hand will allow you to manage note your
notification settings and your
subscriptions too so that's all for this
Bell icon the next important button is
this plus icon as you can see there are
five drop down options that appear here
the first one being new repository
followed by import repository new just
new organization new project so new
repository we've already discussed
previously in this session a repository
is a place where you create your files
for your project it's basically a
storage space right so you repository
can directly interact with your git
right so the new repository option will
allow you to make files a repository to
your GitHub account right the git on the
other hand the tool that which we use to
make local repositories in our personal
computer can be directly pushed on the
local repositories can directly be
pushed onto your GitHub account so
that's what the new repository pattern
allows you to do so but the new project
on the other hand is a place to track
issues features and other tasks that are
related to the code within the
repository you can also connect with the
devops build and deploy process assign
people to tasks and so on by using this
button that is the new project button
so the difference between the new
repository button and the new project
button is that projects in GitHub are
only a part of GitHub but not git but
the new repository option is a part of
GitHub and git so that's the main
difference between new repository and
new project I hope that's clear so the
next button will drop me down some
interactions that I can make with my
profile so if I click on this your
profile option it will redirect me to a
page where I can edit my profile I can
really create my identity using this
page so here if I click the edit profile
I can add a bio about myself I can add
the company in which I'm working in the
location at where I am the website
Twitter username Etc all of that I can
add here all of the information about
myself I can also view the repositories
I'm working on currently or the
repositories I've worked in the past
projects that I'm working on the
packages and the entire contributions
I've been making on GitHub from the last
year
so basically it allows me to build an
identity or it'll help me build my
profile on GitHub so I hope that's clear
now if I click on this button and if I
want to sign out from a profile I can
simply scroll down and click on the sign
out button here and this will sign me
out of my account so that's all for
getting started with GitHub these are
the basics on what GitHub is and what
each of the button and options really do
now if I want to move back to the main
page of my GitHub profile I can simply
click on this octocat that's github's
logo so I'm just going to click on this
octocat logo and here I'm back to my
main page
now before we move on and work on the
different operations and options within
GitHub and learn different things about
GitHub I'm going to give you a brief
overview on how to download the desktop
version of GitHub so I'm simply going to
search for GitHub desktop on my search
engine and I'm going to click on the
first link that's available on this page
now I can simply click on this button
that says download for Windows 64-bit
that's compatible to my current version
of my personal computer if you have a
Mac you can simply click on the Mac
version and download it to your desktop
but as I've already mentioned previously
that we're going to work on the website
version so I'm going to Simply switch
back to this now let's quickly move on
to the next part of the session
create a repository so firstly let us
understand what a repository is it is
simply a storage space for the correct
project that you're working on GitHub is
a very popular Central repository that
allows you to share your files whereas
git allows you to create local
repositories that are present on the
system you are working on
so you can basically push your local
repository into GitHub and share it with
other collaborators via the central one
now that we know what a repository is
and how it works let's go on to the
demonstration part and create a first
Repository
so you can do this in two ways either
you can click on your create repository
button that is present on the left side
or you can as I've already mentioned in
the previous part of the session you can
click on to this plus icon and you can
click on the new repository option so
this will redirect you to a page that
says create a new repository you can add
your repository name I'm going to name
my repository as edureka and it's
available all of your repository names
must be unique from one another to
identify them easily you can also add a
description which is optional I am just
going to add the description this is my
first Repository
and a description allows people or other
collaborators to understand what your
repository is all about but as a good
developer or a good programmer you would
definitely want to add a description and
give an overview of what your repository
is all about there are two options now
available that says private or public
now you can choose your repository to
either be public or private so the
private One lets you decide who can
access your profile whereas the public
One lets anyone View and access your
repository but you can choose who can
commit to it that's the difference
between public and private repository
I'm going to let my repository be public
as of now now if you scroll down you can
see that you can initialize your
repository with three options the first
one being add a readme file the second
one being add a git ignore file you can
always choose a add a readme text file
to your project which often contains
information about the project and other
necessary details the user must be aware
of when he or she is accessing that
particular project now I want a readme
file for my repository so I'm going to
click on this button here that's going
to check it the next option is add a DOT
get ignore file so this file will let
you ignore a list of files when the user
is pushing files to GitHub that's what
this option really does but I'm going to
let this be unchecked for now for your
repository to truly be open source you
will need to license it so others are
free to use change and distribute the
software you can simply click on choose
a license option and pick your the
required license for your project
there are several licenses like MIT GPL
Apache License 2.0 BSD Etc but for this
repository we don't really need a
license so I'm going to untick this too
and now you can see there's a piece of
information that says this will set
master as the default Branch but I'm
going to ignore this for now I'm going
to explain about branches later on in
this session so this is all you have to
do to create your first new repository
you add a name you choose a description
you add an optional description you let
your repository be either public or
private and you initialize a repository
with either of these three options and
I'm simply going to click on my create
robustry option now this will redirect
me to a page with all the information
and the files that are currently present
in my repository you can see here my
repository name is present here with the
optional description that I gave and the
number of files currently we have only
just one file that's the readme text
file and that's present here
so this is all we have congratulations
you just created your first repository
now you can see there are some options
that says issues pull requests actions
projects Wiki security Etc we don't have
to really talk about all of these right
now we will just learn about one option
that says code here so this is really
important if you click on this button
you can see that there's a link that is
available here and https link so if you
copy this link and paste it on your kit
terminal that's present on your computer
you can download this entire project
directly to your local system so that's
what the link is for I hope that's clear
and the next option that says open with
GitHub desktop will allow you to open
this entire repository in your GitHub
desktop version and you can also the
last option that says download zip will
allow you to download this entire
repository in the form of zip files so
all of your project files will be within
that zip file so that's all you have to
really know about your Repository and
I'm going to click on the readme text
file that's available
it will take me to another page with
some extra information about that file
you can see currently we have two lines
and the memory space that is allocated
to this file so we currently have two
lines that is edu Recon this is my first
repository and you can also see the
number of contributors to this project
that is just one that's just me for now
and you can view the history of the
commits or the changes that have been
performed in your file so we'll come
back to that part later on this session
but you can move back to your main page
of this repository by clicking on the
name button here so edureka is the name
of my repository so I'm going to click
on that so now I'm back to the main page
of my Repository
before we learn how to create our first
Branch let us understand what branches
are branches allow you to work on other
features that can be included and merged
with the master Branch if required so
what is the master Branch the master
branch is the main branch where your
project resides on so all of the changes
all of the activities that you do with
your main project lies or is on your
default branch that is named as the
master Branch so what really GitHub
allows you to do is it allows you to
create additional branches so on these
additional branches you can work on the
other features or you can experiment
with your project and if you're happy
with this you can simply merge these
features to your main branch that is
your master Branch this is what branches
are really for so they simply allow you
to work on other features that's what
branches are so let's move on to the
demonstration part and look at how we
can create our own branches so now if
you look on the left corner you can see
a button that says Master So currently
we're on the master to branch and
there's only one branch and the master
branches have already mentioned is the
default Branch so when you create a
repository you're automatically creating
a master Branch so this is where your
project will be residing on and now if
you want to create another Branch say
let's name this Branch one branch
so this is what I want to name my
additional Branch I'm simply going to
name it and I'm going to click on the
enter button so it will redirect me to a
page so this is the exact replica of
your master branch and you can work on
this Branch you can work on any other
feature or you can add something you can
remove something you can really
experiment on this branch and if you're
happy with this you can merge back this
feature or the experimentation that
you've been working on to your master
Branch right so you can see this readme
text file it's exactly the same there's
the name of a repository the description
of the repository you can click on the
readme text file and everything's the
absolute same you can quickly switch
back to the main page but the only
difference is that you're currently on a
branch named Branch one branch you're
not on your master Branch now if you
want to switch back to your master
branch and work on it you can click on
this button and you'll find the master
here you can click on that and it will
take you back to your master Branch so
you can work on your project so the
currently two branches you can see that
and everything's normal everything looks
simple that's all for branches it's
really easy I hope it's clear so you can
look for branches here on this bar here
that's present here you can also create
new ones in the same option so that's
all for branches let's move on to the
next part of the session make a comment
now what are comments
comments simply record changes to one or
more files in your branches so basically
they save the changes that you're making
in your project git always assigns each
comment a unique identification which is
called sha or a hash that identifies the
specific changes so for any changes are
made to your project files you can
simply go back and look at the version
history or the history of the each
commit you've performed on your project
files so that's what really commits are
all about it's extremely easy let's go
ahead and make our first comment now I'm
going to switch to my Branch one branch
and I'm going to make my first comment
I'm going to click on my readme text
file that's the only file currently in
our repository so we'll make the change
in the readme text file there are three
really important icons that are present
in the right corner as you can see the
first one is a PC icon that says open
this file in GitHub desktop so if you
click on this file this entire file will
open in your GitHub desktop version the
next one that is the pencil icon will
allow me to edit this particular file
that is my readme text file and the
third icon is a bin icon which will
allow me to delete this file now what
we'll be working on is the pencil icon
that's the edit this file option I'm
going to click on this and I can simply
view a space or a file that will allow
me to make changes to my readme text
file I'm going to add another line here
that says this is my first comment this
is what I want to add to my readme text
file and if I want to preview the
changes I'm going to click on this
preview changes button you can see that
this is my first repository this is my
first Commit This is my first commit is
the additional piece of information that
will be adding to my readme text file
and it's highlighted in blue so we know
that that's the additional information
I'm happy with this change I'm going to
switch back to my edit file I'm going to
scroll down and if I want to add a
description about the change that I'm
performing to my file I can do that a
good programmer would always add a
description to the change that he's
making to the project file so other
collaborators will view the commit or
they view the change they can read the
extended description and understand what
the change is about so that's a good
habit that you must follow but as of now
we're not going to do that so I'm going
to leave this blank as you can see there
are two radio buttons that are currently
available the first one says commit
directly to the branch one branch this
will allow me to make the change or save
or make the commit directly to my Branch
one branch only so the change that I'm
making currently is only implemented to
my Branch one branch the second option
allows me to create a new Branch for
this particular commit and start a pull
request we're not going to talk much
about this option right now but the
first option is extremely important so
we let this be stuck on to the option
that says commit directly to the branch
one branch and I'm going to click on the
commit changes options and this will
simply implement the entire change to
the file you can see that the change is
implemented the additional piece of line
that says this is my first comment is
added to my readme text file now the
interesting part is if I switch to my
master Branch the change is not
implemented in my master Branch so the
change is only currently present in my
Branch one branch and now if I want to
view the history of the changes that
I've made I've already mentioned in the
previous part of the session that the
history button will allow me to do so so
I'm going to click on this history
button and you can see that I made my
first commit 23 minutes ago and I made
my new comment 41 seconds ago and there
is also a hash number unique hash
identification number that allows me to
distinguish between both of these
changes so all of my comments that I'll
be making on this Branch will be
available here so that's the main point
of a version control system isn't it
understanding and keeping a record of
all the changes that we're performing in
our files and our projects so this is
gives full Justice to the word Version
Control so that is what GitHub is all
about now that we learned how to make
our first comment two let's move on to
the next part of the session
open and merge pull requests so what are
pull requests pull requests let you tell
other developers about changes you've
pushed to branch in a repository on
GitHub so once a pull request is open
you can acknowledge and review the
changes with collaborators and add
follow-up commits after which your
changes are merged into the base Branch
so there are two ways to create a pull
request the first one being pulled
requests from a forked repository and
the second one being pulled request from
a branch within a repository currently
in this demo we will work on the second
one that is pull request from a branch
within a repository now I'm simply going
to switch to My Demo part okay now
currently I'm on my master Branch I'm
going to click on this pull request
option that's here now it says Branch
one has had recent pushes three minutes
ago compare and pull requests I'm not
going to click on that I'm going to
Simply click on the new pull request
option here so this will allow me to
compare the changes there's a base
branch and a compare Branch the based
launch is the master branch and the
compare Branch I will compare my master
Branch to my Branch one branch so this
notification says that the merge between
the branch one branch and master branch
is definitely possible so it's a green
signal so if I scroll down I can view
the difference between both the branches
so the left hand side indicates the
information that is present on the
master branch and the right hand
indicates the information that is
present in my Branch one branch plus
sign indicates the additional
information that is present in my Branch
one branch I'm happy with this so I'm
simply going to scroll up and create the
pull request and click on that I can
also leave a comment and I can preview
the change that's not necessary for now
so I'm going to go ahead and create the
pull request
now this will redirect me to a page so
this page says that the opal request has
been opened and now I can choose to
merge this pull request that is I can
merge the branch one branch to my master
Branch so it says this branch has no
conflicts with the base Branch merging
can be performed automatically and
that's good news right so I'm going to
click on merge pull request and I'm
going to confirm my merge update my
readme text file I'm happy with that so
I'm just going to confirm it now it says
pull request successfully merged and
closed you're all set the branch one
branch can be safely deleted I'm not
going to delete the branch I'm going to
compare both of the branches and see if
my master branch is exactly the same as
my Branch one branch so I'm going to
click on this edureka I'm going to go to
my main page of my repository so my
master branch has the additional piece
of information that says this is my
first comment now if I switch to my
Branch one branch it has the exact piece
of information
so the information that was present in
my Branch one branch has been
successfully implemented to my master
Branch so that's all for the pull
request part two we've reached the end
of the demonstration part now let's
quickly look at the case study of how
Microsoft implemented GitHub I'm pretty
sure most of you have heard of Microsoft
Microsoft cooperation is an American
multinational technology company it
develops manufactures licenses supports
and sells different computer software
consumer electronics personal computers
and other related services
so initially Microsoft was against the
use of the open source because they held
very tightly to the Internet Protocol
they were completely hesitant to adapt
to this new concept of sharing code to
the entire world but in 2010 they
rethought this entire scenario and now
Microsoft is one of the biggest
contributors to open source today about
2000 to 25 000 Microsoft Engineers
maintain
typescript.net Windows terminal dark
Helm and more than a thousand other open
source projects so first what they did
was they released new processes in
measured containment but later on they
released only license software so here
developers can learn from the company's
source code but they couldn't really
build on it eventually the stigma died
and now even close code like dotnet is
open source under an MIT license teams
realize that they need to accept
contributions to get feedback and learn
from other Developers
to organize and understand this approach
Microsoft created their open source
programs office which enables
distribution and centralization of
knowledge so the ospo provides the
resources and maintainers to manage
thousands of repositories and
contributors effectively on GitHub even
though Microsoft invests in its tools
they expect other individuals and
organizations to lead the way Microsoft
believes that github's value isn't in
any one feature but its entire Community
GitHub is the place to collaborate it's
where everyone is and where most of the
entire world's open source is already
happening it's not just a feature but
the whole thing
foreign
let's focus on various git commands so
we'll start with Git init command now
git init is basically to turn a
directory into an empty repository now
the first thing that you need you need a
local repository to work with for those
of you who don't know what exactly local
repository is it is basically a private
copy of the whole repository of the
whole shared repository that you have
and users can perform many operations
with this repository such as ADD file
remove file rename file move file you
can commit changes and you can do a lot
of things with your local repository so
the First Command that we are going to
discuss is basically the git init
command which will turn a directory into
an empty git repository so for that I'll
open my Virtual Machine to show you
practically
so guys this is my virtual machine where
I've installed git if you want to see
the version of git that you have
installed just type in here get hyphen
hyphen version and it is 2.7.2 in my
case now let's go ahead and create the
directory first so I'm going to name my
directory as
edureka hyphen Repository
now I'm going to move into this
directory so I'll just type in here CD
array card repo so now I have a
directory I want to convert it into a
local git repository which is very easy
all I have to do is type in git init and
here we go so initialized empty git
repository in home address
so let's go back to our slides again and
we'll see which command we are going to
discuss next
let's talk about git add now so git add
basically adds all the files to the
staging area for git now git doesn't
track each and every modified file so
whenever you do the commit operation git
looks for the files present in the
staging area only those files present in
the staging area are considered for
commit and not all the modified files so
the first thing you need to do is put
all the modified files all the changes
that you have done into the staging area
and then from staging area you can go
ahead and make a comment right so the
first thing is to add your files to the
staging area which is pretty easy all I
have to do is type in git add in the
file name so let me just quickly open my
Virtual Machine and show you that this
is my virtual machine again guys and
first let me just create a file here let
the name of my file be uh edureka.py
yeah let me just go ahead and type
something in this uh dot py file
so I'll just type in here a equal to 10
small code be equal to 20 and I'm going
to print a plus b pretty basic
save it and go ahead and close it right
now what I need to do is add the changes
that I've done into the staging area
right so all I need to do is type in
here get add and the name of the file
that is edureka.py
so I've successfully done that if I want
to add all the files to the staging area
then I can go ahead and type git add and
a period right so this will add all the
files that are there in my local
repository to the staging area so this
was all about git add let me just
quickly open my slides and we'll see
which is the next command we are going
to discuss now we're going to see the
git commit command so git commit
basically records the changes made to
the files in a local repository so you
have all the modified files in the
staging area so when you make a commit
it'll basically record the changes that
you have made to the files in that local
repository and for easy reference each
commit has a unique ID and it is a good
practice to always give a commit message
right so that helps you in understanding
what change was done in this particular
comment right so I'll just quickly open
my Virtual Machine and I'll show you how
you can commit changes to the local
repository this is my virtual machine
again guys so I'm just gonna type in
here get commit hyphen M and then commit
message so I'll just type in here first
commit
right so I've done that successfully two
files changed five insertions create
mode one double zero six double four
edirecta dot py right so this is the log
for it let me just clear the terminal
and go back to my slides again
let's talk about git status now
so git status Returns the current state
of the repository git status will return
the current working Branch if a file is
in staging area but not committed it
shows with Git status or if there are no
changes it will return nothing to commit
working directories clean right so let
me just open my Virtual Machine again
and I'll show you practically how this
works all right so if I go ahead and
type here get status
so it says on Branch Master nothing to
commit working directory clean so it
gives me the name of my branch and what
is there anything that we need to commit
or something that is already there in
the staging area but I have not
committed that and the working directory
is clean right now so whatever changes
that I've made in my file are there in
the staging area and from the staging
area I've made a commit as well right so
let me just do one thing make some
changes in the file add it to the
staging area but I won't commit it right
so let's see what will be the status
then so for that I'll create one more
file let that be
welcome.py I'll edit this using G edit
you can use whatever editor you want
I'll just type in here print
welcome to edureka close the parenthesis
save it and close it now I'm gonna add
this file to the staging area so I'm
just going to type in here git add I can
go ahead and type a period as well that
will add all the files or I can give the
name of the file that is welcome.py now
if I type git status let us see what
will come so it says new file welcome.py
changes to be committed right so these
changes are present in the staging area
but I have not committed these changes
and it gives me the name of my Branch as
well so I hope you have understood what
exactly git status is so I'll just
quickly open my slides again we'll see
which command we are going to discuss
next let's talk about git config now now
with Git there are many configurations
and settings possible so git config is
basically how to assign these settings
now two important settings are username
and user email address these values set
what email address and name commits will
be from on a local computer and it's
very easy to do that let me just quickly
open my VM and I'll show you that
now there are two things either I can
run the git config command globally or I
can run the git config command on the
current repository settings so let me
show you how you can do it globally so
I'll just type in here git config hyphen
hyphen Global user dot name and the name
of the user so let it be subham
and now I can go ahead and add the email
address as well so I just need to type
in here user dot email and I'll type the
email address
and the parenthesis in the double quotes
right so this is done
and this was done globally now if I want
to execute this on the current
repository what I'll type in git config
I won't use hyphen hyphen Global I will
directly go ahead and type user dot name
so let it be my name I say u r a b h
and I'm gonna add my email address as
well which is sort of at the rate
eduraycar.com
done guys it's that easy now let me
quickly open my slides again and we'll
see which command we are going to
execute next so we've seen how a git
config Works let's discuss about git
branching now
so the first question is why do we need
Branch right so branches are used to
create another line of development and
by default git has a master Branch so
usually a branch is created to work on a
new feature so if you're working on a
web application you might want to work
on the card service right so for that
you'll create a new feature Branch or
any name that you want to give to your
branch and once the feature is completed
it is merged back with the master branch
and we delete the feature branch and
every branch is referenced by head which
points to the latest comment in the
branch whenever you make a commit head
is updated with the latest commit let me
just quickly open my Virtual Machine and
over there we're going to see how to
create a branch how to delete a branch
how to rename how to merge and how to
check out into a different branch
this is my virtual machine again guys
and uh let me just tell you how you can
see all the branches so just type in
here get branch and uh
and it will list all the branches that
are already there so there's only one
master Branch present right now so let
me just quickly go ahead and create a
branch see there are two ways to create
a branch one way is to create a branch
and then you can check into that Branch
another is you just want to create it
but you still want to be in your master
Branch or your current working Branch so
for that uh if you don't want to check
in that Branch just type in here get
branch and give a name to your branch so
let that be feature right and if I go
ahead and type git Branch now I can see
that there are two branches master and
feature right
now I can go ahead and delete this
Branch as well but let me just quickly
uh move into this Branch first so I'll
just type in here git checkout
and the name of my branch that is
featured
and this will Point towards my feature
Branch now I'm in my feature Branch
right so let me just make some changes
in the files that are there in my
edureka repo so you can see that I have
couple of files data rica.py and
welcome.py so let me just make some
change in let it be at arrayca.py right
so I'll just quickly type in here G edit
again you can use whatever editor that
you want
edureka.py and I'll just add one more
variable here let that be 30 and I'll
print one more value
that is C plus a plus b pretty basic
save it and then go back
so in my feature Branch I have made
changes to my edureka.py file right so
what will be my next step is to add
these changes add the modified file in
the staging area so I'll just type in
here git add and a period because I want
all the changes that I've made to my
files should be there in the staging
area so git add and period will add all
the files now I want to commit these
changes so I'll just type in here git
commit hyphen M and a message feature
commit right said now I've done all the
changes and I've added those changes to
the staging area and then finally I've
made a comment as well so it says four
files change 10 insertions create mode
welcome.py welcome.py let me just clear
my terminal now so if I want to merge my
feature Branch with the master Branch
now first I need to check in with my
master Branch for that I'll type in here
get check out
master
now it says switching to Branch master
from feature now I'm in my master Branch
so now I'm gonna type in here get merge
feature and here we go
so you can see that I've merged my
feature Branch with that of the master
Branch it's that easy guys let me clear
my terminal now so what I'll do I'll
quickly create one more branch and and
I'll show you again uh how these things
are happening because there might be
some confusion still so I'll just show
you again how to create a branch how to
check into that Branch how to make some
changes in your local repository adds
that to the staging area make a commit
and then merge it with the master branch
so now uh what I'm going to do is I'm
going to delete my feature Branch
because I've told you earlier as well
once I have merged all my changes to my
master Branch there's no point in
keeping my feature Branch as of now I
delete that branch and there are a
couple of options to do that the safest
one is what I'm gonna tell you right so
I'll just quickly uh type in here get
Branch hyphen D and the name of your
branch which in my case is feature
so it has deleted the branch feature now
there's one more way to delete a branch
instead of hyphen D you can just type in
here get Branch hyphen uppercase D under
the name of the branch but this is not
the preferred option and it's not safe
as well because if you have some
unmerged changes then also it will
delete the branch whereas if you type in
here a lowercase D it will show you a
error that will say that you have some
unmerged changes right so always prefer
a hyphen small D and then feature hyphen
small D and then the name of the branch
that you want to delete right so now I'm
going to show you one more way to create
a branch which is called git checkout
hyphen B and you can give the name to
your branch right so probably I want to
name my Branch again as a new let the
name of my Branch be new so git check
out hyphen B new so this will not only
create a branch called new but it will
also check into that Branch right so we
have also checked switch to a new branch
that is new and over here also if I want
to make changes to my files I can go
ahead and do that all I have to do is
use any editor Let me uh edit say
welcome dot py file
welcome to edureka
I forgot to close the quotation so
that's the change I'm gonna do in this
file right and then I'm gonna get add
period now get commit
I made a mistake there so I'll just type
in here get commit hyphen m
and uh let it be new the name message
all right so it is done now
now what I'm going to do is I'm gonna
show you how to connect to a remote
repository so I have a remote repository
the GitHub account I've created a
repository there so if I want to connect
to that repository what I need to do is
I need to add that origin so I'll just
type in here get remote add origin
and the SSH link that I will just show
you where you can find it so this is my
GitHub account guys and this is my
repository that is git commands if I
want to copy the SSH link I can just go
ahead copy this and paste it here
so we have successfully added the origin
now let me just clear my terminal so we
have connected our local repository with
the GitHub account so we have seen how
branching Works in git let us move
forward and we'll see how to work with
remote repository so over here I've
already told you how to add an origin
right it's a git remote add origin and
what we did we connected a local
repository with a remote repository so
The edirecta Hyphen repo is my local
repository and a remote repository I
just showed you in my GitHub account so
we have already done this git remote
algorithm so now we're going to see how
to clone a remote repository so to
create a local working copy of an
existing remote repository we'll use git
clone to copy and download the
repository to a local computer cloning
is the equivalent of git init when
working with a remote repository git
will create a directory locally with all
the files in repository history
so I'll just open my Virtual Machine and
I'll quickly show you how you can do
that all right so what I'm gonna do is
I'm first gonna create one more
directory so let that be mkdir
and uh the name of the directory Let It
Be
git hyphen repo right and let's move
into this git hyphen Repository
so the first thing I'm going to do is
git clone git clone and the SSH link so
SSH link I've already shown you where
you can find it in your GitHub account
just type in here get clone and the
remote repository URL the SSH link that
I was talking about and let me show you
how you can do that and here we go
so it says cloning into git commands
so let me see what are the files that
are there now so it says git commands
let me go to this get commands
so here we have edereka and readme and
here also you can notice that we have
ederica and readme so I've successfully
done that let me clear my terminal and
now uh I can go ahead and
pull whatever changes I made to the file
that is there in my GitHub account so
let me just show you how to do that
suppose if I have a file edureka and let
me just make some changes here
I'll just type in here
my name is saurabh
so let it be like this
so let's come and change this
so yeah now it'll be reflected in my
Repository
now what I want is I want all the
changes that have been made in my remote
repository in my local machine so for
that I'll just type in here git pull
origin master
and here we go
so let's say that I successfully done
that that means we have pulled all the
files that are there in a report
repository so if I check so I have here
edereca and readme.md files so if I go
ahead and type in G edit edureka
let's see or all right so I can see that
the changes have been reflected my name
is saurabh which was something new that
I added to my file right so this is all
about the pull operation now let us see
how you can push changes to the remote
repository right so if I go ahead and
make some changes or if I create a file
let's say I create a file touch get Dot
py and let that be get one dot py right
so if I make some changes to this
particular file
I'll write here print
I love git
save it close it right so now before we
push to the remote repository we need to
add these changes to the staging area we
need to make a commit and then only we
can push it right so first thing add the
file name so I'll just type in here get
add I can go ahead and type here if
period that will add all the changes
made to the files in the staging area
all the files in the staging area
basically modified files or I can type
the name of my file which in my case is
git1.py so I'll just type in here get
add get1.py and now I'm going to commit
the changes so I'll just type in here
git commit
hyphen m
give a message so I'll just type in here
remote
and it has done successfully so now I
can go ahead and push the changes to my
remote repository so I'll just type in
here get push origin master and here we
go
so it says it is done successfully let
us go back to our GitHub account and
we'll see whether those changes are
reflected there and or not so I'll just
go ahead and run this basically reload
it so you can see that get1.py file is
added here right so let me just quickly
go to the slides again and we'll see
what all topics we have already covered
so we have seen how to perform git clone
we have seen how to pull changes from
the remote repository to the local
computer we have seen how to push the
local comments to the remote repository
now let's talk about few Advanced git
commands focus on git stash git log gets
rebase and get revert so I quickly open
my Virtual Machine and I will explain
you there only so guys let's discuss
about get stash first so to save changes
made when they are not in a state to
commit them to a repository you use git
stash this will store the work and give
a clean working directory if I have to
give you an example imagine when you're
working on a new feature that's not
complete but an urgent bug needs
attention so that's when you can use
gitstash let me just quickly show you
how you can use it
so the first thing I'm going to do is
create a new file so let the name of my
file be
stash Dot py
and I'm gonna make some changes to the
stash.py I'll just type in here print
stash is cool
save it close it and yeah now what I
need to do is add this to the staging
area so I'll just type in here get add
and now let's see the git status
so my git status says that we have new
file which are there in the staging area
that is stash.py right changes to be
committed so these are the changes that
I need to commit So currently it is
pretty dirty so what I can do is I can
put all the uncommitted changes to a
stash let me show you how I can do that
get stash hyphen U and here we go now go
ahead and type get status
nothing to do working directory clean so
it has converted my dirty directory to a
pretty clean one right so with the help
of git stash right now the modification
stashed away by this command can be
listed with Git stash list so I'll just
type in here get stash list here we go
and if you want to inspect it so you can
just go ahead and type git
stash and show
so you can inspect it here right let me
clear my terminal now so now I'm done
I've fixed my bug and now I'm ready to
you know work on what exactly I was
doing before fixing that bug right so
what I can do is I just want to type in
here get
stash apply and here we go
and now if I type git status
you can see that I again got the dirty
directory back
so guys we have seen how gitstash works
now what I'm going to do I'm going to
show you how a git log and git reward
works right so uh for that I'll quickly
create a new repository so let me do
that I'll just type in here
mkdir git hyphen log so let that be the
name of my directory and I'll go into
that directory yeah so now I'm going to
initialize it so I'm just going to type
in here get init it has been done now I
will uh create few files here uh let
that be edureka1 Dot py and I'll just
type in here print
Master
new technologies
at EDU daker
all right save it
and close it
now I'm going to add it here git add
and then finally committed
hyphen m
let's give a message and it is done now
so what I did I created a directory
initialized it then what I did I made a
file inside that in that file I added
that to my staging area that file has
been added to my staging area and then
finally I made a commit right so till
now there is no problem now if I type
here get log let me show you what comes
so you can see that when I type git log
there's a commit hash right there is a
commit that has been made by author
Shivam on this particular date and day
right so this is what basically git log
is it shows the chronological comment
history for a repository this helps you
give context in history for a repository
and it is available immediately on a
recently cloned repository to see the
history as well now I can provide
parameters here as well for example I
can type in here get log hyphen before
and give a date here right similarly I
can tell the git lock to show me the
comments based on the author right so if
I type in here get log hyphen hyphen
author and the name of the author then
it will show the logs based on the
commit made by the author right
similarly if I type in here hyphen
hyphen before right and give a date so
it will give me according to the date as
well so this is how git log Works guys
so now what I'm going to show you is how
to revert to the previous commit right
so for that let me just make some
changes to my file again so the name of
my file is edereka1.py so I'll just type
in here G edit
editorica1.py and I'll type in here one
more print statement that says
don't just simply
learn it
Master it
save it close it now I'm going to add it
to my staging area and finally I'm gonna
commit it
hyphen M last commit all right so this
is done now now what I have to do is
suppose I have made some changes I'm
gonna go back to the previous change
right so I've made some change in my
code I've done the commit as well but I
want to go back to the previous commit
right so for that what we use we use git
revert so let me show you how you can do
that so in order to go back to the
previous comment what I have to do is
just type in here get log hyphen hyphen
one line this will give me one line so
this was my last commit so I want to go
back right so for that I'm just going to
type in here get revert and the hash
comment hash and here we go
so it says that revert last commit so it
has already done that now if I go ahead
and check my EDU
recap.1.py file so it only says Master
new technologies at edureka so this is
how you can even perform the revert
operation just by using in the uh commit
hash so with that you can go ahead and
revert back to the previous commit so I
hope this is clear now let me clear my
terminal so now what I can do is I can
go ahead and revert to the last commit
as well instead of just typing the
comment hash if I just type in here get
reward head
and if I quit so it says that I've
rolled back to the previous version of
my file so if I go ahead and click on LS
command and if I type in here cat
edureka1.py so I've gone back don't just
simply learn it Master it so my last
comment was basically revert operation
right so whatever file changes that have
been done after gitreward will be
reflected as a commit itself right so I
hope you're getting my point so what I
did first I created an empty repository
then in that I created a file editoraker
one dot py made a couple of changes made
two comments then I used get revert with
the hash commit to go back to the
previous commit right so with that I got
only Master new technologies etraca this
one I didn't get at that time then what
I did I showed you get revert head which
will basically take the last comment
right it will reverse the last comment
and the last comment was nothing but the
revert operation that happened that
actually made my file hydra1.py to go
back to the previous version so that was
my last comment right so because of that
it has revert back to the last comment
this is why you can see that edrica1.py
file has both Master new technologies at
the direct and don't just simply learn
it Master it right so I hope I'm clear
with Git log and git reward it's very
easy guys there's not a lot of things to
understand here it's very basic so let's
talk about rebase now so basically
rebase is again one more way of
combining a work between different
branches so what rebasing does it'll
take a set of comments copy them and
store them outside your repository the
advantage of rebasing is that it can be
used to make linear sequence of comments
All right so the comments log or history
of the repository stays clean if
rebasing is done right so let me just
show you how it is done if I just type
in here git rebase Master let me just go
ahead and do that git rebase master
so the current Master is up to date
that's what the message pops up so
basically this command will move all our
work from current Branch to the master
Branch they look like as if they are
developed sequentially but they are
developed parallely
all right so let me just give you a
better example of this so I'll just
create a branch here git branch
say
and Eureka 0 1 0 is the name of my
branch and uh
yeah so if I want to go to this branch
check out add eureka010
yeah I went to that branch and if I make
changes to
endureeka1.py
I'm going to remove this print statement
save it close it clean my terminal and
I'm gonna add it get add period and get
commit hyphen m rebase
all right so this is done now and now
what I can do is I can go ahead and type
in here get
rebase master
so it says that current Branch area 0 1
0 is up to date so this is how you can
perform rebasing so let me quickly go
back to my slides so we have talked
about git stash we talked about git log
git rebase and git revert
[Music]
let's talk about what is Jenkins and how
does Jenkins in this particular picture
Jenkins is an open source automation
server which helps to automate the
process of development rating of testing
deployment packaging and others it is a
server-based system that runs on
software Apache Tomcat it supports VCS
tools like gate bitbucket and also Port
build automation tools like Apache May
one aren't and it helps to make and
facilitate our continuous integration
and continuous delivery process Jenkins
was first released in the year 2011 and
it's a completely open source software
as part of the MIT license Jenkins guys
is a plug-in based software in Jenkins
you have got plugins which basically
used to interact with different
different tools and components in
Jenkins plugins are primary releases in
languages other than Java plugins are
available to integrate Jenkins with most
version control system tools and most
data uses using these plugins we set up
purposes for example you testing you can
do compiling you can do packaging you
can create some reports you can do some
logging all those things are possible
using plugins and Jenkins another
important component that we talk about
in Jenkins apart from plugin is Jenkins
notification which we also call as
mailer using mailer guys we can do
config email notification for any
results you can configure the scenario
for example you want mail notification
when build is successful all those
things can be done using mailers as the
Jenkins component another important
component in Jenkins is the SSH agent
Jenkins also follows a typical Master
Slave topology now why do we need a
master sleeve topology in CI tool like
Jenkins when you work in your
organization we have got multiple teams
which you want to build and deploy their
own pipelines or deploy their own soft
project ports teams don't want that
their piece of code runs on a server is
owned by some other team so what we can
do is we can set up a Master Slave
topology where in Jenkins may run on
server X but even correct multiple other
hours which will act as a stream so when
you are deploying a code executing a
pipeline you can run those code on those
XYZ servers that is the beauty of using
Jenkins Master Slave another good thing
about Jenkins is Jenkins can be deployed
on any operating system you can run
Jenkins on a Windows node on a Linux
node on a Mac OS Jenkins can be deployed
on any operating system and similarly
your Jenkins Master slip can be of any
OS it is not at all dependent both
master and slave need to be of the same
operating system another important core
component of Jenkins call is Java dock
this is out of plugin in Jenkins which
basically is a part of Jenkins score
which helps us to publish results like
build action build directory what is the
expected build output all those things
that we see in Jenkins they all are
happening using Java doc let's talk
about security engineers apart from that
Jenkins have other sort of oscillation
like Project based strategy Matrix based
authorization which helps to make
Jenkins more secure we'll talk more
about this when we talk about Jenkins
overall so as you can see from the
diagram Jenkins is pulling from Source
control system like get bit bucket then
can start deploy it in Jenkins we have
got notification management system using
mailer where is a bit successful we get
a war file a zip file we can Define how
we want the build to be created and it
will save your Center notification
management to a developer that something
happened wrongly with a specific code
lines and you need to rectifier code
that is the beauty of using Jenkins so
Jenkins guide is to the bread and butter
in the industry there are some Jenkins
computer also in the market like gitlab
Jenkins and some others but by default
Jenkins is the two go tool for the CI in
Industry let's talk about what is a
pipeline so far we talked about Jenkins
now let's talk about what is pipeline
injectors now let's say I want to deploy
a code the code may have certain steps
like first step I am building the code
second step I am compiling then code
reviewing then packaging then deploying
how do I automate or combine all these
steps which can be executed in one group
to deploy all these scenarios we create
a delivery pipeline pipeline is nothing
guys it's a combination of different
different steps or different tasks that
we need to perform in order to deploy
our code using a pipeline instead
bringing this task or code deployment
manually we combine them and deploy them
as sort of a one common approach of a
delivery pipeline there are multiple
ways on how you can create a pipeline
the two most common ways or common
approaches of using a pipeline as you
can see from a scenario a declarative
approach and the second one that is used
in the industry is scripted approach
this particular example you are saying
is an example of a Jenkins pipeline
script here you are saying you're first
doing a stage called build stage where
we're combining multiple tasks like
first we are using Java we are printing
something called Echo hello pipeline
then using Mi one as the build
automation tool and time deploy package
and then we are doing a shell script
output of what all things are present in
the particular directory structure but
typical example of syntax of how to
create a pipeline in Jenkins it's
example of a scripted pipeline now let's
talk about the two predominant pipelines
that are used in the IT industry
scripted and declarative let's talk
about what is scripted and declarative
and how they're different from each
other a scripted pipeline is a
traditional dentist pipeline approach
and declarative by pipeline is a modern
day pipeline approach that is used in
the industry in a scripted pipeline the
syntax was very strict and traditional
while in declarative pipeline views
something called as groovy syntax when
you use a declarative pipeline you can
get the code from any version system to
like get and all you create something
called as Jenkins file and you can
download Jenkins file from version to
like it and that can be used to run your
Jenkins hold when you use a scripted
pipeline you define a code something
called as node block but one decorative
pipeline would Define a code in pipeline
blocks as you have seen in the previous
example now let's talk about what is
Jenkins file and how it can be used
Jenkins file is the thing is like a text
file where you define find your entire
structure and syntax of your Jenkins you
know you download the file as in
particular you can just give the path of
your git repo if it is then get
credential private repo and it can pull
all the contents from there and just
execute your code it is a modern
deployment mechanism that we use when we
work with a Jenkins pipeline using
Jenkins pipeline we use certain things
which helps us let's talk about this
first is code review writeration
Venezuela Jenkins pipeline you can
easily review your code your code semi
review by multiple developer before
executed how that works is injections
file is stored generally in a Version
Control System tool like it you can put
something called as PR pull request for
your code review different members of
your team can review the code before you
actually pull the Jenkins file and
deploy you can do an audit Trail or
refer to as login you can log each and
every step output that is being executed
as part of a Jenkins file and can see
what is happening when the Jenkins file
is getting executed it's sort of like a
word boost login then you can find a
single place to store all your data or
all your outputs you no need to have
developers scattered here and there and
writing their own codes for executing a
Jenkins pipeline hold organization can
approach a genius file mythology where
code can be stored in a VCS tool and
from their code can be downloaded and
can work as single source for your
entire pipe let's talk about Jenkins
workflow in particular how does Jenkins
workflow work Jenkins workflow always
starts with the version control system
or you can say source code repository
multiple developers collaborate and they
put their code in a source code system
like git from where Jenkins tried to
pull the data from that Source control
system reposit system there is no
mandatory or source code need to make it
it can be bitbucket SVN whatever your
organization uses with using Jenkins
server we execute tasks like build
deploy compiling packaging code review
all those things happen using Jenkins
server whereas Jenkins actually
installed on setting so Jenkins server
can be any operating system it can be
Windows machine a Linux machine Ubuntu
machine it can be any use of today's
modern world availability using this
Jenkins server would execute our
different different tasks and scenarios
in Champion server only we use the
middle functionality for the feedback
mechanism to be notified developers the
commit or the code deployment successful
it is a failure it has some challenges
what can be done using the feedback
mechanism possible using the Jenkins
server using Jenkins servers we always
first deployed the code on a lower life
cycle environment that is the
recommendation practice you first deploy
a QA server on a testing server and once
the code deployment is successful on the
lower life cycle then we go ahead and
deploy the code on our production server
that is how the entire Jenkins work from
any particularly interacts first now
guys let's talk about Jenkins
installation how to install Jenkins in
our lab to give this example we are
going to use a Centos machine as a
reference to install and configure
changes Jenkins by default runs on port
8080 the default port for running
Jenkins so please make sure that if
you're using any Cloud VMS or any VMS in
your organization then the port 8080
firewall settings are open and
configured so that it can be accessible
Jenkins is a GUI based tool 95 Jenkins
job is completely GUI based we hardly
log into our servers Jenkins got
installed or using a CLI command to
manage or run change it is completely
run unmanaged using Google the graphical
is interface which comes when we install
Jenkins another prerequisite to install
Jenkins is Java need to be present in
our machine we need to make sure that
Java is installed in our machine so guys
let's go to our lab and see how to
install Jenkins so guys as I mentioned
we are going to use a Centos machine so
let's just double check it's a sentence
VM so we have got the centers VM where
we are running Centos 7. first thing let
us install Java this install Dash y you
can install any version of java I am
installing 1.8.0 we can go for any
version
so this completes my Java installation
another good thing about Jenkins here is
Jenkins has got detailed page published
by Jenkins Community where we can see
how to use Jenkins installation for
different different types of operating
system if you go to Jenkins dot IO slash
downloads here you can see the
installation step provided by Jenkins
Community to install Jenkins on
different different operating system you
have got the stable release and the
release which is like Beta release you
can select which little Jenkins you want
to use I will go for the stability that
is Jenkins
2.2.493. ta3 and since I'm going to use
Centos I will click on the Centos
installation step for Jenkins just copy
the steps mentioned here how to install
Jenkins and paste it in your
installation server
these are the RPM dependencies that are
needed to install Jenkins so I'm just
deploying them on my server so that this
dependency are present before install
Jenkins and now let's install Jenkins
exam install Dash y Jenkins
once your Jenkins software is installed
let's start the Jenkins service
Jenkins so start let's validate the
Jenkins service got started
so now we have got Jenkins up and
running a while the guy mentioned
Jenkins runs on port 8080 so let's get
the public IP address of this machine
so the public iPad machine is
35.223.206.46 let's copy this IP address
and try to access Jenkins on Port 8080.
firstly when you install Jenkins Jenkins
for securely ask you to verify the
Jenkins code just go to this path
location and copy the code mentioned
there on your Jenkins server so I will
call Jenkins server install Jenkins I
will just scat this directory structure
and I'll copy this Jenkins secret color
secret key which we need to use to
install Jenkins click on continue
if your key is valid it will prompt the
next page to install plugins to get two
options here either you can install the
plugins recommended by Jenkins or you
can select your own plugins which you
want to install here I will select the
install suggested plugins this step may
take a minute or so since it will
install couple of plugins which I need
for Jenkins to properly function
so you can see as like in the screen
coming up different different plugins
are getting installed in our Jenkins
so once your Jenkins installations
compared asks you to create your first
admin user okay so let's give some
username I'll give you the name as
advoca
give some password you can give any
password of your choice
so I will give some password Here
give some confirm password give the same
path that you give above some name I
will give the name as also educa and
some email address
click on Save and configure
so you got the base URL and it's running
on port 8080 click on Save and finish
start using Jenkins
this is how your Jenkins home screen
looks like when you configure Jenkins
for the first time now let's talk about
some Jenkins Management on the left side
you see a tab called manage Jenkins this
is a place where we do the entire
Administration and management of your
Jenkins you can see I've got plethora of
options here like security configuration
status information tools and actions you
can CLI let's talk about all these tools
in depth let's first talk about
configure system as the name twice using
configure system you can configure and
edit different different Jenkins
Properties or Jenkins configuration
settings so for example how many
parallel tasks you want to execute at
time there's panel of executor you can
give two three whatever you want if you
want you can change the port and specify
the default admin address you can create
your own Jenkins email address from
where your mail notification should go
any Global Properties or variables you
want to link are linked at this place
called Jenkins configuration let's talk
about something about Jenkins security
if you go to configure Global Security
here you see how this you can configure
Security in Jenkins there are multiple
authorization mode you can use in
Jenkins in order to use this
authorization strategy you need to use
the authentication strategies the most
common and most widely used is ldap
integration where you basically
integrate the organization's ldap server
with your Jenkins database once you've
done that you can use different
different authorization strategies which
are used in Jenkins the most predominant
one used in the industry is Project
based metrics authorization strategy now
what does this project-based metrics
authorization strategy means let's say
you have created multiple jobs or
pipelines and Jenkins now all these jobs
are pipelines belong to different
different teams in organization using
project-based metrics authorization
strategy we can restrict that specific
job can only be accessed by team members
of a specific group it is not imperative
that anyone who has excellent Jenkins
can run all the jobs or all the
pipelines using project-based metrics
oscillation strategies you can restrict
that another one that is used is metrics
based security here you restrict the
overall access of check game that means
a user a can have admin rights a user B
can have we only write what type of read
writes you want to give like agent based
feed only right the full job rights full
view right for source code management
right all those things can be done using
metrics based security strategy so the
traditional ways which are not being
used so frequently in the industry are
anyone can do anything that means anyone
who has access to Jenkins is like full
admin another one that is being not used
these days is legacy teamwork legacy
mode is something like you have the
default admin role where you granted the
full control over system or otherwise
you have the only read access like read
or admin in the legacy mode the third
one that was not also being used as
logged user can do anything logged in
user anyone can do anything is almost
like similar here also people who login
can basically do any of the tasks which
is being done in general so in modern
industry for security we only used
metric Space strategy and the
project-based onion strategy these are
two things which are basically used in
modern day world for using Jenkins now
let's talk about how does Master Slave
architecture Works in Jenkins how can we
configure that and how that is possible
let's go back and let's go there
let's go again back to manage Jenkins
you see an option called manage nodes
and clouds let's see how we can set up
Master Slave architecture using junkets
by default the server will install
Jenkins called as the master server
where Jenkins server is running on the
left hand side of the screen you can see
a tab called as blue note using a new
node we can add any nodes belonging to
any operating system which can act as a
Jenkins sling there are multiple ways of
how we can set up authorization between
a master and a slave node the most
common methods of using the Master Slave
nodes are SSH launch agent by execution
of command this is mainly used when
you're using Windows machine as your
slave node for any Linux machine we
generally go by the secure shell as the
authorization strategy we can add as
many nodes as you want as a slave node
there is no limitation of how many slave
nodes we can have in a Jenkins
architecture now let's talk about
plugins and jungles plugins another
important component of Jenkins when you
go to manage plugins you see three
options available here plugins where you
find updates means you are installed and
updates are available
plugins which are installed in your
machine which are already present so if
you guys remember when we installed
plugins we selected an option called
install recommended plugins these are
some plugins which get installed by
default when we select that option then
you've got tab called availability
plugin plugins which are available which
we can use for installation there's a
plethora of plugins which is developed
and managed by the Jenkins Community
which you can use for installing here so
for example let's say we want to install
a plugin to integrate unique Cloud
environment you can find plugin for that
also so if you search for for example
Azure you can file you've got so many
plugins to integrate with the Azure
Cloud similarly if you search for AWS
Cloud you can see so many plugins to
connect with the AWS Services same way
you can create your own plugins also we
need to create our own plugins and you
can upload the plugins which can be used
for you to installation in your own jenk
installation once your plugin
installation has been verified and
updated then it can be used by Jenkins
Community as a whole but initially when
you upload any plugin it is just for you
to use for your own machine now guys
let's talk about how to create a job at
chinkle when you work in Jenkins there
are few terms which we use very often
items jobs project they all are
interchangeable they all carry the same
meaning in terms of Jenkins how to
create a job or item just click on the
new item give some name let's say job
fun for now select just freestyle
project since we are getting a very
basic one click on OK
go on the build app and select how you
want to build step execute a shell
command A bash command now on together
what is your for example I want to
execute a shell command
and let's say I want to print something
Echo
hello from madurica simple job
and just click on apply and click on
Save let's try to build this job in
order to build this job you see the
option called build now click on build
now
see my job is running right now
see my job execution got completed and I
can see my first job got come today
let's click on this one button the first
job execution and click on the console
output here you can see how the job
execution actually happened my job got
successfully completed and you can see a
message getting displayed hello from
attacker so simple way to see a create a
first job and do validate job Bots it is
fully completed or not let's try to
create one more job and then we'll try
to see how that works out let's say I
want to print today's date for example
so let's again say job to for example
again a freestyle project again click on
OK
again click on build
this execute shell and let's say
Echo
hello and let's say date and I want to
store this output in slash temp slash
temp slash and let's click on apply
click on Save and let's click on build
now
you see the job afterwards successful
let's click on this click on Console
output you see it's successfully printed
this and the output got completed you
see all this jobs that we execute has
done in a specific directory structure
on the stringkin server so if I go on
this Jenkins server for example and go
inside this directory structure and you
do LS you can see that this job got
executed in this director structure job
similarly if I go in the temp directory
structure and do an LS you see when we
ran this job we gave the date should be
printed in this edu wake up file if I do
a cat and this file name it displaying
today's data simple example that
whatever jobs we execute gets actually
executed on the Jenkins server we just
seen the front end in the going what is
being done as part of the Jenkins
strategy now how to introduce a source
code management integration in Jenkins
for example let's say I have got some
code in any versioning tool system let's
say like git or sub version of bitbucket
and download data from there too go to
the job or create a new job let's click
on configure
if you go on to source code management
you see an option called git here you
can Define the GitHub repository path
from where you want to download data
your git repository if you're using a
private repository add your get
credentials and from which particular
Branch you want to download or clone
your code you can even add multiple git
Branch from where you want to get the
data that is how this things Works in
Jenkins another Beauty about Jenkins is
build triggers so every time when I
build a job I don't want to go manually
and run the job we can automate this
process using some simple tasks like
build periodically you can define a
schedule where you want your jobs to be
run this schedule defined in a con tab
basis like every five minutes every 10
minutes every hours once in a day we
Define a Quant tab entry for that and
accordingly your jobs will then
automatically in that particular
schedule manner if you want to make your
life more simpler and want to run the
job every time a new commit a developer
makes for that go with the option of
poll SCM in a polo option every time a
new commit happens made by developer at
this specific schedule check for it if
there is no new commit the job will not
build but if there's a new comment the
pole SCM will trigger that job that is
how we automate the things in Jenkins
when you build a job you try to get sort
of pipeline you can create a pipeline on
the basis of multiple scenarios when you
say scenarios WhatsApp to talk about for
example let's say I want the next job to
be executed only if my build is stable
or unstable or failed you can specify a
scenario and then say which job want to
trigger for example I want to Interlink
job 2 and job fund I can give job to
inter build after job to build job one
but only trigger job one only when job 2
is stable so it's like creating a sort
of interdependent jobs where only if my
previous job execution was stable or
successful it will go to the next job
otherwise it will skip the next job that
is how we build a sort of pipeline using
Jenkins in Jenkins your slave nodes or
your Master Slave hierarchy can be a
node in the on-premind machine or it can
be any Cloud vmos so for example you can
have your Jenkins Mass server deployed
on-premise or in your own data center
while your slave machine can be running
in an AWS cloud or an Azure cloud or
even both that is how good Jenkins is
all about Jenkins guys completely open
source Automotive software where there's
no cost of using it or there is no
licensing cost linked with it how do we
automate all those things and deploy
them in a one single code as a pipeline
so now we're going to see an example on
how to create a pipeline type before
using this pipeline we have just make
sure that may 1 is installed and present
in our environment so let's go to our
Jenkins setup we have installed in
configured our Jenkins again just click
on new items as we previously create a
new job or something give some name
let's say I give a name as code
select the option as pipeline click on
OK
I'm going to scroll down you see an
approach called pipeline script okay
basically you write your script or how
you want to deploy your pipeline for
your example here in my scenario you see
we have created this scripted pipeline
here there are three steps so you can
see three stages for our code deployment
the first is we're preparing our code
here as you can see we are downloading
the code from a git repo or a git
repository here you define the GitHub
repo path where you want our port to be
downloaded you can specify whichever
GitHub path you want to use next you can
see we have defined an environment
variable that is build the code that is
getting downloaded from the preparation
step we have named as stage build here
you can see we are running the bin mvn
the default 11 directory and we're
trying to build this particular piece of
code here once our code gets built we
are trying to combine this code and
create an artifact file a jar file you
can Define whatever format of output you
want a jar file a zip file a war file
anything can be defined in this approach
let's click on apply let's click on Save
and let's see how this pipeline looks
like so if you click on build now
you can see different different stages
that happen the first one is preparation
stage in this stage the code would have
been pit pulled from the git repo then
the build side and finally receive
results that something has happened so
let's click on this job and let's click
on the console output you can see the
first stage happened that is pulling the
data from the git repository it was
public report so no code need to be
specified the data got cloned or
downloaded from this git repository once
the data was downloaded it then tried to
combine it so it uses a different
different event plugin like the compiler
plugin for test report the mayweight
Surefire plugin it runs some test queues
as you see it ran some six test cases
where it had no failures in this
particular diaper structure on your
server where you install Jenkins so if
you go inside the active structure where
you install Jenkins and click on LS you
can see the jar file for your code has
been done so using a very simple Jenkins
pipeline you automate the entire process
of compiling packaging building unit
testing and I think it took less than a
minute for the entire thing to happen
probably some couple of seconds for this
entire step level that is how Jenkins
make life so easy overall took says
2.785 seconds and it's 2.785 seconds it
pulled the code from VCS abortion
control system it compiled it it
packaged it it tested it and now
everything looks good it packaged our
code as a chart file so that is how
Jenkins helps us to make our life easier
[Music]
continuous integration is like a
software development life cycle now how
does it work so as you can see here this
pipeline is a logical demonstration of
how a software will move along various
phases or stages in this life cycle
before it is delivered to the customer
or before it is live so just imagine
you're going to build a website which is
going to be deployed on the live web
servers and your product is a website or
a web app now you will have a set of
developers who are responsible for
writing the code which will further go
and build a web app or website so once
the team of developers write this code
and commit the code into the version
control system like get or SVN it goes
to this build phase so this is the first
phase of the pipeline where developers
put in their code and code goes into the
Version Control System having proper
version tags so you know which build has
what kind of version tag so any commit
that happens and you find an error you
can just go to that particular version
and then check the comment so after you
have your version control system face
done then it goes to a build phase where
you compile the code so you get all the
various features of that code from
various branches of repositories then
you merge them and then finally you have
some compiler to compile it and that is
nothing but the build phase so in build
phase what's happening is the code has
been taken from the version control
system and then it is compiled so you
know the code of all the features are
merged and then it is compiled together
once the build phase is done you move on
to the testing phase so in testing phase
we have various kind of testing
depending on the type of the company or
depending on the type of the product
that the company is delivering so one of
the tests in the testing phase is unit
test so what happens in unit test is
that you break your complete software
into small little chunks that is the
complete software is divided into chunks
based on different paths so every part
has a code so it is divided into small
individual units to understand how it
works and when the test is completed you
have an assurance that that particular
part is working
and once each and every part is tested
there is one more test that has to be
performed that is the integration
testing so what happens with integration
testing is now all these individually
tested paths are integrated together and
seen if they can work with each other or
not a software has many features where
they have to work with each other so
that is tested in integration testing
after your testing phase is done you
next move on to the deploy phase and
this is the stage where you deploy your
software onto the staging area or the
test server where you're going to view
the code
so in the deploy phase the code is
deployed successfully and then you can
run another set of strategy tests and
once you think that all the parts are
working and everything is fine you can
finally think that it is acceptable and
then it is deployed onto the production
stage right meanwhile in every step if
there's some error development team
fixes that particular bug and then
pushes it into the version control
system which goes back to the complete
pipeline again so once again if there is
any error reported during testing the
feedback goes back to the dev team they
fix it and that particular product goes
to the complete pipeline again that is
it again goes to the build phase the
testing phase and then it is deployed
onto the plot servers
so this is how in every phase it moves
on throughout the life cycle unless we
get a code or a product which can be
deployed into the production server and
there we can measure and validate that
code
so pipeline is a logical step or a
series of steps which Define how
software development life cycle occurs
so guys this was all about CI CD
pipeline so now that you know a brief
about all the stages in the CI CD
pipeline let me tell you what exactly is
continuous integration
so let's consider a scenario of a team
which has five developers building a
website which has many features so let's
save this website to be a shopping
website right so as we all know that in
a shopping website we have many pages
like you know the user details the past
orders the shopping carts whatever
product a person has added to is card
and so on and then finally we also have
the checkout page and so on right so all
these different pages are known as
features of the software so when you
have multiple features you have to
combine those speeches and merge them
into a single version control system
where you merge the various feature
branches of the code and make a single
pool of the code which can be now
compromising of all the features of
various branches committed by the
developers and then you can build it as
a product and move on to the testing
phase so what are you basically doing
you're combining all the feature
branches that is all the feature codes
into a single branch and once you
combine it successfully so you're then
moving it to the testing phase
now when you merge the code and you
build the code to get a single unit of
the software this is nothing but
integration so integrating means putting
all the code together from all the
contributing developers and then merging
the code and building it as a whole now
you must be wondering why do we call it
as continuous integration the reason is
because everything happens in devops
continuously so everything is automatic
so as soon as the person commits the
code in the Version Control System it
continuously starts merging the process
and then it continuously starts also
building the process simultaneously so
that is the reason that it is known as
continuous integration as nobody has to
wait for anything
after the stage of continuous
integration we have continuous delivery
so as and when we move on to the various
stages in pipeline we finally delivered
the product because as soon as it passes
the test and new features become
functional it can be used as the final
area for the product so everything
happens in a continuous way so the
product is built the product is tested
the product is then deployed onto the
product servers so since it is
continuously integrated built tested and
deployed and delivered this process in
short is known as the CI CD process
now once you have the continuous
integration and continuous delivery in
place the Deployable unit path is called
pipeline now what do I mean by
Deployable unit part I mean the part to
the production servers where it is
deployed onto the live service so now
that you've understood what CI CD
pipeline is let me show you a real life
use case of a company known as Capital
One so Capital One is one of the
well-known brands in America and is
among the top 10 largest banks in the
Nations by deposits it offers a wide
range of financial products and services
to commercial clients small businesses
and individual consumers both online and
inline at branch locations the company
was founded on the vision that the power
of ID and testing could be harmless to
bring the highly customized Financial
products directly to the customers now
the initial challenge was to increase
the automation while ensuring stability
scalability and security as continuous
integration practices were becoming
increasingly mainstream Capital One
formed a continuous delivery tools team
as a part of restructuring the company's
technical organization so this team was
created to support thousands of
developers by providing a solid service
that helps them deliver software
applications more quickly and in a more
agile fashion from a platform
perspective the technical makeup had to
be extremely flexible to meet a wide
variety of use cases but it also had to
be both stable and scalable further as
financial services company Capital One
needed its platform to be secure itself
and promote improved security for the
applications built with it
so with these objectives in mind the new
shared continuous delivery tools team
sought to implement the CI CD practices
that supported security practices
without stifling developer Innovation
with reference to speed and security so
the solution they found out was to
establish a platform for CI CD to
deliver quality software with shorter
time to Market so how did they do that
so the shared continuous delivery tools
team established an internal platform
for automating bills testing and
deployments based on cloud-b's Jenkins
platform they used the role-based access
control plugin to implement an
authorization strategy in which security
roles were assigned to a group of users
after the initial platform rollout the
team made incremental refinements over
the time to improve operational
efficiency they decoupled their Jenkins
Agents from Masters giving them the
ability to scale Masters and agents
independently using the Amazon web
services
the team began using cloud-based Jenkins
operations center to manage multiple
Masters and reduce the administrative
Time by ensuring plug-in compliance and
version consistency with the update
centers so the team settled on a single
default environment for all the groups
on the top of the infrastructure created
by the team a common implementation
pipeline layer was also created so this
is how they use the CI CD pipeline you
know to solve their challenges so after
implementing the CI CD pipeline they saw
that 90 of the pipeline was automated so
this increased deployment frequency with
1300 percent so that's a huge number
isn't it so with the Jenkins platform
they created a service for the
developers that were scalable and stable
as a result of this the time that
developers would have spent managing the
infrastructure was now spent by
developing business applications and
obviously as I said this increased
deployment's frequency by year and year
right and also it was seen that the
quality and the security of all the
repeatable processes was ensured so they
implemented consistent repeatable
processes in an automated fashion every
time a developer wanted to comment or
you know every time a developer wanted
to deploy something so this produced a
more secure software enabling them to
deliver it faster so guys that was the
use case about Capital One so it was
really clear to you right how they face
the challenges to solve the problems
with the help of CI CD and they could
clearly see evident results so in
between the discussion you must have
also observed that I was mentioning
Jenkins platform right so why do you
think that they just use Jenkins why not
anything else so why do you think that
Capital One went for Jenkins platform
why was it so important to them so
Jenkins is a really good automation tool
that you can use now how is Jenkins
gonna help us so let's know so Jenkins
provides us various interfaces and tools
in order to automate the entire process
by entire process I mean the process of
the complete development life cycle so
what happens is that so suppose you have
a get repository where the development
team commits the code and then Jenkins
takes over from this so Jenkins will
pull that code and then it will move it
to the commit phase where the code is
committed to every Branch then Jenkin
moves it and then it goes to the build
phase where it will compile the code now
after the code is compiled and after the
code is validated and reviewed the code
is tested and once all the tests are
done it is finally packaged into an
application it could be either a war
file or a jar file
so what Jenkins does is it is going to
pull your code from your version control
system and then it is going to perform
the build it is going to compile your
code it's going to validate your code
it's going to again review the complete
code after that is done it will perform
the unit testing and the integration
testing and then finally it will package
your application into a war file or a
jar file and then it is ready to be
delivered so that's what Jenkins helps
us to do so now you must have understood
that this is the tool for basic
continuous integration but as I just
said Jenkins role is only till the
application is packaged now if it has to
be delivered then we need some tools to
deliver that product right so now that
you know a lot of theory about cicd
Pipeline and you know the tools for cicd
pipeline that is Jenkins Docker let me
show you practically how a CI CD
pipeline is created and how it is
executed right so let's get started with
our Hands-On so we're going to build a
cicd pipeline using Jenkins and Docker
so for that let me just open my
virtualbox manager where I've installed
Jenkins in my system let's go to my
virtual box
before I start creating my pipeline let
me run the Jenkins service and Docker
right so let me open the terminal
let me enable the Jenkins service and
the docker service before creating the
pipeline so let me type in the command
so for that I'll type in systemctl start
Jenkins
so it will ask me my authentication so
I'll type in my password
and then I'll type in system
CTL enable Jenkins
so as you can see that I've not got the
Privileges because I've not entered the
command as the super user so if you wish
to enter the command as the super user
you have to type in sudo before the
command so you'll type in sudo system
CTL enable Jenkins
and then you'll be asked for the
password so let me type in my password
and then you can see that your Jenkins
server has been enabled now we also want
Docker to run so for that we'll type in
system CTL start docker
again it's asking my authentication so
I'll type in my password
and then Docker has started so our
jankit service and Docker has been
started so now let's open our Jenkins
dashboard so for that I'll open my Local
Host so let me open my Jenkins dashboard
so when you open your Jenkins dashboard
Jenkins always asks for the user ID and
the password mention your user ID and
the password and click on login all
right so this is your Jenkins dashboard
now what I'm going to do is I'm going to
create a pipeline so for that I'll be
using free jobs the first job would be
to build the project the second job
would be to test the project and the
third job would be to finally deploy the
project so for that we'll be creating
free jobs so let me show you how to
create new jobs so for that you have to
go to new item you have to type in the
job names so let me put in the job name
as job1 and choose as freestyle project
and then click on OK
once this is done I'll again go back to
my dashboard and create two more jobs
similarly that is job 2 and job 3. so
let me type in the name job2 and choose
it as freestyle project and click on OK
and when it comes to the configuration
part I'll tell you what you have to
configure soon and then similarly I'll
create a new job that is job free
so let's go back to our dashboard so you
can see that three jobs have been
created now I want to create a pipeline
so what I'll be doing for this pipeline
is I'll be pulling a code from a git
repository and that code is a Java web
code which is basically given to the
development team for the testing so that
is job 2 here and once the testing is
done it will be deployed so for that you
have to mention the repositories URL in
your job to pull that code so let's
mention the repositories URL so for that
I'll go to job one I'll go to configure
and then I'll go to the source code
management tab I'll go to get because
I'm going to mention the repository URL
of a GitHub account
so I'll mention the repositories URL so
this repository has all the files
related to the project so this is where
from Jenkins will pull the code and then
it will perform the build test and
deploy operations in a CI CD pipeline so
I'll click on this and save and apply
similarly I'm going to do it for the
other two jobs also
so we have mentioned our git repository
for all the three jobs now we have to
configure our jobs in such a way that
when it performs the build option it
knows what code it has to compile and
what all has to be installed
respectively
so after mentioning your GitHub
repository URLs for all the three jobs
our first step is to build the source
code to get a VAR file that is an
executable file now what does this mean
basically when the Jenkins server pulls
the code from the gets repository it
uses Maven to install that package so
for that we have to mention the command
of Maven install and then Maven installs
all the dependencies that are required
to compile this application and once the
application passes all the tests
successfully it will then use Docker to
deploy it so this is the process of the
CI CD pipeline that we're going to use
so for that let's go to a job one and
then configure it first so let me open
my job one go to the configure option
go to the build option here go to add
build step choose execute shell because
we're going to execute the shell
commands and then I'll mention the
required commands so the required
commands would be here to you know
mention the directory of our web code
and then also mentioned we will install
so because we want to install all the
dependencies required to compile our
project so let me mention the commands
so as you can see that I've mentioned
the required commands so our first
command is to change the directory of
our web code that is the directory in
which we want the code to work in and
then we are using the command Maven
install to install all the dependencies
required to compile the code after that
we want it to move on to the testing
phase so that the testing is performed
so we're going to change it directly to
the testing phase and then we will
verify the testing results with the help
of this command so this is what you have
to mention in job one after you've
mentioned this you can then save this
job one after that let's configure job 2
job 2 is where we want the testing to be
done and then we want it to be copied to
the docker file so that our application
is deployed onto the plot servers so for
that I'll go again onto the option of
configure
and then I'll go to the build option
I'll go to add build step I'll choose
execute shell and then I'll mention the
required code so after our job one has
been built and tested we want our
application to be deployed onto the
product service so for that we'll build
a Docker container so to do that you'll
use the commands to change the directory
as CD Java web code and then to build a
Docker container you will use the
command Docker build hyphen T devops
pipeline demo so that is the name I've
given you can give any other name that
you want after that you can click on
Save and apply
once this is done let's configure job 3
that is a final stage where everything
is tested and then we just want it to be
deployed onto the plot service but
before that is done we have to check one
thing and that is if our image has been
created previously or not if there is an
image that is previously created then we
have to remove that old files else if it
is not created then we can say that a
new image has been created or if you see
that there is no container existing at
all then you can reply a message that
there is no container existing right so
for that let me just go to the build
option again go to add build step go to
execute shell and then I'll mention the
commands
all right so I've mentioned the required
commands so my container that I've
previously created on job2 that is
devops pipeline demo is running and then
over here I'll check if it is previously
existing or not if it is existing then
I'll remove the files by using the
command RM and if it is not then I'll
print a message saying that the
container does not exist so that's how
you can make sure that your container is
previously existing or not after you're
done with this you can just save and
apply so now that I've told you that we
are creating a pipeline that means all
the jobs must be interconnected so the
pipeline should know after job one is
built what has to be built next and
after job 2 is built what has to be
built next so to do that we have to
mention in the configure options of jobs
so let's go to the first job that is job
one go to the configure option and over
here I'll go to the Post build actions
tab and over here I'll choose build
other projects
so I'll mention the next job that has to
be built after job one that is job two
right so I'll mention job two and then
I'll save and apply
after this is done I'll go back to my
Jenkins dashboard and I'll go to job2
over here
I'll then configure job 2 and then I'll
mention after job 2 is built what has to
be built that is job free
so I'll again go to the postbell actions
go to the build other projects option
and then I'll mention jobs here
so we have mentioned our post Bill
actions so now what will happen is once
job one has been successfully built it
will automatically start building job 2
and once that is done it will
automatically start building job free so
that's how you can build various jobs
one after the other
so as I said that we're going to create
a pipeline so to create a pipeline you
have to go to this plus option here you
have to mention your view name so that
is your pipeline's name so suppose I
mentioned it as devops pipeline so let
me mention the name
and then I'll click on build pipeline
View and then I'll click on OK
now once this is done as I said a
pipeline should know from where it
should start so we're going to mention
job one so that is where our pipeline
will start so I'll go to the pipeline
flow section and then I'll mention the
initial job as job one so that is
already mentioned here and once that is
done I'll click on apply and then click
on OK so that means our pipeline has
been built now if you have to run this
pipeline you just have to go to this
option and run so you can see that if
the job one is in yellow that means it
has started running and job 2 and job 3
are in the sequence
so as you can see that job one has been
successfully built so let me open my
console output and show you so you can
see that our git repository has been
pulled so the code has been compiled all
the maven dependencies are installed
that are required for compiling the
application and finally we can see that
our test results have been successful in
the second job we were creating a Docker
container so let it run so let me show
you how the docker container was created
all right so you can see that our job
too has also successfully built so you
can see that all the artifacts have been
copied and the docker container has been
created so a job 2 was also successful
in building so let's wait for the job
three and once it builds I'll show you
that it has been deployed successfully
all right so you can see that our job 3
has been successfully built so that
means that a CI CD pipeline was
successful
so before I move on to show you the
output let me show you the console
output of Job 3 and you can see that yes
our container did not exist previously
and a new container was created right so
guys that's how you build the CI CD
pipeline now let me show you the results
so I'll go to my local host
so you can see that our application is
running fine
so guys that's how you know you can
build your project and then compile The
Code by installing the required
dependencies and once that is done you
can push it to the test phase where the
product is tested and once it passes all
the tests it is then deployed using the
docker image and once that is also
successful it is deployed onto the
production that you can see in front of
your screen
[Music]
so since Docker is a containerization
platform it's important to understand
what came before containerization or
what is the history of containerization
so before Docker came into the picture
before containerization came into the
picture there was this concept of
virtualization or basically using
virtual machines so virtualization was
this technique of importing a guest OS
on top of an operating system and this
technique was a revelation at the
beginning because it allowed developers
to run multiple operating systems in
different virtual machines all running
on the same host operating system which
nothing but eliminated the need for
extra Hardware resources now the
advantages of virtual machines or of
virtualization are many multiple OSS
could be run on the same machine the
maintenance and Recovery was easy in
case of failure conditions and the third
Point here being the total cost of
ownership was also less due to the
reduced need for infrastructure so as
you you can see here on your screen
right now you can see that there is a
host operating system on which there are
multiple guest os's running which is
nothing but a virtual machine so as most
Concepts have their shortcomings
virtualization also had a few
so running multiple virtual machines on
the same host OS each to the performance
of degradation this is because the guest
OS running on top of the host OS will
have its own kernel own set of libraries
and own dependencies and these take up a
large chunk of the system resources that
is the hard disk processor RAM and other
resources another problem with the
virtual machine which used
virtualization that it takes a lot of
time to boot up so the problem is very
critical in case of radial time
applications so these drawbacks were
always there apart from that you also
have the age-old battle between
development and production teams that
the code works and development and does
not work in Productions because the
developer has a system with their own
set of libraries their own kernel and
the apps running on there whereas the
production team has these resources of
their own so this is a problem that you
can blame upon the difference in the
Computing environment a code that runs
on the developer system might not run on
someone else's computer so this led to a
new technique called containerization
basically a container brings
virtualization to the OS level while
virtualization brings abstraction to the
hardware containerization brings
abstraction to the OS so what you might
notice here is containerization is
basically virtualization but it's more
efficient because there is no guest OS
here it utilizes a host's operating
system shares relevant public libraries
and resources were needed and unlike
virtual machines the application
specific libraries and binaries of
containers run on the host kernel so
each app has its own set of libraries
and binaries in its own little container
which makes processing and execution
extremely fast even if you have to boost
a process it takes only a fraction of a
second because in case of
containerization all the containers
share only the host operating system but
hold all that application related
binaries and libraries in themselves
they are lightweight and faster than
virtual machines so here as you can see
there is your host Os or your host
kernel which is shared by all the
different containers so the containers
themselves only contain the application
specific libraries which are separate
for each container this is what makes
them faster and they do not waste any
resource all these containers are
handled by a containerization Edition
layer which is not native to the host OS
hence a software is needed which can
enable you to run containers on your
host OS and this is also how
containerization solves the difference
in Computing environment problem now a
developer works on containers instead of
virtual machines the app and its
required libraries and binaries all are
in one container so when it is passed on
to the testing team or the production
team it does not matter whether their
host systems have the same libraries all
the dependencies are already present in
the container containing the app
so now that you know what is
virtualization and what is
containerization and why do we need
containerization let's move ahead and
talk about docker
why do we need Docker so challenged I
briefly spoke about earlier is what I
would like to elaborate on in this
section now when you have a project code
in a development lifecycle there are
different different environments you
have your virtual server you have your
staging environment you have your
production environment you have a QA
environment now most of the code that we
deploy today is done using VMS or
virtual machines now how this works is
that a developer or certain developers
will write the code and all of it is
placed in a Version Control System such
as git now this could also be placed in
a staging server depending on your
organization's infrastructure but most
organizations these days use a version
control system so more often than not
the code is placed in that version
control system now this code is never
directly deployed in the production
server it's usually first deployed in a
lower lifecycle server like your QA
server or your staging server now once
it is successful in that that then only
it is deployed on your production server
now this is where the chances of
conflicts increase so the same piece of
code that might be running on your
staging server might not run on your
production server and the reason is very
simple is the difference in the
computational environment this is also
known as infrastructure incompatibility
now in most cases your QA servers or
your QA environment staging environment
are always updated in most organizations
that's the case it usually has the
latest libraries latest binaries latest
jars All That Jazz but the same cannot
be said for the production environment
so in order to face all these challenges
that is faced defensive environment and
lack of optimization of resources we use
Docker so this is where Docker comes
into picture Docker does virtualization
in the software level and we call it
containerization as I had mentioned
before so here as you can see containers
are bundled with their own set of
libraries and binaries but they can
communicate with each other with a fixed
set of protocols the thing about
containers however is that they do not
have their own operating systems which
is a great thing because this is what
makes them lightweight and function on
very little resources which makes them
really fast as well so what exactly is
Docker as you can see on your screens
there are two machines the first one has
three software applications one is
angular based one is react based and one
is Django based now all of them are
using common resources from the systems
libraries Ram processor Etc and the
Frameworks are allocated a location in
the memory now how is it different from
your machine with Docker since you have
a dockerized machine you have a
dockerized system all of your Frameworks
and binaries in library is required by
your angular app your react app and your
Django app can be put into their each
isolated container owners which are
going to run independently from each
other without interfering with another
app and the space in which you had your
framework stored has opened up so now
you can add another software application
there with its own Frameworks and
libraries binaries Etc and that is what
Docker is meant to do it's a tool
designed to make it easier for you to
create deploy and run applications by
the usage of containers Docker is a
containerization solution Docker
containers do not use the guest
operating system they use the host
operating system and on top of that host
operating system there is the docker
engine and with the help of this Docker
engine Docker containers are formed and
these containers have applications
running in them the requirements for
those applications such as all the
binaries libraries Frameworks jars Etc
are also packaged in the same container
there can be multiple containers running
simultaneously as you can see the two
containers here in our example and those
containers have applications running in
them and you don't really have to
pre-allocate any Ram to those containers
these containers allow a developer to
package an application with all its
parts all its needs and then you can
deploy it as one whole package they are
basically lightweight alternatives to
Virtual machines that use the host
operating system this is basically a
general workflow of Docker so you can
see one way of using dock over here what
is happening is that your developer
writes a code and defines an application
requirements or the dependencies in an
easy to write Docker file and then this
Docker file produces Docker images so
whatever dependencies are required for a
particular application is present inside
the image now as we have specified many
times before what are Docker containers
now the docker containers we spoke so
much about are basically the runtime
instances of these Docker images this
particular image is then uploaded onto
Docker hub from where anyone can pull
the image and build a container now
Docker Hub is nothing but the GitHub of
Docker it's like a repository for Docker
images it contains public as well as
private repositories so from the public
repositories you can pull your images
and you can upload your own images as
well to Docker Hub and then as I
mentioned from Docker Hub various teams
such as your QA team or production team
can pull the images and prepare their
own containers as you can see in the
diagram so this shows a great advantage
of Docker is that whatever dependencies
that are there that are required for
your application to run are present
throughout the software delivery life
cycle if you can recall the initial
problem that was there with VMS was
basically that the application worked
fine in a development environment but
when it reached the production
environment it was not working properly
so that particular problem is easily
resolved with the help of this
particular workflow because you have the
same environment throughout the software
delivery life cycle be it a developer
testing QA or production
now before moving on to the next section
let's look at a case study about
containerizing the NASA land information
system framework using Docker now
developed by The hydrological Sciences
laboratory at NASA's Godad space flight
center or gsfc the land information
system or the Lis is a high performance
software framework for terrestrial
hydrology modeling and data assimilation
basically the Lis enables integrating
satellite and ground-based observational
products and advanced modeling
algorithms to extract land surface
States and fluxes now this framework was
very difficult for non-experts to
install due to many dependencies on
specific versions of the software and
compilers this situation then created a
significant barrier to entry for domain
scientists interested in using the
software for their own Computing systems
or in the cloud in addition the
requirement to support multiple runtime
environments across the Allies Community
had created a significant burden on the
NASA team now to overcome these
challenges NASA had deployed Lis using
Docker containers which allowed
installing an entire software package
along with dependencies within a working
runtime environment they also used
Docker swarm which we shall learn about
later to orchestrate the deployment of
the cluster of containers this
installation that used to take weeks or
months was now completed by NASA
officials in minutes either in the cloud
or on premises
now moving on let's look at docker's
workflow and architecture what you see
in front of you is the docker workflow
now the docker engine uses a client
server architecture where the docker
engine is simply a Docker application
that is installed on your local machine
the client server architecture
communicates using a rest API and the
docker Daemon checks the requests to
manage the containers so the docker
architecture includes a Docker client
which is used to trigger Docker commands
a Docker host which runs the docker
demons and a Docker registry which
stores the docker images the docker
Daemon running with the docker host is
responsible for images and containers
we'll understand the concept of images a
little later in this very section as a
part of Docker components so to build a
Docker image you can use the CLI or the
client to issue a bill to command the
docker Daemon which runs on the docker
host
now the docker demon will then build an
image based on your inputs and save it
in the registry which can either be the
docker Hub or a local repository so if
we do not want to create an image then
we can just pull an image from the
docker Hub which would have been built
by different users and finally if we
have to create a running instance of any
Docker image we can issue a run command
from the CLI or the client which will
create our Docker container so this is
basically the overall architecture or
the overall functioning of the docker
architecture as we move forward in this
section things will be more clear to you
so as you can see the heart of the
docker architecture is basically the
docker engine the docker engine is
simply the docker application that is
installed on the host operating system
of your host machine it works like a
client server application which uses a
server which is a type of long running
program called the Daemon process the
second Point here is the command line
interface or the client the next
component here is the rest API which is
used for communication between the CLI
client and the docker daemin so now if
there is a Linux based OS and there is
one Docker client which can be accessed
from the terminal and a Docker host
which runs the docker Daemon we build
our Docker images and run our Docker
containers by passing the commands from
the CLI client to the docker image so
this was all about Docker engine and its
workflow now let's talk about the docker
components which you will be most
acquainted to hearing you have your
Docker file which builds into a Docker
image which you run you get a Docker
container and then you store it in a
Docker registry
let's go ahead and look at all of these
components in a little depth shall we
first of all you have your Docker file
which is nothing but a text document
which contains all of your commands that
you as a developer call on the command
line to assemble an image so basically
to create an image you'll have to write
a Docker file and then build it so
Docker file basically has your set of
instructions which creates the docker
image next you have the docker image
which can be compared to a template used
to create a Docker container so Docker
images are basically the building blocks
of a Docker container these Docker
images are created using the build
command and these are read-only
templates
you can then store them as I had
mentioned before in your Docker Hub or
your local registry next after building
your Docker image what you get is a
Docker container and these are read only
templates
you can then go ahead and store them in
Docker Hub or your local registry as I
had mentioned before Docker lets you
create and share software through Docker
images and you also don't have to worry
about whether your computer can run a
software in a Docker image because a
Docker container is always there to run
it so in case of Docker images you can
use a ready-made Docker image from the
docker Hub or create a new image as per
your own requirements to run in a
container with that let's move on to the
next component which is a Docker
container so the docker containers are
the ready applications created from
Docker images it's the running instance
of a Docker image and this Docker
container holds the entire package
needed to run this Docker application so
a Docker container happens to be the
ultimate utility of Docker and hence
this is the part of Docker which is most
popular and most widely used so if you
talk about the applications every
application is run inside a container so
it is an isolated application platform
that contains all you need to run an
application built from one or more
images
so finally let's move on to Docker
registry which is a storage component
for the docker images now this is where
the docker images are stored which could
either be a user's local repository or a
public repository like the docker Hub
which allows multiple users to
collaborate building an application even
with multiple teams within the same
organization who exchange or share
containers by uploading them to the
docker Hub it helps you control where
your images are being stored and
integrates your image storage with your
in-house development workflow as well
the docker Hub as I've mentioned before
is docker's very own cloud repository
which is similar to GitHub which is kind
of like GitHub but for Docker images
hope you guys are with me so far with
this we have seen the architecture and
components of Docker so now we are going
to talk about two other components of
Docker Docker compose and Dockers form
so first of all let's start with Docker
compose now Docker compose is a tool for
defining and running multi-container
Docker applications which basically
means you can run different or multiple
containers as a single service the
containers are still isolated but they
can interact with each other using a
yaml file which is used to configure
your application services and then with
a single command Docker compose up you
can create and start all the services
from your configuration a great example
of this would be a microservice such as
a shopping app so for example you can
take any online retail stores app it
could be Amazon it could be Flipkart it
could be audio and the idea behind an
app like this is that it's a
microservice app which basically means
it has multiple services in one big
application so all of these Services
individually are easier to build and
maintain and when one service is failing
the entire app is not down so a retail
app like Amazon or myntra or Flipkart or
any app of your choice it'll have
multiple services like it'll have a
login account it'll have your product
catalog service it'll have a shopping
cart service and a checkout service and
this is just the bare minimum and each
of these Services could be scaled up or
down in their own container tested and
built and fixed in their own isolated
containers without interfering with any
other service in that entire application
so using something like Docker compose
you can connect all of these isolated
containers as a single service all right
and the next thing I want to talk about
is Docker swarm now Docker swarm is a
service which allows you to create and
manage a group of either physical or
virtual machines or nodes and schedule
containers each of the nodes is a Daemon
which interacts with others using the
docker API Dockers home is basically a
technique to create and maintain a
cluster of different Docker engines
Services deployed in any node can be
accessed on other nodes in the same
cluster it allows for high availability
of services Auto load balancing
decentralization of access easy
upscaling and downscaling of deployments
and rolling updates so basically how it
works is that your manager nodes know
the status of all the working nodes in a
cluster your worker nodes that accept
tasks sent from the manager node there
is an agent assigned to every single
node to give its task updates to the
manager and finally the workers
communicate with the manager using an
API over the HTTP protocol and that is
how a Docker swarm Works basically
Docker swarm is an orchestration
management tool that runs on Docker
applications it helps you and users in
creating and deploying a cluster of
Docker nodes each node of a Docker swarm
is a Docker Daemon and all the docker
demons interact using the docker API and
each container within the Swarm can be
deployed and accessed by other nodes of
the same cluster so if you consider an
environment having Docker containers if
one of the container fails we can use
the Swarm to correct that failure Docker
swarm can reschedule containers on node
failures
and the Swarm node also has a backup
folder which we can use to restore the
data onto a new swarm so with that we
have come to an end to the workflow and
architecture of Docker so now I'm sure
all of you might be eager for actually
doing something on Docker so let's go
ahead and install docker so here are the
steps to install Docker on Ubuntu on
your Linux systems on your Ubuntu
systems you will be using the command
sudo apt install Docker dot io on your
Ubuntu systems to install Docker on your
system so let's go ahead and start up
with the installation
today I am using the Ubuntu distribution
running on my Oracle VM virtualbox so
we're going to start out by updating so
sudo apt update
and this might take a little bit time so
kindly be patient
this is a good practice before making
any installations any setups on your
machine
all right as you can see all my packages
are upgraded and now we can go ahead and
install Docker so to install Docker we
are going to use the command
pseudo EPT install Docker dot IO
and then click on y
and there's also something which takes
up quite a considerable amount of time
depending on the speed of your machine
as well as the speed of your internet
so on and so forth
Now understand this is just one way of
doing this and this is the method I am
most comfortable with and that's why
this is the method I will be
demonstrating here there are two three
other methods through which you can
download Docker on your machine
and not just in Linux you can also
install Docker desktop on Windows just
in case you do not want to use a virtual
machine
and you can also use a cloud platform
such as an ec2 instance
to run all of these Docker commands that
I shall be showing you later in this
session
so now we are going to start an enable
Docker using sudo system CDL
okay with that we have started and
enabled our Docker machines
and now we are going to go ahead and
check out our Docker version that's
running and as you can see this is the
docker version and this is the docker
build that we are running
all right so moving on we are going to
get to our Hands-On section in this
section we will learn how to create a
Django project setup using Docker and
deploy it on our local host for that
remember you need to have a GitHub
account and Docker installed on your
machine just the way I showed you right
now we are going to build a back-end
rest API with python and Django we're
going to create a new GitHub project
that we are going to use to store The
Source Code for our recipe app API
now this is a wonderful way to show the
versatility of docker
now found this project while browsing
through GitHub and I came to a
conclusion that this was a wonderful way
to show how eclectic how versatile is
Docker as a tool so the first step would
include initializing a new project on
GitHub and coming back to your Linux
machine and navigating to where you wish
to store your API there we are going to
create a Docker file using an editor you
can obviously use the vi editor or Nano
editor if you please they're using the
editor in our working directory we are
going to be creating our dock file our
requirements file and our Docker compose
file which is our yaml file and then we
are going to run our app on the terminal
so this is a basic breakdown of how we
are going to create our Django project
setup using Docker so let's go on
straight to our demo machine and put all
of this into action
so we are going to navigate to GitHub
and we're going to create a new public
project or repository on GitHub so let's
create a new repository and let's call
it
Docker demo API app
and the description we're just going to
write demo app setup we're going to keep
this public and we're going to
initialize this with a readme file all
right so let's go ahead and create our
Repository so as you can see this is a
blank repository Docker demo API app
just the readme file there
so here on our repository page I'm going
to click on this green button code
button and copy the URL from here from
your clone or download option we're
going to copy this to our clipboard
so that we can clone this particular
repository using our terminal I'm going
to move on to the terminal and I'm going
to load my terminal and navigate to the
directory where I want to
store my app
so maybe I'll just save it
here
I'm just going to clone this repository
to my local machine
all right and that's done now you can go
ahead and change your directory to the
Repository
so now I'm going to use the command LS
to see if my repository has been cloned
yes it has been cloned you can see the
docker demo API app clone right here on
my local machine so I'm going to change
my directory
to this repository I just created by
typing the command CD Docker demo API
app now what I'm going to do is I'm
going to open up this project in the
editor like I had mentioned you can use
any other editor you can use the vi
editor the Nano editor any editor that
you're comfortable with
so now I'm going to clear all of this
and I'm gonna use the Nano Editor to
create the docker file first
so just Nano and Docker file there's no
spaces between Docker and file
and I am going to put in this code over
here
all right so this is my Docker file
so the first line of the docker file is
the image that you're going to inherit
your Docker file from with Docker you
can basically build images on top of
other images the benefit of this is that
you can find an image that has pretty
much everything that you need for your
project so you can just add customized
bits that you need just for your
specifications so we are going to create
our Docker file from python 3.7 Alpine
image
now you can find this if you head over
to the docker Hub you can find a list of
available images that you can use to
base your project off of if you search
for python
it'll take you to a list of items
where you can choose this one
so we're gonna use the 3.7 Alpine image
so what it is is that it's basically a
lightweight version of docker
and that's what Alpine means and it runs
python 3.7
and the next line is optional it's
usually a maintainer line but it's
useful just to know who is maintaining
the project
so I'm going to leave that one out but
you can go ahead and put in your
maintenance line you can put your name
or your company's name whatever name you
basically use to keep track to show that
who is maintaining this Docker image
next we are going to set the python
unbuffered environment variable the way
you set unbuffered environment variable
now the way you set an environment
variable in a Docker build file is that
you type EnV and then the environment
variable that you want to set we are
going to set one called Python and it's
got to be in all capitals unbuffered and
then we're going to set it to one so
what this does is that it tells python
to run in the unbuffered mode which is
the recommended mode when running python
within Docker containers the reason for
this is that it doesn't allow python to
buffer the outputs it just prints them
directly and then avoid some
complications and things like that so
when you're running your python
application we're going to install our
dependencies we are going to store our
dependencies in a
requirements.txt list or file which we
are going to create in a moment so that
what we need to do here is we need to
copy our requirements.txt file and what
this does is it says copy from the
directory adjacent to the docker file
copy so we're going to copy the
requirements.txt file further we're
going to copy it on the docker image
which is what this forward slash
requirement txt file means
next we are going to run pip install R
forward slash
requirements.txt so what this does is
that it takes the requirements that we
have just copied and installs it using
pip into your Docker image next we're
going to make a directory within our
Docker image that we can use to store
our application source code so you have
your run mkdir forward slash app all in
lowercase and below that we are going to
type work the forward slash app and
below that we are going to type copy dot
slash app space slash app what this does
is it creates an empty folder in our
Docker image called forward slash app at
this location and then it switches to
that as the default directory so any
application we've run using our Docker
container will run starting from this
particular location unless we specify
otherwise of course next what it does is
that it copies from our local machine
the app folder to the app folder that we
have just created on our image this
allows us to take the code that we
created in our project here and copy it
to our Docker image
so here we have run add user hyphen
capital D and user and finally we are
going to switch to that user by typing
user now this might be a little
confusing because I've added the
username user for our user but what this
command means is that it says add user
which creates a user user hyphen D says
create a user that is going to be used
for running applications only so not for
basically having a home directory that
someone will log in it's going to be
used simply to run up processes from our
particular project
finally this user switches Docker to the
user that we've just created the reason
we do this is for security purposes if
you don't do this then the image will
run our application using the root
account which is not recommended because
that means if someone compromises our
application they can have the root
access to the whole image and they can
go ahead and do other malicious things
whereas if you create a separate user
just for the application then that kind
of limits the scope that the attacker
would have within a Docker container all
right so we can go ahead and write out
we can save this file and move ahead to
creating our requirements file
so this is what your requirements.txt
file will look like it's a fairly it's
just two lines
in the first line here we are installing
Django we are using version 2.2.4 which
is the latest table version that I could
find and that's the version that we are
going to use for the project and then
type Django more than equals to 2.2.4
so basically what it says is that
install Django that is equal to or
higher than this particular version and
we do this to take the minor version
which is the last number and make sure
that we install the latest available
version because typically it's the
version that has the security features
and security fixes and things like that
but typically doesn't have breaking
changes so we can be confident that our
application when we rebuild our Docker
image will have the latest security
patches
even if it does not have the latest
version
okay in the next line we are going to
install the Django rest framework
so we're using
3.9.0
and in the same way which we did with
Django we are going to make the install
one less than
3.10.0 to get the latest version of 3.9
which is whatever is the latest minor
version at the time we build our project
so now we're going to go ahead and save
this file as well
now what we're going to do is we're
going to go ahead and create
an empty directory called
app here for our Docker file to call
because when our Docker file will run
and it's going to ask for an app folder
we're not going to have one then we're
going to face issues
so now if I do an LS here you see we
have a Docker file we have our
requirements file and we have our app
folder
Now understand why this is required for
our Docker file for us to build it now
so now if you try to copy it without the
app folder actually existing it will
give you an error so now we can go ahead
and build our Docker image so we are
already at a terminal if you're not on
your terminal you can load up your
terminal and if you are on Windows it'll
be your command prompt let's make sure
that you navigate to your project folder
which I am already at
and then I'm going to type Docker build
Dot now what that's going to do is that
it's going to build whichever Docker
file is in the root of our project that
we are currently in
the reason we call a Docker file in the
first place is because it's the standard
convention that Docker uses to identify
the docker file within our project
so we're going to wait for it to
complete and it should be fairly quick
because we are using an Alpine image
it's a very lightweight minimal image
that runs python that's how we create a
Docker file for our project and go ahead
and create our Docker compose
configuration for our project
so we're going to use Docker compose as
a tool to allow us to run our Docker
image easily from our project location
so it allows us to easily manage the
different services that make up for our
project so for example one service might
be the python application that we run
another service might be a database for
example
so let's go ahead and create the docker
compose file for our project
so Nano Docker compose dot yaml or yml
so this is a yaml file that contains the
configuration for all services that make
up our project the first line of the
docker compose configuration file is the
version of Docker compose that we are
going to be writing our file for so
version colon and three next we Define
the services that make up our
application right now we only need one
service for a python Django application
so Services colon and in the line below
you're going to type app this is the
name of our service so we are going to
call it out and then we are going to
type build and then context code on
context so what this says is that we are
going to have a service called app and
the build section of the configuration
we are going to set the context to dot
which is our current directory that we
are running Docker compose from next
we're going to type the board configure
duration so we're going to type them in
open speech marks 8080 or is our host to
port 8080 on our image so we're going to
type that and then we're going to add a
volume the volume allows us to get the
updates that we make to our project into
our Docker image in real time so it
basically Maps a volume from our local
machine here into our Docker container
while we'll be running our application
now this means whenever you change a
file or you change something in the
project you will automatically update it
in the container now the good part is
you don't have to restart Docker to get
the changes into effect
so then you have a DOT slash app
forward slash app
so what this does is it Maps the app
directory which we have here in our
project
and this it maps to the app directory in
our Docker image then you have the
command to run your application in your
Docker container
so just make sure that the indentation
is one indent from where your command
starts and then you type the command so
we're going to use to run our
application be sh hyphen see this means
we are going to run the command using
the Shell
the python manage py run server then
we're going to run the server on our
local host at port 8080 so this will run
the Django development server available
on all IP addresses that run on the
docker container
and it's going to run on port 8080 which
is going to be mapped through the pause
configuration on our local machine so we
can run our application
so again we are going to move on to our
terminal and build the docker compose
file and what this does is it will build
our image using the docker compose
configuration and this should not take
too much time so if we're going to build
our Docker file
so now we're going to add sudo
Docker compose
build
and as you can see it's successfully
built
clear it out
so now we're going to use Docker compose
to create the project files for our
application and for that you can either
log in as super user or just type sudo
so Django
admin dot Pi we're gonna start project
now after running the command you will
realize that if you go into your empty
app folder which is the folder that you
created initially
and list out it's not empty anymore it
has another app folder in it which
contains all of our app setup I will
show you
there you go you have these files one
two three four five python files
which is basically your Django apps
entire setup and all of this Docker
created and set it up for you so a
Teensy little detail let's just
open up our
settings.py file and we're gonna go down
to allowed hosts and just in case it's
empty between the close brackets you can
go ahead and add your local host or put
in the asterisk which allows all hosts
all right so I'm just gonna do that
and then I'm gonna go back
and now all we have to do is deploy the
application through your terminal
so we're gonna go sudo
Docker compose
up
so now as you can see your Django
application is up and running
and all of this was possible because of
Docker
[Music]
with the ever-changing requirements of
modern software development it is known
that kubernetes is one system that will
fulfill these needs if you are someone
who wants to get started with kubernetes
you're definitely in the right place let
us start off by understanding what it
really is with the help of a simple
example imagine you're building a web
application so within this web
application the structure might be
monolithic it might compromise several
modules components or even layers but
externally it is a single web
application or you could also say it is
a single service despite having
different modules the application is
built and deployed as one service for
all platform that is desktop personal
computer mobile Etc within a single
container that is a virtual machine
let us assume the web application grows
over time and becomes large it is now
extremely challenging to make changes
over the code accurately also the size
of the application can slow down the
startup time moving on what if you want
to add new updates to this web
application you must redeploy the entire
application as one and during this
process in case you stop and try to
restart the service the customers on
your website will not be able to access
anything for that period of time which
could eventually lead to a potential
loss of money also bugs in any one of
the components can bring down the entire
process and since all the components of
the application are identical bugs can
definitely impact the availability of
the application
now what if you have a sudden spike in
traffic and have to serve 20 times the
number of your usual customers in such a
situation you have to very quickly
deploy new servers which takes a lot of
effort later when traffic dies down to a
normal level you now have 20 times the
number of your usual servers it can also
be challenging to scale when different
modules have conflicting updates or
resource requirements so instead of
creating a monolithic application we can
Define each component as a separate
Loosely coupled service and then we
could virtualize each of these services
using a Docker container not only can
the application start easily but the
services can also be deployed
independently with Docker containers we
can easily scale any number of instances
up and down according to the
applications requirements but this is
definitely easy to manage when we're
dealing with just three or four
containers now let's say we had to deal
with thousand such containers how do we
really keep track of the containers that
are healthy and the ones that are not
orchestrating and managing these
containers at the same time can be an
extremely tedious task and this led to
the need for container orchestration
tools so this is where kubernetes comes
into the picture kubernetes was
originally designed by Google and is now
maintained by the cloud native Computing
foundation with kubernetes you package
your website files as containers or pods
which you can run on any instance in
your kubernetes cluster as you can see
pods are running on nodes which are
physical systems having Hardware
components that are a part of the
kubernetes cluster this implies that
every service that you create runs on
separate containers and this will allow
you to work in manage services
independently
pods can be running on any available
node in the cluster which has enough
resources for a pod to work in simple
words kubernetes is an open source
system for automating deployment rolling
updates scaling services and managing
containerized applications kubernetes
works with Amazon ec2 Rackspace gcp IBM
Cloud Etc
also hyper growth companies like GitHub
reworks Cloud boost Walmart are becoming
acclimated to building shipping and
managing containers via kubernetes an
average anyone with kubernetes knowledge
can expect to earn between 5 to 6 lakhs
per annum in India whereas they can
expect an average salary of one forty
thousand dollars per annum in USA as the
Fortune 500 are used in kubernetes the
demand for expertise in this field is
rapidly increasing which in turn
requires it professionals who can work
on the store so what are you waiting for
begin your journey with edureka's
interactive kubernetes certification
training that is curated to help you
learn how to set up your own kubernetes
cluster configure a networking between
pods and secure the cluster against
unauthorized access
foreign
why do we need kubernetes now to
understand why do we need kubernetes
let's understand what are the benefits
and drawbacks of containers now first of
all containers are good they are
amazingly good right any container for
that matter of fact a Linux container or
a Docker container or even a rocket
container right they all do one thing
they package your application and
isolate it from everything else right
they isolate the application from the
host mainly and this makes the container
fast reliable efficient lightweight and
scalable now hold the thought yes
containers are scalable but then there's
a problem that comes with that and this
is what is the resultant of the need for
kubernetes even though containers are
scalable they are not very easily
scalable okay so let's look at it this
way you have one container you might
want to probably scale it up to two
containers or three containers well it's
possible right it's going to take a
little bit of manual effort but yeah you
can scale it up you're not going to have
a problem but then look at a real world
scenario where you might want to scale
up to like 50 to 100 containers then in
that case what happens I mean after
scaling up what do you do you have to
manage those containers right you have
to make sure that they're all working
they're all active and they're not
talking to each other because if they're
not talking to each other then there's
no point of scaling up itself because in
that case the servers would not be able
to handle the roads if they're not able
to talk to each other correct so it's
really important that they are
manageable when they are scaled up and
now let's talk about this point is it
really tough to scale up containers well
the answer for that might be no it might
not be tough it's pretty easy to scale
up containers but the problem is what
happens after that okay uh once you
scale up containers you'll have a lot of
problems like I told you the containers
first of all should have to communicate
with each other because there are so
many in number and they work together to
basically host the service right the
application and if they are not working
together and talking together then the
application is not hosted and scaling up
as a waste so that's the number one
reason and the next is that the
containers have to be deployed
appropriately and they have to also be
managed they have to be deployed
appropriately because you cannot have
the containers deployed in just random
places you have to deploy them in the
right places you cannot have one
container in one particular cloud and
the other one somewhere else so that
would have a lot of complications well
of course it's possible but yeah it
would lead to a lot of complications
internally you want to avoid all that so
you have to have one place where
everything is deployed appropriately and
you have to make sure that the IP
addresses are set everywhere and the
port numbers are open for the containers
to talk to each other and all these
things right so these are the two other
points the next point or the next
problem with scaling up is that auto
scaling is never a functionality over
here okay and this is one of the things
which is the biggest benefit with
kubernetes the problem technically is
there's no Auto scaling functionality
okay there's no concept of that at all
and you may ask at this point of time
why do we even need Auto scaling okay so
let me explain the need for auto scaling
with an example so let's say that you
are an e-comce portal okay something
like an Amazon or a Flipkart and let's
say that you have decent amount of
traffic on the weekdays but on the
weekends you have a spike in traffic
probably you have like 4X or 5x the
usual traffic and in that case what
happens is maybe your servers are good
enough to handle the requests coming in
on weekdays right but the requests that
come on the weekends right from the
increased traffic that cannot be
serviced by your servers right maybe
it's too much for your servers to handle
the load and maybe in the short term
it's fine maybe once or twice you can
survive but they will definitely come a
time when your server will start
crashing because it cannot handle that
many requests per second or permanent
and if you want to really avoid this
problem what do you do you have to scale
up and now would you idly keep scaling
up every weekend and scaling down after
the weekend right I mean technically is
it possible will you be buying your
servers and then setting it up and every
Friday would you be again buying new
servers setting up your infrastructure
and then the moment your weekday starts
would you just destroy all your servers
whatever you built would that would you
be doing no right obviously that's a
pretty tedious task so that's where
something like kubernetes comes in and
what kubernetes does is it keeps
analyzing your traffic and the load
that's being used by the container and
as and when the traffic is reaching the
threshold Auto scaling happens when if
the servers have a lot of traffic and if
it needs you know more such servers for
handling requests then it starts scaling
up the containers on its own there's no
manual intervention needed at all so
that's one benefit it with kubernetes
and one traditional problem that we have
with scaling up of containers okay and
then yeah the one last problem that we
have is the distribution of traffic that
is still challenging without something
that can manage your containers I mean
you have so many containers but how will
the traffic be distributed load
balancing how does that happen you just
have containers right you have for three
containers how does the load balancing
happen so all these are questions we
should really consider because
containerization is all good and cool it
was much better than VMS yes
containerization it was basically a
concept which was sold on the basis of
scaling up right we said that VMS cannot
be scaled up easily so we told use
containers and with containers you can
easily scale up so that was the whole
reason we basically sold containers with
the tagline of scaling up but in today's
world our demand is ever more that even
the regular containers cannot be enough
so scaling up is so much or in so detail
that we need something else to manage
your containers correct do we agree
there we need something right and that
is exactly what kubernetes is so
kubernetes is a container management
tool all right so this is open source
and this basically automates your
container deployment your container
scaling and descaling and your container
load balancing the benefit with this is
that it works brilliantly with all the
cloud vendors with all your public Cloud
vendors or your hybrid Cloud vendors and
it also works on premises so that is one
big selling point of kubernetes right
and if I should give more information
about kubernetes then let me tell you
that this was a Google developed product
okay it's basically a brainchild of
Google and that pretty much is the end
of the story for every other competitor
out there because the community that
Google brings in along with it is going
to be huge or basically the Head Start
that kubernetes would get because of
being a Google brain child is humongous
and that is one of the reasons why
kubernetes is one of the best container
management tools in the market period
and given that kubernetes is a Google
product they have written the whole
product on go language and of course now
Google has contributed this whole
kubernetes project to the the cncf which
is nothing but the cloud native
Computing Foundation or simply Cloud
native Foundation right you can just
call them either that and they have
donated their open source project to
them and if I have to just summarize
what kubernetes is you can just think of
it like this it can group like a number
of containers into one logical unit for
managing and deploying an application or
a particular service so that's a very
simple definition of what kubernetes is
it can be easily used for deploying your
application of course it's going to be
Docker containers which you will be
deploying but since you will be using a
lot of Docker containers as part of your
production you will also have to use
kubernetes which will be managing your
multiple Docker containers right so this
is the role it plays in terms of
deployment and scaling up scaling down
is primarily the game of kubernetes from
your existing architecture it can scale
up to any number you want it can scale
down anytime and the best part is the
scaling can also be set to be automatic
like I just explained some time back
right you can make kubernetes kubernetes
would analyze the traffic and then
figure out if the scaling up needs to be
done or the scaling down can be done and
all those things and of course the most
important part load balancing right I
mean what good is your container or
group of containers if load balancing
cannot be enabled right so kubernetes
does that also and these are some of the
points on based on which kubernetes is
built so I'm pretty sure you would have
got a good understanding of what
kubernetes is by now right a brief idea
at least if you have any doubts then I
would request you to put your comments
or queries on the comment section below
all right my edrica team will get back
to you at the earliest so moving forward
let's look at the features of kubernetes
Okay so we've seen what exactly
kubernetes is how it uses Docker
containers or other containers in
general but now let's see some of the
selling points of kubernetes or why it's
a must for you let's start off with
automatic bin packing when we say
automatic bin packing it's basically
that kubernetes packages your
application and it automatically places
containers based on their requirements
and the resources that are available so
that's the number one advantage the
second thing service Discovery and load
balancing there is no need to worry I
mean if uh you know if you're if you're
going to use kubernetes then you don't
have to worry about networking and
communication because kubernetes will
just automatically assign containers
their own IP addresses and probably a
single DNS name for a set of containers
which are performing a logical operation
and of course there will be load
balancing across them so you don't have
to worry about all these things so
that's why they say that there is a
service Discovery and load balancing
with kubernetes and the short feature of
kubernetes is that storage orchestration
with kubernetes you can automatically
Mount your storage system of your choice
you can choose that to be either a local
storage or maybe on a public cloud
provider such as a gcp or AWS or even a
network storage system such as NFS or
other things right so that was the
feature number three now feature number
four self healing now this is one of my
favorite parts of kubernetes actually
not just kubernetes even with respect to
Docker swarm I really like this part of
self-healing what self-healing is all
about is that whenever kubernetes
realizes that one of your containers has
failed then it will restart that
container on its own right it will
create a new container in place of this
crashed one and in case your node itself
fails then what kubernetes would do in
that case is whatever containers were
running in that failed node those
containers would be started in another
node right of course you would have to
have more nodes in that cluster if
there's another node in the cluster
definitely room would be made for this
failed container to start a service so
that happens so the next features batch
execution so when we say batch execution
it's that along with Services kubernetes
can also manage your batch and cie
workloads which is more of a devops role
right so as part of your cie workloads
kubernetes can replace your containers
which fail and it can restart and
restore the original state that is what
is possible with kubernetes and secret
and configuration management that is
another big feature with kubernetes and
that is the concept of where you can
deploy and update your secrets and
application configuration without having
to rebuild your entire image and without
having to expose your secrets in your
stack configuration or anything right so
if you want to deploy and update your
secrets only that can be done so it's
not available with all the other tools
right so kubernetes is one that does
that you don't have to restart
everything and rebuild your entire
container that's one benefit and then we
have horizontal scaling which of course
you people might aware of already you
can scale your applications up and down
easily with a simple command the simple
command can be run on the CLI or you can
easily do it on your GUI which is your
dashboard your kubernetes dashboard or
Auto scaling is possible Right based on
the CPU usage your containers would
automatically be scaled up or scaled
down so that's one more feature and the
final feature that we have is automatic
rollbacks and rollouts now kubernetes
what it does is whenever there's an
update your application which you want
to release kubernetes progressively
rolls out these changes and updates to
the application or its configurations by
just ensuring that one instance after
the other is send these updates and it
makes sure that not all instances are
updated at the same time thus ensuring
that yes there is high availability and
even if something goes wrong then the
communities will roll back that change
for you so all these things are enabled
and these are the features with
kubernetes so if you're really
considering a solution for your
containers for managing your containers
then kubernetes should be your solution
right so that should be your answer so
that is about the various features of
kubernetes now moving forward here let's
talk about few of the myths surrounding
kubernetes and we are doing this because
a lot of people have confusion with
respect to what exactly it is so people
have this misunderstanding that
kubernetes is like Docker which is a
containerization platform right that's
what people think and that is not true
so this kind of a confusion is what I
intend to solve in the upcoming slides I
will talk about what exactly kubernetes
is and what kubernetes is not let me
start with what it's not now the first
thing is that kubernetes is not to be
compared with Docker because it's not
the right set of parameters which you
are comparing them against Docker is a
containerization platform and kubernetes
is a container management platform which
means that once you have containerized
your application with the help of Docker
containers or Linux containers and when
you are scaling up these containers to a
big number like 50 or 100 that's where
kubernetes would come in when you have
like multiple containers which need to
be managed that's where kubernetes can
come in and effectively do it you can
specify the configurations and
kubernetes would make sure that at all
times these conditions are satisfied so
that's what kubernetes is you can tell
in your configurations that at all time
I want these many containers running I
want these many pods running and so many
other needs right you can specify much
more than that and whatever you do at
all times your cluster master or your
kubernetes master would ensure that this
condition is satisfied so that is what
exactly kubernetes is but that does not
mean that Docker does not solve this
purpose so Docker also have their own
plugin I wouldn't call it a plug-in uh
it's actually another tool of theirs so
there's something called as Docker swarm
and Docker swarm does a similar thing it
does contain a management like Mass
container management so similar to what
kubernetes does when you have like 50 to
100 containers Docker swarm would help
you in managing those containers but if
you look at who is prevailing in the
market today I would say it's kubernetes
because kubernetes came in first and the
moment they came in they were backed by
Google and they had this huge Community
which they just swept along with them so
they have like hardly left any in any
market for Docker and for Docker swarm
but that does not mean that they are
better than Docker because they are at
the end of the day using Docker so
kubernetes is only as good as what
Docker is if there are no Docker
containers then there's no need for
kubernetes in the first place so
kubernetes and Docker they go hand in
hand okay so that is the point you have
to note and I think that would have also
explained the point that kubernetes is
not for container using applications
right and the last thing is that
kubernetes is not for applications with
a simple architecture okay if your
architecture if your applications
architecture is pretty complex then you
can probably use kubernetes to uncomplex
that architecture okay but if you're
having a very simple one in the first
place then using kubernetes would not
serve you any good and it could probably
make it a little more complicated than
what it already is right so this is what
kubernetes is not now speaking of what
exactly kubernetes is the first point is
kubernetes is robust and reliable now
when I say robust and reliable I'm
referring to the fact that the cluster
that is created the kubernetes cluster
right this is very strong it's very
rigid and it's not going to be broken
easily the reason being the
configurations which you specify right
at any point of time if any container
fails a new container would come up
right or that whole container would be
restarted one of the things will
definitely happen if your node fails
then the containers which are running in
a particular node they would start
running in a different load right so
that's why it's reliable and it's strong
because at any point of time your
cluster would be at full force and at
any time if it's not happening then you
would be able to see that something's
wrong and you have to troubleshoot your
node and then everything would be fine
so kubernetes would do everything
possible and it pretty much does
everything possible to let us know that
the problem is not at its end and it's
giving the exact result that we want
that's what kubernetes are doing and uh
the next thing is that kubernetes
actually is the best solution for
scaling up containers at least in
today's market because the two biggest
players in this market are Docker swarm
and kubernetes and Docker swarm is not
really the better one here because they
came in a little late even though Docker
was there from the beginning kubernetes
came after that but Docker swarm which
we are talking about came in somewhere
around 2016 or 2017 right but kubernetes
came somewhere around 2015 and they had
a very good Head Start they were the
first ones to do this and their backing
by Google is just icing on the cake
because whatever problem you have with
respect to Containers if you just go up
and if you put your error there then you
will have a lot of people on github.com
and GitHub queries and then on stack
Overflow who will be resolving those
errors right so that's the kind of
Market they have so it's backed by a
really huge Community that's what
kubernetes is and to conclude this slide
kubernetes is a container orchestration
platform and nothing else all right so I
think these two slides would have given
you more information and more clarity
with respect to what kubernetes is and
how different it is from Docker and
Docker swarm right if you guys still
have any doubts then I would request you
to put your comments in this comment
section below and my team will get back
to you at the earliest okay so now
moving on uh let's go to the next topic
where we will compare kubernetes with
Docker swarm and we are comparing with
Docker swarm because we cannot compare
Docker and kubernetes head-on okay so
that is what you have to understand if
you are this person over here if you are
Sam who is wondering which is the right
comparison then let me reassure you that
the difference can only be between
kubernetes and Docker swamp Okay so
let's go ahead and see what the
difference is actually let's start off
with your installation and configuration
okay so that's the first parameter we'll
use to uh compare these two and over
here Docker swarm comes out on top
because Docker is a little easier you
have around two or three commands which
will help you have your cluster up and
running that includes the node joining
the cluster right but with kubernetes
it's way more complicated than Docker
swarm right so you have like close to 10
to 11 commands which you have to execute
and then there's a certain pattern you
have to follow to ensure that there are
no errors right yes and that's why it's
time consuming and that's why it's
complicated but once your cluster is
ready that time kubernetes is the winner
because the flexibility the rigidness
and the robustness that kubernetes gives
you cannot be offered by Docker swarm
yes Docker swarm is faster but yes not
as good as kubernetes when it comes to
your actual working and and speaking of
the GUI once you have set up your
cluster or you can use a GUI with
kubernetes for deploying your
applications right so you don't need to
always use your CLI you have a dashboard
which comes up and the dashboard if you
give it admin privileges then you can
use it you can deploy your application
from the dashboard itself everything
just drag and drop click functionality
right with just click functionality you
can do that the same is not the case
with Docker swamp you have no GUI in
Docker swamp Okay period so Docker swarm
is not the winner over here it's
kubernetes and yes going to the third
parameter scalability so people again
have a bad misconception that kubernetes
is better it is the solution for scaling
up and it is better and faster than
Docker swarm well it could be better but
yes it's not faster than Docker swarm uh
even if you want to scale up right
there's a report where I recently read
that uh the scaling up in Docker swarm
is almost five times faster than the
scaling up with kubernetes so that is
the difference but yes once your scaling
up is done after that your cluster's
strength with kubernetes is going to be
much stronger than your Docker swarm
cluster strength that's again because of
the various configurations that should
have been specified by then that is the
thing now moving on to the next
parameter we have is uh load balancing
requires manual service configuration
okay this is in case of kubernetes and
yes this could be shortfall but with
Docker swarm there is inbuilt load
balancing techniques which you don't
need to worry about okay even the load
balancing which requires manual effort
in case of kubernetes is not too much
there are times when you have to
manually specify what are your
configurations you have to make a few
changes but yes it's not as much as what
you're thinking and speaking of updates
and rollbacks what kubernetes does is it
does the process scheduling to maintain
the services while updating okay uh yeah
that's very similar to how it works with
Docker swarm wherein you have like
Progressive updates and service Health
monitoring happens throughout the update
but the difference is when something
goes wrong kubernetes goes that extra
mile of doing a rollback and putting you
back to the previous state right before
the update was launched so that is the
thing with kubernetes and the next
parameter we are comparing those two
upon is data volumes so data volumes in
kubernetes can be shared with other
containers but only within the same pod
so we have a concept called pods in
kubernetes okay now pod is nothing but
something which groups related
containers right a logical grouping of
containers together so that is a part
and whichever containers are there
inside this pod they can have a shared
volume okay like storage volume but in
case of Docker swarm you don't have the
concept of power at all so the shared
volumes can be between any other
container there is no restriction with
respect to that and dog as one and then
finally we have this parameter called
logging and monitoring so when it comes
to logging and monitoring kubernetes
provides inbuilt tools for this purpose
okay but with Docker swamp you have to
install third-party tools if you want to
do logging and monitoring so that is the
fallback with Docker swamp because
logging is really important one because
you will know what the problem is you'll
know which container failed what have
happen that is exactly the error right
so logs would help you give that answer
and monitoring is important because you
have to always keep a check on your
nodes right so as the master of the
cluster it's very important that there's
monitoring and that's where kubernetes
has a slight advantage over Docker sperm
okay so that's my take on the difference
between kubernetes and Docker swarm and
I would also love to hear what's your
take on this right I would like to know
if you would use kubernetes or Docker
swarm and I would also know why if you
prefer Simplicity or if you prefer
stability that choice could be yours and
it's something that's arguable right
it's a debatable topic so I would like
to hear what are your thoughts so please
drop in your comments at the bottom we
have a comment section right our videos
so put it right there and probably in
one of our future videos we will discuss
about that but before I finish this
topic there is this one slide I want to
show you which is about the statistics
so this stat I picked it up from this
platform line which is nothing but a
company that that writes about tech okay
and what they've said is that the number
of news articles that were produced
right in that one particular year had 90
of those covered on kubernetes compared
to the 10 on Docker swamp amazing right
that's a big difference that means for
every one blog written or for every one
article written on Docker swarm there
are nine different articles written on
kubernetes and similarly for web
searches for web searches kubernetes is
90 compared to Docker swarms 10 and
Publications GitHub Stars the number of
comments on GitHub all these things are
clearly won by kubernetes everywhere so
kubernetes is the one that's dominating
this market and that's pretty visible
from this stat also right so uh I think
that pretty much brings an end to this
particular topic now moving forward let
me show you a use case let me talk about
how this game this amazing game called
Pokemon go was powered with the help of
kubernetes I'm pretty sure you all know
what it is right you guys know Pokemon
go it's the very famous game and it was
actually the best Game of the Year 2017
and the main reason for that being the
best is because of kubernetes and let me
tell you why but before I tell you why
there are a few things which I want to
just talk about I'll give you a overview
of what Pokemon go is and let me talk
about a few key stats so Pokemon go is
an augmented reality game developed by
Niantic for your Android and for iOS
devices okay and those key stats read
that they've had like 500 million plus
downloads overall and 20 million plus
daily active users now that is massive
daily if you're having like 20 million
users plus then you have achieved an
amazing thing so that's how good this
game is okay and then this game was
actually initially launched only in
North America Australia New Zealand and
I'm aware of this fact because I'm based
out of India and I did not get access to
this game because the moment news got
out that we have a game like this I
started downloading it but I couldn't
really find any link or I couldn't
download it at all so they launched it
only in these kind trees but what they
faced right in spite of just releasing
it in these three countries they had
like a major problem and that problem is
what I'm going to talk about in the next
slide right so my use case is based on
that very fact that in spite of
launching it only in these three
countries or in probably North America
and then in Australia and New Zealand
they could have had a meltdown but
rather with the help of kubernetes they
used that same problem as the basis for
their success so that's what happened
now let that be assessments and uh
before I get to that let me just finish
this slide one amazing thing about
Pokemon go is that it has inspired users
to walk over 5.4 billion miles an hour
okay yes do the math 5.4 billion miles
in one year that's again a very big
number and it says that it has surpassed
engineering Expectations by 50 times now
this last line is not with respect to
the Pokemon Go the game but it is with
respect to the back end and the use of
kubernetes to achieve whatever was
needed okay so I think I've spent enough
time over here let me go ahead and talk
about the most interesting part and tell
you how the backend architecture of
Pokemon go was okay so you have a
Pokemon go container which had two
primary components one is your Google
big table which is your main database
where everything is going in and coming
out and then you have your programs
which is run on your java Cloud right so
these two things are what is running
your game mapreduce and cloud data flow
or something which was used for uh
scaling up okay so it's not just the
container scaling up but it's with
respect to the application how the
program would react when there are these
increased number of users and how to
handle increased number of requests so
that's where the map reduces the
Paradigm comes in right the mapping and
then reducing that whole concept so this
was their one deployment okay and when
we say into Phi it means that they had
their server capacities which could go
up till five times okay so technically
they could only serve X number of
requests but in case of affiliate
conditions or heavy traffic load
conditions the max the server could
handle was 5X because after 5x the
server would start crashing that was
their prediction okay and what actually
happened at Pokemon go on releasing in
just those three different geographies
is that the moment they deployed it the
usage became so much that it was not X
number of times which is technically
their failure limit and it is not even
5x which is the server's capability but
the traffic that they got was up to 50
times 50 times more than what they
expected so you know that when your
traffic is so much then you're gonna be
brought down to your knees that's a
definite and that's a given right this
is like a success story and this is too
good to be true kind of a story and in
that kind of a scenario if the requests
start coming in are so much that if they
reach 50x then it's gone right the
application is gone for a toss so that's
where kubernetes comes in and uh they
overcome all the challenges how do they
overcome the challenges because
kubernetes can do both vertical scaling
and horizontal scaling at ease and that
is the biggest problem right because any
application and any other company can
easily do horizontal scaling where you
just spin up more containers and more
instances and you set up the environment
but vertical scaling is something which
is very specific and this is even more
challenging now it's more specific to
this particular game because the virtual
reality would keep changing whenever a
person moves around or walks around
somewhere in his apartments or somewhere
on the road then the ram right that
would have to increase the memory the in
memory and the storage memory all this
would increase so in real time your
server's capacity also has to increase
vertically so once they have deployed it
it's not just about horizontal
scalability anymore it's not about
satisfying more requests it's about
satisfying that same request with
respect to having more Hardware space
more RAM space and all these things
right that one particular server should
have more performance abilities that's
what it's about and kubernetes solved
both of these problems effortlessly and
Niantic were also surprised that
kubernetes could do it and that was
because of the help that they got from
Google I read an article recently that
they had a Niantic slab he met with some
of the top Executives in Google and in
gcp right and then they figured out how
things are supposed to go and they of
course met with the heads at kubernetes
and they figured out a way to actually
scale it up to 50 times in a very short
time so that is the challenge that they
were presented and thanks to kubernetes
they could handle 50 times the traffic
that they expected which is like a very
one-off story and which is very very
surprising that you know something like
this would happen so that is about the
use case and that pretty much brings an
end to this topic of how Pokemon go used
kubernetes to achieve something because
in today's world Pokemon go is a really
reward game because of what it could
right it basically beat all the
stereotypes of a game and whatever
anybody could have anything negative
against the game right so they could say
that these mobile games and video games
make you lazy they make you just sit in
one place and all these things right and
uh Pokemon go or something which was
different it actually made people walk
around and it made people exercise and
that goes on to show how popular this
game became if kubernetes lies at the
heart of something which became so
popular and something that became so big
then you should imagine how big
kubernetes or how useful kubernetes is
right so that is about this topic as
said before kubernetes Master controls
your nodes and inside nodes you have
containers okay and now these containers
are not just contained inside them but
they are actually contained inside pods
okay so you have nodes inside which
there are pots and inside each of these
spots there will be n number of
containers depending upon your
configuration and your requirement right
now these pods which contain a number of
containers are a logical binding or a
logical grouping of these containers
supposing you have an application X
which is running in node one okay so you
will have a pod for this particular
application and all the containers which
are needed to execute this particular
application will be a part of this
particular pod right so that's how pod
works and that's what the difference is
with respect to Docker swarm and
kubernetes because in Docker swamp you
will not have a pod you just have
containers running on your node and the
other two terminologies which you should
know is that a replication controller
and service your replication controller
is the Master's resource to ensuring
that the requested number of pods are
always running on the nodes right so
that's like your confirmation or an
affirmation which says that okay this
many number of PODS will always be
running and these many number of
containers will always be running
something like that right so you see it
and the replication controller will
always ensure that's happening and your
service is just an object on the master
that provides load balancing across a
replicated group of PODS right so that's
how kubernetes works and I think this is
good enough introduction for you and I
think now I can go to the demo part
right guys if you guys have any doubts
so I would request you to again put it
in the comment section and my team will
get back to you and in the meanwhile let
me get to the final part of this video
wherein I will show you how to deploy
applications on your kubernetes via
either your CLI or either via your yaml
files or via your dashboard
okay guys so let's get started and for
the demo purpose I have two VMS with me
okay so as you can see this is my Cube
Master which would be acting as my uh
master in my cluster and then I have
another VM which is my Cube node one
okay so it's a cluster with one master
and one node all right now for the ease
of purpose for this video I have
compiled the list of commands in this
text document right so here I have all
the commands which are needed to start
your cluster and then the other
configurations and all those things so
I'll be using these I'll be copying
these commands and then I'll show you
side by side and I will also explain
when I do that as to what each of these
commands mean now there's one
prerequisite that needs to be satisfied
and that is the master should have at
least two core CPUs okay and 4GB of RAM
and your node should have at least one
core CPU and 4GB of ram so just make
sure that this much of Hardware is given
to your VMS right if you're using Ubuntu
to our Linux operating system well and
good but if you're using a VM on top of
a Windows OS then I would request you to
satisfy these things okay these two
criterias and I think we can straight
away start let me open up my terminal
first of all okay this is my node I'm
going back to my master
okay
yes
so first of all if you have to start
your cluster you have to start it from
your Master's end okay and the command
for that is Cube idiom init you specify
the Pod Network flag and the API server
flat uh we are specifying the port
Network flag because the different
containers inside your pod should be
able to talk to each other easily right
so that was the whole concept of
self-discovery which I spoke about
earlier during the features of
kubernetes so for the self-discovery we
have like different pod networks using
which the containers would talk to each
other and if you go to the documentation
the kubernetes documentation you can
find a lot of options there you can use
either Calico pod or you can use a
flannel Port Network so when we say Port
Network it's basically acronymed as cni
okay container network interface okay so
you can use either a Calico cni or a
flannel cni or any of the other ones
these are the two popular ones and I
will be using the calco cni okay so this
is the network range for this particular
pod and this will have to specify over
here okay and then over here we'll have
to specify the IP address of the master
so let me first of all copy this entire
line
and uh before I paste it here let me do
an ifconfig and find out what is the IP
address of this particular machine of my
master machine the IP address is
192.168.56.1.1 okay so let's just keep
that in mind and let me paste the
command over here in place of the master
IP address and I'm going to specify the
IP address of the master okay which I
just read out it is
192.168.56.1.1 and the Pod Network
I told you that I'm going to use the
Calico pod so let's copy this network
range and
paste it here so all my containers
inside this particular pod would be
assigned an IP address in this range
okay now let me just go ahead and hit
enter and then your cluster would begin
to set up
so it's going as we expected
so it's going to take a few minutes so
just hold on there
okay perfect my kubernetes master has
initialized successfully and if you want
to start using your cluster you have to
run the following as a regular user
right so we have three commands which is
suggested by kubernetes itself and that
is actually the same set of commands
that even I have here okay so I'll be
running the same commands this is to set
up the environment and then after that
we have this token generated right the
joining token so the token along with
the init address of the IP of the master
if I basically execute this command in
my nodes then I will be joining this
cluster where this is the master right
so this is my master machine this is
created the cluster so now before I do
this though there are a few steps in the
middle one of those steps is uh
executing all these three commands and
after that comes bringing up the
dashboard and setting up the Pod Network
right minus the calco Pod so I'll have
to set up the Calico pod and then after
also set up the dashboard because if I
do not start the dashboard and this
before the nodes then the node cannot
join and I will have have various other
complications so let me first of all go
ahead and run these three commands one
after the other okay since I have the
same commands in my text doc I'll just
copy it from there okay I'll say Ctrl C
paste enter okay and I'll copy this line
so remember you have to execute all
these things as regular user okay you
can probably use your sudo but yeah
you'll be executing it as your regular
user and it's asking me if I want to
overwrite the existing what was there in
this directory I would say yes because
I've already done this before but if you
are setting up the cluster for the first
time you will not have this prompt okay
now let me go to the third line copy
this and paste it here
okay perfect now I've run these three
commands as I was told by kubernetes now
the next thing that I have to do is
before I check the node status and all
these things let me just set up the
network okay the poor Network so like I
said this is the Line This is the
command that we have to run to set up
the Calico Network okay to all of the
nodes to join our particular Network so
it would be copying the template of this
calico.tml file which is present over
here in this docs okay so hit enter
and yes my thing is created Calico Cube
controller was created now I'll just go
back here and see at this point of time
I can check if my master is connected to
the particular pod okay so I can run the
cube CDL get nodes command Okay this
would say that I have one particular
resource connected to the cluster okay
name of the machine and this role is
master and yeah the status is ready okay
if you want to get an idea of all the
different pods which are running by
default then you can do the cube serial
get pods along with a few options okay
you should specify these flags and they
are
all namespaces and with the flag O
specify wide okay so this way I get all
the pods which are styled by default
okay so there are different services
like hcd for cube controllers for the
Calico node for the hcd master for every
single service there's a separate
container and Port started okay so
that's what you can understand from this
part okay that is the safe assumption
now that we know the cluster the cluster
is ready and the Masters part of the
cluster let's go ahead and execute this
dashboard okay remember if you want to
use a dashboard then you have to run
this command before your nodes join this
particular cluster because the moment
your nodes join into the cluster
bringing up the dashboard is going to be
challenging and it will start throwing
errors okay it will say that it's being
hosted on the Node which we do not want
we want the dashboard to be on the
server itself right on the master so
first let's bring the dashboard up so
I'm going to copy this and paste it here
okay Enter
great my kubernetes dashboard is created
now the next command that you have to
get your dashboard up and running is
Cube CDL proxy okay with this you will
get a message saying that uh it's being
served at this particular port number
and yes you are right now there you can
if you access localhost what was the
port number again localhost yeah
127.0.0.1 is localhost okay followed by
port number 8001 okay
yeah so right now we are not having the
dashboard because it is technically
accessed on another URL but before we do
that there are various other things that
we have to access I mean we have to set
okay because right now we have only
enabled the dashboard now if you want to
access the dashboard you have to first
of all create a service account okay uh
the instructions are here okay you have
to first of all create a service account
for your dashboard then you have to say
that okay you are going to be the admin
user of this particular service account
and you have to enable that
functionality here you should say
dashboard admin privileges and you
should do the cluster binding okay the
cluster role binding is what you have to
do and after that to join to that or to
get access to that particular dashboard
we have to basically give a key okay
it's like a password so we have to
generate that token first and then we
can access the dashboard so again for
the dashboard there are these three
commands well you can get confused down
the line but remember this is separate
from the above okay so what we did
initially is uh ran these three commands
which kubernetes to execute and after
that the next necessity was bring up a
pod so this was that command for the Pod
and then this was the command for
getting the dashboard up and right after
that run the proxy and then on that
particular port number it'll start being
served so my dashboard is being served
but I'm not getting the UI here and if I
want to get the UI I have to create the
service account and do these three
things right so let's start with this
and then continue I hope this wasn't
confusing guys okay I can't do it here
so let me open a new terminal
okay sure I'm gonna paste it and yes
service account created let me go back
here and execute this command where I'm
doing the role binding I'm saying that
my dashboard will should have admin
functionalities and that's going to be
the cluster role okay cluster admin and
then the service account is what I'm
using and it's going to be in default
namespace okay so when I created the
account I said that I want to create
this particular account in default
namespace so the same thing I'm
specifying here
okay dashboard admin created good so
let's generate the token that is needed
to access my dashboard okay before I
execute this command let me show you
that once
so if you go to this URL right slash API
slash V1 slash namespaces
let me show it to you here okay so this
is the particular URL where you will get
access to the dashboard okay login
access to the dashboard localhost 8001
API B1 namespaces slash Cube system
slash Services slash https kubernetes
dashboard colon slash proxy okay
remember this one that is the same thing
over here and uh like I told you it's
asking me for my password so I would say
token but let me go here and hit the
command and generate the token so this
is the token I'm gonna copy this from
here till here I'm gonna say copy and
this is what I have to paste over here
all right so sign an update yes perfect
so this is my dashboard right this is my
kubernetes dashboard and this is how it
looks like whatever I want I can get an
overview of everything so there is
workloads if I come down there is uh
deployments I have option to see the
pods and then I can see what are the
different Services running among most of
the other functionalities okay so right
now we don't have any uh bar graph or
pie graph showing you which clusters up
which pod is up and all because I have
not added any node and there is no
servers that is running right so I mean
this is the outlay of the dashboard okay
you will get access to everything you
want from the left you can drill down
into each of these namespaces or pods or
containers right now if you want to
deploy something through the dashboard
right through the click functionality
then you can go here okay but before I
create any container order before I
create any pod or any deployment for
that matter of fact I have to have nodes
because these will be running only on
nodes correct whatever I deploy they run
only on nodes so let me first open up of
my node and get the node to join this
particular cluster of mine now if you
remember the command to join the node
got generated at the master end correct
so let me go and fetch that again so
that was the first command that we ran
right this one so let's just copy this
and paste this one at my node end
this is the IP of my master
and it will just join at this particular
port number let me hit enter let's see
what happens okay let me run it as a
root user okay
okay perfect successfully established
connection with the API server and it
says this node has joined the cluster
Right Bingo so this is good news to me
now if I go back to my master and in
fact if I open up the dashboard
there would be an option of nodes right
so initially now it's showing this
master master is the only thing that is
part of my nodes let me just refresh it
and you would see that even node hyphen
1 would be a part of it right so there
are two resources two instances one is
the master itself and the other is the
node now if I go to overview
you will get more details if I start my
application if I start my service or
containers then all those would start
showing up here right so it's high time
I start showing you how to deploy it to
deploy it using the dashboard I told you
this is the functionality so let's go
ahead and click on this create
and yeah mind you from the dashboard is
the easiest way to deploy your
application right so even developers
around the world do the same thing for
the first time probably they create it
using the yaml file and then from there
on they start editing the yaml file on
top of the dashboard itself or they
create or deploy the application from
here itself so we'll do the same thing
go to create an app using functionality
click functionality you can do it over
here so let's give a name to your
application I'll just say at Eureka demo
okay let that be the name of my
application and I want to basically pull
an engines image okay I want to launch
an engine service
so I'm going to specify the image name
in my Docker Hub okay so it says either
the URL of a Public Image or any
registry or a private image hosted on
Docker Hub or Google container registry
so I don't have to specify the URL per
se but if you are specifying a Docker
Hub if you're specifying this image to
be pulled from Docker Hub then you can
just use the name of the image which has
to be pulled that's good enough right
engines is the name and that's good
enough and I can choose to set my number
of ports to one or two in that way I
will have two continuous running in the
Pod right so this is done and the final
part is actually without the final part
I can straight away deploy it okay but
if I deploy it then my application would
be created but I would just not get the
UI I mean I won't see the engine service
so that I get the service I have to
enable one more functionality here okay
the servers here click on the drop down
and you will have external option right
so click on external this would let you
access this particular service from your
host machine right so that is the
definition so you can see the
explanation here an internal or external
service can be defined to map an
incoming port to a Target Port seen by
the container so engines which would be
hosted on one of the container ports
that could not be accessible if I don't
specify anything here but now that I've
said access it externally on a
particular port number then it would get
mapped for me by default engines runs on
port number 80. so the Target Port would
be the same but the port I want to
expose it to that I can map it to
anything I want so I'm going to say 82
all right uh so that's it it's as simple
as this this way your application is
launched with two pods so I can just go
down and click on deploy and this way my
application should be deployed
my deployment is successful there are
two pods running uh so what I can do is
I can go to the service and try to
access the UI right so it says that it's
running on this particular port number
eight two one five three so copy this
and say localhost
three two one five three okay hit enter
uh bingo so it says welcome to Jenkins
and uh I'm getting the UI right so I'm
able to access my application which I
just launched through the dashboard it
was as simple as that so this is one way
of launching or making a deployment uh
there are two other ways like I told you
one is using your CLI itself your
command line interface of your Linux
machine which is the terminal or you can
do it by uploading the yaml file uh you
can do it by uploading the yaml file
because everything here is in the form
of yaml or Json okay that's like the
default way so whatever deployment I
made right that also those
configurations are stored in the form of
yaml so if I click on view or edit yaml
all the configurations are specified the
default ones have been taken so I said
the name should be a draca demo that is
what has been taken over here that is
the name of my deployment okay so kind
is deployment the version of my API it's
uh this one extension slash V1 beta1 and
then under metadata I have various other
lists so if you know how to write a yaml
file then I think it would be a little
more easier for you to understand and
create your deployment because yaml file
is everything about lists and maps and
these xaml files are always lists about
maps and maps about lists so it might be
a little confusing uh so probably we'll
have another tutorial video on how to
write a yaml file for kubernetes
deployment but I would keep that for
another session okay let me get back to
this session and show you the next
deployment okay the next deployment
technique so let me just close this and
go back to overview okay so I have this
one deployment very good okay so let's
go to this yeah so what I'll do is let
me delete this deployment okay or let me
at least scale it down because I don't
want too many resources to be used on my
node also because I will have to show
two more deployments right so I have
reduced my deployment over here and I
think it should be good enough
great uh so let's go back to the cube
setup this document of mine
so this is where we got right
we could check our deployments we could
do all these things so one thing which I
might have forgotten is showing the
nodes which are part of the cluster
right so this is my master
yeah so I kind of forgot to show you
this Cube CDL get notes so the same view
that you got on your dashboard you get
it here also I mean these are the two
nodes and this is the name and all these
things okay
and I can also do the cube CTL get pods
it should tell me all the pods that are
running uh at Eureka demo is the pod
which I have started okay this is my
report now if I specify with the other
flags right with all namespaces and with
wide then all the default pods which get
created along with your kubernetes
cluster those will also get displayed
let me show you that also just in case
okay yeah so this is the one which I
created and the other ones are the
defaulted deployments that come with
kubernetes the moment you install uh set
up the Clusters these get started okay
and if you can see here this particular
that this particular demo which I
started is running on my node one along
with this Cube proxy
and this particular Calico node so these
two services are running on master and
node and this one is running only on my
node one right uh you can see this right
the Calico node runs both on my node
over here and on my master uh and
similarly the cube proxy runs on my node
here add on my master so this is the one
that's running only on my node okay uh
so getting back to what I was uh about
to explain you
the next part is how to deploy anything
through your terminal
now to deploy your same engines
application through your CLI we can
follow these set of commands Okay so
there are a couple of steps here first
of all to create a deployment you have
to run this command Okay Cube serial
create deployment engines and then the
name of the image that you want to
create this is going to be the name of
your deployment and this is the name of
the image which you want to use so Ctrl
C and let me go to the terminal here on
my master I'm executing this command
Cube serial create deployment okay so
the deployment engines is created if you
want we can verify that also over here
so under deployments right now we have
one entry into Ray car demo and yes now
you can see there are two engines and
adobeca demos so this is pending I mean
it would take a few seconds so in the
meanwhile let's just continue with the
other steps once you have created your
deployments you have to create the
service okay you have to say uh which is
the node Port which can be used to
access that particular service right
because deployment is just a deployment
you're just deploying your container if
you want to access it like I told you
earlier from your local from your host
machine or all those things then you
have to enable the node Port if you want
to get your deployments on your terminal
you can run this command Cube CTL get
deployments okay
engines also comes up over here right if
you want more details about your
deployment you can use this command Cube
CDL describe you'll get like more
details about this particular deployment
as to what is the name uh what is the
port number it's sort of signing on and
all these things okay let's not
complicate this you can probably use
that for your understanding later
so once that is done the next thing that
you have to do is you have to create the
service on the nodes you have created
the deployment but yes create the
service on the nodes using this
particular command Cube CDL
create service and say node Port okay
this means you want to access it at this
particular port number you're doing the
port mapping 80s to 80. okay container
Port 80 to the internal node port 80.
okay so service for engines is created
and if you want to check uh which of the
deployments are running in which nodes
you can run the command Cube CTL get SVC
okay this would tell you okay you have
two different Services iteraker demo and
engines and they are running on these
port numbers and on these nodes right so
kubernetes is the one which got created
automatically a Eureka demo is the one
which I created okay engines is again
the one which I created kubernetes comes
up on its own I'm just specifying here
because this is a container for the
cluster itself okay so let's just go
back here and then yes and similarly if
you want to delete a deployment then you
can just use this command Cube CDL
delete deployment followed by the name
of the deployment right it's pretty
simple uh you can do it this way
otherwise from the dashboard also you
can delete it like how I showed you all
your click over here and then you can
click on delete and then if you want to
scale it you can scale it so both of
these deployments of mine have one part
each right so let's do one thing so
let's just go to the engines service
and here let's try accessing this
particular service
localhost
okay perfect here also it says welcome
to manjings right so uh with this you
can understand that the port mapping
worked and by going to service you will
get to know on which port number you can
access it on your host machine right so
this is the internal container Port
mapped to this particular Port of mine
okay now if if not for this if this
doesn't work you can also use the
cluster IP for the same thing cluster IP
is going to basically uh the IP using
which all your containers access each
other right so for your pod you will
have an IP so whatever is running in
your containers that will again be
accessible on your cluster IP so so it's
the same thing right so let me just
close these uh pages and that's how you
deploy an application through your CLI
so this comes to our last part of this
video which is nothing but deployment
via yaml file so for again deployment
via yaml file you have to write your
yaml code right you have to either write
your yaml code or your Json code correct
so this is the code which I have written
which is uh in yaml format and and in
fact I already have it in my machine
here so how about I just do an LS yeah
there is deployment.yaml right so let me
show you that
so this is my yaml file okay so here I
specify various configurations uh
similar to how I did it using the GUI or
write it using the CLI it's something
similar to jizzer I specify everything
in one particular file here if you can
see that I have specified the API
version okay so I'm using extensions dot
slash B1 or beta1 okay I can do this or
I can just simply specify version one I
can do either of those and then the next
important line is the kind so kind is
important because you have to specify
what kind of file is is it a deployment
file or is it for a pod deployment or is
it for your container deployment or is
it the overall deployment what is it so
I've said deployment okay because I want
to deploy the containers also along with
the Pod so I'm saying deployment in case
you want to deploy only the pod which
you realistically don't need to okay why
would you just deploy a pod but in case
if you want to deploy a pod then you can
go ahead and write pod here and then
just specify what are the different
containers okay but in my case it's a
complete deployment right with the pods
and the services and the containers so I
will go ahead and write other things and
Under The Meta data I will specify the
name of my application I can specify
what I want I can put my name also over
here like Warden okay and I can save
this and then the important part is this
pack part so here is where you set the
number of replicas do you remember I
told you that there's something called
as replication controller which controls
the number of PODS that you will be
running so it is that line so if I have
a set two over here it means that I will
have two poles running of this
particular application of vardan okay
what exactly am I doing here under spec
I have saying that I want two containers
so I have intended a container line over
here and then I have two containers
inside so the first container which I
want to create is of the name front end
okay and I'm using an engines image and
similarly the port number that this
would be active on is container Port 80.
all right and then I'm saying that I
want a second container and the
container for this could I could rename
this to anything I can say back end and
I can choose which image I want I can
probably choose a httpd image also okay
and I can again say the ports that this
will be running on I can say the
container Port that it should run on is
uh port number is 88 right so that's how
simple it is all right and since it's
your first video tutorial the important
takeaways from this yaml file
configuration is that under spec you
will have to specify the containers and
uh yes everything in Json format with
all the indentations and all these
things okay even if you have an extra
space anywhere over here then your yaml
file would thrown in by an error so make
sure that is not there make sure you
specify the containers appropriately if
it's going to be just one container well
and good it's two containers make sure
you uh indent it in the right way and
then you can specify the number of PODS
you want to give a name to your
deployment and mainly establish read
these rules okay so once you're done
with this uh just save it and close this
yaml file okay so this is your
deployment.yaml now you can straight
away upload does yaml file your
kubernetes okay and that way your
application would be straight away
deployed okay now the command for that
is Cube CDL create hyphen f and the name
of the file okay so let me copy this and
then the name of my file is
deployment.yaml so let me hit enter
perfect so my deployment the third
deployment Warden is also created right
so we can check our deployments from the
earlier command that is nothing but Cube
CDL get deployments okay it's not get
deployment.yaml sorry
it's get deployments and as you can see
here there is an edirect demo there is
engines and there is vardhan and the
funny thing which you should have
noticed is that I said I want two
replicas right two pods so that's why
the desire is to currently we have to up
to date is one so okay up to date is two
brilliant available is zero because
let's just give it a few seconds in 23
seconds I don't think the board would
have started so let's go back to our
dashboard and verify if there's a third
deployment that comes up over here
okay perfect so uh that's how it's gonna
work okay so probably it's gonna take
some more time because the container is
just restarting so let's just give it
some more time
this could well be because of the fact
that my node has very less resource
right so I have too many deployments
that could be the very reason uh so what
I can do is I could uh go ahead and
delete other deployments so that my node
can handle these many containers and
pods right so uh let me delete this
particular deployment engines deployment
and let me also delete this edureka demo
deployment of mine
okay now let's refresh and just wait for
this to happen
okay so what I can do instead is I could
have a very simple deployment right so
let me go back to my terminal and let me
delete my deployment okay and let me
redeploy it again so Cube CDL delete
deployment
okay so what then the deployment has
been deleted okay so let's just clear
the screen and let's do G edit of the
yaml file again and here let's make
things simpler
let me just delete this container from
here
let me save this all right and close
this now let me
create a deployment with this okay so
Walden is created
let me go up here and refresh let's see
what happens
okay so this time it's all green because
it's all healthy my nodes are successful
or at least it's going to be successful
container creating
perfect so two PODS of mine are up and
running and uh both my pods are running
right and both are running on node one
pods two out of two those are the two
deployments and replica set and then
Services right so it's engines which is
the base image which is being used so uh
well and good this is also working so
guys yeah that's about it right so when
I try to upload it maybe there was some
other error probably in the yaml file
they could there could have been some
small mistake or it could have been
because my node had too many containers
running those could have been the
reasons but anyways this is how you
deploy it through your yaml file
foreign
[Music]
let's look into the architecture of
kubernetes kubernetes architecture has
mainly three components the masternodes
the worker nodes and the distributed key
value stores like etcd talking about the
master node the master node is
responsible for managing the kubernetes
cluster and it is the entry point for
all the administrative tasks so we can
communicate to the master node via the
CLI or GUI or apis so for the fault
tolerance purposes there can be more
than one Master node in the cluster and
if we have more than one Master node
then there would be high availability
mode and only one of them will be the
leader performing all the operations all
the other masternodes would be the
followers of that node also to manage
the cluster state kubernetes uses etcd
and all the master nodes connect to it
so etcd is a distributed key value store
which I'll tell you in a little while
also let me tell you that the key Value
Store can be the part of the master node
and it can also be configured externally
and in that case masternodes would
connect to it so now that I've told you
what a master node is and what are the
responsibilities of the master known
let's discuss the components of Master
nodes a master node has mainly four
components the API server the scheduler
the control manager and etcd so now let
me tell you about all these components
one by one so starting with API server
all the administrative tasks are
performed via the API server within the
master node a user sends the rest
commands to the API server which then
validates and processes the request
after executing the request the
resulting state of the cluster is stored
in the distributed key value store after
that we have scheduler so as the name
suggests the scheduler schedules the
work to different worker nodes the
scheduler has the resource usage
information for each worker node and
also knows about the constraints that
the users may have set before scheduling
the work the scheduler also takes into
account the quality of service
requirements data locality affinity and
taffinity and many other such parameters
and then the scheduler schedules the
work in terms of PODS and services
the next component that we have is
controller manager now what do you think
are the responsibilities of controller
manager well as the name suggests
controller manager manages different
non-terminating control loops which
regulate the state of the kubernetes
cluster now each one of these control
loops know about the desired state of
the object it manages and then they
watch their current state through the
API servers now in a control Loop if the
current state of the object it manages
does not meet the desired State then the
control Loop itself takes the corrective
steps to make sure that the current
state is same as the desired state so
what does the controller manager do it
basically makes sure that your current
state is same as the desired state after
that the last component that we have is
etcd so as I mentioned earlier etcd is a
distributed key value store which is
used to store the cluster state so
either it has to be a part of the
kubernetes master or you can configure
it external finally so guys that were
the various components of masternode now
let's move on to the next component of
kubernetes architecture that is the
worker node a worker node is a machine
or a virtual machine or any physical
server which runs the applications using
pods and is controlled by the master
node so you understand the master sleeve
concept it has its own machine but yes
it is controlled by the master node now
pods are scheduled on the worker nodes
which have the necessary tools to run
and connect them so you can see that the
Pod is basically the scheduling unit in
kubernetes it is a logical collection of
one or more containers which are always
scheduled together and to access the
applications from the external world we
have to connect to the worker nodes and
not the master nodes so now that I've
briefed you about the worker node let's
discuss the various components of worker
node so a worker node has mainly three
components the cubelet the Q proxy and
container runtime so let's start the
discussion session with container
runtime so a container runtime is
basically used to run and manage a
container's life cycle on the worker
node so some examples of container
runtimes that I can give you are the
container rkt lxt Etc now it is often
observed that Docker is also referred to
as container runtime but to be precise
let me tell you that Docker is a
platform which uses containers as
container runtime all right so now that
you've understood what container runtime
is let's move on to the second component
that is cubelet so cubelet is basically
an agent which runs on each worker node
and communicates with the master node so
if you have 10 worker nodes then cubelet
runs on each and every worker node then
it receives the port definition via
various means and runs the containers
associated with that pod it also makes
sure that the containers which are part
of the pods are healthy at all times so
what does a cubelet do it basically runs
in all the containers it make sure that
your containers which are part of
various spots are always healthy at all
times so the cubelet connects to The
Container runtime using the container
runtime interface which consists of
various protocol buffers grpc apis and
libraries so as you can see on the
screen the cubelet connects to the CRI
shim to perform containers and image
operations now CRI implements two
services that is the runtime service and
the image service the image service is
responsible for all the image related
operations while the runtime service is
responsible for all the Pod and
container related operations so these
two Services have two different
operations to perform now let me tell
you something interesting here container
runtimes used to be hard-coded in
kubernetes but with the development of
CRI kubernetes can now use different
container runtimes without the need to
recompile so any container runtime that
implements CRI can be used by kubernetes
to manage pods containers and container
images
so now let me give you an example of a
CRI shims so dokashim and CRI container
are two examples of CRS ship with
dockershim containers are created using
Docker installed on the worker nodes and
then internally Docker uses container to
create and manage containers with CRI
container we can directly use docker's
small Offspring containers to create and
manage containers so these were two
simple examples of CRI shims now let's
move on to the third component of worker
node that is Q proxy so Q proxy is the
network proxy which runs on each worker
node and then listens to the API server
for each service Point creation or
deletion so for each service Point Q
proxy sets The Roots so that it can
reach to it so guys that were the
components of the worker node so now let
me move on with the session by
discussing the third component of
kubernetes architecture that is etcd so
as I mentioned before kubernetes uses
the atcd to store the cluster state so
atcd is a distributed key value stored
based on the raft consensus algorithm so
the raft allows a collection of machines
to work as a coherent group that can
survive the failures of some of its
members so basically this algorithm
Works in such a way that even if some of
the members fail to work it can still
work at any given time one of the nodes
in the group will be the master and rest
of them will be the follower as I told
you before in the starting of the
session that there can be only one
master and all the other Masters have to
find follow that Master right not only
this but let me tell you that atcd is
written in the co-programming language
besides storing the cluster state
etcd is also used to store the
configuration details such as the
subnets and the config Maps
[Music]
what is Docker Docker is a
containerization platform that packages
your application and all its
dependencies together in form of a
Docker container to ensure that your
application works seamlessly in any
environment it is basically an open
platform for developing shipping and
running applications now Docker enables
you to separate your applications from
your infrastructure so you can deliver
software very quickly with Docker you
can also manage your infrastructure in
the same ways you manage your
applications by taking advantage of
docker's methodologies for shipping
testing and deploying code very quickly
you can significantly reduce the delay
between writing code and running it in
production basically provides the
ability to package and run an
application in a Loosely isolated
environment called a container the
isolation and security allow you to run
many containers simultaneously on a
given host as we have already discussed
containers are lightweight and contain
everything needed to run an application
so you don't really need to rely on what
is currently installed on the host you
can easily share containers while you
work and be sure that everyone you share
with gets the same container that works
in the same way now there are some
important features of Docker that I'd
like to address the first feature is
that it is extremely fast and there is a
consistent delivery of your applications
now Docker streamlines development life
cycle by allowing developers to work in
standardized environments using local
containers which provide your
applications and services containers are
great for continuous integration and
continuous delivery workflows
now let me explain this with a tiny
example your developers write code
locally and share their work with their
colleagues using Docker containers now
they use Docker to push their
applications into a test environment and
execute automated and manual tests when
developers find bugs they can easily fix
them in the development environment and
redeploy them to the test environment
for testing and validation now when
finally testing is complete getting the
fix to the customer is as simple as
pushing the updated image to the
production environment
this is how easy it is to use Dockers
and to get your work done the second
feature is that it has a responsive
deployment and scaling Docker containers
can run on a developer's local laptop on
physical or virtual machines in a data
center on cloud providers or even in a
mixture of environments its portability
and lightweight nature also makes it
very easy to dynamically manage
workloads scaling up or tearing down
applications and services as business
needs dictate in your real time the
third feature is running more workloads
on the same Hardware Docker is
lightweight and fast it provides a
viable cost effective alternatives to
hypervisor based virtual machines so you
can use more of your compute capacity to
achieve your business goals it is
perfect for high density environments
and for small and medium deployments
where you need to do more with fewer
resources so there is a relationship
between kubernetes and Docker we're
going to discuss them now so if Dockers
is containers and kubernetes is
container orchestration why would anyone
ask should I use Docker or kubernetes
now in reality the two tools are
actually complementary to each other and
help build Cloud native or microservice
architectures Docker is mostly used
during the first days of a containerized
application it really actually helps
build and deploy the applications
container or containers in cases where
the applications architecture is very
simple Docker can address the basic
needs of the application's life cycle
management now if the case or the
scenario where the application is broken
down into multiple microservices with
each of these services having their own
life cycle and operational needs
kubernetes comes into the picture it is
not only used to create the application
containers it actually needs a container
platform to run and Docker being the
most popular one it runs on that
kubernetes integrates with a large tool
set built for and around containers and
uses it in its own operations containers
created with Docker or any of its
Alternatives can be managed scaled and
moved by kubernetes which also ensures
failover management and health
maintenance of the system as you can see
in the diagram the container life cycle
has been explained you can see that in
the docker there is the build and
deployment of the application that has
been successfully done once the docker
has been successfully deployed then
kubernetes takes the responsibility to
Monitor and scale the application there
are great benefits to using kubernetes
with Docker firstly the applications are
easier to maintain as they are broken
down into smaller parts also these parts
run on an infrastructure that is much
more robust and the applications are
more highly available
your applications are able to handle
more load on demand improving the user
experience and reducing resource waste
as applications become more scalable
there is no need to pre-allocate
resources in anticipation of load peak
times now here both Docker and
kubernetes are backed by strong open
source communities and are part of the
cloud native Computing Foundation a
Linux Foundation project aiming to
advance container Technologies and align
the industry around specific standards
now that we understand the docker and
kubernetes have a really great
relationship together let's move ahead
and address the main topic of today's
session that is kubernetes versus Docker
firstly we will compare them in terms of
setup and installation as you can see on
the screen kubernetes really requires a
series of many manual steps to set up
the master and worker node components in
a cluster of nodes it's really hard and
complicated to install kubernetes but on
the other hand that is installing Docker
is a matter of one-liner commands on
Linux platforms like Ubuntu and Centos
so compared to kubernetes Docker is much
more easy to install and set up moving
on to the next difference that is
locking kubernetes provides no native
storage solution for log data whereas
logging driver plugins are available in
Docker 17.05 and much higher so in terms
of monitoring kubernetes applications
you can use various open source tools
like Prometheus grafana influxdb Etc
Docker on the other hand also has tools
like Prometheus Docker API etc for
monitoring purposes
kubernetes supports up to 5000 nodes
whereas Docker supports only up to 2000
nodes
and in terms of compatibility kubernetes
is more comprehensive and highly
customizable whereas Docker is less
extensive and customizable
so for all of you all who don't know
what load balancing is it is an
efficient distribution of network or
application traffic across multiple
servers in a server farm so usually each
load balances sits between client
devices and back-end servers receiving
and then Distributing incoming requests
to any available server capable of
fulfilling them as you can see in
kubernetes you must manually configure
your load balancing settings whereas in
Docker it does Auto load balancing that
is it is automated
now there are various companies that use
kubernetes and Docker companies like
Google Shopify udemy slack Robinhood
delivery hero stack share new bank Etc
use kubernetes every day on the other
hand various companies like Twitter
Business Insider Spotify PayPal eBay
Oxford University press the New York
Times New Relic all of these companies
and organizations make use of docker now
that we've compared kubernetes and
Docker let us understand in what
situation to use kubernetes and docker
so what are the choices when it comes to
kubernetes in Docker what happens if you
do choose to use only one and not the
other so there are three scenarios that
I'll be addressing the first one is
Docker without kubernetes the second one
is kubernetes with a Docker and a third
one is kubernetes with Docker we've
already discussed the benefits of using
kubernetes with Docker but we'll just
brief up that again so firstly let's
understand the scenario where Docker can
be used without kubernetes Docker by
itself will allow you to build container
images manage them in a registry run
containers and communicate with them and
eventually put them together in a
multi-container application using Docker
compose so is Docker all that you need
now when it comes to producing and
managing container images and putting
containers into operation at the runtime
level the answer for the most part is
definitely yes it has become the
industry standard for creating
containers and getting them off the
ground now under Dockers current and
developer-centric definition of its
Mission however it has moved away from
earlier attempts to go beyond its core
functions it is no longer attempting to
compete with kubernetes and other
companies at the infrastructure or
orchestration level now let's talk about
the second scenario kubernetes does not
include functionality for creating or
managing container images and it is not
by itself run containers it really needs
to work with an external container
source and runtime it is however capable
of using containers from a variety of
sources and it is compatible with
runtimes other than Docker so it is not
inherently dependent on Docker and
Docker alone
what kubernetes does is it provides a
rich and a very flexible and Powerful
framework for defining applications and
orchestrating containers at scale it is
very well designed for key Enterprise
level tasks such as automated scaling
maintaining High availability and
operating in a multi-platform
environment it also has a large
community of users and developers with a
corresponding large number of add-ons
and support tools
so finally in the third scenario what is
the best choice this is a tricky
question and the answer is most obvious
that is use both the truth here is that
all the kubernetes can use other
container sources and runtimes it is
designed to work well with Docker and
much of kubernetes documentation was
very well written with Docker in mind
the most basic kubernetes use case is
kubernetes plus Docker and kubernetes
includes docker-centric tools such as
compose which converts Docker compose
commands in settings so that can be used
by kubernetes for its part dockerism
raised kubernetes and has in fact
offered its very own integrated
kubernetes distribution so the bottom
line is that kubernetes and Docker are
both industry standards in their
respective core areas of expertise and
together they provide a well integrated
platform for container management
deployment and orchestration at scale it
was never really a question of
kubernetes versus Docker it was always Q
communities and Docker and today this is
even more true
[Music]
so what is configuration management why
do you need configuration management now
system administrators usually perform a
repetitive tasks such as installing
servers configuring those servers so on
and so forth they can automate this task
by writing scripts where it is a very
hectic job when you are working on a
very large infrastructure so meet our
team we have Dave the developer Sam the
sysadmin and Alice the manager how the
workflow goes is Dave passes updates to
the operations team which is Sam so when
Dave is done with a certain software
update he pings Sam to deploy it on the
servers but Sam then has to deploy the
update on each and every individual
server and it's going to take him a
significant amount of time just to
deploy this new update to servers
meanwhile the manager Alice comes in
with great news
they've been getting traffic on all of
their servers and she wants Sam to add
200 more servers to their system but Sam
hasn't even updated the current 20
servers with Dave's changes how is he
going to do that for 200 more servers
now there are a couple of issues here
that Sam is facing a few major ones are
having to configure a large number of
servers together
scaling to new servers manually without
any configuration management tool and
the fact that there is always a
development and deployment environment
mismatch which is going to halt his work
from time to time now to solve this
problem configuration management was
introduced to workplaces configuration
management is the practice of handling
changes systematically so that a system
maintains its Integrity over time
configuration management or CM ensures
that the current design and build state
of the system is known good and trusted
and doesn't rely on tacit knowledge of
development teams it allows access to an
accurate historical record of System
state for project management and audit
purposes
so with configuration management you
overcome the following challenges you
figure out which components to change
and when the requirement changes you
also redo an implementation because the
requirements have changed since the last
implementation you can also revert to a
previous version of the component if you
have replaced it with new but flawed
versions and finally you can replace the
wrong component just in case you
couldn't accurately determine which
components needed replacing so now in an
automated scenario Dave passes updates
to Sam and scaling up is no longer an
issue and adding 200 more servers as
said by the manager is not really a
problem with the help of configuration
management
Sam can easily scale deploy and
configure as many servers as his manager
wants the best example for this that I
know is that of the New York Stock
Exchange a software glitch had prevented
the NYSE from Trading stocks for almost
an hour and a half that is 90 minutes
this led to millions of dollars of loss
a new software installation was actually
the cause of this problem the software
was installed on eight of nyse's 20
trading terminals however despite being
tested out the night before it failed to
operate in the morning properly on its
eight terminals so there was a need to
switch back to the old software as a
result of proper configuration
management
NYSC recovered from that situation in 90
minutes which is pretty fast for a
scenario like this had the problem
continued longer the consequence would
have been far more severe now I hope you
understand the importance of
configuration management which is
considered the back backbone of devops
it allows more frequent software
releases in the safest and most reliable
way possible configuration management is
all about bringing in consistency in the
infrastructure it is done by ensuring
that the current design System state and
environment is known trusted and agreed
upon by everyone it also helps record
all the changes made in the system
configuration management tools provide
an easy approach to manage and configure
servers it enables users to manage and
configure the entire infrastructure and
its environment it makes writing
individual scripts for servers time and
time again just to get little things
done Obsolete and it does so by one
Concept in particular infrastructure as
code now here is a shell script and a
configuration management tool script
that you see in front of you both are
adding a user to a host but while the CM
tool script is easy to understand and
write as you can see it's almost pure
English the shell script is harder to
understand and you'll have to learn to
write shell scripts of your own in the
first place now infrastructure as code
enables the automation of it operations
which is to build deploy and manage by
provisioning of code rather than
manually handling each phase of
different environments basically what it
does is that it Bridges the environment
differences CIS admins have encountered
every time they had either tried to
deploy new code or set up new servers
and it obviously has its own advantages
for example provisioning servers using
infrastructure as code is way easier
than writing shell scripts shell scripts
will require workflow definitions as you
just saw whereas the CM tool scripts
have predefined workflows so that
basically makes the CM tool script easy
to understand and write while
eliminating the requirement for learning
how to write shell scripts separately
now there are a few very popular
configuration management tools such as
salt stack ansible CF engine and chef
but the one that we want to talk about
today is puppet
now puppet is a configuration management
tool that helps orchestrate provision
deploy and configure the entire
infrastructure of an organization it
enables users to concentrate more on
making delivery faster and more reliable
rather than continually fixing mistakes
it defines distinct configurations for
each and every host and continuously
checks and confirms whether the required
configuration is in place and is not
altered and if altered puppet
immediately reverts back to the required
configuration on the host it also helps
in Dynamic scaling up and scaling down
of machines while providing control over
all of your configured machine so a
centralized or a master server or repo
based change gets propagated to all
automatically so puppet uses a Master
Slave architecture the diagram in front
of you depicts what basically happens
so let me break it down for you a little
bit there's something known as a puppet
agent which sends facts to the Puppet
Master the facts are basically your key
value data pair that represents some
aspect of the slave state like its IP
address its uptime operating systems or
whether it's a virtual machine or not
the Puppet Master uses these facts to
compile a catalog that defines how the
slave should be configured the catalog
is a document that describes the desired
state for each source that the Puppet
Master manages honestly and finally the
puppet slave reports back to the master
indicating that the configuration is
complete which is visible in the puppet
dashboard the Puppet Master Slave
communicate with each other through a
secure encrypted Channel with the help
of SSL the puppet slave asks for the
Puppet Master certificate after
receiving the Puppet Master certificate
the master request for a sleep
certificate once master has signed the
slave certificate the slave request for
configuration or data and finally the
Puppet Master sends the configuration to
the puppet slave and that is how this
architecture works now let us take a
look at a few important puppet
components starting out we have
manifests now each slave has got its
configuration details in Puppet Master
written in the native puppet language
these details are written in the
language which puppet can understand and
these are termed as manifests now they
are composed of puppet code and their
file names using the dot PP extension
they are basically like programs of
puppet for example you can write a
manifest in puppet master that creates a
file and installs Apache server on all
puppet slaves connected to the Puppet
Master next you have a module now puppet
module is a collection of manifests and
data such as your fax files templates
Etc and they have a specific directory
structure modules are used for
organizing your puppet code because they
allow you to split your code into
multiple manifests hence they are also
known as self-contained bundles of code
and data now the next component of
puppet is a resource now resources are
the fundamental unit for a modeling
system configuration each resource
describes some aspect of a system like a
specific service or a package next you
have a factor which gathers your basic
information or facts about a puppet
slave such as Hardware details network
settings operating system type version
IP addresses Mac addresses SSH keys and
more these packs are then made available
in Puppet Masters manifests as variables
then you have something known as an M
Collective it is a framework that allows
several jobs to be executed in parallel
on multiple slaves it performs various
functions such as interacting with
clusters of slaves whether in small
groups or very large deployments using a
broadcast Paradigm to distribute
requests where all slaves receive all
requests at the same time requests have
filters attached and only slaves
matching the filter can act upon the
requests you can also use Simple command
line tool tools to call the remote
slaves and write custom reports about
your infrastructure using the M
Collective next and the final component
I'm going to talk about is a catalog
which basically describes the desired
state of each manage resource on a slave
it is a compilation of all the resources
that the Puppet Master applies to a
given slave as well as the relationships
between those resources now catalogs are
compiled by a puppet master from
manifests and slave provided data which
includes fax certificates and
environment if one is provided as well
as an optional external data such as an
external slave classifier exported
resources and functions the master then
serves the compiled catalog to the slave
when requested now that you know about
puppet its architecture and the various
components of puppet let's understand
why should you opt for puppet or a
configuration management tool like
puppet now if you're a poker Enthusiast
or a card enthusiast or you've ever
played online games then you must have
heard about something called Zynga it's
the world largest social game developer
its infrastructure uses tens of
thousands of servers in both public
cloud and private data centers early on
they were using manual processes
including kickstarters and post installs
to get hundreds of servers online now
during the process they faced a number
of different problems first of all was
scalability and consistency the platform
was experiencing phenomenal growth and
its infrastructure needed to keep Pace
with the industry script based Solutions
and manual approaches were not really
sufficient for these needs the second
problem they were facing was portable
infrastructure Zynga needed a way to
leverage a consistent configuration
management approach in both their public
Cloud infrastructure and their own data
centers now given the diversity of the
various Zynga gaming properties it was
important for the team to be able to
quickly match the right configuration to
the right machine hence flexibility
turned out to be another problem and
finally infrastructure insights as the
organization matured it became more
important to have an automated method of
visualizing the properties of each
machine now this company was smart
enough to quickly realize the need for
an automated process even before they
hit rapid scaling and that's when they
got puppet into the picture
so with puppet the first thing they
could expect was speed of recovery the
production operations team could rapidly
deploy the right configuration to the
right box if a system got
inappropriately reconfigured puppet
automatically reverted it back to its
last stable State and provided the
details necessary to manually remediate
a system rapidly the next thing was the
speed of deployment apart from the speed
of recovery the speed of deployment was
also increased manifold puppet provided
significant Time Savings in a way the
operations team delivered services for
the gaming Studios the next benefit of
using puppet was consistency of servers
puppets model driven framework ensured
consistent deployments according to Mark
Stockford who is the vice president of
production operations at Zynga it was
evident that they had experienced many
ways to save time the beauty of using
puppet is that according to him was that
it allowed them to deliver consistent
configuration patients across their
servers in a very short period every
time and finally having a model
different approach using puppet made it
easy for them to share configurations
across the organization which helped
enable developers and operations teams
to work together and ensure new Service
delivery was of extremely high quality
hence it furthered collaboration over a
dozen people from the team got trained
in puppet this knowledge had been
disseminated throughout the team under
the operations team within each
individual gaming studio so speed of
recovery speed of deployment consistency
of servers and collaboration were the
four main benefits of having used puppet
so first of all guys this is my AWS
Management console I am going to launch
a virtual machine with ec2 using this
option
all right here I'm going to pick my Ami
just filtering it out to free tier only
and if you go down
you'll get an option of Ubuntu Server
18.04 which I am going to select
and once you've chosen your Ami in type
I'm going to choose the t2 micro
and next move on to configure instance
details all right so the number of
instances we'll be needing is two one
for master and one for agent
so going on next to add storage add tags
this is unnecessary you could obviously
go ahead and add tags for yourself but I
don't find it significant for this
particular demonstration so I'm going to
move on straight to configure Security
Group
so here what I'm going to do is I'm
going to add Rule and what we need right
now is something that will allow all TCP
connections for our agent
so I'm going to go ahead and click on
TCP or you can always go ahead and use a
custom TCP for your agent
okay so I'm going to do that and here
select from anywhere all right and once
I am done with this I am going to review
and launch
okay
so basically I created a new key pair
and I launched the instances now your
instances are going to be launched in a
while
okay
so all right what I'm going to do is I'm
going to edit the names of both of the
instances I'm going to name one as
master
and I'm going to name the other one as
slave
all right
so both the instances are running we'll
start with the master first so now that
we have connected to our machines using
putty we're going to start by installing
ntp date packages to sync all the date
and time on your servers so first
we're gonna sudo apt update
and then we are going to apt install
your
ntp date
now what you need to take care of while
running these commands that all of your
commands in this installation are run as
root
so all of your commands are going to
proceed by sudo
all right now we have installed our
intimidate packages and synced our data
time on our servers and all of this is
being done in your master machine next
we are going to set up hosts file on our
machines and remember to run all these
three commands on both of your machines
right so now on our masters we're going
to edit the host file and add the IP
address of the master itself in the
hosts file and name it puppet
so we're gonna open Nano
all right
on the master we're going to edit the
host file and add the IP address of the
master itself and we are going to name
it puppet
and in the agent we are again going to
go
and Nano again we're going to open the
hosts file
add the master
and add the puppet agent
and then we're going to write out of
both
okay
next what we're gonna do is we are going
to add puppetrepo or repositories on
both of your machines so your puppet
uses Port 8140 to communicate through
SSL and we are going to open it through
the ufw command so ufw enable
and then click on why all right
and then ufw
and we are going to allow our port
number 8140
all right and remember to do that on all
of your machines that we are using right
so here again
ufw enable
and ufw allow 8140
okay
now we're going to add the puppet 6
repository on all of the machines
so we're going to app update first
we're going to get our puppet 6 package
and we are going to update our system
again
and we're going to be doing the same
thing on all of the machines that we
have right
all right moving on we are going to
install the puppet server on the Master
machine so for that we are going to apt
install
an extension y your puppet server okay
now after this installation what we're
going to do is we're going to change the
memory allocation for the puppet server
Now the default setting is 2GB
so we are going to basically set it to
half of it or one-fourth of it which is
1GB or 500 MB according to the memory
allocated to your particular VM that you
have whipped up all right
so
once this is done what I'm going to do
is I'm going to open the default puppet
server file and change one line
all right
so everything is pretty simply explained
here here we have this one line which
says modify this if you would like to
change the memory allocation enable jmx
Etc so here we have our 2GB which we are
going to change to 1G also remember you
can change it to
502 MB as well yeah so it's either half
or one fourth and remember to press I
before you try to make any edits to it
go into insert mode remember to be
logged in as a root user
so I'm going to click on escape to get
out of insert mode now you can see I'm
not able to insert anything
right and then I'm just going to
exit it
all right next what we have to do is
edit our configuration file of puppet
so we're going to Puppet Labs and puppet
Dot
conf
conf
okay
so here you will have to add the
following lines just going to add
Main
cert name equals puppet
server equals puppet
environment will be production
and the Run interval
then I'm going to escape and
get out of this
now that you have edited the
configuration file you're going to need
to set up the puppet server certificate
so for that
you have to type
you have to go to Puppet Labs
and certificate setup
and this might take some time and once
it's done you can start and enable your
puppet server so we're going to do a
system CTL
start puppet server
followed by systemctl enable puppet
server
so now that we've started it we are
going to enable it
all right now you can see something
along the lines of executing and your
puppet server is enabled now this was
all about setting up the puppet server
on the master
so now we are going to install the
puppet agent the same way that we
installed the puppet server by using apt
install
and now since I'm using an AWS instance
you can see this takes very little time
as opposed to if I were using an actual
VM yeah so now I'm going to go into the
puppet configuration file on the agent
machine and
make a few changes
I think I forgot a space here all right
we are here hit the eye so you can make
edits in there
like we had done previously we have main
we have
certification name we have
server
we have environment and runtime
so sort name will be
puppet agent
server will be puppet
environment will be production
and run interval will be 15 minutes
we're going to hit escape and we're
gonna quit the vi editor
next what we're going to do is we are
going to start and enable the puppet
server on the agent machine
so for that we're going to use this
command
op slash puppet lab slash bin slash
puppet you're going to call resource
service you can call the master ensure
it's running
and enabled so now you can see that your
service that is puppet it's running it's
enabled and you can see its provider now
running the puppet service on the agents
will generate send the certificate for
authentication on the master server so
now we're going to move on to signing
the certificate right we had seen this
in our architecture section for that
we'll first
move on to our Master machine
okay so finally here's
an extra bit which is signing the
certificate
this is a small little section which
consists of not more than three commands
first of all you're going to list all
the certificates left for signing on the
Master machine
for that you're going to use the puppet
server CA list command then you can
choose either to individually sign the
certificate or all of them together so
you can either use this command and put
the sort name or you can do a hyphen all
and to verify
you can open the agent and use the test
command and with that your puppet
installation and setup is complete
[Music]
what is ansible well ansible is a simple
open source ID engine which automates
the application deployment infra service
orchestration cloud provisioning and
many other I.T tools not only this but
ansible also is very easy to deploy
because it does not use any agent or
Custom Security infrastructures so
ansible uses Playbook to describe
automation jobs and playbooks use very
simple language that is the yaml so what
do you think this yaml is well yaml is a
human readable data serialization
language and is commonly used for
configuration files but it can be also
used for many applications where data is
also stored so this means it is very
easy for the humans to understand read
and write isn't it so the advantage of
yaml file is that even the it
infrastructure support guys can read and
understand the Playbook and debug if
needed so it's basically a really simple
language and makes it really easy for
the humans to read write and understand
now moving on with the session as
ansible is designed for the multi-trade
deployments ansible does not manage one
system at a time it models the it
infrastructure by describing all of your
systems which are interrelated so
ansible is completely agentless which
means that ansible works by connecting
your nodes to SSH by default but if you
want any other method for connection
like curve loss ansible also gives you
that option so after connecting your
nodes ansible pushes some small programs
called as the ansible modules and
ansible runs that modules on your notes
and removes them when they are finished
so it also manages your inventory in
simple text files those are basically
the host files that you can see on the
screen where ansible uses the host files
where one can group the host and control
the actions on a specified group in the
playbooks right
next as I was telling you about ansible
and ansible is the tool for
configuration management you should also
understand configuration management
before you understand how to deploy it
with the tools so talking about
configuration management configuration
Management in terms of ansible means
that it maintains the configuration of a
product performance by keeping a record
and updating detailed information which
describes an Enterprise hardware and
software so such information typically
includes the exact versions and updates
that have been applied to install
software packages and the locations and
also the network addresses of the
hardware devices so for example if you
want to install the new version of the
weblogic server on all the machines
present in your Enterprise it is not
feasible for you to manually go and
update each and every machine right so
you can install the weblogic server in
one Co on all your machines with the
ansible playbooks and inventory written
in most simple way so all you have to do
is that you have to list out the IP
addresses of your notes in the inventory
and write a Playbook to install the
weblogic server after that you have to
run the Playbook from your control
machine and then it will be installed on
all your nodes so guys now that you've
seen what configuration management is
let me show you how does ansible work so
basically ansible works by connecting
your notes and pushing out small
programs called as ansible modules to
them so ansible then executes these
modules over SSH by default and then
removes them when finished so your
library of modules can decide on any one
machine and there are no servers emails
or databases required so the management
know that you can see on the screen is
basically the controlling node which
controls the entire execution of the
Playbook it's the node from which you're
running the installation and the
inventory file provides the list of the
host where the ansible module needs to
be run and the management node does a
SSH connection and then it executes
these small modules on the host machine
and installs the product
so the beauty of ansible is that it
removes the modules once those are
installed so that it effectively
connects to the host machine and then it
executes those instructions and if it's
successfully installed then it removes
the code which was already copied on the
host machine and that was which was
executed so guys that's how ansible
works
now if you talk about the features of
ansible let's take a look at them one by
one so the first is that it is agentless
by agentless I mean that there's no kind
of software or any kind of agent
managing your note unlike puppet or Chef
where you need to install a puppet agent
or a chef client on all your note
machines but for ansible what you just
have to do is you just have to install
ansible in your control machine and then
you're good to go after that it is built
on top of python and this helps to
provide a lot of functionalities of
python right now coming to SSH ansible
uses SSH for secure connections now SSH
is a very simple passwordless Network
Authentication Protocol which is very
secure so our responsibility is to just
generate a public key in your control
machine and copy the same key onto your
node machines so coming to the final
feature the ansible is also push-based
architecture for sending configurations
so in the case of ansible where you want
to make any kind of configuration
changes on your notes all you have to do
is you just have to write down those
configurations and then you just have to
push them all at once in your notes so
if you talk in simple terms what it does
is that it gives you full control on
whenever you want to make changes on
your notes and also makes it very easy
and fast to set up and needs very
minimal requirements so guys the
features of ansible are it's agentless
it's built on top of python it uses SSH
and also uses the push-based
architecture
now as in the features section I was
talking about that ansible is a
push-based architecture what do you
think is the difference between the
push-based architecture and the pool
based architecture well tools like
puppet and Chef are basically pull based
configuration management tools whereas
ansible is push-based configuration
management tool now in the case of
puppet and Chef their agents present the
agent software that puppet has and is
basically called the puppet agent and in
case of Chef it is known as the chef
client so what this agent actually does
is that it keeps on pulling the central
server periodically for any kind of
configuration information and whatever
it finds it pulls those changes and then
gets them affected on your node machines
whereas in the case of ansible since
there are no agents present whenever you
want to make any changes you can make
those changes directly and you can push
those configurations directly whenever
you want to as you've got full control
over it not only this but also in a
push-based configuration management
system so as you can see on the screen
there's a version control system which
manages all the changes in the code base
and then we have a server so basically
the server will have ansible or any
other configuration management tool
system installed there and then we'll
have a bunch of nodes so these bunch of
nodes are basically the collection of
web servers and application service and
also the database servers and all these
are connected to the main server via SSH
now what happens here is that these main
servers constantly pull the version
control system for any changes in the
code Base by this I mean that it
constantly keeps on checking the status
of Version Control System at regular
intervals of time and whenever there are
any kind of changes so specifications
are sent to the main server and then the
server decides on how to take those
configuration changes according to the
changes in the code base now in the
situation where you want to write all
these new configurations you can
directly push them all at once onto your
node machines so you've got full control
here and whenever you want to make any
changes you can make the required
changes now coming to the full base
configuration the relation between the
version control system and the central
server is the same it basically
constantly pulls for the changes to the
version control system for any changes
in the code base and whenever there are
changes it pulls those changes but the
main difference between both the
architectures is that from a push-based
configuration management tool it has got
an agent present and then this agent or
the client is a software that manages
all the nodes so what this agent does is
that it constantly pulls the central
server for any configuration changes by
this I mean that it keeps on checking
the central server for any changes in
the configuration that needs to be made
and whenever it finds any kind of
configuration changes it pulls those
changes and then only it gets affected
onto the node machines so at this point
you have to wait for an agent to pull
those changes and only then the
configuration changes will get affected
onto your nodes so the basic point is
you have to be a little bit dependent on
the agent here and the Improvement tools
right so as I was chanting from the
beginning of the session that ansible is
really simple it's very easy to learn
because it gives us the Simplicity of
push-based system but there's a
disadvantage with the system that's
because you cannot achieve full
automation with the push-based system
because it's not usually possible to
board a server and have it configure
without some sort of client or server
protocol and also it gives you a lack of
scalability as well as when you're
dealing with hundreds of servers a
push-based system will start showing its
limits unless you make use of trading or
multi-processing alright guys so those
were the differences between the push
based configuration and the pull-based
configuration so ansible is basically a
push-based configuration management tool
now moving on with the session as I've
talked so much about ansible you would
be interested to install it so how do
you think that you can install ansible
so before I start telling you the steps
to install ansible let me tell you that
there are basically two types of machine
when we talk about deployment those are
the control machine and the remote
machines so the control machines are the
machines from where we can manage other
machines and the remote machines are
ghost machines which are handled or
controlled by the control machine so
they can be multiple remote machines
which are handled by a single control
machine so for managing remote machines
we have to install the ansible on the
control machine so now let me tell you
the steps to install ansible so for that
you just have to type in the command
sudo apt hyphen get update
so this command will basically update
your repositories now once that is done
you have to you know install the common
software properties so for that you'll
type in the command sudo apt hyphen get
install software hyphen properties
hyphen common so you can see that you
know on the screen that your
installation is going on now once that
is done you'll type in the command sudo
apt hyphen add hyphen Repository
space PPA colon ansible slash ansible
so this will basically include your
ansible's repository so you can see a
description about ansible
now if you want to continue you just
have to press on enter so once you press
on enter you can see that unit is
getting continued now once this is done
you again have to update your
repositories so for that type in the
command sudo apt get update
now once you're done updating your
repositories you have to install ansible
so for that you'll type in the command
sudo apt hyphen get install and simple
so this will basically install the
ansible tool
so you can see that your ansible has
been installed so guys that's how you
can install ansible
now let's move on with the session so
now let me just give you a summary of
the commands that you need when you
install ansible you just have to update
your repositories once you're done with
that you have to install the common
software properties once that is also
done you'll add a new repository on the
name of ansible and then you'll update
your own repositories and then finally
you'll install ansible so these are the
commands that you have to use now moving
on with the session what do you think
this host inventory is well inventory
defines a group of hosts which are alike
in anyway so for example you would want
to group your web servers in one group
and application service in another right
so group can have multiple servers or a
single server that can be a part of
multiple groups so the name of the group
is enclosed in square brackets and the
server names can be their DNS names or
IP addresses so by default ansible looks
at the inventory file but that can be
modified by passing the inventory part
to the ansible command line so we can
modify the way ansible connects to our
host by supplying the additional
information in the inventory file so as
you can see on the screen this is just a
list of IP addresses on my notes and
also you can name the groups of your
different IP addresses so for example if
I want to group my web servers together
and my data servers together all I have
to do is I have to write a group name
between the two square brackets and a
big group together so whenever you want
to make configuration changes just on
the web server and not on the database
server all you have to do is you just
have to mention the group name on the
host and then it will automatically
configure your web server so guys that
was about host inventory now if you talk
about another key term in ansible that
is ansible modules so model are those
executable plugins that get the real job
done so usually modules can take the key
value arguments and run in a customized
way depending upon the arguments
themselves so a module can be invoked by
command line or can be included in the
ansible Playbook now let me tell you
some commands related to ansible modules
now if you want to use the modules from
command line you have to type in the
command ansible all hyphen amping now if
you want to use the Ping module to Ping
all the hosts defined in the inventory
then you have to type in the Command
ansible Web servers hyphen M command
hyphen a LS so this will basically list
all the modules present and then it will
ping now if you want to flush iptable
rules on all the hosts in the inventory
then you have to type in the command
ansible hyphen I inventory all hyphen M
command hyphen a iptables f hyphen knife
and Bikram hyphen iPhone ask hyphen
Beckham hyphen pass so this will
basically tell ansible to execute the
command with pseudo privileges
so if you're more than people to gather
facts about the host then you have to
type in the command ansible all hyphen M
setup and also if you want to extract
particular facts in the documentation of
the setup module you have to type in the
command ansible hyphen doc setup so Guys
these few commands that you can use with
ansible modules
now moving on with the session let me
tell you about the yaml files because
that is the file that has been used so
ansible uses yaml Syntax for expressing
ansible playbooks ants will use a CML
because it's really simple for humans to
understand read and write when compared
to the other data formats like XML or
Json so every AML file optionally starts
with three hyphens and ends with three
dots so as you can see on the screen
here it starts with three hyphens and
ends with three dots now you can also
use abbreviation in yamls to represent
dictionaries so as you can see on the
screen it basically follows the pattern
of a key value pair so here the key
value is James and it has various values
like its name row number division sex
everything so the key James has many
values like the name James John the roll
number to be 34 the division to be B and
the sex to be male not only this but we
can also represent the list in yaml file
so every element of the list should be
written in a new file with the same
indentation Asians starting with the
hyphen and a space so for example as you
can see on the screen after writing
countries we have written America with
the hyphen and the space and then the
next value comes in the next line with
the hyphen and a space that is China
similarly Canada and Iceland so if you
have to represent this list in an
abbreviation form it can be represented
in the form of countries colon and in a
square bracket you can type in America
China Canada and Iceland right now
further talking about yaml files you can
also represent a list of dictionaries so
yaml uses the pipeline to include new
lines while showing multiple lines and
uses the greater than symbol to suppress
new lines while showing multiple lines
so due to this we can read and edit
large lines and in both the cases
indentations are ignored so guys that
was about the yaml files now talking
about ansible playbooks let me tell you
how to write an ansible Playbook but
before that let's see what exactly
ansible Playbook is so as I've told you
before that the playbooks are the files
where ansible code is written and these
are written in the yaml format so yaml
stands for yet another markup language
and playbooks are one of the core
features of ansible and tell ansible
what to execute so they're like the
to-do lists for ansible that contains a
list of tasks and playbooks contains the
step which the user wants to execute on
a particular machine so playbooks run
sequentially and playbooks are the
building blocks for all the use cases of
ansible so each Playbook is an
aggregation of one or more plays in it
and playbooks are structured using the
place so they can be more than a plain
set of Playbook and the function of a
place to map a set of instructions
defined against the particular host so
since the ml is a strict type of
language we need to take Extra Care
while writing the yaml files so now let
me tell you how can we create a Playbook
so as I mentioned before Yaman starts
with three hyphens and then we have
different yaml tags so now let's go
through each and every yaml tag so the
different tags that are included are the
name hosts words and tasks so the name
tag specifies the name of the ansible
book so basically I'm talking about as
in what this Playbook will be doing or
any logical name that can be given to
this Playbook coming to the host tag the
host tag specifies the list of host or
the host group against which we want to
run the task the host field or tag is
mandatory and it tells ansible on which
host to run the listed task so the task
can run on the same machine or on a
remote machine and one can run the task
on multiple machines and hence the host
tax can have a group of host entry as
well coming to the vars stack the vars
tag lets you define the variables which
you can use in your playbook and usage
is very similar to the variables in any
programming language
now coming to the final task that is the
task all the playbooks should contain
tasks or a list of tasks to be executed
so the tasks are the list of actions one
needs to perform so task field contains
the name of the task and this works as a
help text for the user well it's not
mandatory but proves useful in debugging
The Playbook so age task internally
links to a piece of code called module
now let's move on to the Hands-On part
where we'll basically you know create a
Playbook and then I'll show you the
working of ansible so let's get started
with our Hands-On part so first we have
to create an SSH connection right to
create an SSH connection with the node
so for that you have to type in the
command SSH hyphen Keygen so this will
basically you know generate your key so
once you type in the command you can see
an output that it is generating a RSA
key pair now if you have to mention the
directory where you want to store the
key but if you press enter it will it
will store it as a default location so
let me just press enter and once you
press enter you can see an output that
it has created a directory for SSH now
you have to enter the pass phrase so for
that let me just press enter again and
again we have to mention the same pass
phrase so for that let me just mention
it again and you can see that you know
our key has been generated
all right now once your key has been
generated you have to create an host
inventory file so we're creating a host
inventory file so that we mention the
specifications of the node IP address
that we want to connect with so for that
you'll type in the command so for that
you'll type in the command sudo and then
you'll mention the directory of your
host so I've just mentioned the
directory and then I'll press on enter
so it will ask for my password so I'll
mention the password
so as you can see on the screen this is
the default host inventory file now as
we have to mention the name of a servers
I'll just mention it in the square
brackets let's say I mentioned the name
to be test servers so for that I'll type
in test servers and then I'll mention
the name of the node so let me just type
in the name of the node so over here my
name of the node is K node so I've
mentioned that now once this is done
just save this file and close this file
so for that you'll press the keyboard
shortcut Ctrl X and then press on y so
this will save your host inventory file
so once you're done with that you know
you have to mention the SSH generated
key in the K note that is the node that
we just configured right so for that you
will type in the command SSH hyphen copy
hyphen ID space hyphen I space and the
name of the node so over here my name of
the node is K node once I'm done with
that I'll press on enter and you can see
that you know it will ask for a
confirmation so let's just say yes so
you can see the output that our key has
been copied
once you're done with that you'll be
asked for your password so let's just
type in the node password and you can
see you know your key has been
successfully added to your node
now before I move forward I've just
cleared my terminal so just in case if
you want to know the command the command
is clear so once you type in clear
terminal gets cleared now moving on with
the Hands-On now once your key has been
generated you have to you know create a
Playbook so before I start creating a
Playbook let's just check if our test
servers are running or not so for that
you have to type in the command ansible
space hyphen M ping and the name of the
server so here I've given test servers
so I'll just type in that command and
then I'll press on enter now once this
is done you can see an output that you
know it is successful so now let's start
creating playbooks so to start creating
playbooks you have to first create a
yaml file so for that you have to type
in the command sudo Nano let's say I
give the name to be edureka demo dot
yaml so this opens up IML file now let's
start mentioning the specifications
required so first I'll mention the host
so as I mentioned before that yaml file
starts with three hyphens so let me just
mention three hyphens and then I'll
press on enter so let's start by
mentioning the yaml tags so let's start
with the host so for that I'll type in
hyphen host space colon I'll mention the
name of the server that is the test
service
then I'll mention Bikram and over here
you make sure that you know you do not
miss out the space so it has to come in
one go
and then let's say I put it true then
I'll mention variables so for that I'll
type in the command words and then you
have to make sure that you know you're
maintaining the syntax so I'll type in
words mention a colon and then inside
which I'll type in ansible
hyphen become hyphen pass colon to
edureka that is basically our password
then I'll again mention task colon space
colon and then inside which I'll mention
the name that is basically the name of
the task so for that I'll type in hyphen
name space colon install engines
and then let's mention the package also
space colon space PKG is equal to
engines
space state is equal to installed
and then I'll put in a space and then
after that so let me just put in spaces
and then I'll mention notify colon I'll
press on enter I'll go inside that I'll
mention hyphen start engines
after that let's press on enter and now
let's mention the handlers so for that
let's again maintain the spaces
space colon inside which will again
mention the name so for that we'll
mention name space colon start space
engines and then mention the service so
that is name engines state is equal to
start it so guys this was my yaml file
to create a Playbook so for that I'll
press the keyboard shortcut Ctrl X and
then I'll press on y so this will save
my file now once you have saved this
file you have to run your playbook so
for that you'll type in the command
ansible
hyphen Playbook
and the name of the yaml files over here
it's edirect demo so I'll mention any
breaker demo.yaml and then I'll press on
enter
so you can see on the screen that the
Playbook is running
so as you can see the Playbook has run
so basically it mentions name of the
server that is the test servers it
mentions the task so we wanted to
install the engine so it has mentioned
the task then it also mentions the
running hand the node that is the K node
over here and then it also mentions
whether it is unreachable failed or not
so guys this is how Playbook runs now
once the Playbook runs it automatically
installs your engines container that is
basically the task that we mentioned now
to check whether it is running on the
other node or not you have to switch to
your other nodes so let me just switch
to my other node
now now to check whether engines has
been installed or not on the other node
let's type in the command
PS walks that is wa ux space pipeline
space grep the name of the file that we
install that is engines now let's press
on enter
so you can see the output that you know
it has been installed
so guys that's how ansible basically
works it's really simple to do right so
you just have to mention on one node and
all the actions will happen on all the
other nodes connected to it
foreign
let's start with understanding why
should you use terraform there are some
great pre-existing tools like ansible
Chef puppet and salt stack but then what
makes terraform better than these tools
first reason is that terraform is a
server orchestration tool all the above
mentioned tools were created for Server
configuration meaning that their primary
goal is to install and manage software
on the already existing server terraform
concentrates more on server provisioning
with software container deployment left
to Docker and Packer
the next reason is the immutable
infrastructure
which chefs salt puppet or ansible any
software update must be run in place
thus every server grows a unique record
of updates throughout its life cycle
this can quite often lead to so-called
configuration drift when the differences
in these configurations lead to bugs
terraform addresses the issues by
utilizing an immutable infrastructure
approach where every new update of any
parameter creates a separate
configuration snapshot this way updating
the development environment goes
smoothly and is completely bug proof
the next reason is the declarative code
style wild Chef or ansible forces you to
write step-by-step procedural
instructions for teaching the desired
State terraform prefer describing the
desired each state of the system and the
tool itself deals with reaching the goal
state
but why is it better with the procedural
code you need to think of all the recent
events and processes that took place in
order to write clear instructions but
with terraform you simply order the tool
to do something with the currently
active state of the system this is why
the code base remains quite small and
easily understandable
the fourth reason is the client only
architecture
terraform leverages the cloud provider's
API for provisioning the infrastructure
which removes the need of additional
Security checks running a separate
configuration management server and
multiple software agents ansible does
this by connecting through SSH yet the
capabilities are quite Limited
so what is terraform
terraform is an open source
infrastructure as a code tool developed
by hashicor it is used to define and
provision the complete infrastructure
using an easy to learn declarative
language called HCL or hashicorp
configuration language terraform
supports multiple providers such as AWS
Azure kubernetes Google cloud and many
more
now let's understand terraform better
through a use case
Rapido is an agile Innovative company
they are providers of strategic Cloud
Consulting Cloud migration devops
automation managed services and security
Consulting
so once the client was looking for a
cloud agonistic tool to orchestrate the
infrastructure onto multiple public
clouds they wanted the infrastructure to
be up in a matter of few minutes the
client had different environments like
production staging and QA to test
database backups were also included as a
part of infrastructure setup
devops engineers at rapidder debuted and
analyzed the client's requirements and
came up with a solution of using
terraform to codify the provisioning of
infrastructure onto public Cloud not
only infrastructure but rapidly also
automated the application deployment
process by integrating gitlab to improve
the software development and accelerate
the software delivery the solution
provided by rapidder also took care of
database backup for a situation when the
infrastructure is destroyed using this
solution the client is now able to test
their application and deploy it on
production environment without downtime
so the overall benefits that a client
could experience include the client's
ability to create the infrastructure
over multiple public clouds within a
matter of few minutes next is that the
client has the flexibility to launch the
infrastructure as and when needed
furthermore the automated deployments
help the client push the application
into production seamlessly thereby
reducing the time to Market
also the automated backup of the
database ensures that they always have a
copy of their latest database
moving ahead let's understand terraform
life cycle
it includes initialize plan apply and
Destroy terraform init initializes the
working directory which consists of all
the configuration files terraform plan
is used to create an execution plan to
reach a desired state of the
infrastructure terraform apply then
makes the changes in the infrastructure
as defined in the plan and the
infrastructure comes to the desired
state
terraform destroy is used to delete all
the old infrastructure resources which
are marked tainted after the apply phase
now let's see how terraform works
terraform core uses two input sources to
do its job
the first input source is a terraform
configuration that you as a user
configure here you define what needs to
be created or provisioned the second
input source is a state where terraform
keeps the up-to-date state of how
current setup of infrastructure looks
like
so what terraform code does is it takes
the input and it figures out a plan of
what needs to be done it compares the
state and checks what is the current
state and what is the configuration that
you desire in the end result it figures
out what needs to be done to get that
desired state in the configuration file
the second component of Architecture is
provide us for specific Technologies
this could be Cloud providers like AWS
Azure or other infrastructure as a
service platform it gives you the
possibility to create infrastructure on
different levels terraform has over
hundreds of providers for different
Technologies and each provider then
gives terraform user access to its
resources
for example AWS provider gives you
access to hundreds of AWS resources like
ec2 instances the AWS users Etc with
kubernetes provider you can access
Commodities resources like services and
deployments namespace Etc
so this is how terraform works and this
way it tries to help you provision and
cover the complete application setup
from infrastructure all the way to the
application
moving ahead let's see how to install
terraform
now to install terraform let's go to its
official website
so the system I'm using is a 64-bit
Linux system so
I'll install terraform for that I'll
copy the link first
then what we need to do is open the
terminal
then we write wget and we paste the link
we had copied
and enter
so now terraform will get installed
okay so terraform has been installed
since it's a zip file let's unzip it now
and this is the file name let's copy
that
now let's move terraform to a location
let's see what the location will be
so we'll be moving terraform to this
location
okay now that it has been successfully
moved let's see the terraform version
that we have
so it's 14.5 okay
so by writing terraform and entering
you'll see what all commands terraform
provides you
so these are all the commands it also
includes init plan apply destroy as we
discussed before there's something also
called as validate that you can use
and several other commands
now that we have installed terraform in
our system let's move ahead with our
demo
so let's just clear screen
okay
now what we need to do is we are going
to create a directory
and we are going to name it terraform
demo
okay
so let's move into that directory now
now what we are going to do is we are
going to create a configuration file
terraform configuration file over here
so let's write G edit and the file name
so let's name it demo one dot TF so this
is the extension that we need to put for
any terraform configuration file that we
are going to create here and hit enter
okay so the G edit file terraform
configuration file has been created here
is where we are going to put all the
configuration we need so initially let's
try to create an ec2 instance in Amazon
web services and then destroy that ec2
instance okay so what we need to do here
is terraform provides you with
everything every support that is there
for your code online
so let's say AWS provider
okay so you can see the usage here this
is what we need to Define that we want
to create or use AWS service
let's paste this here okay
so it asks for region access key and
secret key so what are access key and
secret key here we'll get that from
Amazon web services let's see how
so I'm going to create a user over here
I'm gonna give it terraform
and I'm going to give it a programmatic
axis
along with that
administrator access
and next
so let's give this as my demo
and value as terraform
okay let's just keep it to terraform
so this is the summary
and let's create it
so user has been created and this CSV
file is what we need to download to get
the you know the access key and the
secret key
okay
we have the file here let's open it
so this is the access key here let's
copy this
and we are going to paste it here
and we have the secret key
again let's copy and paste it here okay
so for region let's create this for us
East 2 okay now we are going to put what
resources we need since we are creating
ec2 instance let's
so what we need is just this part for
now for creating our instance let's
paste it here
okay so it says we want to create AWS
instance let's change this to
and this Ami is for us to S2
so every time you change the region Ami
is going to change so let's find it for
us East 2.
so let's check
so we are on uscs2 right now
and you can see there are no instances
running
okay so suppose we want to create an
instance we want to launch one
so here you can see the Ami for
uscs2 let's copy that
and we'll paste this here
okay
so I think this is enough we don't need
to add anything else we have provider
AWS and we have given the region
axis and secret key
we want to create AWS instance
and Ami and instance type as T2 micro
let's keep it that way okay
let's save the file
and close it
now let's go back to the terminal
and now what we are going to do is
initialize it
then let's apply terraform plan
okay you can see here one in the plan
which is the easy two instance we want
to create
and then finally terraform apply
so it asks for an approval from you so
you just write yes over here
okay it says one added let's go back to
AWS and see
if the ec2 instance in Us East 2 has
been created
okay so you can see the instance has
been created here
okay yeah so one instance is running and
we are in USC's too so the instance has
been created
okay
now what we need to do is we want to
destroy this instance that we have
created now simply what we need to do
here is terraform
destroy and this will destroy that
instance
okay you can see one destroyed over here
let's see if it has been destroyed
so you can see we had an instance over
here but it has been destroyed now
so this was a very simple and basic demo
that I have shown you here
now let's do something a little more
complicated let's try to add some more
services that AWS provides you
so let's clear the screen okay
now what I'm gonna do is we have the
same G edit file over here you can see
the demo one
okay so we have written a code partially
we heard let's include some more things
and additional features in this code
itself so just remember that this
instance has been already destroyed so
while we initialize plan and apply this
instance is going to be created again if
we keep this code okay
so I want to create this instance again
but apart from that what I want to do is
I want to create another ec2 instance in
some another region so let's see how to
do that
for that let's copy this okay so all I
need to do is suppose I want to create
another instance in the US East one so
let's change this the access key secret
key is going to remain same because we
are making it through the same user okay
let's create another resource here
because we want to create one more
instance
okay let's name this Something Like Us
East one
okay and just remember the Ami is going
to change for us East one okay so let's
search for that Ami now
okay
yeah okay there is no instance over here
so suppose we want to create one
so you can see the Ami here for Linux so
let's copy this and
so this is the US East one let's paste
this here so you can see the Ami divorce
from region to region okay
so we have done this now what we need to
do is how will this resource understand
that it wants to create an ec2 instance
in Us East one so for that what we are
going to do is we are going to create an
alias
so let's put this as
okay so this is done we have added the
Alias over here
so let's refer this to this part okay
this Alias that we have created how we
are going to do that is by
writing provider is equal to AWS Dot
whatever Alias we have provided here
copy paste it
okay
so this is done
now apart from this let's see we want to
create an S3
so
okay S3 bucket
okay so for S3 this is what we need
so let's copy these
and paste it here okay so let's change
this to
something let's keep this
so one thing to remember is that the
bucket name has to be unique so let's
try to make a unique name over here
okay let's keep this let ACL be private
let's change this to my terraform bucket
and let's keep this
Debbie and me okay apart from this what
we are going to do is we are going to
add versioning over here
so why we are adding versioning here is
because we want to preserve retrieve and
restore every version of every object
stored in our Amazon S3 bucket so that's
the reason we have added the versioning
block over here okay so looks fine
let's save this
okay so we have one ec2 instance in Us
East 2 another in Us East one and one S3
bucket so remember that in case of Us
East 2 we haven't used any areas so what
it is going to do is it is going to
refer to this default one where there
are no Alias nothing provided for it you
know to refer so it's going to refer
this by default uses too okay let's save
this
and let's close this
okay
let's open the terminal and
let's initialize again
then plan
okay so you can see three over here to
ec21s3
let's apply it
okay so you can see three added
let's check it now in AWS
okay so we are in U.S east one so we
will have one ec2 here
okay so we can see one instance running
okay so we have one instance running
over here let's check it in Us East 2
okay so we have one instance
successfully running over here as well
and let's check our S3 bucket
okay so you can see
we have one S3 bucket created over here
successfully great
now let's say we want to add a VPC one
subnet and also want to create
relational database system okay so more
three things VPC subnet and RDS
so what we can do is we can actually add
to this rather than creating another
file
so let's open this again
okay
So Below this let's start adding but one
thing to remember is that we haven't
destroyed any of this yet the two ec2
instances and S3 we haven't destroyed
them so what we are going to do is we
are going to add to this okay so what we
are going to add let's start with VPC
so
okay
so let's copy this this is what we need
to create a VPC
and paste it here below
okay so it says resource we want to
create is AWS VPC let's give this a name
as
Dev
okay cider block okay let's keep this
and let's keep this default okay let's
change the tag name to something else
let's keep it
Dev VPC okay
so that's done now suppose you also want
to create subnet
so let's search for that
okay let's copy this
and paste it here
let's again change a little bit
let's give it an EMS sub let's keep the
side a block same
and let's change it to Dev
subnet okay
so one thing to see over here is we need
VPC ID over here but since we haven't
created a VPC yet how will we give the
VPC ID over here so there is a way to do
that so what we are going to do is let's
just copy the whole thing that we have
over here copy this
and paste it here
we don't want the double inverted commas
okay
so this is how we are going to give the
VPC ID to our subnet okay it's the same
as this over here just AWS VPC dot Dev
dot ID okay this is how it will get the
ID of the VPC okay so this is done apart
from that as we said suppose we want to
create RDS let's see what to do for that
so you can see the basic usage here
let's copy this
and let's make some changes
we want engine version as 8 point
0.19
okay
and let's change this to eight point
zero
okay
so we are going to add one more line
over here which is we don't want to
create a snapshot of this RDS that we
are creating okay suppose this is some
dummy RDS that we have created we don't
want any backup for this so we are going
to provide it that
we don't want a snapshot okay
okay
so we have written Skip's final snapshot
and we have kept it true okay so it
won't create a snapshot
okay so that is done
let's maybe change the name to my
terraform DB okay
username let's keep this the same okay
this is just an example so overall what
all we have added to this we have
created one VPC subnet and RDS let's
save this
okay and close it
let's go back to the terminal
and initialize it
then terraform plan
okay so you can see here that three to
add
so we already have two ec2 1s3 and apart
from that we are adding one VPC subnet
and RDS okay so let's apply
okay so we can see three added let's go
and check
let's check this in our AWS console
so we can see that S3 is still running
the one we had created before so the
three things we have created now is VPC
subnet
and RDS okay let's check those now
so we can see the VPC we have created is
now available it's running and this one
is a default VPC that was already
present apart from that let's check
subnets
so see our subnet is present over here
and it's running rest are default okay
apart from that let's check RDS
so we can see one database instance here
let's
see
okay so you can see the RDS we had
created for MySQL
that's running now
okay now suppose everything that we have
created now we want to destroy it okay
imagine how much time would it take to
destroy all of these through console now
let's see how we are going to destroy it
through terraform
so all we need to do here is terraform
destroy
so what all will it destroy everything
that we have created in that file
everything is going to be destroyed
which includes our two ec2s one S3 VPC
subnet and RDS okay everything has to be
destroyed through one single command
okay let's see
so you can see six over here
so let's see if it destroys
everything
okay so it says six destroyed let's
check one by one okay
now currently we are an RDS let's
refresh this and see
okay so you can see the RDS has been
deleted now it is destroyed so
let's next see vpcs
so we can see the VPC we had created is
deleted the one present over here is the
default one let's check the subnet
okay again the subnet we had created has
been destroyed and these are just the
default subnets that are present apart
from that let's see ec2
So currently we are in Us East 2 right
okay so you can see there are no
instances running over here
okay so the instance we had created has
been destroyed now let's check S3
as you can see no buckets over here so
the S3 we had created is also destroyed
okay now let's go back to ec2 and we are
going to change our region to us East
one
so remember we had created
one more instance and different region
using Alias let's see if that has been
destroyed as well okay so you can see
there are no running instances
okay so the instance we had created in
Us East one also has been destroyed
this is just one example of how multiple
Services AWS provides and you can do
everything through infrastructure as
code rather than going through console
okay so you can do this with other Cloud
platforms as well like Azure Google
clouds and there are many more so this
was just a simple example for AWS
foreign
[Music]
application used for event monitoring
and alerting now what it really does is
it records real-time metrics we'll talk
about metrics later on in the session
now this metrics that it records is
stored in a Time series database built
using a HTTP pull model and with this
you also have flexible queries and
real-time alerting Prometheus basically
pulls these metrics which are in a key
value format and stores the data As Time
series allowing users to query data and
alert in a real-time fashion at given
intervals Prometheus will hit targets to
collect metrics aggregate data show data
or even alert if some thresholds are met
in spite of not having the most
beautiful graphical user interface now
that we know that Prometheus is
basically an alert monitoring system now
let's move on and understand why we need
Prometheus as I've already mentioned in
the introduction it basically
contributes to the devops system by
monitoring Cloud native applications and
infrastructure and also by watching over
hundreds of microservices now it really
works well for recording any purely
numeric time series data now another
really important benefit of Prometheus
is that it fits both machine-centric
monitoring as well as monitoring of
Highly Dynamic service oriented
architectures this is a real Advantage
now in a world full of microservices its
support for multi-dimensional data
collection and querying is a particular
strength now another advantage of
Prometheus is that it is designed for
reliability to be the system you go to
during an outage to allow you to quickly
diagnose problems now each Prometheus
server is Standalone not depending on
network storage or other remote Services
you can easily rely on it when other
parts of your infrastructure are broken
and you do not really need to set up an
extensive infrastructure to use it now
you You Must Be Wondering Why Prometheus
really because there are other time
series databases that can be used to
write so one of the noticeable
differences between Prometheus
monitoring and other time series
databases is that it is a full based
tool unlike nagios sensui Etc now
because it is a pool based tool what it
really does is it actively scrapes
Targets in order to retrieve metrics for
them for example the node exporters or
application exporters so Prometheus
expects to retrieve metrics via HTTP
calls done to certain endpoints that are
defined in Prometheus configuration node
exporter and application exporters will
listen on particular ports and
Prometheus server initiates a HTTP call
to this particular exporter and switch
system or application metrics from the
end now that we know why Prometheus is
so important let's move on and
understand continuous monitoring in
Prometheus monitoring applications and
application servers is an important in
part of today's devops culture and
process now why a continuous monitoring
you obviously want to continuously
monitor your applications and servers
for application exceptions server CPU
memory usage or storage spikes and you
also want to get some type of
notification if the CPU or memory usage
goes up for a certain period of time or
a service of your application randomly
stops responding so that you can perform
appropriate actions against those
failures or exceptions this is why
Prometheus is used in continuous
monitoring now with monitoring the
number of tools out there such as Amazon
cloudwatch nagios New Relic and many
others but Prometheus is a very popular
free and open source project for
monitoring dozens of servers as I've
already mentioned even microservices now
though we know what is prometh is used
for in continuous monitoring let's move
on and check out the Prometheus
architecture
so in Prometheus terms we call the main
monitoring service the Prometheus server
you can see that on the screen and the
things that Prometheus monitors are
called targets so the server is the
monitoring service and the targets is
what the Prometheus monitors so I hope
that is clear so in short the Prometheus
server monitors targets I said earlier
targets can also refer to an array of
things it could be a single server or a
target for probing of endpoints over
HTTP https DNS TCP or it could simply be
a HTTP endpoint that an application
exposes through which the Prometheus
server gets the application's health
status from so now let me explain a
little bit about metric each unit of
Target such as current CPU status memory
usage or any other specific unit that
you would like to monitor is called a
metric so Prometheus server collects
metrics from targets over HTTP stores
them locally or remotely and then
displace them back in the Prometheus
server now the Prometheus server scripts
targets at an interval that you define
to collect metrics from specific targets
for example for a minute or 30 seconds
or whatever you want you can give the
target as and then it will finally store
it in a Time series database you can see
the storage box that is in between that
is your time series database now you
define the targets to be scraped and the
time interval for scraping metrics in
the Prometheus dot yml configuration
file you get the metrics details by
querying from the Prometheus time series
database with the Prometheus stores
metrics and you use a query language
called prompt ql in the Prometheus
server to query metrics about the
targets so in other words you ask the
Prometheus server via prom ql to show us
the status of a particular Target at one
particular time as you can see in third
white box it accepts queries that is it
accepts prompt ql queries now Prometheus
provides client libraries in a number of
languages that you can use to provide
health status of your application but
Prometheus is not really only about
application monitoring you can use
something called exporters to monitor
third-party systems now exporter is a
piece of software that gets existing
metrics from a third-party system and
eventually export them to metric format
that the Prometheus server can
eventually understand now a sample
metric from a Prometheus server could be
the current usage of free memory or file
system free via a node exporter in the
Primitive server we learn all of this in
the demo part of this session now I hope
Prometheus server is clear to you so the
main component here is the Prometheus
server which is divided into three parts
the retrieval part the storage part and
the HTTP server so the HTTP server
really accepts queries and displays it
in the graphical user interface also
we'll be learning about alert manager
later on in this session so I hope all
of these are clear let's move on to the
next part of the session and learn about
Prometheus metrics and its types so here
each unit of a Target such as the
current CPU status memory usage or any
other specific unit that you would like
to monitor is simply called the metric
so the server collects the metrics and
stores them either locally or remotely
and then eventually displays it on the
Prometheus graphical user interface now
moving on let us check out the four
different types of metrics that are
available the first one that we'll be
talking about is the counters a counter
is simply a cumulative metric that
represents a single monotically
increasing counter whose value can only
increase or be reset to zero on restart
now for example you can use a counter to
represent the number of requests served
tasks completed or errors you always
have to remember this to not use a
counter to expose a value that can
decrease now for example do not use a
counter for the number of currently
running processes instead user gauge
we'll be talking about gauges in a bit
now client Library usage documentation
for counters are go Java Python and Ruby
let's move on and understand the next
metric type that is gauges I'm sure you
must be aware of gauge now a gauge is a
metric that represents a single
numerical value that can arbitrarily go
up and down gauges are typically used
for measured values like temperatures or
current memory usage if you check out a
speedometer in a car it looks like a
gauge right that is a gauge that's
exactly what we're talking about here so
the client Library uses documentation
for gauges are go Java Python and rupee
the next metric type is the histogram
now a histogram samples observations and
counts them in configurable buckets it
also provides a sum of all observed
values a histogram with a base metric
name of Base name exposes multiple time
series during a scrape now when you're
operating on buckets remember that
histogram is cumulative now moving on to
the next metric we have summary which is
very similar to histogram a summary
samples observations while it also
provides a total count of observations
and a sum of all observed values it
really calculates configurable quantiles
over a sliding time window now a summary
with a base metric name of Base name
exposes multiple time series during a
script
now the difference between summary and
histogram is that here you do not know
what the range of values will be upfront
so you cannot use histograms whereas in
histogram you know what the range of
values will be upfront so you can just
Define it on your own now with this
let's move on to the demo part of this
session and install Prometheus on our
Linux virtual machine so I'm just going
to check out my Ubuntu machine number
so the first thing what we're going to
do is we're going to install Prometheus
from the official website so I'm going
to go to my web browser so I'm just
gonna type in Prometheus in my web
browser and I'm going to click the first
link
and as you can see this is the official
website of Prometheus I'm gonna click on
the download button so once I click on
the download button it will redirect me
to a page so as you can see here there
are a lot of products Prometheus alert
manager Black Box exporter and a lot
more if you keep scrolling down right
now we're going to be downloading
Prometheus so we're just going to stay
here and I'm just going to click on the
operating system as Linux and this will
just show the tar file for the Linux
operating system I'm just gonna right
click on that and I'm going to click on
copy link location and I'm going to
switch back to my terminal and click on
wget
and this will download Prometheus for me
in my Linux operating system
now that it's downloaded I just want to
see the list of files in my tar file so
as you can see there's the Prometheus
Linux star file which is present
currently in my Prometheus folder
now what I'm going to do is I'm going to
copy this file and then untie it using
the command tar hyphen x v f and then
I'm just going to paste this file
this will immediately enter the file for
me once that is done now I'm just going
to list down the number of files that
are currently present in my Prometheus
file now you can see that there's an
file which has been untied here clear my
screen
okay now I'm again going to just list it
so you can see it clearly now I'm just
going to copy this file and enter this
directory
so I'm just going to list down all the
files that are present you can see
there's console libraries this consoles
license notice there's a file that's
named Prometheus so this is the
application the Prometheus application
so I've already mentioned before that
the prometheus.yml configuration file
really just defines the targets that has
to be scraped right and also the time
interval for scraping metrics now let's
just check if all of those parameters
that I just mentioned are there in the
configuration file so I'm just going to
use scat command for it you can see that
there's a scrape interval parameter of
15 seconds you can set the scrape
interval to every 15 seconds default is
every one minute so I also have an alert
manager configuration right now we
haven't set that up but it's just there
you can also load your own rules and
evaluate them according to the global
evaluation interval and you can see by
default it is currently active on the
9090 Port okay and by default scrap
configuration containing exactly one end
point to scrape is given that is the
Prometheus itself right as I've already
mentioned before Prometheus is a tool or
an application that will scrape out
metrics from different applications but
by default right now we have the
Prometheus itself to be scraped and that
is currently present on the port number
9090 so I hope that is clear we will
learn about the rules in the next part
of the session we'll also learn about
the alert manager configuration in the
later part of the session so both of
these will be clear later but I hope the
scrap interval and you know the scrap
configs are clear to you now let's move
on to the next part now I'm just gonna
clear the screen
now I'm just going to execute Prometheus
by using Point front slash Prometheus
and this will immediately activate and
get my Prometheus application running
if we move back to my web browser and
just type in localhost
9090 this will redirect me to my
Prometheus application as you can see
now if I come out of my terminal right
okay and I reload my page you can see
that it is enabled to connect to my
Prometheus so what I'll have to do is I
will have to create a Prometheus service
and get that running so let's learn how
to do that I'm just going to clear my
screen
so now to be able to run Prometheus as a
service I'm going to use the copy
command I'm just going to use the CP
hyphen R command I'm just going to copy
all of these files that are consisting
in this folder to the destination folder
and I'm going to use this command hyphen
user local hyphen bin Prometheus now for
any application to be run a service you
will have to compulsorily have a DOT
service file so let's go ahead and
create one so to create the service file
I'm just going to use the command sudo
VI and I'm going to enter that
particular folder
so as you can see I just typed out a
tiny piece of code and as you can see
here this is my executable command and
this is my configuration file let's go
ahead and save this
okay great so now that our service file
has been created I'm just going to clear
my screen and now we're just going to
start our service file
now once that is done I'm just going to
check the status of Prometheus it is
active and running great
now if I want to stop Prometheus I'm
just going to type in the command sudo
service Prometheus stop okay now that I
refresh my page you can see that
Prometheus is up and running so that's
great I hope it's clear how to create
your own service file so you can get
Prometheus up and running I hope it was
very clear and I hope you understood how
to install Prometheus into your virtual
machine now in the next part of the demo
I'm just going to brief you all about
the graphical user interface of
Prometheus so stay tuned
okay so now we're gonna check out the
user interface of Prometheus so in the
previous session we installed Prometheus
and we got the instance running now if I
click on Prometheus this is where it
will take me now you can see that there
are three options that says enable queue
history use local time enable
autocomplete you can use the local time
if you'd like to enter some time to
evaluate the queries so there's a search
bar here for you to type in the
expression that you'd like to execute
now the expression or query that you'd
like to execute will be is usually
executed in two different forms that is
the table form and the graph form as of
now since you don't have any expression
and query you it says no data queried
yet now on the top you can see three
options that says enable query history
use local time enable autocomplete so
this option will allow you to use your
local time since I'm in India and it's
10 8 right now so that's my local time
now if I want to also enable my query
history tree that is all the queries
that I have executed in the past so this
option will simply enable me to check
out the queries that I've executed in
the past so I'm just going to untick
that because I don't have any queries
that I've executed in the past anyway
another interesting thing in this page
is that you can add several panels right
so you can execute different queries all
at once this is great and this option
add panel has been added in the recent
times so this is the new user interface
you also have a classic user interface
I'll shift to that and you can see that
here you can enable query history that's
the only option that's common from the
new user interface also you have a bar
that will allow you to type in your
expression or query you can also insert
metric at cursor now since Prometheus is
written in golang so you have a lot of
metrics that are related to that you
also have metrics related to Prometheus
now if I want to execute any of these
metrics I can just click that and I can
execute it you can see that the instance
localhost of Port 9090 is currently up
and running and the job is Prometheus I
can view the graph too if I'd like to
now let's just execute another query I'm
just gonna click go info and this will
just give me the version of the golang
and the job Etc so you can just really
execute all the queries directly from
this option in your classic UI but as
you can see the new user interface that
option is unavailable now let's move on
and check out the other options that are
available in this panel let's click on
alerts currently we have no alerts as
I've already mentioned before that
alerts are simply conditions that you
will have to specify and if these
conditions are satisfied so the alert
will be triggered to that particular
meet you know or manager so currently as
we have none it just shows nothing here
so these options will allow me to check
my in active alerts my pending alerts my
firing alerts so usually it's an
inactive state if the condition is not
satisfied if the condition has been
satisfied then it'll go to the pending
stage and finally if it exceeds the
critical limit Or the critical amount it
will enter the firing stage so you can
view the alerts based on these three
states that is the inactive State
pending State and firing State moving on
we have graph so this is the same as the
home page of Prometheus right so we've
talked about that so once I click the
status option you can see that there are
seven different options that drop down
the first one is runtime and build
information this just really gives me
you know the information about
Prometheus that is the start time of
Prometheus the working directory the
configuration reload extra information
about my runtime and build is given in
this option now the alert managers there
are none so it just says endpoint now
you can also see the Go version here you
can see the build user build date all of
that the version of the build
information also is given here so this
is what the runtime and build
information is all about the net next
option in the status is tsdb status what
is tsdb tsdb is simply the time series
database so this option allows me to
check out my databases status right the
number of series the number of chunks so
I can just view the statistics of the
head cardinality so I can just view the
top 10 label names with value count I
can view the series count by Metric
names and I can also check out the label
names with high memory usage in fact I
can also you know take the queries from
this top 10 series count by Metric names
I can just execute it as a query and you
can see that this gives me the data of
all the and this just executes the
information and gives me the data the
list of data that allows me to check out
my HTTP request duration in seconds so
also I can view this in graph so I'm
just going to click on that
and this just gives me information of
all of the data that is available in the
table in terms of graph so I hope this
is clear now moving on we'll check out
command line Flags this is just the
command line flags that I can use to
really make changes in my service file
we created a service file right to run
this particular service of Prometheus so
so this just really gives all the
information about my service file so you
can see that there's config file
currently in this location there is also
query timeout and you can view all of
that all of the extra information here
you can check out the web listen address
it's in port number 9090 you can also
check out the web page title currently
the web page title is Prometheus time
series collection and processing server
you can change this if you'd like to you
could also check out the web read
timeout so all the information basically
about this service that we've created is
given in the command line Flags so
moving on we will check out a
configuration file this configuration
file we've already seen before right
this is our prometheus.byml file right
so this is simply the same file that is
given here you can see global alerting
scrap configs you can check out the
targets and then finally the port number
okay so if you'd like to copy this you
could just copy to clipboard moving on
to the next option we have rules there's
none right now so it shows nothing later
on in the session we'll be creating our
tools so stay tuned for that now the
next option is targets there's only one
primitive service that is up and running
and that is of port number 9090 so the
state is up you can see the instance you
can see the job and the last scrape the
scrape duration and if there are any
errors that will appear here before we
move on to the next option so I'm just
going to show you the metrics of this
instance the Prometheus instance that
we're running I'm just gonna click on
metrics now you can see that Prometheus
defines a very nice text-based format
for its metrics I'll just zoom this a
bit so you can view it better so now you
can see that the data is relatively you
know human readable and we even have the
help and type decorators to really
increase the readability right as I've
already discussed before Prometheus has
only four types of metrics right to keep
everything really simple that is counter
gauge histogram summary now the type in
help attributes here help us provide
user-friendly hints for each metric in
the output so that's what this help and
type decorators really do they just
increase readability you can't really
view all of these metrics and understand
what they are so they're just helping
you with the readability you can see
that the help here says go GC duration
seconds a summary of the pause duration
of garbage collection cycles and a
metric type of summary here you can see
the type is gauge and you can see all of
that information here the type is
counter also here the type is counter
gauge again so that's what really the
type will do so the type will just give
you an Insight of what type of metric it
is so help will just really describe the
metric and type will just give you the
type of metric so I hope this is clear
so I hope Matrix was easy to understand
now that we finished targets let's move
on to service Discovery and this just
shows you so currently there's just one
end point that has been scraped and that
is our current running active instance
that is Prometheus now it just shows the
address the metrics path and the scheme
the job and just show some labels of the
target so that's all service Discovery
is all about and if you'd like to check
out and know more about Prometheus just
click the help option in the panel and
this will redirect you to the Prometheus
official website now the classic UI we
already discussed and this is all for
the graphical user interface of
Prometheus I hope it was easy to
understand I hope it was easy to learn
now let's move on to the next part of
the session okay so now in this part of
this session I'll be downloading the
Prometheus node exporter so for all of
you all who don't know what the node
exporter is it really is a way to
measure various machine resources now
the machine resource can be your memory
disk or even your CPU utilization so
it's a very famous application of
Prometheus so without Much Ado let's go
ahead and download our node exporter I
will go to my web browser and I'm going
to the official website of Prometheus
so the first link will redirect me to
the official website of Prometheus I'm
going to click on the download button
and I'm going to scroll down and look
for node exporter
so I'm just going to copy the link
location I'm quickly going to move back
to my terminal and I'm going to download
node exporter
so now that the node exporter has been
downloaded now let's just check if the
file is available and it is so I'm just
going to untar this start file with the
command tar xzf and I'm just going to
copy this
and this command will just untie my file
okay so I'm just going to clear the
screen now let's just move ahead and
check out what are the files that are
there in our node exporter file so we
have license node exporter and notice
now what I'm going to do is I'm going to
use the sudo CP command so I'm going to
copy the node exporter file to my user
local bin location so with the help of
the copy command that is the CP command
I'm just going to create a copy of the
node exporter file into my user local
bin location okay
once that is done I'm just going to
check out the files that are present in
my user local bin location you can see
that there is a node exporter file that
has been copied right so I've just
copied this file into this location
that's all I've done moving on we'll now
create a service file so we created a
service file for a Prometheus
application right so even for the node
exporter I'm going to create a service
file using the command sudo VI
so I'm just going to name this as node
exporter.service
so I have written down a piece of script
the same way we did to a Prometheus
service file here you can see the
description which says Prometheus node
exporter service you also have the type
you have execution start which is
currently present in the user local bin
location now once that is done I'm just
going to save this and I'm going to
reload my system
now I'm just going to start the service
you'll have to make sure of one thing
and that is the node exporter so what
you're naming the service file you'll
have to initiate the same name so as you
can see here the service file is named
as node exporter so I'm just going to
type the same thing here and I will
start the service once I start the
service I'm just going to check its
status and you can see that it's active
and running so that's great
so the node exporter is currently
running on address
900 ports so we're just going to check
it out
and it opens so you can check out the
metrics that are currently running in
our node exporters so this is the end
point as to where our Prometheus will
scrape all of its data so now that we
have node exporter up and running let us
configure the node exporter into our
prometheus.yml file so every time you'll
have to configure a particular Target
you will have to go to the
prometheus.yml file and add the target
there okay so what I'm going to do is
I'm going to move to my
directory
okay once that is done I'm just going to
list out all the files so you have the
Prometheus file here
you can see the Prometheus dot yml file
right I'm just going to clear my screen
so now I'm just going to view all the
files that are present in Prometheus as
you can see that there's the yml file
here so we're going to use the command
sudo VI
Prometheus dot yml to edit our
configuration file
so now you can see there is only
currently one job and that is Prometheus
we have to add another job to our file
so here I'm adding node exporter as my
job name and the target is with IP
address
9100 right so make sure the indentation
is correct so once the indentation is
correct we have applied a Target we have
added a Target to our configuration file
and we can just save this
once that is done I'm just going to
clear my screen
so now I'm going to restart my
Prometheus and check the status it's
active and it's running so we can just
go back to our page
so you can see here the node exporter
has been added to our service Discovery
with address as localhost
9100 it's localhost because we are
currently working on our machine if you
want to add another machine's port
address you can do that too in
Prometheus by mentioning their address
and now you can just go back and you can
check out the configuration file has its
changes so it has another Target that's
named node exporter and also its port
number is 9100 so if I move back to my
classic UI 2 I can check out the metrics
that are here if you notice there are a
lot more metrics than before right you
also have the node metrics if I want to
check out the node CPU seconds in total
I can execute it right
so now that I've executed it you can
check out the cpu's total number of
seconds it is hosting the node exporter
for you can also check out the username
info of the node you can see that the
instance name is given here the machine
that we're currently working on the Node
name is edureka the system name is Linux
the version is given here and also so
the entire node's username info is given
if we execute this query so this is
great right like now your node export
has been successfully configured into
Prometheus let's move on to the next
part of the session before moving on to
the next part of the session and
configuring alert manager let's
understand querying a little bit more in
detail so Prometheus as we all know
fundamentally stores all of its data
time series right now each time series
is identified by a metric name we've
also discussed this before and as well
as a set of key value pairs that
Prometheus calls labels okay now the
metric name indicates the overall aspect
of a system that has been measured let
me just execute this query Prometheus
HTTP request duration seconds total okay
now you can see that labels serve to
differentiate subdimensions of a metric
such as you know the Handler the code
the instance the job that it's currently
running on finally a sequence of samples
forms the actual data for a series okay
each sample consists of a time stamp and
a value where timestamps have
millisecond precision and your values
are always 64-bit floating Point values
the best part of Prometheus queries is
that you can add regular Expressions you
can specify a time limit a certain time
limit you can also use different
functions we'll see all of this so now I
could add a certain filter to my query
now if I want to just execute the
queries with label code 400 I'm just
going to add that filter here and
execute the query and this will
automatically display the HTTP requests
in total with code 400 and you can
really you know add whatever you'd like
to in your filters so that is one way to
work with queries and also as I've
already mentioned you can also add
regular Expressions right so you can use
not equal to is equal to and many other
things
so for example if you want to list all
the series no matter which metric name
or job with the Handler name starts with
API okay so you can just simply run this
query
what we have done in this regular
expression is so this is obviously a
regular expression right so in this
regular expression it needs to end with
a full stop and a star okay since
regular Expressions always match a full
string in Prometheus now the resulting
time series is usually a mix of series
with different metric names now you know
that how to select your time series by
the metric names as well as a
combination of their label names
so in the next part we will learn how to
calculate rates or Deltas of a metric
over time now one of the most prominent
frequent functions you will use in
Prometheus is the rate function now
instead of calculating event rates
directly in the instrumented service in
Prometheus it is a very common thing or
a common way to track events using raw
counters okay and to let Prometheus
server calculate rates ad hoc during
query time this has a number of
advantages such as not losing rate
spikes between scrapes as well as being
able to choose Dynamic averaging windows
at query time counters start at zero
when a monitored service starts and get
continuously incremented over the
service processes lifetime occasionally
when a monitored process restarts its
counters reset to zero and begin
climbing again from there graphicing raw
counters is usually not very useful as
you will only see an over increasing
line with occasional resets now we will
just use another query for this example
now I'll just explain you how the rate
function really works and for this I am
just going to use the query
Prometheus
HTTP request duration seconds count okay
and I'm just going to specify the job as
Prometheus
and you can also view the graphs
it looks something like this
okay now to make counters useful we can
use the rate function to calculate their
per second rate of increase this is
really why we use rate function we need
to tell rate function over which time
window to average the rate by providing
a range selector after the series
matcher okay now for example to compute
the per second increase of the above
counter metric as average over the last
five minutes we can just add the rate
function
and just specify though time limit is 5
minutes
just going to close the braces and
execute the function okay
you can check out the graph now you can
see now the result is much more useful
right rate function is very smart and
automatically adjusts for counter resets
by assuming that any decrease in a
counter value is a reset the variant of
rate function is I rate function while
rate function averages the rate over all
samples in the given time window that is
five minutes in this case I rate
function only ever looks back two
samples into the past okay it still
requires you to specify a Time window
like five minutes to know how far to
maximally look back in time for those
two samples now I rate function will
really react faster to rate changes and
is thus usually recommended for use in
graphs in contrast the rate function
will provide smoother rates and is
recommended for use in alerting
Expressions okay now let's execute The
Irate function for this
you can see that the graph would look
like this okay uncovering short
intermittent tips in the request rates
rate and eye rate functions always
calculate a per second rate so here
sometimes you'll want to know the total
amount by which a counter increased over
a window of time but still correct for
counter resets you can achieve this with
the increase function okay for example
to calculate the total number of
requests handled in the last R you can
use the increase function
and I will change this to wanna
now that we know how queries work how
you can use regular expressions and how
you can really use functions in your
queries let's move on to the next part
of the session that is custom roles okay
so time series queries can quickly
become quite complicated to remember and
type using the expression browser all
the time right so what you can do is
sometimes in some situations you'd want
to use some queries very frequently
right but you don't want to really type
the same query over and over again so in
this case rather than remembering and
typing this particular query every time
we can create a recording rule that will
run at a chosen interval and make the
data available as a Time series so how
we're going to do it I'm just going to
so this is a pretty popular query that
I'm going to execute that is
node memory
rebuytes okay
I'm just going to execute this query
this will just give me how much of free
bytes is available in the node exporter
you can see the free number of bytes
here but what if I want to execute this
in terms of you know percentage
so here you can see the number of memory
that the job node exporter is using and
it's just four so I'm just multiplying
by 100 to check in percentage form so
you can see that it is only using four
bytes of memory now if I want to see how
much of memory is remaining I'm just
going to minus the entire query with 100
so you can see that 95 percent of bytes
is left over now what if I want to
execute this query very often right so
it'd be very time consuming for me to
type the whole thing out and then
execute it right so but there's an
easier way to make this a query right so
it'll just drop down in your uh drop
down list and then you can just search
for it and execute it right
so how do we do that so what we're going
to do is we're quickly going to shift
back to our terminal I'm just going to
clear my screen okay and I'm just going
to
I'm just going to move into the user
local bin a Prometheus directory and now
here we have to create a new file called
Prometheus rule so that's what I'm gonna
do
and since it's a yml file I'm just going
to add the yml extension
here I'm just going to add the name as
custom rules
and I'm just going to specify my rule
here
so I just type the whole thing out I
have named this custom rules and I've
recorded this query as node memory
memory free in person and the expression
we've already executed is the same one
I'm just going to go ahead and save this
I'm just going to clear the screen
so Prometheus usually comes with a tool
called prompt tool and this prompt tool
can be used to really check your rules
files and you can do really a lot of
things with chrome tool for now we're
just going to use this command which I
typed out and we're going to execute
this command to check if our file is
running okay and you can see that it
checks the Prometheus rules file and it
says its success now let's go ahead and
add the Prometheus rules.yml reference
to the Prometheus yml rule file section
I'm just going to clear the screen
now I'm just going to enter the
prometheus.yml file
so here I'm just going to enter the
Prometheus underscore rules.yml file
so now I'm just going to save this file
now once that is done I'm just going to
restart the system
and I'm just going to check the status
too
okay great is up and running so we're
just going to switch back to Prometheus
and I remember naming it free bytes I'm
just going to switch the classic new
eyes so I can look for it and it was
named node
memory
memory free in person right this is
exactly what we named our query and I'm
just going to execute this
and you can see that
it's 95 so this is how easy it is to
make your own queries and execute them
okay so now that we've created our you
know custom rule so let's just go check
out our custom Rule and you can see that
it's here and the state says okay so
that's great now let's go ahead and
create our first alert rule okay the
same way that we created our custom rule
we're going to be creating our alert
rules this is exactly why I explained
you custom rules in the beginning so I'm
just going to switch back to my terminal
I'm going to clear the file I am going
to so I'm just going to go to the
Prometheus underscore rules.byml file
and once that is done so I'm just going
to type in my alert in this Prometheus
underscore rules.yml file and once I'm
done typing so I'll just explain you the
alert after that
so I've created a rule named alert rules
and this alert will allow you to
recognize when your instance is done so
we have currently the Prometheus
instance that is running and we also
have the node exporter that's running so
if any of these instances go down for up
to one minute and then the alert will be
up okay so the severity is pretty
critical and we have given annotations
too where we have mentioned the instance
so this just gives a summary of what
instance is going down and a description
to of the job type and also we have
mentioned the job type and that it has
been down for more than one minute okay
so once this is done which is going to
save this
I'm going to clear my screen so the same
way that we executed The Prompt tool
command we're just going to execute that
so we're just going to execute this
command
and it says two rules fun that's great
now I'm just going to restart my
Prometheus and I'm going to check the
status
okay it's up and running let's switch
back to our Prometheus
and reload it
now you can see that there is now an
alert rule okay which has the alert
instance down and the expression is is
up is equal to zero so this expression
that says up is equal to 0 is pretty
interesting because now if I go to my
Prometheus and just execute up it'll
just tell me what are the jobs that are
currently up and running now the value
is currently zero so if the expression
turns 0 so then the alert will be
activated so here this is what the
expression really means so here if the
up is equal to 0 for more than a minute
then the alert will be immediately
activated now as I've already mentioned
before the three states that is the
inactive stage pending stage and then
the firing stage if none of the alerts
are up then it's just going to be in
inactive State and if for instance one
of the jobs go down okay then it will
immediately move to the pendant state so
once it immediately moves to the pending
stage then it will wait for a minute
because we have specified the condition
that it should be in the pending state
for a minute and if it goes beyond a
minute then it should go to the firing
stage okay so once it reaches the firing
stage all of the managers that are
involved in getting the notification
about this alert then they will be
informed about it immediately so I
really hope you understood how to create
your own rules how to configure them
into your prometheist rules.yml file and
then just really you know understand how
they really work now let's move on and
install and configure our alert manager
we'll have to just simply go to the
Prometheus official website click on
download and then you'll get an option
called alert manager I'm just going to
click on that and then I'm just going to
copy the link location of Linux because
I've been clearly using Linux machine so
I'm just going to copy that link
location clear my screen
and
so I'm just going to download alert
manager
so once I copy the link location what
I'm going to do first is I'm just going
to move back directories
so now I'm just back to my root I'm just
going to move into my Prometheus
directory so I'm just going to download
my alert manager here
okay as you can see that the download is
complete I'm just going to list out the
files and you can see that there's an
alert manager the tar alert manager file
I'm just going to untar this using the
command
and you can see that using the command
pseudo tar hyphen xvf and then I'm just
going to paste the link and you can see
that the files have been untied I'm
going to clear my screen
and I'm just going to list out the files
yeah so the alert manager file has been
untaught I'm just going to enter into
this directory and check out the files
that are currently in this
you can see that there's an alert
manager executable file you have alert
manager.yml the configuration file you
have an amp tool so now what we're going
to do is we're going to be running alert
manager as a service the same way we ran
Prometheus and node export as a service
we're going to also run alert manager as
a service so what I'm going to do first
is I'm just going to copy my files into
the user local bin
so now I'm just going to create the
alert manager service file if you
remember we also created a Prometheus
service file and also node exporter
service file so we're just going to do
the same we're going to create an alert
manager service file
so once you create your alert manager
service file this is what you'll have to
type out I've typed out the script so
you have a description you also have the
type of the service and you're just
going to mention the location of the
configuration file and where the
execution will begin from okay once this
is done I'm just going to save this file
gonna clear the screen
so I'm just going to shift locations to
the user local bin alert manager
directory so I'm just going to list out
the files and you can see there's the
alert manager.yml file so here is where
we'll have to make a few changes that is
the alert manager configuration file so
I'm just going to make some changes
there
so this is the default file I'm just
going to make some changes here and I'm
just going to explain what the changes
are about as you can see that you have
receivers where you know you have to
configure your email ID that is your
email ID from where you're sending the
message and the receiver who's going to
receive the message right so you're just
going to add a two email ad you're going
to add a from email ID and you're also
going to add a username you're going to
add identity that is the email ID and
then you're also going to add the Gmail
password now that all of my email IDs
are in Gmail so I'm just going to
mention the public Gmail host server so
I'm just going to add in all my
credentials and cut to the next part of
the session
and as we have already seen before
there's a tool that is called amp tool
in the alert manager so the amp tool is
just an executable that allows you to
view or to modify the current state of
the alert manager so I'm going to
execute this command user local bin
alert manager amp tool and check the
configuration file and then just give
the location of my service file okay and
once that is done it's going to check so
I'll just check if it's successfully
configured once that is done you can
just see that there's one receiver on
this end okay I'm just going to clear my
screen
and I'm going to start my alert manager
so once I've made changes to my service
file so now I'm just going to execute my
command user local bin alert manager amp
tool now for all of you all who don't
know what amptool is amptool is an
executable that allows you to view or to
modify the current state of the alert
manager okay so once this is done this
is successful so you can just check the
config file with the help of amp tool
and once that is successful you can just
see that there's one receiver so what
I'm going to do is I'm just going to
clear my screen
and I'm going to start my alert manager
service
now that the alert manager is up and
running you can see that it's currently
running on port number 9093 so I'm just
going to check out the port where the
alert manager is up and running as you
can see that there is four different
options that are given alerts silences
status help and you can just filter your
alerts group your alerts and you can
also see the status of your alerts and
you can also see the active pending and
expired alerts you can get help from
this page too so you can just see your
alert manager in general and now let's
so now with the alert manager active
let's move on to the demo part
okay so now that we have set up our
alert manager so we'll quickly shift
back to our terminal and I'll clear my
screen
so now we have to configure alert
manager into Prometheus I'm just going
to move out from this directory and I'm
just going to enter the Prometheus
directory
I'm just going to list out the files
that are present here so I'm just going
to edit my prometheus.yml file
and here as you can see I'm going to
specify my alert manager
so I'm just going to specify my alert
manager to
port number 9093 and once I'm done with
that I'm just going to save this file
and now I'm going to restart Prometheus
okay it's up and running so I'm just
going to check if my endpoint has been
successfully scraped in my runtime and
build information and if I scroll down
you can see that so the alert manager
has an end point right and this is it so
that's great we have Prometheus repeat
we have alert manager up and running now
let's just go check the status of all
our targets
we have node exporter which is up we
have Prometheus which is up okay now
let's go to our alert stage we have one
alert rule that we've currently written
and it's inactive now let's get into the
demo part and let's so let's just stop
one of the instances I'll think I'm
going to stop my node exporter instance
and once that starts so let's just see
how the alerts go by when one of the
instance is shut down okay
so I'm just opening a new terminal and
what I'm going to do here is I'm just
going to
okay now what I did is I just entered my
Prometheus directory I just listed down
the files and we have the node exporter
file I just changed the directory to the
node exporter and listed down the files
and currently the node exporter is up
and running obviously but now what I'm
going to do here is I'm just going to
stop this service okay before that I'll
just clear my screen
and I'm just going to stop the service
and I'm just going to check the status
so it's inactive and once that is done
I'm just going to quickly shift to my
status
of the target so you can see that one of
the target is down
and I'll just execute the up query and
you can see that the node exporter has
the value 0 that is it's down okay I'm
just going to check the alerts you can
see that the alerts is in the pending
state right there's zero in an active
and there's just one pending so it's
just going to be in the pending state
for a minute if you remember we had
specified that for a minute it's going
to wait if the instance is going to be
down so once it crosses the one minute
threshold it's going to go to the firing
stage here it's going to alert the
manager that is me currently I will
receive the mail saying that one of the
instance is down and it's a critical
alert that you will have to address
immediately okay so that's how it really
works now let's reload the page and you
can see that after a minute it's entered
the firing State okay and there are none
in pending state so it was in pending
state for a minute and then it moved to
the firing state
so in the previous session we learned
about how to create our own alerts in
Prometheus and how to be notified about
these alerts so that we can take
discrete actions whenever necessary so
in the next part of the session I'm just
going to integrate Prometheus with one
of the most famous visualization Tools
in devops that is grafana so before I
explain you about grafana I'm just going
to briefly explain you about node
exporter and how we're going to be
displaying its metrics in grafana so
node exporter is simply an exporter that
collects technical information from
Linux nodes right such as CPU disk
memory statistics I'm sure you must be
aware of this by now so the metrics that
the node exporter exposes of the Linux
machine that we're currently working on
that is Ubuntu all of these metrics are
going to be visually represented in
grafana we know that Prometheus doesn't
have like a great UI right all of the
information is very textual and it's not
very like visually appealing link so
with the help of grafana I just really
want to you know expose all of these
metrics in a graphical representation
like gorgeous or you know bar graphs
anything it could be anything so before
we download grafana onto our Linux
machine I am quickly going to be making
some changes in the yml file before that
I'm just going to check if my Prometheus
is up and running
it is and I'm also going to check if my
node exporters are up and running it is
so that's great I'm just going to clear
my screen
okay so now that is done so I'm just
going to list down the files that are
currently in the Prometheus directory
and I'm just going to shift to my
Prometheus file so I'm just going to
move into this directory I'm going to
list the files okay great so so we're
just going to make some changes in the
prometheus.yml file I'm just going to
okay now you can see that you have our
alert manager configuration our rule
files script configs a job name but here
in the static config so I'm just going
to add the node exporter as a Target too
I'm just going to add the IP address of
currently it's running on localhost
9100
okay great just going to save this
and I'm going to restart Prometheus
kids up and running
I'm just going to clear the screen okay
great
just going to go down so now I'm just
going to go back to my Prometheus
directory and
now let's go ahead and download grafana
I will click on my web browser and I
just go to the official website of
grafana
so this is the official website of
grafana I'm just going to click on the
download button
so on this page you can just click
download and then if you scroll down you
can see Linux so these are some of the
commands that you can use to download
grafana on your Linux machine so I've
already made a list of all of these so
I'm just going to use them to download
grafana
and
okay now that grafana has been installed
I'm just going to clear my screen
and I'm just going to check if it's up
and running
so I'm just going to use this command to
start the grafana server and then I will
check
the status of it
and it's up and running currently it's
running on Port 3000 I'm just going to
quickly shift to my web browser and
check
as you can see that grafana is up and
running that's great if you've newly
installed grafana the email address and
the password by default is admin
okay and you can obviously create your
own password so once you're prompted
with your credentials admin then you can
just create your own password and submit
it but I'm just going to skip this for
now and this is how the grafana user
interface really looks like okay you can
see that there's a search button there's
some icons on the left hand side and
there are also panels that you can play
around with so you can create a
dashboard that will allow you to create
beautiful visualizations for your data
source right so once that is done I'm
just going to add my first data source
just going to click this option in the
basic panel
and here as we are going to be working
with Prometheus we want our data source
to be Prometheus I'm just going to
select this and it says data source
added so you can name a data source
anything you'd like but I'm just going
to let it be Prometheus for now and our
URL of Prometheus is
Google host
90 90
right once that is done I'm just going
to check if everything looks fine yes it
does so for adding a data source you all
you have to do is name it and then add a
URL that's how easy it is I'm just going
to save and test this
and this is great it's working right so
once I've saved and tested it so I'm
just going to click the grafana icon
that's present on the left corner left
top corner and this will redirect me to
my home page of grafana okay now that we
are done with this I'm just going to
click the plus icon and click on the
option import so what we're going to do
is we are going to import the dashboard
from grafana.com so if I search for
grafana node exporter dashboard and
click on the first link that's available
it will redirect me to a page which has
the name node exporter full right so
what it really does is it has an ID the
node exporter has an ID so I'm just
going to copy this ID to the clipboard
and I'm quickly going to import this ID
here okay and I'm going to load it
because we're using the node exported
dashboard in grafana I'm just going to
copy that ID and I'm I'm going to go
down if you want to change the name you
can do that and I'm going to select
Prometheus as my data source and I'm
going to import this
once that is done you can see a
beautiful representation of your CPU
your system load your RAM used and the
root FS the swap used all of the
information is beautifully been
represented in gauges right these are
gauges okay and these are the graphs
currently you can see that it's just
been used here you can see busy system
busy user and all of how long it's been
idle for all of the information about
this CPU this is basic CPU information
you have storage disk you have eight
panels here you can see that here so
there's several different panels that
have already been created by grafana and
all you have to do is import it and add
your data source that is your Prometheus
right and this is way better than
viewing it on Prometheus because
Prometheus has everything in text and in
terms of metrics so it is very boring to
view it like that I'd obviously prefer
grafana to just view all of this
foreign
why do we need continuous monitoring
there are four major reasons as to why
we need to practice continuous
monitoring the first and obvious reason
is it allows us to have better Network
visibility and transparency continuous
monitoring offers devops teams Clarity
on the state of the ID infrastructure by
automatically collecting and analyzing
data to reflect a possible outages and
important Trends it also facilitates
rapid responses a primary aspect of
continuous monitoring is implementing an
alert system that immediately notifies
the right people the minute an IT
incident emerges so this basically
enables timely response to security
threats or functional stop gaps
minimizing damage and eventually
allowing faster restoration of the
system to Optimal operational levels the
third reason is it minimizes system
downtime now consistent system
monitoring and quick necessary alerts
help maintain system uptime by raising
the alarm when there is a service outage
or any application performance issues
the fourth reason is it helps in
assisting with healthy business
performance reduction in system downtime
also obviously minimizes negative impact
on customer experience thus safeguarding
the organization against losses in
revenue or credibility also continuous
monitoring tools can be used to track
user reactions to software updates which
is useful for several teams like the
development team quality assurance team
sales marketing Etc now that we know why
we need continuous monitoring let's
understand what is continuous monitoring
continuous monitoring also sometimes
refer to as continuous control
monitoring is an automated process by
which devops Personnel or anyone can
observe and detect compilence issues and
security threats during each phase of
the devops pipeline outside develops the
process may be expanded to do the same
for any segment of the ID infrastructure
in question it basically helps teams or
organizations monitor detect study key
relevant metrics and find ways to
resolve said issues in real time as we
have already seen in the previous slide
continuous monitoring comes in the end
of the devops pipeline now once the
software is released into production
continuous monitoring will notify
developers and quality assurance teams
in the event of specific issues arising
in the production environment so what
does continuous monitoring really help
in it basically assist the it
organizations devops teams in particular
with procuring real-time data from
public and hybrid environments this is
especially helpful with implementing and
fortifying various security measures
incident response threat assessment
computers and database for n6 and root
cause analysis it also helps provide
General feedback on the overall health
of the it setup including off-site
networks and deployed software moving on
let us check out out the continuous
monitoring tools and devops now there
are several devops tools for the
monitoring ecosystem that help both
development and operation teams work
together effectively now the first one
is the monitoring tools it should come
as to no surprise that we're starting a
discussion of devops tools with the
focus on the set we know best that is
monitoring tools a good monitoring
platform lets you monitor infrastructure
and application performance whether
on-prem in the cloud or across
containerized environments so you have
complete visibility into every system
all the time now some of the tools are
sensui negios Prometheus now sensui is a
flexible and scalable Telemetry and
service Health checking solution for
monitoring servers containers Services
applications functions and connected
device Prometheus on the other hand
relies on the pull method to collect
information with the built-in database
nagios is the Legacy monitoring tool
that introduced more monitoring
practices to a generation of operators
the next one on the list we have devops
configuration management tools now
configuration management tools allow you
to automate the provisioning and
deployment of systems info's desired
configurations and remedicate
configuration drift by modeling your
infrastructure as code you can apply
software delivery practices like Version
Control automated testing and continuous
delivery to infrastructure and
applications some of the tools are
ansible Chef puppet ansible is written
in Python which is agentless and
utilizes an imperative approach Chef on
the other hand is written in Ruby which
also relies on an imperative
configuration management approach puppet
on the other hand relies on a
declarative configuration management
approach using a domain specific
language and an agent Master
architecture moving on we have devops
alerting tools this provides both
actionable and informational system
alerts and can be customized to fit the
complexities of your system now for
example your alerting system needs to be
sensitive enough to cover an outage but
not so sensitive that you're catching
frequent and unnecessary problems that
users aren't really going to see there
are some alerting tools like page Duty
servicenow slack page duty is an on-call
management platform with add-ons for
analytics event intelligence and
automated incident response servicenow
on the other hand utilizes automated
workflows for itsm as well as customer
service and business processes
slack on the other hand lets you
consolidate alerts into the same
platform you use for group chats and
collaboration next on the list we have
metric storage once you have automated
configuration management alerting and
monitoring you will have a whole lot of
data at your disposal to learn from The
Challenge is how do you securely store
and analyze it you obviously need a
storage system that lets you Aggregate
and learn from system capacity user
Behavior service levels security risks
and much more this is exactly where
metric storage comes into the picture
the different types of tools are
influxdb Splunk AWS influx DB is simply
a Time series database that's suited for
long-term data storage Splunk on the
other hand uses a search engine database
model to store and query data AWS on the
other hand supports a wide range of
storage purposes including relational
and non-relational databases a data
warehouse for analytics a Time series
database a ledger database to store
trend transactions and much more in this
session along with grafana we will also
learn how to configure influx DB to get
real-time data so stay tuned for that
moving on to the last type of continuous
monitoring Tools in devops we have
visualization tools now a visualization
tool might be considered essence of your
devops dual chain for monitoring you get
to basically combine all of your data
here sort and finally visualize it that
is display it on customizable dashboards
now what really visualization tools do
is they provide context and meaning that
allow you to track changes and
improvements over time and they also
give management a real-time view that
helps guide strategic decisions
customization options make it easy for
team members to design and share their
own dashboards grafana can be used on
top of a variety of different data
stores including graphite influx DB and
elasticsearch now that we know that
grafana is a visualization devops tool
let's move on to the next part of the
session introduction to grafana
firstly let us understand what is
grafana grafana is a multi-platform open
source analytics and interactive
visualization web application it
basically provides charts graphs and
alerts for the web when connected to
supported data sources grafana is
expandable through a plugin system it
allows you to query visualize alert on
and understand your metrics no matter
where they are stored create explore and
share dashboards with your team and
Foster a data-driven culture also
grafana has become the world's most
popular technology which is used to
compose observability dashboards with
everything from Prometheus and graphite
metrics to logs and application data to
power plants and beehives with all of
this said let's move on and check out
some of the features of grafana the
first one is to visualize from heat maps
to histograms graphs to Geo Maps grafana
has a plethora of visualization options
to help you understand your data
beautifully the second feature is alert
and grafana seamlessly Define alerts
where it makes sense while you're in the
data Define thresholds visually and get
notified via slack pager Duty and much
more you can also unify in grafana bring
your data together to get better context
this is because grafana supports dozens
of databases natively and the best part
is that you can mix them together in the
same dashboard the next important
feature is that it is open source
grafana gives you various options it's
completely open source and backed by a
Vibrant Community use grafana cloud or
easily install it on any platform the
next important feature is that grafana
can extend you can discover hundreds of
dashboards and plugins in the official
library of grafana because of the
passion and momentum of their Community
new ones are added almost every week the
last feature of grafana is collaboration
you can bring everyone together and
share data and dashboards across teams
it basically empowers users and helps
Foster a data-driven culture
with all of this said let's dive right
into the demo and install grafana into a
local machine now I will be working on
my Linux virtual machine that is Ubuntu
and if you are working on it too then
you can follow the same procedures to
download grafana onto your local machine
now to do that we will firstly go to the
official website of grafana so here you
can see how you can install grafana on
Ubuntu so there are some set of commands
that you'll need to incorporate to
download grafana onto your system and
you'll have to just follow that now I'm
going to open my terminal
so once we install grafana onto our
local machine I am going to explain you
the entire graphical user interface of
grafana and how it really looks how we
can make dashboards how we can make new
panels in your dashboards Etc all of
that we'll be learning in this part of
the session moving on now let's go ahead
and install grafana into our system
so have a set of commands already in my
notepad so I can just copy paste it and
I can make the entire process more
easier so I'm going to copy the first
command in the notepad
and this will allow me to download the
packages for grafana so now that's done
I'm going to download grafana and all of
its packages and and then I'm going to
start and enable grafana server
now the grafana server has started you
can see here and its status is active
and running so that's great so in the
next step I want to know on which Port
is my grafana instance running on so for
that I'm just going to type in the
command sudo
Alice of hyphen I hyphen uppercase P so
this command will give me a list of
instances that is currently running on
my VM and as you can see grafana
instance is running on Port 3000 so now
I'm going to open my local browser and
I'm going to
type localhost
and since the grafana server is running
on Port 3000 I'm going to type that and
this will redirect me to the login page
of grafana and as you can see the
grafana page will prompt you to enter
the username and password by default the
username and password of your Port 3000
is admin so the username is admin the
password is admin and I'm gonna log in
now if you want a new password and you
don't want it to be admin you can just
type in your new password and confirm it
and then submit it for now I'm just
going to skip that
now you can see the home page of grafana
and it's very interesting because all of
these that you can see the blue pick
boxes that you can see are all panels
you can really play around with these
panels you can view them you can edit
them you can share them you can
duplicate these panels you can remove
these panels you can also move around
these panels so this is really
interesting even the welcome to grafana
panel can be moved around can be edited
it can be shared you can remove it you
can make changes you can really play
around with all of these panels in
grafana and if you scroll down you can
see a Blog which will give you the
latest updates about grafana and its
changes that are occurring every day
also there are a list of dashboards on
the left hand side this also is a panel
and you can just view edit share them
and do whatever you want with it
currently you can see there's only one
dashboard that I've worked on and that's
the world map dashboard so this is the
home page of grafana and if you want to
to return back to the home page of
grafana at any point of time when you
are working on this application you can
simply click on the logo on the left
hand corner and this will just redirect
you to this main home page now if you
can see on the right hand side there are
two symbols the first one which says
dashboard settings and the next one
which says cycle View mode now the
dashboard settings is pretty interesting
because if you click on it because you
can access to the general configurations
of the application you can name your
main refiners whatever you'd want to you
can also add a description to the home
dashboard you can add tags you can also
place it in a general folder which is
pretty interesting you can edit it you
can Auto refresh it based on time Etc
you also have annotations variables
links Json model all of these are not
really necessary because this is a
beginner tutorial I'm not going to
discuss any of these in details but this
is what the home settings allow you to
do the dashboard settings now the cycle
View mode is pretty interesting now if
you want the left hand panel you can see
a left hand panel with certain icons
that are present if you want to not view
this you can just click on the cycle
View mode and that will disappear and
even if you want the Home tab to
disappear you can just click on the
cycle View mode and it will just um you
know hide everything and if you want to
get back the panels you can just click
the escape button on your keyboard and
everything will return back to normal so
I hope this is clear I hope this home
page was really easy to understand now
let's move on and check out these icons
that are present on the left hand side
so if you want to search any of the
dashboards that you've created you can
obviously click on the search button and
then just search it by name so you can
sort them based on alphabetical order as
I've already mentioned there's only one
dashboard currently that I've worked on
that is named world map
let's move on to the next icon the plus
icon which will give me four different
options that says create dashboard
folder import now if I want to create a
new dashboard I can click on this
dashboard option and it will redirect me
to a page where it says new dashboard
and you can add a new panel now the
panels are pretty interesting because
you can really play around with these
panels you can increase its size you can
decrease its size you can move around
with these panels by just hovering your
cursor over the top part of the panel
and then just you can really move around
and play around you can also add panels
to this by clicking on that and this
will redirect me to another page where I
can select I can give my panel title I
can add a description to my panel I can
choose what kind of visualization I want
to represent my data in if I want to
represent it in a graph I can click the
graph if I want to express it in a Gorge
form I can do that too by clicking on
The Gorge option it's really interesting
you can really pick your options out
there are various options you can also
view the plugin list options there are
several thousands of plugins that are
available on grafana which you can
download on your OPM and and you can
view these panels here so that's
interesting moving on we also have
queries you can select your database
from this option currently the default
data source is selected you can also
write your queries as you'd like to
display them and that will be expressed
in the visualization form that you have
selected we can remove these panels by
clicking on the remove button if you
want to add a new panel you can just
click on this add panel option you can
add as how many ever as you'll need so
there's no limit to this
so you can add herniable panels you'd
want to using the add panel option you
can also save this dashboard you can
name it as whatever you'd want to and
you can choose a folder so we're going
to learn about that in the next part so
you can create different folders and you
can save different dashboards to each of
these folders that's where the folder
option will allow you to do and you can
also check out the dashboard settings
we've already discussed this and you can
also again view this in a cycle View
mode and there's a Time range that you
can pick using this option which will
allow you to display data in terms of
time and finally you can zoom out and
you can also refresh the dashboard if
you've made any changes you can save it
and refresh your dashboard and the
Incorporated changes will display
immediately I hope this is clear the new
dashboard option is a pretty important
option and this is what you'll really be
working on in most of this application
as I've already mentioned previously you
can create as many folders as you'd like
to so I can name my folder anything I'd
want to and then just create one and
whenever I make a new dashboard I can
save those dashboards that I'd want to
save in that particular folder so this
will allow me to really manage my
folders also there's an option called
playlist where I can create playlists to
cycle dashboards and TVs without user
control and the last options will allow
me to view my snapshots so with this I
hope the folder option is clear so this
option will allow me to import dashboard
from the file or you know from the
official website of grafana the next
option is the explore option so here you
can really just explore the entire
database and you can just add new
queries you can run queries so this
option will allow me to write queries
for my data and the options or the
answer the results of these queries will
be displayed here so the explore option
is a little different and it's pretty
interesting moving on we have alerting
alerting is another really important
option in in grafana so what really
grafana alerting allows you to do is it
allows you to attach rules to your
dashboard panels so when you save the
dashboard grafana extracts the alert
rules into a separate alert rule storage
and schedules them for evaluation you
can view all of these alerts or you can
search for alerts in this option so in
the alert tab you can also specify alert
rule notifications along with a detailed
message about the alert rule this can be
done via the channels so you can create
rules and you can just search for them
alerting is a really important option
here moving on we have configuration now
this configuration part is extremely
important because this will allow you to
add a data source into your grafana
right so you can click on the option add
data source and this will redirect you
to a page where you can see a series of
different databases you can see time
series databases you can see logging and
document databases you can see
distributed tracing you can see SQL
Cloud Etc so you can just select any of
them and just create one you can see
that in Time series databases you have
Prometheus you have graphite you have
influx DB and in SQL you have MySQL post
GRE SQL Microsoft's SQL server in Cloud
you have Cloud watch Azure rafana cloud
and you have many other options that you
can really select so this is great so
you can just add your data source using
the data sources option in configuration
also you can control and view the
different users that are working in your
grafana by clicking on the users option
and you can also search the user by
their name email login Etc you can
create teams so you have a particular
project and you want to work with a
particular team you can just create any
number of teams that you'd like grafana
provides n number of plugins that you
can download from the internet and you
can incorporate these in your dashboards
or in your panels so these are really
interesting you can search them by name
or type and you can also download more
plugins on grafana.com by clicking on
this option moving on we have
preferences you can update your
organization profile you can also switch
on to different themes here if I click
on the live theme and then save it it
will change my grafana into a light
theme and because I prefer the dark mode
I'm just going to switch back to that
now the last operation in the
configuration is the API Keys you can
add a new API key that's all this option
really allows you to do finally you have
the options server admin here you can
view the users you can view your
organizations so this option really just
allows you to check out your system
settings that are configured in grafana
also you can just check out the
statistics of the users admins viewers
the active sessions the total number of
playlists your total number of snapshots
how many dashboard tags you have how
many alerts you've got and how many
users have been active since a
particular amount of time that is here
30 days all of the statistical
activities that are occurring in grafana
can be viewed in this option and finally
you can also upgrade your grafana to
grafana Enterprise so this is what the
entire graphical user interface is about
there are other two options that I'd
like to talk about is this icon that you
can see on the left corner this will
allow me to sign out this will allow me
to change my password and also change my
preferences now if you'd like to edit
your profile make some you know really
nominal changes or you'd like to change
some preferences you can click on this
option and just do that you can also
change your password here if you'd like
to and if you click on this question
mark icon you can see the version of
grafana currently we're working on
version
7.3.7 and these are all the community
support keyboard shortcuts all of this
is given in The Help option so I'm just
going to click on my grafana logo and
move back to my grafana homepage with my
home dashboard
so I hope this part of the demo really
gave you an overview of how grafana
looks like how grafana works or how you
can get started really with grafana and
also I hope it helped you install
grafana into your local machine because
that is really important once you
install it and once you configure it you
can just get started with it now before
we get into the demonstration let us
just go check out what we're gonna
create
so firstly we'll be creating a world map
which will show us the confirmed cases
of covid-19 over the past one year so
all the panels that we've really created
here is the data of the past one year
itself as you can see on the left bottom
corner of the world map panel there is a
white tiny box which is called as a
legend and as you can see cases with
less than 100 cases is represented in
green color and 100 to 10 000 cases is
represented in orange color and cases
above 10 000 is represented in red color
so so only the confirmed cases has been
represented on this world map now if I
scroll down you can see a bar graph
which represents the confirmed cases
death cases and the recovered cases of
India so the green one represents
confirmed cases the Blue Line represents
the recovered cases and the yellow one
represents the death cases this is again
representing the cases over the past one
year in India
so if I scroll down I've created three
more panels of the confirmed cases in
India death cases in Indian recovery
cases in India and this is the average
number over the past one year this is
not the total or the sum of all the
cases and have represented these panels
in the form of gauges now moving on I
have also made four different panels
representing each of the state cases
okay so here we have Maharashtra
Karnataka Delhi and Kolkata and this is
the graph of their recovered cases death
cases and confirmed cases so this is a
panel which represents the state
Maharashtra and the recovered confirmed
and death cases are represented in the
form of a graph so this is what we're
going to create I hope you guys like it
without Much Ado let's get started
so before we begin we have to install a
data source into our local machine right
so for this demonstration I'm going to
pick influx DB influx DB is basically a
Open Source Time series database which
is optimized for fast High availability
storage and retrieval of Time series
datas in terms of fields you can use
influx DB for operations monitoring
application metrics Internet of Things
sensor data and also real-time analytics
that's what we are going to do right so
to install influx DB we can go to their
official website and check out the
commands to install influx DB on Ubuntu
so these are some of the commands that
is given in the official website I have
just made a list of it in my notepad so
that will be easy for me to install it
you can also follow the same procedure
else you can just go to that official
website and get the commands to install
in your operating system
so I'm going to switch back to my
terminal
so these are some of the commands that
will allow me to install in fluxdb into
my one two so I'm just going to copy
this and I'm going to paste this
and once that is done you can copy the
remaining commands that will allow me to
install influx DB and start it on my
server
now the last command I'm going to copy
paste the last command
and this will start my influx DB now to
check on which Port my influx DB is
currently running on I am just going to
type out the list of current running
instances
my bad it's sudo LS of hyphen I hyphen p
and this will give me the current
running instances as you can see that
influx DB is running on server 8086 so
that's great we have influx DB running
we also have grafana server running on
Port 3000 now that we have both our
grafana server and influx DB up and
running we can move on and create our
database and activate it
I'm just going to clear the screen
so now I'm just going to type in flux
and I'm going to enter my shell of
influx here I want to check out the
databases that are currently present on
my influx database so it will show me
two databases that is internal database
and covered so internal database is a
default influx DB database and covid is
a database that I have created manually
in the past before now if you want to
create your own database like the one
which I just created covet you can just
type create database covid-19 and click
enter this will automatically create the
database covert now if I click on show
databases it will show me that there is
a new database covid-19 now I can use
the database covid-19 with the command
use covid-19 and if I click on show
measurements it has none right now so
it's not going to show any measurements
now we'll switch to grafana and we will
activate
our covid-19 database into influx DB so
now I will switch to my home page of
grafana and click on add your first data
source and this will redirect me to my
database page which will give me a list
of all the databases that's available
since we'll be working on influx DB I'm
just gonna select this and this will
redirect me to a page where I can name
my influx DB database as whatever I'd
like to name it and it will also prompt
me to enter a URL so as we already know
the influx DB is currently active on
port number 8086 I'm gonna write that
and if I scroll down it will also show
influx DB details and here I can select
my database so I'm just going to type
covid-19 okay so that's clear there's
name influx DB1 we will also add in the
URL with port number 8086 and we will
also given the details of the database
that we're currently working on that is
covid-19 and we will just click on Save
and test data so it says data source
updated data source is working this is
great now I'm simply going to go ahead
and create my first dashboard I'm going
to create my first panel and the first
panel that we're creating is the world
map data so by default you won't have
the world map visualization you will
have to go to the grafana plugins
website and you will have to download
the world map panel by yourself so you
will have to go to grafana plugins and
click on panel option here and you can
see that the first panel that will turn
up is the world map panel I'm going to
click on the world map panel and going
to click on the installation and you can
see that there is a command which will
allow me to install the world map panel
so I'm just going to copy this
I'm gonna quit from this and I'm going
to type in sudo and I'm going to paste
the command that I just copied and this
will immediately download install the
grafana world map panel into my grafana
application
after the refiner world map panel has
been installed successfully I will have
to restart my grafana for this I will
click on sudo service
grafana server
restart
and this will automatically restart my
grafana server now if I switch back to
my dashboard and reload this page
and if I just click the world map you
can see that the panel has been added so
I'm just going to click on that panel
now what we're going to do is we are
going to get our data set for this I'm
just going to download my data set from
track corona.live.api
so the reason as to why I'm using this
data set is because the world map plugin
has a format that needs to be followed
so there is latitude longitude there's
the name of the country and also there
is a metric the metric is another
parameter that needs to be followed
which will allow you to give you a
certain radius for the particular
country that you are representing on the
world map so all of this format is
clearly satisfied on this track
corona.api database and all of this
format is satisfied in this country
level data on track corona.live API so
I'm just going to download this link
location and directly download it on my
terminal and I will just do this by
typing in the command sudo wget now that
I copied the location of the data set
I'm just going to download it directly
on my virtual machine with the command
sudo wget and I'm going to paste the
link location
and this will automatically download the
data set into my local machine
now you can see that the countries.csv
has been successfully downloaded and
saved and I can view this
countrys.csv.1 when I list it down I'm
going to clear my screen
so once I clear my screen
now I will go and I will start coding I
will create my first python file I will
name this world map Dot py
and I will start editing my file
first I will have to import pandas is PD
I will also require a client for my
influx DB database so I'm just going to
type from so as you can see that I've
already typed out the entire code so I
can now explain it to you and as I've
already mentioned firstly you'll need to
create a client and you will have to
incorporate two parameters for this
client that is the host and the port
number so that's what the code does in
this line then you'll have to connect
the client to the database and uh since
our database name is covid-19 I'm going
to pass that and then you'll have to
read the CSV file so whatever the name
is of your CSV file just add that here
then I'm going to drop the null values
and then print it and in this for loop I
am just going to iterate over this DF
and for every row inside this DF I will
add a value inside my influx server now
influx server is a no SQL database so
you need to send in the Json format
database so this Json body has basically
three fields three really important
Fields the first one is measurement the
second one is tags and the third one is
Fields so measurement is uh the table or
the name in which the data is stored and
for now we're just going to name the
measurement as covet map the tags are
the property which is repeated now what
I'm going to do is to explain this I'm
just going to download this CSV file now
once I download the CSV file I can see
the location there's a location that is
the country name and there's also code
for that country for that particular
country you also have the latitude
longitude data you have confirmed dead
recovered updated Etc
so the second field that is the tag
field is a property which is repeated
several times for example the country
names the country names have obviously
repeated several times because based on
the case and the confirmation or the
debt there's a country name right so
obviously here in this database the
country has been repeated several times
so if there are cases from the United
States there are many cases of United
States so that particular country has
been repeated several times tags is
basically property which has been
repeated now I'm going to keep the tag
as the country and because it's in row 0
the country name I'm just going to
mention that row 0. so the next one is
fields and this is anything except the
tags so anything which is not a tag is
basically a field so there are four
different parameters here that's name
latitude longitude and Metric I've
already mentioned this earlier in the
session so the name is again in row 0
the name of the country the latitude and
longitude is given in Row 2 and Row 3 so
0 1 2 3 right so Row 2 and 3 I've
specified that Row 2 here and then the
metric is given in row 4. so the metric
here can be either confirmed dead or
recovered so you can either represent
the dead cases in your world map or you
can also represent the recovered cases
in the world map but right now I'm just
going to represent the confirmed cases
on the world map so I'm just going to
pick that as my metric and that's on row
4. so I've just mentioned that then I
will finally pass on the Json body to my
client and then I'm just going to print
done
now I hope this piece of code is cleared
I'm just going to save it
once that is done I can just run this
file so it's Python 3 world map Dot py
okay great so it's clear
and I'm going to install pandas and
influx DB so I'm just gonna click fifth
three install
pandas in flux DB
and this will automatically install
pandas and influxdb for me because I
already have both of these installed it
says requirement already satisfied if
you don't it will download for you now
moving on I will clear the screen and
execute my world map python file
so it has 230 columns and eight rows now
I will enter my influx shell and I will
check out my databases I'm just going to
use my database go with 19 and after
that I'm just going to check out my
measurements so we just created one
measurement in the world map python file
and that is covered map so that's given
here now moving on we will
check out
the data information in our covid map
measurement as you can see there's time
there's name of the country this
latitude this longitude that's great
okay now we'll switch back to our
dashboard and create our world map panel
so I'm just going to create a dashboard
add a new panel and then here I'm going
to click on visualization as world map
the panel is right here so first what
I'm going to do is I'm going to select
my influx DB1 then I'm going to select
my measurement as covid map and I'm
going to add the fields there are three
Fields I'm going to remove this field
the next field that I'm going to add is
longitude and I'm going to add another
field that is metric then I will add
name I'm going to remove this too
and format it as a table
once that's done
I am going to check out my map data
options now in the map data options I'm
going to click the location data and
select table and in the field mapping
option the table query format should
always be in terms of coordinates and as
soon as I click that you can see that
there are circles that appear in the
world map now the location name field is
name now once I specify the location
name field as name and refresh my
dashboard so when I hover my cursor on a
certain area then it will show me the
name of the country so you can see that
the green ones have 10 plus cases so the
legend that is present in the it will
give me an idea of what each color
represents so the green currently
represents more than 10 cases and orange
will represent cases that are between 0
to 10 and red will represent cases that
are less than 10 so we'll just change
this threshold you can change the
threshold options here so I'm just going
to keep the threshold as 100 and 10 000
and I'm just going to save this so the
cases that are less than 100 should be
in green color so I'm just going to
apply that so the cases that are more
than 10 000 I'm just going to represent
that in red color so I'm going to click
on the red color I'm going to apply
and now you can see that the countries
with more than 10 000 cases are going to
be in red color but the countries
between 100 to 10 000 cases are going to
be represented in orange color and the
countries with less than 100 cases are
going to be represented in green color
so this looks pretty good right now
we'll just go to the setting and then
just change the name of this to world
map
okay I want to have a transparent
background so I'm just going to use this
option to switch to that and I think
everything looks good I think a map is
ready I'm just going to save this
and I'm just going to save it in the
general folder
once that is done I will elongate my map
so you get a clear picture of how it
looks this looks good right now let's
quickly move on to the next part of this
demo now for the next part of this demo
we're going to go to kaggle and I will
look for covid-19 data sets so in the
search keyboard I'm just going to search
for novel coronavirus 2019 data set and
I'm going to click the first data set
and if I scroll down this data set has a
serial number observation date the state
the country this is perfect for me for
the next part of the session so I'm just
going to download that
and then once I finish downloading it
you can see that you have an observation
date you have the state country the last
update confirmed debts and recovery so
here in this particular data set you can
see there's a serial number there's an
observation date you have the state you
have the country you have the last
update confirmed case the deaths and
Recovery based on that particular date
now this is great for me so I'm just
going to paste this data directly into
my file as you can see the covid-19 data
set is already here I'm just going to
place this in that directory the edureka
directory I'm just going to click clear
and now I'm just going to check my LS
you have the covid-19 data set here so
now I'm just going to create a file
that's named graph to write my python
code and I will edit it
so I'll just type in my python code and
then explain it to you in the end so it
won't take up much time
now as you can see that I've already
created a client to my influx DB and
then I add in the database covid-19 and
I'll have to pass a variable to read the
CSV file that is covered 19data.csv and
then I'll drop the null values after
that I will iterate through the Json
body which has three fields that we've
already talked about the first one is
the measurement field and I'm going to
name this as kobit metric I also have
the tags here so here I'm going to
specify the country and state we have
the country in row three so zero one two
and three and we have the state in the
Row 2 so I'm just going to specify Row 3
here and Row 2 here so I hope that is
clear in the fields we have confirmed
which is in the fifth row that's in the
sixth row and recovered in the seventh
row so fifth sixth and seventh this key
is a pretty interesting key the time key
so here as you can see the time column
is outside the fields you can add key
which is called as time here and you can
use observation date that is present in
the Row 1 column you can see that the
observation date is given in the Row 1
column but because it's in the form of
string data we will have to convert it
to the timestamp format and this is
Possible only with the help of the date
time python Library so we can just
convert this into UTC timestamp in this
line of code so what it really does is
it takes out the time component from the
string and then you can format it in
this particular format that is the month
date and yeah so this is the format that
we are going to present it in so so it's
percentage M front slash percentage D
from slash percentage y so that's month
date and Year I hope this is clear now
we'll just quit and execute this
chord
graph.py and as you can see here there
are 15
000 rows and eight columns now this is
done it will send a message okay I'm
going to clear the screen in some cases
you will have to restart your grafana
server so that the data is incorporated
into your application else you could
just check your databases
so here I'm going to check the
measurements and you can see that covid
metric is present so select start from
covid metric
I'm going to just limit it to 20 now and
this will show me the time confirmed
cases the country the debts recovered
and the state okay now that this is
clear
we can get back to a dashboard and
create a new panel I'm just going to
name this panel
covid-19 India
okay now what I'm going to do is I'm
going to select now I'm just going to
select my influx DB1 database
and I'm going to select metric I'm going
to specify the country and I'm just
going to specify India here and in field
I'm just going to add confirmed debts
and I'm going to add another field
recovered
okay and if you want to show the legend
you can do it or you can just choose not
to I'm just going to show the legend
here
and that's it you can just save it and
apply it and you can see the bar graph
and the other important thing here is
you'll have to specify the time
sometimes it'll just be for today or you
know as default it'll just be for
because this data is from the last year
you'll have to specify the data for the
last one year and the graph will
eventually display so I'm just going to
apply this I'm just going to shift my
world map above it
I'm just going to stretch this out
okay so this looks fine
now I'm just going to make gorgeous so
I'll just add a new panel and I will
select influx DB1 measurement as metric
where country is equal to India okay and
the field value here right now will just
be confirmed now I'm just going to name
this
confirmed cases in India okay and I do
want to display the panel with the
background so I'm just going to let it
be displayed with the background and now
I'm just going to select my Gorge okay
great now because this is confirmed
cases I want this to be in yellow color
I don't want it to be in any other color
so I'm just going to change the
threshold color to yellow and I'm going
to apply
okay great I'm gonna add another panel
so this time it's going to be for the
depths and I'm going to just select
gauge gonna rename my panel title to
death cases
in India and as you can already see the
time ranges for the last one year so
that's great this is already in red I
want the death cases to be represented
in Red so I'm just going to apply I'm
gonna just shift this here and resize it
okay great and the last one is the
recovered cases right so I'm just going
to
work on that too I'm going to select
measurement as covet metric where
country is equal to India
and the field value is recovered okay
this is great I'm going to change the
threshold to Green
okay great
and everything looks fine just we will
change the
okay I'm gonna apply
I'm just gonna reposition this again
so this looks okay
if you want to move all of these down
you can just pull the world map up and
then just pull the covid-19 cases up to
now you can see the confirmed cases
death cases and recovered cases this is
all the mean value now using the new
panel I'm going to represent four states
I'm just going to select measurements go
with metric where state is equal to
Maharashtra and field value is equal to
confirmed and I'm just going to add more
field values deaths
and then recovered okay wait this looks
fine and I'm just gonna name this
okay
I'm gonna represent this in graph I'm
okay with that
I want to display this without a
background so I'm just gonna shift this
down as much as I can okay this looks
okay I'm just gonna make this for three
more States
metric where state is equal to Karnataka
field value confirmed
deaths
and finally recovery
okay and this is Karnataka so I'm going
to name that
and I want to display this without a
background looks neat and clean
there's one thing you could do is you
could add Alias because you'd want the
graph to be represented properly so this
will just add earlier's I'm just going
to add that's here
and finally we will add
recovered
okay this looks great
also I'm gonna add alias in the
Maharashtra state so confirmed
deaths
and finally
recovered
okay
and we will apply this
okay looks good so we have two more
states to work on we will work on West
Bengal
confirmed I'm going to add alias
and then I'm going to add another field
which will give me debts
and this will also give that
I'm gonna add another field
recovered and this I can just try it
recovered
okay
and I'm gonna just add
West Bengal here display without a
background
and apply
finally this is our last panel that
we're going to work on
and I will add for Delhi
confirmed I'm going to add alias
confirmed
I'm going to add two more fields and I'm
sure you must be very much familiar with
how this works now
okay I'm going to display this without a
background and I'm going to name this
panel Deli
that's it guys this is how easy the
tutorial is this is how easy it is to
use grafana you have your world map you
have covid-19 analysis of India you have
the confirmed cases you also have the
confirmed death and Recovery cases which
is represented in terms of cages and
also you can represent all those State
analysis of covid-19 in terms of a graph
now if you want to change the name of
your dashboard you can simply go to
dashboard settings and then just change
it to whatever you want so I'm just
going to say covid-19 analysis India
okay and I'm just going to save
dashboard
okay great
and this is all now in the next part
we're just going to learn a little bit
about variables
so we're moving on to the last part of
this session and we are going to learn
about grafana variables if you're not
new to programming or coding you must be
very much aware that variables are just
placeholder for a value right so you can
use variables in metric queries and also
in panel titles so what variables in
grafana do is they give you the ability
to create more interactive and dynamic
dashboards let us go ahead and check out
how this happens
now I'm back to my terminal I'm just
going to type in flux now instead of
hard coding things like server
application and sensor names in your
metric queries you can use variables in
place of them
now before we move ahead I'm just going
to
use my covid-19 database and I'm going
to write a query show tag values with
key is equal to Country and this will
display me a list of all the countries
that are present in my covid-19 database
and this is what exactly variables do so
this is exactly what grafana allows you
to do if you have multiple identical
data sources or servers right you can
make one dashboard and use variables to
change what you are viewing so what this
really does is it simplifies your entire
data now I'm just going to click on the
logo and I am going to if you remember I
have already mentioned this before that
if you click on the dashboard settings
you will get the option of variables on
the left hand side so I'm just going to
click on the variables and then I'm
going to add a new variable I can name
this variable whatever I'd like to I can
just name it as covid countries
so all the rules that we had learned
syntax rules that apply to variables
also applies here in grafana
so the type is query you can also use
constants data sources systems anything
that you'd like I'm just going to type
out the query right so I'm just going to
use that and my data source is influx
DB1 and I'm just gonna you know type the
same query now if I scroll down it will
show me a preview of all the values
right so all of these values are shown
here you can just add this variable and
just display it in a dashboard that's
how simple it is so if you have any
particular values that you'd like to
display so this is what values really
does it allows you to you know simplify
your data and then visualize it and
represent it in grafana dashboards you
can do this with iot sensors you can do
it with you know if you want to analyze
your systems performance all of that can
be simplified with variables so I'm just
going to save this and I can use these
variables later on in this session I
hope grafana variables was easy to
understand with this we come to the end
of today's session I hope you had a
great time understanding about grafana I
hope this grafana tutorial give you a
brief overview of how it works how you
can really get started with grafana in
general and this is just like a simple
tutorial you can create as many projects
and demos you'd like too it's really
easy it's very interesting
foreign
what do you think is the history behind
it what do you think are the major
reasons that affected the growth of
selenium let's have a look guys so this
selenium was originally developed by
Jason Huggins in the year 2004 as an
internal tool while he was working at
thoughtworks so Huggins later joined the
programmers or the testers of
thoughtworks and try to develop selenium
RC so this tool was developed in the
year 2004 by a two developers namely
Jason Huggins and Paul Hammond so it was
open sourced in the same year guys so
selenium RC was the first one to be
found among the web driver grid and
selenium ID in 2005 Dan fabulous and
Nelson Sproul made an offer to accept
the series of patches that would
transform selenium RC into what it is
known for today so in these days we call
it a selenium web driver right so Dan
fabulish and Nelson sprawl are the major
reason for giving us the idea about how
RC works now correspondingly in the Year
2007 Huggins joined Google and also he
helped in developing the stabilization
process of RC and at the same time Simon
Stewart at thoughtworks developed a
superior browser automation tool called
the Webdriver okay so this is the tool
which we currently use and also the
current version of selenium that we're
using is three but recently in 2009 a
September or October I think they had a
meeting or a conference related to the
version release and they actually kind
of spoke about the new version that is
selenium 4. I don't think it is
releasing anytime soon but yeah they
gave us a hint of that so yeah we'll be
looking forward to work on selenium 4 as
well so the current version in the
market is selenium 3. also in the year
2009 there was a conference that was a
test automation conference where people
tried to merge two products that the
selenium RC and selenium Webdriver to
call it as a new project called selenium
2.0 so this is the second version which
was used between 2005 to 2010 I believe
don't remember the exact time span but
yeah selenium 2 was released by then and
in 2008 selenium grid was developed by
Pat light body so this was about the
history behind the growth of selenium as
you can see that the growth began in
2004 itself but to be officially
released and to be officially available
to People by the name selenium web
driver it took close to five to six
years right so this is exactly how
selenium came into the Mainframe now
moving ahead let's take a look at what
exactly is selenium like I mentioned
selenium is an open source tool which is
used for automating the tests that are
carried out on the web browsers also it
can be reframed in this way it is an
open source portable framework for
automating applications that is mainly
web-based applications and it can be
tested across different browsers namely
Chrome Firefox Safari and just not the
traditional ones if the tests can also
be carried out in different OS platforms
that is Windows Mac and Linux and the
selenium test scripts can be integrated
with the tools such as test NG junit and
so on for managing test cases and Chin
rating reports it can also be integrated
with Maven Jenkins and Docker to achieve
continuous testing so why exactly
software testing or automation testing
is required today's world of technology
is completely dominated by machines and
their behavior is controlled by the
software empowering it will the machines
behave exactly as we want them to the
answer to this is what we call software
testing or automation testing at the end
of the day it is the application success
rate which is going to control your
business growth the same thing can be
said even for web applications because
most of our businesses today are
completely dominated on the Internet or
it completely belongs to the Internet so
this is exactly what is selenium now
moving on let's take a look at why
exactly selenium is required why not any
other tool so some major uses I would
say or benefits of using selenium is it
is easy to automate testing across web
applications suppose you're working on
or testing any of the software
application that is web-based
application I say and you find a bug and
you don't know how to resolve it until
unless you've come across selenium so
selenium is one such tool which acts
like a key to our success rate so
automating testing across web
applications is like a really tedious
task which can be fulfilled by using
selenium and also it has a wide variety
of language support if you're
comfortable using C C plus plus or C
sharp these are considered as a basic
languages right okay not C sharp but
still C C plus plus Java python right so
these four languages are like the basic
languages which can be supported by any
of the software so apart from these four
standard languages selenium also proudly
supports Ruby c-sharp spark Pearl and
many more all right now talking about
implementing the test cases like I
mentioned automating the testing cases
across different web applications of
browser is not an easy task but by using
selenium you can Implement those test
cases very easily and also you can
perform or write the test scripts on
different platforms like mac Windows
Linux and so on also one major reason
why you should go for selenium is it is
easy to understand and is open source
open source as in you don't have to pay
for this version it is not a paid
version easy to understand as in it is
explained in simple terms where any
normal human being can understand now
this was about why we need selenium now
moving on let's understand what are the
major features of selenium so I think
you guys have understood what exactly
selenium is and how it supports testing
across different platforms right so
these are also considered as features
but apart from that we'll talk about
some notable information related to
selenium so selenium can be considered
as a leading cloud-based testing
platform which helps testers to record
their actions and Export them as a
reusable script and easy to use
interface and like you already know that
it supports operating systems browsers
and different programming languages
right or also apart from this it also
supports parallel test execution which
reduces time and increases the
efficiency of the test cases also
selenium requires fewer resources as
compared to other automation testing
tools you don't have to have a different
plugin for each and every test cases
that you were working on right apart
from this we also have something called
the selenium commands which are
categorized in terms of different
classes which makes it easier to
understand and Implement also selenium
Webdriver does not require the server
installation as the test scripts
interact directly with the browser
also so these were some notable features
of selenium now moving on let's
understand the components
so we have namely four components guys
we have something called selenium ID
selenium RC selenium Webdriver and
selenium grid now let's understand them
in detail so talking about selenium ID
selenium IDE or integrated development
environment is primarily a record run
tool that a test case developer uses to
develop selenium test cases
selenium IDE is a simple record and
Playback kind of tool which comes as an
add-on for Firefox only but now I think
it is also used for a chrome driver as
well it is used for Prototype testing
and the test cases written in ide can be
exported in many programming languages
like Ruby Java c-sharp Python and so on
edit and debug options along with the
record are also available in the IDE
also guys this is an excellent tool for
beginners to understand the syntax of
selenium Webdriver it is also considered
as a simplest framework in the selenium
suite and is the easiest to learn guys
it is like I mentioned a Firefox plugin
and you can easily install it on your
system this is exactly why a selenium
IDE was used now talking about selenium
RC
selenium RC or selenium remote control
was the first tool of the selenium Suite
it was known as the JavaScript executor
and RC was a tool which made selenium
famous in the market all the credits to
selenium being a masterpiece or selenium
being very famous around the globe is
because of selenium RC it was the first
tool that provided the support for
multiple programming languages also this
supported almost all major vendors of
browsers like Firefox Chrome Internet
Explorer and so on so the first version
was called selenium 1 followed by which
we have selenium 2 3 and now I think
we're expecting four as well in the
market also using RC you can perform
cross browser testing all right so this
was about selenium RC now moving ahead
let's understand what exactly is
selenium Webdriver okay so talking about
selenium webdrivers selenium Webdriver
was the first cross-platform testing
framework that could control the browser
from the OS level in contrast to IDE
selenium where driver provides a program
farming interface to create and execute
test cases so these test cases can be
written in such a way that the web
elements on the web page can be
identified and then perform actions on
those elements also selenium Webdriver
is an upgrade to RC because it is much
faster and it is faster because it makes
direct calls to the browser and it does
not need a server between the client and
the server RC on the other hand needs a
RC server to interact with the web
browser so each browser has its own
driver on which the application runs so
different kinds of web drivers are
Firefox driver which is popularly called
as geeko driver Chrome driver Internet
Explorer driver HTM unit driver Safari
driver and so on also this supports air
programming languages like Java c-sharp
PHP Perl Ruby dot net and so on also it
supports testing across different
platforms like Firefox Chrome and so on
also cross browsing was introduced in
this version also guys selenium RC and
selenium Webdriver together was combined
to make it call a selenium 2 so like I
mentioned this I just wanted to remind
you guys of it again now talking about
selenium grid so what exactly is
selenium grid selenium grid was
developed by Patrick lightbody and
initially called it posted QA which is a
part of the version 1 of selenium and
also it was used in the combination with
RC to run test cases on remote machines
in fact with grid multiple test scripts
can be executed at the same time on
different machines parallel execution
can be achieved easily with the help of
Hub node architecture and one machine
will assume the role of Hub and whereas
the others will be the nodes okay so
this is still in use guys grid is used
by most of the companies as well and it
works both with selenium Webdriver and
RC however maintaining a grid with all
required browsers and operating system
is a tedious task for this there are
multiple online platforms that provide
an online selenium grid access to run
your selenium automation script so this
is about the suite of tools or selenium
components
foreign
so here we will be installing the latest
version of java so here you must keep in
mind that the Java version must be 8 or
above
after that we will be installing the
Java from the official website that is
Oracle once after the installation we
will also configure Java by setting the
path in the system after that we will
check whether the Java is installed
successfully or not now let's do the
demo
so install Java we must go to a official
website of Oracle so here as you can see
we have Java downloads Oracle so just
click on that and you can see here Java
18 and Java 17 are available now so we
have to download the software according
to our operating system so I am using
Windows so let me go to Windows here
so I'll install here the 64 installer so
let me click on this
one of our folk has asked me a question
let's see
okay they have asked me that whether we
can install other sizes too
yes according to your system's
compatibility you can install whichever
the feasible sizes so you can install 84
installer too
I hope you understood Kitty
once it get downloads just click on the
executable file and it asks you for the
setup procedure now here it is asking
for the installation process so let's
click on next
and this is the path where the Java jdk
is installed so until and unless if you
have a good reason to change it then you
can change it or else I'll suggest you
to keep Hazard default folder so let's
click on next
as you can see here the Java is
successfully installed let's close this
window
now let us configure this Java
now let's go to the file explorer let us
find where the Java is installed so go
to see folder program files
and you can see here the Java folder so
double click on that
you can see the version
18.0.2 have been installed so that is
the latest and stable version that is
available today now double click on this
two
so you will find the win folder here so
this is the main folder so double click
on this two
you can see many files has been
installed here so let us copy the path
from this folder so copy this path now
go to settings
find for environment variables
so you can see here edit the system
environment variables okay let's click
on this
as you can see here we have the
environment variables so click on that
you have those system variables so you
have to change the path over here
so it will be edited in the whole system
now you can see the path over here so
just click on that
and add a new and paste to the copied
path over here
now just click on OK
now let's check whether Java is
installed properly or not so let's go to
command prompt
now let's type Java space hyphen version
to check which version has been
installed
yeah you can see here
18.0.2 has been installed now let's
check for Java C
so you can see here the Java has been
installed successfully we have all the
Java packages here
so now we have successfully installed
Java now let's install the second
prerequisite that is Eclipse editor
here as you can see first we have to
install or download the eclipse from the
official website after that we will get
a zip file that has been installed after
that we have to extract from the zipper
file then we will be running the
application now let's see the demo
now for Eclipse download we have to go
to our official website that is
www.eclipse.org now let's click on this
link
you can see the Eclipse IDE 2022 so I'll
download this
this is the last button download
this might take you a few minutes
so once this executable file is
downloaded just click on this
okay now you can see here the eclipse
installer has been installed and this
will give you some of the options to
click on
so if you want to work on the Java codes
or you want to become a Java developer
and work on some codes then you can
click on this or you have many other
options you can use C C plus plus or it
will be feasible for php2 so in this
session I will be using the Java
developers so I'll click on this to
install
I'll click on install
so let's wait till it gets installed
so in between installation it might also
ask you for the agreement so you can
also agree the policies
and make sure you have checked all these
buttons also so it is very important so
to create a shortcut in your desktop so
you if you want to create a shortcut in
your desktop then you can click on this
or you can uncheck this
okay as you can see the installation
completed successfully let's launch this
now it is asking you for the workspace
now if you want to keep the default then
you can keep it until and unless you
have a good reason to change it so in my
case I will keep it as it is I'll launch
it
so as you can see here the editor has
been opened so if you want to create a
new Java project you can click on a
create new Java project and you can fill
the name the project name and you can
create some packages classes
Etc and that will be shown in this
session too so don't worry stay tuned
so now we have successfully installed
both of the prerequisite that is Java
and Eclipse now the main concept is
selenium Webdriver come let's see how to
install that
the first and the main thing you have to
see is the latest table version so at
this point of time we have the stable
version has 4.3.0 so you can see the
latest version whenever you install and
the next thing you have to see is the
browser driver if you are working on
selenium you have to have a browser
drivers to execute some of the code or
the selenium scripts so that you need
some of the browser drivers and finally
we will also see the configuration of
the selenium on our Eclipse so let's see
the demo
so go to browser and type selenium org
and you will get a selenium's official
website so click on the selenium
and you can see the home page of
selenium and click on downloads you can
see here the latest table version is
4.3.0 so just click on this
so this is a jar file that will be
downloaded the next thing that we have
to download is the Java client and if
you scroll down you will get the Java
client over here that is also the
version 4.3.0 just click on that too
now after downloading these both now go
to file explorer
go to downloads
and I am creating a folder in C drive
called as selenium Webdriver
and now I will move this both jar file
as well as the Java client over here
now as you can see here the Java client
is a zipped file so let's unzip this and
extract all the files over here
as you can see we have downloaded the
selenium server jar file as well as the
Java client tool now Java client is a
zipper file so now I have extracted also
now what will I do is I'll go to C
folder
now I have created a selenium Webdriver
folder inside the C folder
now I will copy both of the files over
here
so let's copy this and I will go to see
folder
now you can see here I have created a
selenium Webdriver folder so I'll paste
it over here
now copy the jar file to
okay
now once we have installed both the
files over here now let's go and install
the browser driver 2. as I am using
Chrome here as a browser so I will
install a chrome driver
so let's write Chrome driver
so click on the first link here
to check which version are you using
just go to this menu
click on help and Google Chrome and you
can see the version over here that is
OneNote 3.0.5060
we have to install the same Chrome
driver version so that is the reason we
checked now now let's go to the official
website and let's click on this version
and the latest table version over here
is this so let's click on this version
or click on the version which you need
mine is Windows and if your operating
system is under Linux or Mac then you
can install according to it so I'll
install the win
as I said here the Chrome driver is a
ZIP file so let's extract this over here
and copy the extracted file
and go to the folder where our selenium
files are located
so let's create a folder over here
called Chrome driver
now paste this chrome files inside this
folder
okay now all set now this is how we
install the selenium Webdriver now let's
see the demo within small example
so now let us go to our eclipse and
start coding with the small example
okay now let's start a new Java project
so click on create new Java project
so let's write a project name has
selenium demo one
so here just make sure the Java SC is
minimum of 1.8
so let's finish
so we have created a Java project name
now let's create a package over here
right click on this
click on new and you can see here the
package over here right so click on that
let's write package name as edureka
you can write whichever name you wish to
now as we created the package here let's
create a class over here so I'll click
on new here
class
so I will give the class name has
selenium script
and don't forget to click this public
static word mean because it is very
important
yeah and you can see here the package
name is edureka and the class name is
selenium script and the public static
void Main
and now let's start writing the code now
we want to the Chrome driver to be
installed over here
and yeah before that we just include the
selenium files over here so just right
click over here and you can see the
build path over here right just click on
this and configure build path
and you can see the library over here
just click on that and add external jar
files because we have not added the
selenium jar files till now so let's
click on this the jar file and open
then apply and close it now we have
included the referenced libraries over
here now let's start writing the code so
first we want the selenium drivers to be
included so let's write system
dot set property you can see here
straight properties
just write set property inside the
quotes just write Webdriver dot Chrome
dot driver
and after this just give a comma and
include the Chrome driver path over here
so for that let's go to the file
explorer and go to the selenium
Webdriver folder and you can see the
Chrome driver folder here just double
click on this and you can see the files
over here just copy the path
and go to the editor again
and paste it over here
and please don't forget to write this
Chrome driver dot EXE
let's close this
now just write
web driver
space driver
which is equals to
new Chrome Drive
closest to and as you can see we have a
error over here just hover on the Chrome
driver
and you will get a quick fixes over here
just import the Chrome driver
and do the same for the web driver too
and import the web Drive
now we expect the output to be showed as
a website so I will give
www.edureka.com as a output website now
I want to get a edureka.com as a output
website so that just write driver dot
get
and write the website whichever you want
to be executed I will give the edureka
link over here
and next I have to print the title
whichever comes over the tab so I will
just print the title over here for that
system dot out dot print Ln
driver
dot get
title
just close this two
for automatically waiting of the browser
we just write the driver to quit just
driver dot quit
and let's run this and see
just run
select the folder to be saved
as you can see the edureka page has been
opened here
you can see the courses over here too
and the browser got exited automatically
and also you can see the title over here
that just got printed from the tab
so this is the final output that we just
got now I will just take out this code
and just run
as you can see here I just took out the
code from that script and the browser
will stays for long time and until and
unless you close that window the browser
does not close so that is why I took out
the code over there
and you see here the website is staying
for a long time that is a reason
and you can see the title again printed
over here
[Music]
now let us have a look at what exactly
is an arguments and how it works so
nagos is basically a tool used for
continuous monitoring of systems your
application your services and business
processes Etc in a devops culture right
now in the event of failure nagias can
alert technical staff of the problem
allowing them to begin remediation
processes before outages affect business
processes end users or customers so I
hope you're getting my point it can
alert the technical staff of the problem
and they can begin remediation processes
before outages affect their business
process or end users or customers right
with argue is you don't have to explain
why an unseen infrastructure outage
affect your organization's bottom line
right so let us focus on the diagram
that is there in front of your screen so
Naga is basically runs in a server
usually as a Daemon or a service and it
periodically runs plugins residing in
the same server what they do they
basically contact hosts on servers or on
your network or on the Internet now one
can view the status information using
the web interface and you you can also
receive email or SMS notification if
something goes wrong right so basically
nagis Damon behaves like a scheduler
that runs certain scripts at certain
moments it stores the results of those
scripts and will run other scripts if
these results change I hope you're
getting my point here right now if
you're wondering what are plugins these
are nothing but compiled executables or
scripts it can be pearls create shell
script Etc that can run from a command
line to check the status of a host or a
service now nages uses the results from
the plugins to determine the current
status of the host and services on your
network now let us see our various
features of nagios let me just take you
through all these features one by one
it's pretty scalable and secure and
manageable as well it has a good login
database system it automatically sends
alerts which we just saw it detects
network errors and server crashes it has
easy writing plugins you can write your
own plugins right based on your
requirement your business need then you
can monitor your business process and ID
infrastructure with a single pass guys
issues can be fixed automatically if you
have configured it in such a way then
definitely you can fix those issues
automatically and it also has support
for implementing redundant monitoring
hosts so I hope you have understood
these features there are many more but
these are the pretty attractive features
and why an argument is so popular is
because of these features let us now
discuss the architecture of nagivist in
detail so basically nagios has a server
agent architecture right now usually in
a network a Naga server is running on a
host which we just saw in the previous
diagram right so consider this as my
host so Naga server is running on a host
and plugins interact with local and
remote so here we have plugins so these
will interact with the local resources
or services and these will also interact
with the remote resources or services or
hosts right now these plugins will send
the information to the scheduler which
will display that in the GUI right now
let me repeat it again nagus is built on
a server agent architecture right and
usually in argue is server is is running
on a host and these plugins will
interact with the local host or services
or even the remote Hostess Services
right and these plugins will send the
information to the scheduler an argue as
process scheduler which will then
display it on the web interface and if
something goes wrong the concerned teams
will be notified Via SMS or through
email right so I think we have covered
quite a lot of theories so let me just
go ahead and open my sentivise Virtual
Machine where I've already installed
nagios so let me just open my sentos
Virtual Machine first so this is my
Centos virtual machine guys and this is
how the nagios dashboard looks like I'm
running it at Port 8000 you can run it
wherever you want I've explained that in
the installation video how you can
install it now if you notice there are a
lot of options on the left hand side you
can you know go ahead and play around
with it you'll get a better idea but let
me just focus on few important ones so
here we have a map option here right if
you click on that then you can see that
you have a local host and you have a
remote host as well right so my Nargis
process is monitoring both the localhost
and the remote host the remote host is
currently down that's why you see it
like this when I'll be running it I'll
be showing you how it basically looks
like now if I go ahead and click on host
you will see all the hosts that I'm
currently monitoring so I'm monitoring
edureka and localhost edureka is
basically my remote server and localhost
is currently on which my inaugus server
is running right so obviously it is up
and the other server is down if I click
on Services you can see that these are
the services that I'm monitoring for my
remote host I'm monitoring CPU load ping
and SSH and for my localhost I'm
monitoring current load current users
HTTP ping root partition SSH swap usage
and total processes you can add as many
services as you want all you have to do
is change the host.cfg file which I'm
going to show you later but for now let
us go back to our slides we'll continue
from there so let me just give you a
small recap of all things we have
discussed so we first saw why we need
continuous monitoring we saw various
reasons why Industries need continuous
monitoring and how it is different from
the traditional monitoring systems then
we saw what is exactly continuous
monitoring and what are the various
phases involved in implementing a
continuous monitoring strategy then we
saw what are the various continuous
monitoring tools available in the market
and we focused on nagos we saw what is
an arguments how it works what is its
architecture right now we're going to
talk about something called as nrpe now
gives remote plugin executor which is
basically used for monitoring remote
Linux or Unix machines so it'll allow
you to execute nagus plugins on those
remote machines now the main reason for
doing this is to allow an arguments to
monitor local resources you know like
CPU load memory usage Etc on remote
machines now since these public
resources are not usually exposed to
external machines an agent like NRP must
be installed on the remote Linux or Unix
machines so even I have installed that
in my Centos box that's why I was able
to monitor the remote Linux host that
I'm talking about now if you notice the
diagram here so what we have is
basically the check underscore NRP
plugin residing on the local monitoring
machine this is your local monitoring
machine which we just saw right so this
is where my your server is now the check
underscore in RP plugin resides in a
local monitoring machine where your nag
us over is right so the one which we saw
is basically my local machine or you can
say where my Naga server is right so
this check underscore and RP plugin
resides on that particular machine now
this NRP daymen which you can see in the
diagram runs on remote machines the
remote Linux or Unix machine which in my
case was edureka if you remember and
since I didn't start that machine so it
was down right so that NRP Daemon will
run on that particular machine now there
is a secure socket layer SSL connection
between monitoring host and the remote
host you can see it in the diagram as
well the SSL connection right so what it
is doing it is checking the disk space
load HTTP FTP remote services on the
other host right then these are local
resources and services so basically this
is how an rpe Works guys you have an
check underscore NRP plugin residing in
the host machine you have NRP Diamond
running on the remote machine there's an
SSL connection right yeah you have SSL
connection and this and RP plugin
basically helps us to monitor that
remote machine that's how it works let's
look at one very interesting case study
this is from bitnetics and I found it on
the inaugus website itself so if you
want to check out go ahead and check out
their website as well they have pretty
cool case studies apart from bitnatics
also there are a lot of other case
studies on their website so bit netix
provides basically Outsource it
management and Consulting to non-profit
or small to medium businesses right now
bidnetics got a project where they were
supposed to monitor an online store for
an e-commerce retailer with a billion
dollar annual revenue which is huge guys
now it was not only supposed to you know
monitor the store but it also needed to
ensure that the cart and the checkout
functionality is working fine and it was
also supposed to check for website
deformation and notify the necessary
staff if anything went wrong right seems
like an easy task but let us see what
are the problems that bitnetics faced
now bitnetics hit a roadblock upon
realizing that the client's data center
was located in New Jersey more than 500
miles away from their staff in New York
right so there was a distance of 500
miles between where their staff is
located and the data center now let us
see what are the problems they face
because of this now the two areas needed
are unique but at the same time a
comprehensive monitoring for their Dev
test and Broad environment of the same
platform right and the next challenge
was monitoring would be hampered by the
firewall restrictions between different
application sites functions Etc so I
think a lot of you know about this
firewalls is basically sometimes can be
a nightmare right apart from that most
of the notifications that were sent to
the client were ignored because mostly
those are false positive right so the
client didn't bother to even check those
notifications now what was the solution
so the first solution they thought is
adding SSH firewall rules for Network
Operation Center personal and Equipment
second is analyzing web pages to see if
there's any problem or deformation
occurrences the third and the very
important point was converting
notification to nagos alerts and the
problem that we saw false positive was
complete greatly removed with this
escalation logic where converting us
notifications to an RGS alerts and
escalations with specific time periods
for different groups right I hope you're
getting my point here now configuring
event handlers to restart Services
before notification which was basically
a fix for 90 percent of the issues and
using naga's score on multiple servers
at the NOC facility and each nagus
worker was deployed at the application
Level with direct access to the host so
whatever targets worker or agent or
remote machine we have was deployed at
the application Level and had the direct
access to the host or the master
whatever you want to call it and they
have implemented the same architecture
for production quality assurance staging
and development environments now let's
see what was the result now because of
this there was a dramatic reduction in
notifications thanks to the event
Handler's new configuration then there
was an increase in uptime from 85
percent annually to 98 annually which is
significant guys right then they saw a
dramatic reduction in false positive
because of the escalations logic that I
was just talking about then fourth point
is estimating the need to log into
multiple boxes and change configuration
files thanks to naga's configuration
maintained in a central repository and
pushed automatically to appropriate
service fourth point is estimating the
need to log into multiple boxes and
change the configuration files and that
happens because the inaugus
configuration maintained in a central
repository or a Central master and can
be pushed automatically to all the
states to all the servers or slaves or
agents whatever you want to call it so
this was the result of using nagos right
now is it time to check out a demo where
what I'll be doing is I'll be monitoring
couple of services actually more than a
couple of Services of a remote Linux
machine through my nagios host which I
just showed you right so from there I'll
be monitoring a remote Linux host called
edureka and I'll be monitoring like
three four Services you can add whatever
you want and let me just show you what's
the process once you have installed
nagus what you need to do in order to
make sure that you have remote host or a
remote machine being monitored by your
inaugus host now in order to execute
this demo which I'm going to show you
you must have lamb stack on your system
right Linux Apache MySQL and PHP and I'm
going to use Center OS 7 here let me
just quickly open my sentos Virtual
Machine and we'll proceed from there so
guys this is my sentos virtual box where
I've already installed Lagos as I've
told you earlier as well and this is
where my inaugus host is running or you
can see the nagus server is running and
you can see the dashboard in front of
your screen as well right so let me just
quickly open the terminal first let me
clear the screen so let me just show you
where I've installed in Argos this is
the path right if you notice in front of
your screen it's in user local nag OS
what I can do is I'll just clear the
screen and I'll show you what all the
directories are inside this so we can go
inside this HC directory and inside this
I'm gonna go inside the objects
directory right so why I'm doing this is
basically if I want to add any command
for example I want to add the check
underscore NRP command that's how I'm
going to monitor my remote Linux host if
you remember in the diagram right so
that's what I'm going to do I'm going to
add that particular command I've already
done that so let me just show you how it
looks so I'll just type G editor you can
choose whatever editor that you like and
go inside the commands.cfg file and let
me just open it so these are the various
commands that I was talking about now
you can just have a look at all these
commands so this is to basically notify
host or by email if anything goes down
anything goes wrong in the host this is
for service basically it'll notify if
there's any problem with the service
through email this will check if my host
machine is alive I mean is it up and
running now this command is basically to
check the disk space like the local disk
then the load right you can see all of
these things here swap FTP so I've added
these commands and you can have a look
at all of these commands which I've
mentioned here and the last command you
see is I've added manually because all
these commands once you install you get
it by default but the NRP check
underscore NRP which I'm highlighting
right now with my cursor is something
which I have added in order to make sure
that I will monitor the remote Linux
host now let me me just go ahead and
save this right let me clear my screen
again and I'll go back to my now give us
directory Let Me Clear My screen again
now basically what this will do is this
will allow you to use a check underscore
and RP command in an argue with service
definitions right now what we need to do
is update the NRP configuration file so
use your favorite editor and open
nrp.cfg which you will find in this
particular directory itself so all I
have to do is first I'll hit LS and then
I can just check out this Etsy directory
now if you notice there is an nrp.cfg
file right I've already added it so I'll
just go ahead and show you with the help
of G edit or you can use whatever editor
that you prefer now over here you need
to find this allowed host directive and
add the private IP address of your nagos
server to the comma delimited list if
you scroll down you will find something
called a loud host right so just add a
comma and start with the IP address of
the machine that you want to monitor so
current let me just open it once more so
I'm going to use sudo because I don't
have the Privileges
now in this allowed host directory all I
have to do is comma and the IP address
of the host that I want to monitor so it
is
192.168.1.21 just go ahead save it come
back clear the terminal now save and
exit now this configures an RP to accept
requests from your nagos server why it's
private IP address right and then just
go ahead and restart NRP to put the
changes into effect now on your server
you need to create a configuration file
for each of the remote hosts that you
monitor as I was mentioning before as
well now where you're going to find it
in HC servers directory and let me just
go ahead and open that for you let me go
to the servers directory now if you
notice here there is edirecta.cfg file
this is basically the host that I'll be
monitoring right now if I go ahead and
show you what I have written here is
basically first what I have done is I've
defined the host it's basically a Linux
server and the name of that server is
Eddy raker allies whatever you want to
give this is the IP address maximum
check attempts the periods I want to
check it 24 7 notification interval is
what I've mentioned here and
notification period so this is basically
about all my hosts now in that host what
all services I'm going to monitor I want
to monitor generic services like Ping
then I want to monitor SSH then I'm
going to monitor CPU load as well these
are the three services that I'll be
monitoring and you can find that in your
HC servers directory over there you have
to create a proper configuration file
for all of the hosts that you want to
monitor Let Me Clear My terminal again
right so I hope you have understood this
guys and if you have any questions or
doubts go ahead and mention that in the
comment section just to show you my
remote machine as well let me just open
that so this is my remote machine guys
over here I've already installed NRP so
over here I'm just going to show you how
you can restart an RP system CTL restart
nrpe dot service
uh here we go it's asking for the
password I've given that so my NRP
service has started actually I've
restarted again I've already started it
before as well let me just show you how
my nag years dashboard looks like in my
server now this is my dashboard again if
I go to my host tab you can see that we
are monitoring two host edureka and
localhost edrica is the one which I just
showed you which is up and running right
I can go ahead and check out this map
Legacy map view as well which basically
it tells me that my editor is a remote
host then also I have various services
that I'm monitoring so if you remember I
was monitoring CPU load ping and SSH
which you can see it over here as well
foreign
[Music]
devops exactly is to understand that we
need to understand what Azure is and to
understand azeri would be using this
particular analogy now if you talk about
power consumption it's fairly simple I
mean you consume electricity and you pay
for the amount or the units that you've
consumed if you've used maybe 100 units
you'd be charged for 100 units and the
important part here is you're not
concerned about where the electricity
has come from who maintains it how is it
generated how is it brought to your
house and stuff like that I mean even if
you know about these things it's not
your concern how it is done I mean even
if you do not have any knowledge about
it you can still survive very well
similar as Azure or any cloud service
provider that is there in the market let
me tell you how now if you talk about
software development you normally have
to take care of a lot of things like
building a software building the
infrastructure in first place having
servers in place making sure that your
servers your resources are monitored and
maintained now the problem here is if
you are doing all these things you won't
be able to focus on your business and if
you wish to focus on your business you
probably might need to hire quite a few
people plus servers and stuff like that
is actually costly what if I could tell
you that you could actually go ahead and
rent all these services like you rented
the electricity and paid only for the
units you used wouldn't that be nice
well this is what Azure does it is a
cloud service provider which actually
lets you rent compute storage database
analytics and a number of other services
you only pay for the services you use
and only for the time duration you use
those services for and that is why this
analogy which is very similar to how
electricity is consumed and used I hope
the idea is clear to you what Azure is
to give you a little more specific
definition it is a popular cloud service
provider which provides services I just
talked about and it is one of the best
in the market it is a Microsoft product
and we all know that Microsoft is a big
fish when you talk about software market
and that is why Azure is one of the
leading cloud service providers it is
second only to AWS and it is actually
catching up with AWS in a very good way
now AWS also has a devops approach and
as I've already mentioned it is azure is
very much neck and neck with AWS it also
has its devops services and these
services that are provided by Azure are
very good when you talk about comparing
those with AWS devops Services let's try
to understand how Azure devops Services
go well with each other and to do that
let's take a look at some of the
components which Azure has to offer to
you well when you talk about Azure
components these are the major
components which Azure has to offer to
you you have your Azure pipelines you
have your Azure boards as your artifacts
Azure repos and Azure test plans
if you remember the definition of devops
I talked about continuous integration
and continuous deployment let me throw
in some light on what it is exactly now
when you talk about continuous
integration and continuous deployment
what you mean to do here is you wish to
continuously build softwares and
continuously deploy those to the end
users again if there are any releases
any changes that need to be done those
should be implemented right away and the
latest version should be available to
all the people that are working on that
software I mean if I actually go ahead
and build a particular software now if
there are like 15 people working on that
and if I make some changes to that
software that change should be given to
everyone right so this is where
continuous integration and deployment
comes into picture it aims at keeping
everyone on the same page and ensuring
that software development happens at a
faster rate and to do that you have
something called as Azure pipelines now
when you talk about continuous
integration and deployment you need a
line maybe a tunnel where you can
actually carry out these processes right
that tunnel that line that pipeline is
something that is provided by Azure
pipelines I mean you can actually go
ahead and build any kind of software and
use different kinds of platforms that
Azure supports and when I say Azure
supports well Azure supports so many
platforms that I believe there is hardly
any software or any framework that you
wish to use that you cannot using
Microsoft Azure and that is why building
pipelines with Microsoft Azure becomes
very easy now if you talk about building
pipelines in general it is a very simple
and a very easy to implement kind of a
process because it is just click survey
when you talk about Microsoft Azure and
once the pipeline is ready integrating
it with different softwares becomes very
very easy now again if you are building
a software you might be having maybe
hundreds of people working on that
software in that case you need a
software or something that would let you
keep track of all these things help you
plan better help you we have everyone on
the similar pages and help you keep
track of all these things now this is
where Azure boards come into picture it
readily integrates with power bi now if
you people know what power bi is it is a
data visualization and business
intelligence tool and it is one of the
best in the market it is very neck and
neck with Tableau which again is a great
tool and what Microsoft Azure ports do
is they let you integrate power bi into
Azure devops environment where you can
actually go ahead and take a look at the
data in real time and have various
graphs charts so that you are assessing
or the way in which you keep track of
all these things becomes a lot more
easier next we have Azure artifacts and
what it does is it lets you take into
consideration various packages or feeds
that you generate now when I say this
you can do that from both private and
public resources apart from that whether
it's Marvin whether it's news and
whatever it is everything is taken care
by your Azure artifacts when you're
talking about continuous inter duration
and deployment your repositories become
very important and Microsoft Azure lets
you have any kind of repositories that
are there if you take a look at Azure
reports it has almost everything that is
needed to deal with repositories if you
are building a software it needs to be
tested and with Azure test plans you can
do end-to-end testing and it ensures
full coverage for your tests so these
are some of the components that ensure
DeVos processes carried out very well
with Microsoft Azure and that is why
Azure and devops go very well hand in
hand now let's move further and take a
look at the demo part so guys in order
to understand how Microsoft Azure works
or how the Azure devops stuff works we
would be switching into Azure portal now
Azure as I've already mentioned is a
cloud service provider and we are going
to see how Microsoft Azure can be used
to carry out devops operations to do
that you can actually go ahead and visit
azure's website just open the website
and then you have this place called as
portal where you can click in now guys
if you have an account it would ask you
to sign in just go ahead and sign into
your account and you are more than good
to go for people who are completely new
to Microsoft Azure let me tell you a
thing that they have something called as
a free tier where you can actually go
ahead and create a free account you have
to enter in your credit card details but
Microsoft Azure won't charge you for
that once you create an account
Microsoft Azure gives you certain free
credits which you can use for a
particular duration of time now I cannot
see my credit share but normally you can
actually go ahead and take a look at the
amount of credits that Microsoft Azure
has to offer to you it is somewhere
around if I'm not only it is close to
200 if I'm not wrong yeah that is what
it is so probably you would be able to
utilize the services for free and how do
you do that you use that credit that is
made available to you so if there is a
particular service that you want to use
you can pay the bills using the credit
that is made available to you and you
won't be charged for the services that
you use there are certain limits on the
services that you use suppose if you
actually go ahead and create a server
and you let it open for a long time
create a database on top of it you might
be charged heavily for that but I would
suggest that you actually go ahead and
take a look at certain documentation
that Microsoft Azure has to offer to you
and as long as you stay in the specified
limits you won't be charged anything so
I would suggest that you actually go
ahead and put in your credit card debit
card details and get an account created
once you create an account this is what
the Microsoft Azure portal or the
interface looks like so we have our
account which is up and running now we
are going to go ahead and Carry Out
devops practices right so how do we do
that we switch into devops you can just
come to the search console and type
devops here Azure devops or you can just
go ahead and type Azure devops somewhere
here
the reason I'm doing this is I want to
show you how the interface looks like so
this is what it would look like when you
start
and if you click on start free you would
be redirected to this page now again
guys first you need to create an
organization think of organization as an
environment I mean if your company is
actually planning to build a software
they would be having a structure to do
that right so if your company is
planning to actually go ahead and build
a particular software they would be
having a template a structure to do that
right so when I say an organization I'm
talking about a structure how do you
create an organization it is very simple
here's an option here if you click on
new organization it would ask you for
the name of the organization you enter
the name and your organization gets
created now that we have an organization
our next step should be to go ahead and
create a project how do we do that again
click on Project there would be an
option here once you create an
organization saying that you want to
create a project you should say yes on
it again that is how the interface looks
for somebody who has started fresh if
not this is what it would look like so I
can just go ahead and enter the project
name let's say
demo project for today
I'm bad at naming conventions so forgive
me for that description
sample project would do again guys the
complexity of project is not important
here I'm just trying to explain certain
stuff to you so please pardon this
simpleness of this particular project
again guys once you do enter in the
details it gives you an option do you
want it public or private when you say
public anyone on the internet can have
access to it more of an open source kind
of stuff if not you can stick to your
private thing and you can give access to
the people you want to take a look at
your project having said that um if you
select private you can just say create
project and your project gets created
okay guys so the project is created it
took a longer while than normal if your
internet is not working that well at
times it takes this time so once the
project is ready guys you are actually
free to go ahead and do the stuff now
these are the components that we talked
about at the bottom you can see these
components you have your boards Repose
pipelines test plans artifacts and you
can actually go ahead and create all of
those let's start by creating a pipeline
first guys so all you have to do is say
new pipeline
now again guys how do you actually go
ahead and create a pipeline what do you
do for that do you go ahead and use an
existing piece of code that you have do
you have repositories that you want to
do or do you want to type in your own
code you can do anything you want you
can use Azure reports which are made
available to you for free
um GitHub we all know is a popular place
to have your repositories kept and if
you wish to do that you can import
repositories from your GitHub as well to
do that you have to actually just go
ahead and click on this icon that is
what I'm going to do in this session I'm
going to use a GitHub repository so you
have to click on this thing and you have
to authorize your GitHub repository or
your GitHub account rather now my
account is already authorized with Azure
so I don't think it should ask me for
reauthorization you can actually go
ahead and authorize your GitHub account
once you do that you're free to actually
go ahead and import a repositories from
your GitHub now if I go to GitHub
This Is My Demo account so I have one
repository that I have it here my
repositories you can see this is a
python repository guys as I've already
mentioned you can use any platform you
want when you talk about Azure python is
something that is simple and I wish to
stick to that now this repository is
something that is made Available to You
by Microsoft Azure as well do not worry
I would be sharing in this link if not
you can visit Microsoft Azure docs for
Azure devops and you would get this link
as well so once you have this link you
have to Fork this thing into your GitHub
for people who do not know what GitHub
is well think of it as a storage place
where you can actually go ahead and
store your repositories now repositories
is nothing but it is a piece of code or
a software which you can actually go
ahead and store it here which you can
give access to others as well something
that is called as open source where
others can actually go ahead and take in
your piece of code maybe modify it and
give it to you or modify it and use it
for their own purpose so this is what
GitHub does it is a place where you can
put in your codes you can have a master
Branch for it and you can give access to
certain people who can make changes to
that code now how do people make changes
to it they have this option which is
called as forking your GitHub repository
when I say forking it that means I'm
actually creating a copy or a branch of
this particular repository and I'm using
it for my own sake once I make changes
to it I can submit those changes to the
main repository or to the master branch
of it so this is what GitHub is and the
process which we saw here it would ask
you to authorize your GitHub account and
if you've already forked that particular
repository it would be available here in
the suggestion as you can see it has
been forked here so I have this
repository here which I've selected
so it opens my yml file which is there
which I can actually go ahead and edit
what does this thing do it considers
python Django framework it tests a
Django product or project rather on
multiple versions of python and it has
steps that lets you analyze code save
build artifacts deploy them and more so
this is what this piece of code does for
you guys and won't be getting into the
details of what this code does but again
it's a sample repository you can
actually go ahead and study it a little
more once you do that all you have to do
is just run this piece of code as simple
as that guys you have to run it what
this code does is it creates a pipeline
for you and when I say it creates a
pipeline it lets you probably actually
go ahead and make codes make changes to
the codes and submit those to the master
Branch we would be doing that not worry
about that as you can see Microsoft
Azure gives you continuous feedback and
update as to what is happening where it
is happening and how it is happening
this might take maybe half a minute or
so because quite a few things that are
happening as you can see it's telling
you what has happened right I mean the
processes you can see the tasks
happening
it is performing tests and stuff like
that see these are the number of
processes that have taken place you've
prepared a job it has initialized an
agent to take care of it initialize the
job got resources for it used python
version exported the project path then
it installed certain prerequisites it
ran tests it published the test results
and then this is what happened post job
get sources if you wish to have more
details you can click on this icon and
it would give you the details about what
has happened here so you can do that as
well
or you can just probably come back to
Pipelines
and see that there's a pipeline that has
been created now guys if I actually go
ahead and commit changes to this piece
of code that is my yml file or the files
that I have in this repository those
would be reflected here you can see the
status here we are going to do that but
let me show you some other things as
well if I click on analytics it should
give me analytics of what has happened
here whether they have any failures
whether the code ran properly and stuff
like that test failures is not available
because we do not have a plugin here
when I say plugin okay did not find any
test results is what it is saying so
let's just go ahead and do certain
things and probably this would be more
clear to you guys do not worry about
that going back to the History part
if I take a look at the summary part it
would tell me what has happened here
test succeeded build pipeline succeeded
manually queued and this is what has
happened now guys can I just go ahead
and put in certain other details can I
make certain changes here let's try to
understand what can be done now again I
have this gate repository with me right
so let's just go ahead and play with it
a little and commit certain changes to
do that first go to the pipeline
this is the pipeline I'm concerned about
and probably I must be getting certain
details that I'm looking for they're not
here so let's go back go back
go back a little
projects projects
links again
here you have this option called as
status badge now what is the status
badge see this is what the status badge
looks like it tells that okay whether
the pipeline you built was built
correctly or not so if you are a
developer you would be wanting your code
to be green right I mean everything
should be checked green everything
should be nice and ready and successful
so this is what the badge does so we're
going to go ahead and use this piece of
batch to add to our code so that now
this is a sample stuff I mean it's not
that important but if we do go ahead and
make or add this badge to our piece of
code it would show that a certain change
was made in the code or in the file and
that would be reflected in the pipeline
something that is important when you
talk about CI CD so let's do that with
the simple example so I've come to the
pipelines here
my pipeline and then I scroll down
you would be having this readme file
where you can make changes how do we do
that
I say edit
and then I add in a piece of code here
say let's add a new
piece of code somewhere here maybe
okay there you go again guys do not get
into the details of the complicacy of
the stuff it is fairly easy and do not
judge me for that either I mean as I've
already mentioned my aim is to just go
ahead and show you how the concept or
the stuff works so it is very basic guys
I've just gone ahead and made some
change to this piece of code I've
actually just gone ahead and added the
batch here so I'm just gonna go ahead
and say commit changes update readme
file yes please do that commit directly
to the master Branch or should I commit
it to the new branch that I did let's
say master Branch for now
so the change was committed here so if I
come here and if I just go to Pipelines
and select my pipeline open it
yeah there you go the change has been
reflected update readme so this file was
updated so again guys this is what I'm
talking about
continuous integration and deployment
that is what it means I mean if I make
change to this piece of code and if
somebody else wants to access it it or
my software should give me the latest
copy right so this is what this piece of
code is doing in my pipeline if I just
go ahead and pick a certain piece of
code I make change to it that change
gets reflected and this is what devops
does and this is what Azure helps you do
it lets you bring in the whole GitHub
concept into Azure and it helps you go
ahead and work with CI CD in a much
better way again I talked about some
other stuff as well Repose you have your
boards you have quite a few things that
you can do if you talk about overview in
general you have something called as
dashboard so if I click on this icon
here
so I have this dashboard that is created
by default oh there is no dashboard here
or if there is a dashboard it does not
have widgets so I'm gonna add a widget
let's say I need a visit for build
history so I click on it and I say add
so I said done editing
so if I just refresh this thing
so if I just come here and select this
particular Pipeline and I say save
it will reflect the changes that has
happened
what happened with or what is happening
with this particular dashboard or with
this particular project so it would give
me details like these and as I've
already mentioned when you create
dashboards you can actually go ahead and
integrate power bi which is a very
popular business intelligence and bi
tool basically so you can actually go
ahead and take in or consider all those
features as well apart from that you
have all the other stuff that you can
actually go ahead and play with again to
use analytics or to understand how
analytics work with this thing you
actually would have to go ahead and do
two things one you would have to go
ahead and integrate your plugin for
analytics once you do that you can go
ahead and Implement quite a few things
to do that you can actually come here
and you can do quite a few things you
can take your GitHub connections you can
integrate your GitHub with your Azure
you can actually go ahead and integrate
quite a few other things see you have
options like service connections Asian
pools retention parallel jobs and stuff
like that so you can take care of all
those things as well boards again you
can go ahead and add items here items as
in you can put in states where you want
to put in details for certain things
like there are particular bugs that have
particular priorities based on your
project you can add in those items
saying that okay um this is the
criticality of those items and stuff
like that it is fairly is you can just
click on create a new item whether it's
a bug whether it's a feature issue
whatever it is you can go ahead and you
can add that here and you can keep track
of all those things in the form of
dashboards or boards here
Repose you can actually go ahead and
when you click on files you can see the
reports that you're using and stuff like
that you can go to the marketplace and
you can take in the reports that
Microsoft Azure supports pipelines we've
seen what it is test plans you can go
ahead and launch proper test plans as
well and you can create your own
artifacts or your feeds as well by using
Azure artifacts so yeah guys this is
pretty much what this session had to
offer to you people we did go ahead and
create a pipeline we saw how to commit
changes and how those changes get
reflected when you use an Azure Pipeline
and we also saw what dashboards are and
how do they work
foreign
[Music]
devops and cloud or devops on cloud why
do we need devops on cloud Now
understand that devops and Cloud go hand
in hand they can be implemented
individually of course but devops
becomes twice as much efficient and
beneficial when clapping with a cloud it
can help an organization deliver new
software features much faster in a more
effective manner many organizations try
to fix their application development
processes by shifting from waterfall to
devops now they have this understanding
that devops alone won't be that
effective so public and private Cloud
Solutions are now evolving together with
devops this brings in products at a
faster rate to the market through quick
access to development environment and
streamline developer processes
infrastructure as code and automation
together reduces the cloud complexity
and maintenance of servers and resources
which are Ops Team previously was very
concerned about the security also highly
increases with automated repeatable
processes that serve to eliminate error
that can cause further issues and even
more importantly it builds security
controls from the very beginning to the
very end of the process now because you
don't have any servers and your
continuous operations are cloud-based it
also eliminates a lot of downtime and
last but not the least scalability which
is one of the most important factors for
applications as they are developed when
devops and Cloud are clubbed together it
reduces the cost of infrastructure and
Global reach also increases with this
now that you know why you need a cloud
platform let's go ahead and look at our
Cloud platform of choice today which is
AWS so what is AWS now back in the day
handling and storing data was way
different than it is now companies
preferred storing data using private
servers and that was obviously for
security reasons but however with better
usage of the internet the trend friend
has seen a paradigm shift for Industries
as they are moving the data to the cloud
now this enables companies to focus more
on core competencies and stop worrying
about storing and computation for
example back in the day if a streaming
platform or a search engine platform
anything with a high volume database
suffered a corruption it would take days
before their operations resumed they
would face problems scaling up and only
then would they realize the need for a
highly reliable horizontally scalable
and distributed system is what they need
but now with cloud services and public
Cloud platforms this wouldn't be a
problem now since every company has
started to adopt cloud services it can
be claimed that the cloud is the Talk of
the Town and AWS in particular is the
leading cloud service provider in the
market AWS which stands for Amazon web
services is an amazon.com subsidiary
which offers Cloud Computing Services at
an extremely affordable rates therefore
its customer base is strong and it
targets everyone from individuals to
startups to take joints running the it
landscape if you might wonder what cloud
computing is is basically the use of
remote servers on the internet to store
manage and process data as opposed to an
actual physical server or a personal
computer to do the same as we are
talking about AWS it is kind of an iaas
or infrastructure as a service which
basically gives you a server in the
cloud that you have complete control
over in IAS you are responsible for
managing everything starting from the OS
completely up to the application you are
running so now that we have discussed
about devops and AWS let's move on to
RCI CD pipeline now what is a pipeline
or cicd pipeline it's nothing but a
series of steps that must be performed
in order to deliver a new version of the
software at its Bare Bones at its most
basic you have your build test and
deploy stage the CI CD basically stands
for continuous integration and
continuous deployment so CI is basically
continuous integration which basically
means bringing together all the
developers working copies to a shared
main line all the developers working on
Parallel branches of a certain upgrade
of an application merge their changes
into one main branch and CD stands for
continuous delivery and deployment now
while the CI includes building and
testing of your application continuous
deployment is about the processes that
have to happen after the code is
integrated for the app to be delivered
to the users these processes involve
testing staging and deploying the code
so at the end of this session what we
aim to do is build a CI CD pipeline for
our demo app on AWS so for that we will
have to look at a few components of AWS
so devops when implemented on AWS
becomes a lot more efficient and
effective for a productive life cycle
and the steps that are involved in AWS
devops are code commit code pipeline
code build code deploy and optionally
code staff so first of all we have AWS
code commit which is a fully managed
Source control service somewhat like
GitHub that hosts secure and highly
scalable git based repository without
the need of operating the system it's
mainly designed for developers who are
supposed to store and version their code
securely and reliably for example you
have your it administrators that store
their scripts and configurations and
your web designers who can store their
HTML pages and images
Etc the code commit is fully managed has
great availability is secure scalable
and patens up your development life
cycle for people not aware of code
comment think of it as versioning in
your S3 but how it's different is that
S3 supports versioning but not
collaborative file tracking features
which could commit obviously does it
manages batches of changes across
numerous files made by multiple
developers parallely how it works is
that you create a repository in AWS code
commit service via the console or CLI
and later using git from the development
machine you can run git clone to connect
the local repository and the AWS code
commit repo you can then modify your
files on your development machine via
the local repository and then run git
add git commit and push it to the AWS
code comment repository as you do with
GitHub again like git even a git pull
can be used here to synchronize the
files in AWS could come at repository
with your local wrapper which ensures
that you are working with the latest
version of the files next on our list we
have AWS code pipeline which is a
combination of continuous integration
and continuous delivery services for a
quicker and more reliable infrastructure
and application updates it automatically
builds tests and deploys a user code
Whenever there is a code change and it
is completely based on user-defined
release process models it also
integrates with AWS services like AWS
code Comet Amazon S3 code deploy elastic
Beanstalk Ops works and AWS Lambda you
can configure the pipeline either with
your CLI or your graphical user
interface and like most services on AWS
even with pipeline you only have to pay
for what you use all of this is great
and all but why should you use code
pipeline simple by automating your
software build test and release
processes AWS code pipeline enables you
to increase the speed and quality of
your software updates by running all new
changes through a consistent set of
quality checks it automates your release
process it speeds up your delivery with
quality it allows you to choose your
tools of choice establish consistent
release processes and provide a pipeline
history detail as your source of Truth
code pipeline basically breaks up your
workflow into a series of stages like
your Source build test and deploy and
gives you a revision option as a
Deployable content each stage can
process only one revision at a time even
though multiple revisions can be
processed in the same Pipeline and each
stage will have at least one action to
be performed which is some kind of task
performed on the artifact once all of
the actions that are configured in a
stage is complete the stage is
considered as complete after stages come
complete it transitions the artifacts
created in that stage to the next stage
of the pipeline where you can manually
enable or disable it so it prevents
changes from running through an entire
pipeline an approval action is granted
only by the IAM user and if an action
fails it does not transition to the next
action or stage at all so when a
developer completes her working on his
code he or she commits it to the source
repository and code pipeline
automatically detects the changes and
builds those changes after that the
build code is deployed to the staging
server for testing and then additional
tests such as integration or load tests
are run by the code pipeline once all
the tests are run if the code receives
manual approval then AWS code pipeline
deploys the tested and approved code to
the production instances also every time
when a user creates a pipeline the code
pipeline creates a folder for that
pipeline in an S3 artifact bucket in
that particular region to sort input and
output artifacts next on our list we
have AWS code build now this is a fully
managed build service in the cloud which
compiles your source code runs unit
tests and produces artifacts that are
ready to deploy it eliminates the need
to provision manage and scale your own
build servers it provides pre-packaged
build environments for the most popular
programming languages and build tools
and skills automatically to meet your
Peak build requests now one prerequisite
is that you must provide the AWS code
build with a build project which should
include information as to where to get
the source code from build environment
build commands and where to store the
build output how it works is that the
code build will use the build project to
create a build environment AWS code
build then downloads the source code
from the build environment and performs
tasks that you would specify in the
build specifications if there is any
build output the build environment
uploads its output to the S3 bucket and
while the build is running the build
environment sends information to code
build and cloudwatch logs you can use
the code build console AWS CLI or AWS SD
case to get summarized Bill information
from code build and detailed build
information from cloudwatch logs as well
now that was all about code build now
finally let's talk about code deploy now
AWS code deploy is a service that
coordinates your application deployment
and updates across the fleet of AWS ec2
of any size it automates your code
deployment to any instance handles the
complexity of updating them it also
avoids downtime during application
development and rolls back automatically
if failure is detected apart from that
it also integrates with third-party
tools and AWS to make your job easier it
has six primary components to be
specific you have app application
revision compute platform deployment
group deployment configuration and the
code deploy agent now the basic skeleton
of how your deployment workflow pursues
is as you can see on your screen you
basically create an application with a
unique name and set up a deployment
Group by specifying the instances to
which you want to deploy your
application and the deployment type if
you're using the Lambda platform you
just deploy your deployment group's name
followed by which in your deployment
configuration you specify the success or
failure condition of deployment and to
how many instances you want to deploy
parallely then you upload the
application specification file to the S3
and deploy your application as specified
with the help of your code deploy agent
which is running on each instance on an
ec2 platform or as specified in your
specification file to the deployment
group when you're using a Lambda
platform and finally you check your
deployment results and if you face any
bugs or issues you can always roll back
and redeploy finally let's discuss a
little bit about codestar which is
basically a cloud-based development
service that provides tools that you
need to quickly develop build and deploy
applications on AWS it's basically a
very templatized format where you start
developing on AWS in certain minutes and
you could choose from a variety of
project templates to all of the software
delivery is easily managed and you can
work across your team very securely
using the code stuff and apart from all
of this you also are provided with a
project management dashboard to monitor
your application continuously all you
have to do as an AWS code star admin is
create a project and add users your
users or team members will come at
changes which in turn will be built and
deployed and through consistent
monitoring of the application if there
are any updates required or any bugs the
developers take a decision make updates
fix bugs and then the loop closes back
in on the team and that's about it the
development process takes little to no
time using Code stop now that we have
spoken about all of these different
components of AWS devops I hope building
the CI CD pipeline would seem a tad more
tangible to you if you're new Learners
you can always sign up for a free tier
which is free for the entire year
obviously there are limitations to all
of the services Beyond which you shall
be charged but that rarely happens so if
you're starting out with AWS and want to
try out all of its features and services
the free tier is a pretty good idea we
already have a video on how to create a
free tier on the AWS console if any of
you want to know how to create a free
account you can just go ahead and check
that out so we're going to be building a
CI CD pipeline using AWS using Code
Pipeline and I'll try to be as slow and
verbose as possible as comprehensive as
possible so as to help you guys to
follow me each step of the way so what
we'll be doing here is that we'll be
creating a demo application platform as
a service application and we shall be
deploying it using the code pipeline so
let's go ahead and and create our
application first so to create an app we
will be using the Amazon elastic
Beanstalk which basically has a bunch of
templates ready for you now since this
is a demo about code Pipeline and not
web development we'll be creating a
pretty rudimentary app so let's go ahead
and type elastic Beanstalk using elastic
Beanstalk we can make simple apps very
very quickly and if you have already
created an application created a few
applications they are going to appear
right here on your screen but since we
have created no applications using this
particular account this is a screen that
will greet you so we're going to go
ahead and click on create application
so let's call this deployment app
we're not very original are we we're not
going to put in any application tags you
could put them if you like but currently
they are completely unnecessary on
platform let's pick PHP and it
automatically fills up the platform
Branch inversion with the latest ones
available then I'm just going to be
using the sample application code and
click on create application now this is
a platform as a service app so basically
once I click on create application I
wouldn't have to worry about any
background processes or creating the
infrastructure elastic Beanstalk is
going to do that for me on its own
and here it's gonna show you all of the
steps that are taking place like create
environment is starting using elastic
Beanstalk as Amazon S3 storage bucket
for environment data so on and so forth
now this will take some time so let's go
ahead and parallely do something else
I'm going to open another tab
now the AWS console
okay thankfully you did not ask me to
sign in again so while my app gets
created what I'm gonna do is I'm gonna
create the pipeline so I'm going to type
in code pipeline
and the Funda is simple even here guys
you're just going to put in certain
details and your pipeline will be
created for you now this is your
dashboard this is where you all of your
recent pipelines will appear since in
this free tier we have no pipelines
created hence you have no results to
display so first things first I'm going
to create a pipeline so click on this
big orange button which says create
pipeline I'm going to give the pipeline
a name let's just call it
demo pipeline
so as a default I'm just going to fill
in a role name AWS
code pipeline
service role
us
East
demo pipeline then click on next
so for Source provider I am going to use
GitHub
and for that I'll have to connect to
GitHub
and will ask me to authorize the code
Suite
okay
okay once you have successfully
authenticated the account
you can go ahead and search for the
repository here I'm going to choose the
demo hyphen code deploy app which is
actually a sample app I have forked from
the AWS code pipeline repository you can
go ahead and look for it as well it will
be available in the AWS code pipelines
GitHub account yeah
add in the branch let's select Master
branch
and click on next Now we move on to the
build stage because I have nothing major
to build I'm just going to go ahead and
pick skip build stage then you will be
prompted with a notification which will
ask you if you're sure to skip your
build stage now since I have nothing
major to build yes I am going to skip
this build stage and for deployment here
we are going to choose a deploy provider
so here I'm going to use elastic
Beanstalk
and put in my application name and my
environment name now our application
name was deployment app and our
environment name would be deployment app
environment this is something which gets
created automatically so we're going to
go ahead and click on next and finally
we are at the review stage where you can
go ahead and review all of the details
that you have just put in for your code
Pipeline and I am just going to create
this pipeline it's pretty simple to do
now this might take a few moments so
kindly be patient
and then you are greeted with a
notification which says success
congratulations your pipeline has been
created now here you can go ahead and
release changes if you want our IM role
is in place that we had created earlier
our pipeline is a success now all we
have to do is wait for our application
to get created and then we can go ahead
and deploy our application
all right with that our application has
been created it has okay health and you
can see the platform here you can see
the running version you can see all the
details all the recent information
regarding the app now this is a platform
as a service it basically runs all of
your processes in the background now if
you would have gone ahead created this
app configured your IM user configured
everything else that was needed this
would have taken you a lot of time but
because you are using a platform as a
service feature of AWS this entire
process is automated and your work is
simplified so if you go down and look at
the recent events one thing you can see
is your instance deployment is completed
successfully just in the line above that
you can see that your new application
was deployed to running ec2 instances so
basically an instance is running on ec2
so let's go ahead
and go to ec2 click on instances running
there is one instance running obviously
we all know which one that is and this
is your deployment app let me go ahead
and click on it here you can see the
instance State it's running and your
pipeline your Source has succeeded your
deployment has succeeded you go here you
can go down take a look at your IM role
your subnet ID
so with that your instance has been
deployed and you realize that your demo
pipeline has worked
[Music]
foreign let's compare AWS devops versus
azure devops
so let's talk about the services that it
provides the Amazon web services code
pipeline is a continuous delivery
service for fast and reliable
application updates code pipeline builds
tests and deploys your code every time
there is a code change based on the
release process models you define on the
other hand Azure devops services for
teams to share code track work and ship
software Azure devops provides unlimited
private git hosting Cloud bill for
continuous integration agile planning
and release management for continuous
delivery to the cloud and on-premise it
also includes a broad integrated
development environment support
in the next comparison we will talk
about what category of tech stack do
they belong to as you can see Amazon web
services code pipeline belongs to
continuous deployment category of the
tech stack while Azure devops can be
primarily classified under integrated
development environment tools
now let's talk about the features of
Amazon web services and Azure as you can
see Amazon web services provides
workflow modeling AWS integration and it
also has some pre-built plugins in it
whereas Azure on the other hand provides
agile tools like kanban boards backlogs
scrum boards and it also provides
reporting tools like dashboards widgets
power bi Etc
and it also allows you to integrate with
Git which provides free private
repositories and pull requests
now why do people like AWS is because it
is extremely simple to set up whereas
Azure on the other hand is a complete
package it's full and it's very powerful
now let's move on and check out the uses
of AWS in azure as I've already
mentioned AWS is extremely simple to set
up it also has several managed Services
it can be easily integrated with GitHub
and it also has parallel execution
deployment is completely automatic and
there are manual steps available so
you're never lost Azure on the other
hand supports open source it also has
several Integrations that is it can be
easily integrated with GitHub and
Jenkins it also has several project
management features and the most
important plus point of azure is that it
is free for stakeholders I hope the
users between AWS and Azure are
extremely clear now let's move on and
talk about AWS and Azure clients so
Amazon web services is used by companies
like play HQ affirm curro education up
ready for and bitbank incorporation
whereas Azure has clients like Microsoft
Kingsman software evodeck digital Muse
energy to Market and QR point
now the next important difference is
with what tools that these cloud
services can be integrated with Amazon
web services as you can see on the
screen can be easily integrated with
GitHub Jenkins Amazon ec2 Amazon S3 AWS
elastic Beanstalk run scope and Cloud
base Azure on the other hand can be
easily integrated with kit and GitHub
Docker slack Jenkins Trello and visual
studio
now let's move on and compare the salary
of an AWS devops engineer and Azure
devops engineer as you can see on the
screen already the median salary for an
AWS devops engineer is rupees 4 lakh 67
000 rupees whereas the median salary for
an Azure devops engineer is about rupees
6 lakhs 4 000 rupees a year
[Music]
let's address devops interview questions
and answers
they are different topics that we'll be
covering in the session today that is
questions on General devops programming
Version Control continuous integration
continuous testing configuration
management continuous monitoring and
finally containerization and
virtualization
so without Much Ado let's talk about
General devops interview questions and
answers the first question is what are
the fundamental differences between
devops and agile
so the first feature that we'll be
addressing today is the agility
so it devops the agility is seen in both
development and operations whereas the
agility in agile is only seen in the
development side
the second difference is between the
processes or practices in devops it
involves different processes such as
continuous integration continuous
development continuous testing Etc
whereas in agile it involves practices
such as agile scrum agile kanban Etc
the key Focus area in devops is
timeliness and quality whereas in agile
timeliness is the main priority
the next feature that we'll be talking
about is release Cycles or development
Sprints and develops smaller release
Cycles are implemented with immediate
feedback whereas in agile there is only
smaller release Cycles
the next feature that we'll be talking
about is the source of feedback in
devops feedback is from self monitoring
tools whereas in agile it's from
customers and finally the scope of work
in devops is agility and the need for
automation whereas in agile it's only
agility I hope the fundamental
differences between devops and agile is
clear based on the different features
that are available
the next question is what is the need
for devops according to me this answer
should start by explaining the General
market Trend instead of releasing big
sets of features companies are trying to
see if small features can be transported
to their customers through a series of
release chains this has many advantages
like quick feedback from customers
better quality of software Etc which in
turn leads to high customer satisfaction
so to achieve this companies are
required to lower failure rate of new
releases increase deployment frequency
shorten lead time between fixes faster
mean time to Recovery in the event of
new release crashing devops fulfills all
of these requirements and helps in
achieving seamless software delivery you
can give examples of companies like Etsy
Google and Amazon which have adopted
devops to achieve levels of performance
that were Unthinkable even five years
ago they are doing tens hundreds or even
thousands of code deployments per day
while delivering world-class stability
reliability and security
the next question is how is devops
different from agile or software
development life cycle now agile is a
set of values and principles about how
to produce that is how to develop
software example if you have some ideas
and you want to turn these ideas into
working software you can use the agile
values and principles as a way to do it
but that software might only be working
on a developer's laptop or in a test
environment
now if you want to quickly easily and
repeatedly move that software into
production infrastructure in a safe
simple and yet efficient manner devops
is definitely the way to do that you
need to understand devops tools and
techniques you can summarize by saying
agile software development methodology
focuses on the development of software
but devops on the other hand is
responsible for development as well as
the deployment of the software in the
safest and most reliable way possible
now moving on which are the top devops
tools how do all of these work together
the most popular devops tools are get
for Version Control System Jenkins for
continuous integration selenium for
continuous testing puppet Chef ansible
are the configuration management and
deployment tools nagios is used for
continuous monitoring finally Docker is
used for containerization you can also
mention any other tool if you want but
make sure you talk about all the tools
that you can see on the screen right now
now the second part of the answer has
two possibilities
now if you have experience with all of
the tools that I just mentioned then you
can just say that I've worked on all of
these tools for developing good quality
software and deploying this software as
easily frequently and reliably if you
have experience only with some of the
tools then you can just mention those
tools and say that I have specialization
in these tools and have an overview
about the rest of the tools now let's
talk about a generic logical flow where
everything gets automated for seamless
delivery however this flow may vary from
organization to organization as per the
requirement firstly developers develop
the code and this source code is managed
by version control system tools like git
developers send this code to get
repository and any changes made in the
code is committed to this particular
Repository now Jenkins pulls the score
from the repository using the git plugin
and builds it using tools like Anto
Maven configuration management tools
like puppet deploys and Provisions
testing environment and then Jenkins
releases this code on the test
environment on which testing is finally
done using tools like selenium once the
code is tested Jenkins sends it for
deployment on the production server even
the production server is provisioned and
maintained by tools like puppet after
deployment it is continuously monitored
by tools like nagios Docker containers
provide a testing environment to test
the build features I hope how all of
these work together is clear
now let's move on to the next question
what is the difference between
continuous delivery and continuous
deployment
continuous delivery ensures code can be
safely deployed onto production whereas
in continuous deployment every change
that passes the automated test is
deployed to production automatically
continuous delivery also ensures
business applications and servers
function as expected on the other hand
in continuous deployment it makes
software development and the release
process faster and more robust
in continuous delivery it delivers every
change to a production-like environment
through rigorous automated testing but
in continuous deployment there is no
explicit approval from a developer and
requires a developed culture of
monitoring I hope the differences
between continuous delivery and
continuous deployment is clear you can
see so much to the interviewer he's
going to be more than happy
now what are the advantages of devops
now for this answer you can use your
past experience and explain how devops
helped you in your previous job now if
you do not have any such experience then
you can simply mention the technical and
business benefits of devops
the technical benefits of devops is it
allows continuous software delivery
there are also less complex problems to
fix and also there is a faster
resolution of problems the business
benefits on the other hand are it
enables faster delivery of features
there is more stable operating
environments and most importantly
there's more time available to add value
rather than to fix or maintain
what is the most important thing devops
help us to achieve the most important
thing that devops helps us achieve is to
get the changes into production as
quickly as possible while minimizing
risks in software quality assurance and
compilence this is the primary objective
of devops however you can add many other
positive effects of devops for example
clearer communication and better working
relationships between teams that is
worthy operations and development teams
collaborate together to deliver good
quality software which in turn leads to
higher customer satisfaction
moving on explain what the use case
where devops can be used in industry or
real life
there are many industries that are using
devops so you can mention any of these
use cases like Amazon Google Netflix Etc
or you can simply talk about
Etsy
Etsy is a peer-to-peer e-commerce
website focused on handmade or vintage
items and supplies as well as unique
Factory manufactured items
Etsy initially struggled with slow
painful site updates that frequently
caused the site to go down it affected
sales for millions of etsy users who
sold Goods through Online Marketplace
and risks driving them to the competitor
with the help of a new technical
management team
Etsy transitioned from its waterfall
model which produced 4r full site
deployments twice weekly to a more agile
approach today it has a fully automated
deployment Pipeline and its continuous
delivery practices have reportedly
resulted in more than 50 deployments a
day with fewer disruptions so basically
Etsy employed continuous integration
pipeline into their daily deployment
activities and this eventually resulted
in more deployments per day
the next question is explain your
understanding and expertise on both the
software development side and the
technical operations side of an
organization you have worked in the past
now for this answer share your past
experience and try to explain how
flexible you were in your previous job
you can refer this example devops
Engineers almost always work in a 24 bar
7 business critical online environment I
was adaptable to on-call duties and was
available to take up real-time life
system responsibility I successfully
automated processes to support
continuous software deployments I have
experience with public or private clouds
tools like Chef or puppet scripting and
automation with tools like Python and
PHP and also a background in agile
so this is how you can explain your
employer about how you have expertise
and understanding on both the software
development side and the technical
operation side of your organization
moving on how will you approach a
project that needs to implement devops
now the following standard approaches
can be used to implement devops in a
specific project
so in stage one you have to have an
assessment of the existing process and
implementation for about two to three
weeks to identify areas of improvement
so that the team can create a roadmap
for the implementation in stage two you
create a proof of concept once it is
accepted and approved the team can start
on the actual implementation and roll
out of the project plan finally in stage
3 the project is now ready for
implementing devops by using tools like
Version Control tools integration tools
testing tools deployment tools delivery
tools and monitoring tools followed in
each step now by following the proper
steps for Version Control integration
testing deployment delivery and
monitoring the project is now ready for
devops implementation
now let's move on to the second part
that is programming here I'll be
discussing some programs that are
commonly asked in a devops interview
without Much Ado let's check them out
so the first program that is commonly
asked is fistbuzz implementation it is a
very simple programming task where you
have to print the numbers from 1 to 100
and if the number is a multiple of three
then you should print the message Fizz
if it's a multiple of five then it
should print the message buzz if it's a
multiple of both then it should print
the message fizzbuzz this is the logic
of the program it's extremely simple and
commonly asked to so fizzbuzz is one
program that you should be thorough with
as you can see in the output the
multiples of three print quiz the
multiples of five print bus and the
multiples of both five and three that is
15 Prince fizzbuzz I hope this is clear
you can see that they just used simply
an if condition alif condition and then
simply print the message
so one important programming task that
is often asked that is questions based
on trees so the first one is to detect
the mirror of a tree so here you have to
check for two trees to be mirror images
so the following conditions must be two
the first one is their root node scheme
must be the same the second condition is
the left subtree of left tree and right
sub tree of right trade have to be
mirror images and the third condition is
that the right subtree of left tree and
left subtree of right tree have to be
mirror images
so here you have a binary tree node
which is class node and then given two
trees say root 1 and root 2 return true
if there are mirror of each other
now the base case here is if both are
empty then just return true now if both
are non-empty compare them recursively
note that in recursive calls we pass
left of One Tree and right of the other
tree and this function this if condition
will check that then finally we create
the tree and see if both of these are
mirror images of each other if it is
true then it's going to print one if it
is not symmetric then it's going to
print 0. if neither of the above
conditions like two conditions that have
been satisfied if it is true then root 1
and root 2 are not mirror images so then
you can return false now you're creating
the tree here and then just checking if
it is symmetric if it is then it's going
to print one else it's going to print
zero because it is symmetric you can see
here that the node has one and the left
node and right node has 2 2 left to left
has three and left of right has four and
right of left has four and right of
right has three so it is symmetric it's
going to print one and that's your
output
the next common question is finding the
maximum height of a tree so here you
recursively calculate height of left and
right sub trees of a node and assign
height to the node as maximum of the
heights of two children plus one so this
is where the algorithm says and first
thing you'll have to do is you'll have
to firstly create a Constructor to
create a new node and that's what you're
going to initialize initially after that
you compute the maximum depth or the
height of a tree and the number of nodes
along the longest part from the root
node down to the farthest Leaf node then
you simply return the maximum height of
the tree now we're just going to create
a tree and the height will be always
plus one including the root
so this is the driver program to test
the above function and once you create
your tree you can simply print the
height of the node and this says four
which is absolutely right
moving on we have to know another
important program or logic of the
serializing and unserializing a tree
so the question goes by given the route
to a binary Implement serialized root
which serializes the tree into a string
and deserialize which deserializes the
string back into the tree so firstly
you'll have to create the binary tree
using the node class and once that is
done to serialize this tree into a
string you can see the serialized
function here and to serialize it into a
string you can consider the null nodes
as hash starting from root depth first
search is used to Traverse through the
tree now if a node exists its value is
appended to the string and the left
subtree is traversed this process really
continues until there are no more
children left then our hash is appended
to the array and the recursive function
is existed then the right subtree is
traversed and the same process is
followed all over again the final
resulting is string and that is returned
to deserialize a string to form a tree
the first word in the space separated
string is considered as the root when a
hash is encountered it is considered as
an empty node and none is returned in
the remaining cases a new node with a
value as the word is formed and the
remaining string is Towers so this
function is recursively called until the
end of the string is reached
you can see that string after
serializing the node is written and the
assert is true
depth first search is really important
given a binary tree you have to Traverse
it using depth first search using
recursion unlike linear data structures
which have only one logical way to
Traverse them trees can be traversed in
two different ways that is usually depth
first search or breadth first search if
you're into coding I'm sure you must be
very much aware of TFS and BFS
depth first search is basically a
technique used for traversing a
particular triograph here back tracing
is used for traversal that is in this
traversal first the deepest note is
first visited and then back traces to
its parents nodes if no sibling of that
particular node really exists in graphs
usually there are cycles and
disconnectivity unlike graph tree does
not contain cycle and are always
connected so a DFS of a tree is
relatively very easy so as you can see
here firstly we can begin from a node
then Traverse is adjacent or children
without caring about cycles and if we
begin from a single node and Traverse
this way it is guaranteed that we
Traverse the entire tree as there is no
disconnectivity
so you can do this in three different
orders in order traversal or pre-order
traversal or you know even post order
traversal
so the first case is to print in order
in case of a binary search trees in
order traversal gives nodes in a
non-decreasing order to get nodes of
binary search trees in non-decreasing
order a variation of in order traversal
where in order traversals reversed can
be used
the next function is post order
traversal which is useful to get the
post-fix expression of an expression
tree this is simple too
and finally pre-order pre-order
traversal is used to really create a
copy of the tree it is also used to get
the prefix expression of uh expression
tree I hope this is clear you're just
going to print each of the order in
which the binary tree is
so it really traverses the tree in a
particular order post order Travis is
the left tree then the right tree and
then visits the root whereas in in order
Traverse is the left tree then it visits
the route and then traverses the right
sub to you
whereas in pre-order it visits the route
first then traverses the left subtree
and then finally Travis is the right
subject and then it prints accordingly
to that order
so you have a tree of one two three four
five and then accordingly it just
Sprints according to its traversal and
how it moves recursively it prints the
order of the tree
so here you can see that in pre-order
Tower select first traverses the root
and then it traverses the left subtree
that's two and then four and then
finally it will come back to the right
subtree that is then it touches five and
then comes back to three the same way in
order and post order Tower cells is
printed too
on the other hand a level order
traversal of a tree is breadth first
traversal for the tree
so we're going to use Q with the help of
queues we are going to implement breadth
first search here we're going to firstly
create an empty queue for level order
traversal and then we will append the
queue with the root
we will Loop the Q until the length of
the queue is greater than 0 we will Loop
this entire function
and we will print the queues data
enqueue the nodes children first left
then right children DQ a node from q and
finally we will print the level order
traversal of the binary tree
the next important function or program
is the topological sort
topological sorting for directed acyclic
graph is a linear ordering of vertices
such that for every directed Edge UV
vertex U comes before V in the ordering
topological sorting for a graph is not
possible if the graph is not a directed
a cyclic graph
so in depth first search we print to
vertex and then recursively call DFS for
its adjacent vertices but in topological
sorting we need to print a Vertex before
its adjacent vertices that's the
difference between DFS and depth first
search and topological sort we can
modify DFS to find topological sorting
of a graph in DFS we start from a Vertex
we first print it and then recursively
called EFS for its adjacent vertices
into Political sorting we use a
temporary stack we do not print the
vertex immediately we first recursively
call to political sorting for all its
adjacent vertices then push it onto a
stack finally print contents of the
stack it's important to know that a
Vertex is pushed to stack only when all
of its adjacent vertices are already in
the stack so I hope topological sorting
makes sense so what they really do here
is you're really having a function you
have to create a class to represent a
graph then you are adding a function to
add an edge to the graph a recursive
function is called to use the
topological sort so you have to mark the
current node as visited and recursively
visit all the vertices adjacent to that
particular vertex finally you push the
current vertex to stack which stores the
final result
and then you have to write down the
function to do the topological sort that
is you repeatedly call the above
function and then you finally call the
function to do this topological sort you
also should call the recursive function
also you should call the recursive
helper function to store topological
sorting starting from all vertices one
by one then finally you print the
contents of the stack here we are
printing the contents of the stack in a
reverse order so it's 543210
you'll have to create the graph and then
finally call the function which will
eventually print the graph in a reverse
order I hope this is clear
moving on to another important topic
that is sets you'll have to know basic
functions like removing duplicates here
you're defining a function named remove
duplicates first you're going to convert
the array into set and then eventually
into list you're going to define the
array and then print it
so if you know python I'm sure you must
be very aware of this next is to detect
duplicates here we're simply going to
use set which is going to be traversing
through the entire list of elements that
you've mentioned and in this particular
list that you've defined if the count of
the data within the set is repeating or
has occurred more than once then you're
simply going to not include them in your
output
so if the data or the elements within
the list has occurred more than once
then it is simply going to keep a count
of it and going to display that in its
output
strings are another important topic so
in strings it's extremely easy there's
simple programs with the help of slicing
in Python you can reverse a string by
using -1 and it's just going to print
the reverse of a particular string that
you're going to incorporate so the
length of a string can be obtained by
using the Len method You're simply going
to use it with print and then it will
simply print the number of elements in
your string another important program
that you will have to be thorough with
is permutations different kinds of
permutation and combination programs can
be asked in your devops interview a
Python program to print all permutation
using Library functions you can use the
permutations you can import the
permutations library and then use it the
types of permutations that can occur
with the three numbers say one to three
and this will print the different
combinations that can occur with the
three numbers that's one two three
so permutations is another important
topic that you must be aware of a Python
program to print all permutation using
Library functions permutation can be
used if you are very familiar with
python so here as you can see I want the
permutations of three numbers one to
three I'm going to Define a variable and
use the function permutations with three
numbers that is one two three and this
will print out a list of all the
possible permutations of these three
numbers you can see the output so these
are some of the programs even quick sort
merge saw those are really important
you'll have to know the basic functions
of programs that can be asked
I'm not saying that these programs are
compulsively asked in a devops interview
but these are the ones that have
commonly been asked so that's why I have
made a list of all of the programs so I
hope this is clear and let's move on to
the next topic without Much Ado
now let's move on to the third part of
this devops interview questions and
answer session so the first question
that we'll be addressing in the Version
Control is what is Version Control and
its benefits
now Version Control is a system that
records changes to a file or set of
files over time so that you can recall
specific versions later Version Control
Systems consists of a central shared
repository where teammates can commit
changes to a file or set of files then
you can mention the uses of Version
Control Version Control allows you to
revert files back to a previous state it
also allows you to revert the entire
project back to a previous state it can
compare changes over time see who last
modified something that might be causing
a problem also it allows you to check
who introduced an issue and at what time
now let's move on and talk about some of
the benefits of version control system
now with the Version Control System all
the team members are allowed to work
freely on any file at any time it will
later allow you to merge all the changes
into a common version also all the past
versions and variants are neatly packed
up inside the Version Control System
when you need it you can request any
version at any time and you'll have a
snapshot of the complete project right
at hand another benefit is that every
time you save a new version of your
project your version control system
requires you to provide a short
description of what was changed
Additionally you can see what exactly
was changed in a files content this
allows you to know who has made what
changes in the project at watch time
the last benefit is that the distributed
Version Control System like git allows
all the team members to have complete
history of the project so if there is a
breakdown in the central server you can
use any of your teammates local get
Repository
the next question is describe the
various branching strategies you have
used
this question is asked to test your
branching experience to tell them about
how you have used branching in your
previous job and what purpose does it
serve you can refer to the points that
I'll be addressing now the first type of
branching is feature branching a feature
Branch model keeps all of the changes
for a particular feature inside of a
branch when the feature is fully tested
and validated by automated tests the
branch is then merged into Master the
second type of branching is Task
branching in this model each task is
implemented on its own Branch with the
task key included in the branch name it
is easy to see which code implements
which task just look for the task key in
the branch name the third type of
branching is release branching once the
develop branch is acquired enough
features for a release you can clone the
branch to form a release branch
creating this Branch starts the next
release cycle so no new features can be
added after this point only bug fixes
documentation generation and other
release oriented tasks should go in this
Branch once it is ready to ship the
release gets merged into master and
tagged with the version number in
addition it should be merged back into
develop branch which may have progress
since the release was initiated
in the end just mention that branching
strategies vary from one organization to
another so you know the basic branching
operations like delete merge checking
out a branch Etc
the next important question is what is
git
git is a distributed Version Control
System it can track changes to a file
and allows you to revert back to any
particular change its distributed
architecture provides many advantages
over other version control systems like
SVN one major advantage is that it does
not rely on a central server to store
all the versions of a Project's files
instead every developer clones a copy of
a repository with a local repository and
has the full history of the project on
his or her hard drive so that when there
is a server outage all you need for
Recovery is one of your teammates local
get repository there is a central Cloud
repository as well where developers can
commit changes and share it with other
teammates as you can see in the diagram
the distributed Version Control has a
central repository and each of the
working copy or each of the developer
have their own local repositories where
they can make changes to regularly and
then if they want to push any of the
changes to the central repository where
their other teams can look upon it they
can easily do that using git
moving on to the next question in
Version Control how do you push a file
from your local system to the GitHub
repository using git
now firstly you have to connect the
local repository to your remote
repository by using the command git
remote add origin and the web address
the example is given below git remote ad
origin and the copied web address of my
GitHub URL that is https github.com
kavya githubtest.get
secondly you have to push your file into
a remote repository by using the command
git push origin master that is you're
pushing your file into the master branch
the next question is how is a bear
repository different from the standard
way of initializing a git repository you
can either do this by using the standard
method that is by using the git
initialization command git i n i t
and this will allow you to create a
working directory a DOT git subfolders
created with all the git related
revision history the second way of doing
this is by using the bear way that is
get init hyphen hyphen pair it does not
contain any working or checked out a
copy of source file bear repositories
store git revision history in the root
folder of your repository instead of the
dot git subfolder this is the difference
between the standard method and the
barrier so in git how do you revert a
commit that has already been pushed and
made public there can be two answers to
this question so make sure that you
include both of these because they can
be used depending on the situation the
first way is to remove or fix the bad
file in a new commit and push it to a
remote repository this is the most
natural way to fix an error once you've
made the necessary changes to the file
commit it to the remote repository for
that you can use git commit hyphen n and
commit message type in the commit
message
the second way is to create a new Comet
that unders all changes that were made
in the bad commit and to do this you can
use the command git revert name of the
bad commit I hope this is clear
let's move on to the next question what
is git stash now a git developer working
with the current Branch wants to switch
to another Branch to work on something
else but the developer does not want to
commit changes to your unfinished work
the solution to this particular issue is
get stashed git stash takes a modified
tracked files and saves them on a stack
of Unfinished changes that you can
reapply at any time
the next question is what is git bisect
how can you use it to determine the
source of a bug I would suggest you to
First give a small definition of get
bisect get bisect is used to find the
commit that introduced a bug by using
binary search the command for git bisect
is git bisect the sub command and option
commands now since you've mentioned the
command explain what this command will
do this command uses a binary search
algorithm to find which commit in your
Project's history introduced a bug you
use it by first telling it it's a bad
commit that is known to contain the bug
and a good commit that is known to be
before the bug was introduced then get
bisect picks a commit between these two
endpoints and asks you whether the
selected commit is good or bad it
continues narrowing down the range until
it finds the exact commit that
introduced the change I hope git bisect
and how it is used to determine a bug is
easy and understandable
moving on what is gitry base and how can
it be used to resolve conflicts in a
feature Branch before much
according to me you should start by
saying gitry base is a command which
will allow you to merge another Branch
into the branch where you are currently
walking on and move all of the local
commits that are ahead of the rebase
branch to the top of the history of that
particular Branch now that you've
defined what gitry base is Give an
example or explain with an example to
show how it can be used to resolve
conflicts in a feature Branch before
March
now since you have defined Gatorade base
now explain it with an example if a
feature Branch was created from master
and since then the master branch has
received new commits gitrebase can be
used to move the feature Branch to the
tip of Master the command effectively
will replay the changes made in the
feature Branch at the tip of Master
allowing configs to be resolved in the
process when done with care this will
allow the feature Branch to be merged
into Master with relative ease and
sometimes as a simple fast forward
operation
how do you find a list of files that is
changed in a particular commit
for this answer instead of just telling
the command explain what exactly this
command will do so you can say that to
get a list files that has changed in a
particular commit use the command git d
i f f hyphen tree space hyphen R hash
to get a list files that has changed in
a particular commit use command git diff
3 hyphen R and the hash the particular
hash given the commit hash this will
list all the files that we change or
added in that particular commit The
Hyphen R flag makes the command list
individual files rather than collapsing
them into root directory names only you
can also say that the output will also
include some extra information which can
be easily suppressed by including two
flags git diff tree hyphen no commit ID
hyphen name only hyphen R hash here
hyphen no commit ID will suppress the
commit hashes from appearing in the
output and hyphen name only will only
print the file names instead of the
parts
I hope this is clear
moving on how do you set up a script to
run every time a repository receives new
commits through push
there are three ways to configure a
script to run every time a repository
receives new commits through push one
needs to Define either a pre-receive
update or a post receive hook depending
on when exactly the script needs to be
triggered pre-receive hook in the
destination repository is invoked when
commits are push to it any script bound
to this hook will be executed before any
references are updated this is a very
useful hook to run scripts that help
enforce development policies update hook
Works in a very similar manner to
pre-receive hook and is also triggered
before any updates are actually made
however the update Hook is called once
for every commit that has been pushed to
the destination repository finally a
post receive hook in the repository is
invoked after the updates have been
accepted into the destination repository
this is an ideal place to configure
simple deployment scripts invoke some
continuous integration systems dispatch
notification emails to repository
maintainers Etc
hooks are local to every git
repositories and are not versioned
scripts can either be created within the
hooks directory inside the dot kit
directory or they can be created
elsewhere and links to those scripts can
be placed within the directory
now let us look at continuous
integration interview questions
the first one is what is meant by
continuous integration
continuous integration is a development
practice that requires developers to
integrate code into a shared repository
several times a day each check-in is
then verified by an automated build
allowing teams to detect problems early
I also suggest that you explain how you
have implemented this in your previous
job you can refer to the diagram that is
given here here as you can see
developers first check out code into the
private workspaces and then when they
are done with it they commit the changes
to the shared repository that is the
Version Control Repository
then the continuous integration server
monitors the repository and checks out
changes when they occur The Continuous
integration server then pulls these
changes and builds the system and also
runs unit and integration tests
after this The Continuous integration
server will now inform the team of the
successful build if the build or test
fails The Continuous integration server
will alert the team the team will then
try to fix the issue at the earliest
opportunity this process keeps on
repeating and this is the entire
continuous integration pipeline in short
moving on why do you need a continuous
integration of development and testing
for this answer you should focus on the
need of continuous integration my
suggestion would be to mention the most
two important points firstly continuous
integration of development and testing
improves the quality of software and
reduces the time taken to deliver it by
replacing the traditional practice of
testing after completing all of the
development it also allows development
team to easily detect and locate
problems early because developers need
to integrate code into a shared
repository several times a day that is
more frequently each check-in is then
automatically tested
moving on to the third question what are
the success factors for continuous
integration here you have to mention the
requirements for continuous integration
you should include the following points
that is continuous integration allows
you to maintain a code repository it
automates the build makes the build
self-testing in continuous integration
everyone commits to the Baseline every
single day every commit that is made to
the Baseline should be built keep the
build fast
it is important to test in a clone of
the production environment it also makes
it easy to get to the latest
deliverables everyone can see the
results of the latest bills and finally
it automates deployment
the fourth question explain how you can
move or copy Jenkins from one server to
another approach this task by copying
the jobs directory from the old server
to the new one there are multiple ways
to do that you can firstly move a job
from one installation of Jenkins to
another by simply copying the
corresponding job directory you can also
make a copy of an existing job by making
a clone of a job directory by a
different name you can also rename an
existing job by renaming a directory it
is also important to keep in mind that
if you change a job name you will need
to change any other job that tries to
call the rename job
in the fifth question explain how can
you create a backup and copy files in
Jenkins the answer to this question is
really direct to create a backup all you
need to do is to periodically backup
your Jenkins home directory this
contains all of your build jobs
configurations your slave node
configurations and your build history to
create a backup of your Jenkins setup
just copy this directory you can also
copy a job directory to clone or
replicate a job or rename the directory
I hope this is clear now let's move on
to the sixth question of the session
explain how you can set up Jenkins job
my approach to this answer will be first
to mention how to create Jenkins job go
to Jenkins top page select new job then
choose build a freestyle software
project then you can tell the elements
of this freestyle job firstly optional
SCM such as CVS or subversion by your
source code resides optional triggers to
control when Jenkins will perform bills
some sort of build script that performs
the build where the real work really
happens optional steps to collect
information out of the build such as
archiving The artifacts and or recording
Java dock in test results optional steps
to notify other people or systems with
the build result such as sending emails
IMS updating issue trackers Etc
moving on mention some of the useful
plugins in Jenkins some of the useful
important plugins are made into project
Amazon ec2 HTML publisher copy artifact
join green balls these plugins a feel
are the most useful plugins if you want
to include any other plugins that is not
mentioned you can add them as well but
make sure you first mention the plugins
that are on the screen right now
the next important question is how will
you secure Jenkins if you have any other
ways other than the one that I'm just
going to talk about please mention it
so the standard way to secure Jenkins is
firstly to ensure that the global
security is on ensure that Jenkins is
integrated with your company's user
directory with appropriate plugin also
ensure that the Matrix of project Matrix
is enabled to fine-tune access then
automate the process of setting rights
of Privileges and Jenkins with custom
Version Control skipped limit physical
access to Jenkins data or folders
periodically Run Security audits on same
Jenkins is one of the most popular tools
that are used extensively in devops
there are many other ways to secure
Jenkins if you have any other way you
could just explain that or you could
just explain the standard method
now let's move on to the continuous
testing questions
first question is what is continuous
testing continuous testing is the
process of executing automated tests as
part of the software delivery pipeline
to obtain immediate feedback on the
business risks associated with in the
latest build in this way each build is
tested continuously allowing development
teams to get fast feedback so that they
can prevent those problems from
progressing to the next stage of
software delivery life cycle this
dramatically speeds up a developer's
workflow as there's no need to manually
rebuild the project and rerun all tests
after making changes
moving on to the next question what is
automation testing what are the benefits
of automation testing automation testing
or test automation is a process of
automating the manual process to test
the application or system under test
automation testing involves use of
separate testing tools it involves use
of separate testing tools which lets you
create test scripts which can be
executed repeatedly and does not require
any manual intervention
some of the benefits of automation
testing are it suppose execution of
repeated test cases it aids in testing a
large test Matrix it also enables
parallel execution encourages unattended
execution improves accuracy thereby
reducing human generated errors and
finally saves money and time
the next question is how to automate
testing in devops life cycle
in devops developers are required to
commit all the changes made in the
source code to a shared repository
continuous integration tools like
Jenkins will pull the code from the
shared repository every time A change is
made in the code and eventually deploy
it for continuous testing that is done
by tools like selenium in this way any
change in the code is continuously
tested unlike the traditional approach
let's move on and understand what are
the key elements of continuous testing
tools
the key elements of continuous testing
tools are risk assessment policy
analysis requirements traceability
Advanced analysis test optimization and
service virtualization
in risk assessment it covers risk
mitigation tasks technical depth quality
assessment and test coverage
optimization to ensure the build is
ready to progress towards next stage
whereas in policy analysis it ensures
all processes aligned with the
organization's evolving business and
compliance demands are met in
requirements traceability it ensures
true requirements are met and rework is
not required an object assessment is
used to identify which requirements are
at risk working as expected or require
further validation also in advanced
analysis it uses Automation in areas
such as static code analysis change
impact analysis and scope assessment to
prioritization to prevent defects in the
first place and accomplishing more
within each iteration in test
optimization it ensures test yield
accurate outcomes and provide actionable
findings aspects include test data
management test optimization management
and test maintenance service
virtualization ensures access to
real-world testing environments so it
basically enables access to the virtual
form of the required testing stages
cutting waste time to test environment
setup and availability
moving on to the next question which
testing tool are you comfortable with
and what are the benefits of that tool
here mentioned the testing tool that you
have worked with and accordingly frame
your answer I will mention an example
with selenium I have worked on selenium
to ensure high quality and more frequent
releases some advantages of selenium are
that it is free and open source it also
has a large user base that helps
communities the major advantage is that
it has cross browser compatibility that
is Firefox Chrome Internet Explorer
Safari Etc it also has great platform
compatibility like in Windows Mac Linux
Etc it supports multiple programming
languages like Java c-sharp Ruby python
Perl Etc it also has fresh and regular
repository developments and finally the
most important point it supports
distributed testing
moving on to the next question what are
the testing types supported by selenium
selenium supports two types of testing
the first one being regression testing
it is the act of retesting a product
around an area where a bug was fixed the
next one is functional testing it refers
to the testing of software features
individually
the next question is what is selenium
IDE it is an integrated development
environment for selenium scripts it is
implemented as a Firefox extension and
allows you to record edit and debug
tests selenium IDE includes the entire
selenium core allowing you to easily and
quickly record and play back tests in
the actual environment that they will
run in now include some advantages to in
your answer with autocomplete support
and the ability to move commands around
quickly selenium IDE is the ideal
environment for creating selenium tests
no matter what style of tests you prefer
moving on to the next question what is
the difference between assert and verify
commands in selenium first we will talk
about assert command asset command
checks whether the given condition is
true or false let's say we assert
whether the given element is present on
the web page or not if the condition is
true then the program control will
execute the next test step but if the
condition is false the execution would
stop and no further test would be
executed the verify command also checks
whether the given condition is true or
false irrespective of the condition
being true or false the program
execution does not halt that is any
failure during verification would not
stop the execution and all the test
steps would be executed
assert command is best used when the
check value has to pass the test to be
able to continue to run whereas verify
command is used to check non-critical
things
have to launch browser using Webdriver
the following syntax can be used to
launch browser you can use the Command
Web driver driver is equal to new
Firefox driver function or web driver
driver is equal to new Chrome driver
function or you can also use web driver
driver is equal to new Internet Explorer
driver function
moving on to the last question of
continuous testing
when should one use selenium grid for
this answer my suggestion would be to
give a small definition of selenium grid
it can be used to execute same or
different test scripts on multiple
platforms and browsers concurrently to
achieve distributed test execution
so this eventually allows testing under
different environments and saving
execution time remarkably
now let's move on and check out how much
you know about configuration management
what are the goals of configuration
management process
the purpose of configuration management
is to ensure the Integrity of a product
or system throughout its life cycle by
making the development or deployment
process controllable and repeatable
therefore creating a higher quality
product or system the configuration
management process allows orderly
management of system information and
system changes for purposes such as to
revise capability improve performance
reliability or maintainability extant
life reduce cost reduce risk and
liability or correct defects let's move
on to the next question that is what is
the difference between asset management
and configuration management
asset management is concerned with
finances whereas configuration
management is concerned with operations
scope is everything you own in asset
management but scope is everything you
deploy in configuration Management in
Asset Management it is interfaces to
purchasing and leasing whereas in
configuration managements it is
interfaces to ITIL processes in Asset
Management you can maintain data for
taxes but in configuration management
you maintain data for troubleshooting
lifecycle is from purchase to disposal
in Asset Management whereas in
configuration management lifecycle is
from deployment to retirement
in Asset Management relationships are
only incidental whereas in configuration
management relationships are all
operational
moving on to the third question what do
you understand by infrastructure s code
infrastructure s code is a type of it
infrastructure that operations teams can
use to automatically manage and
provision through code rather than using
a manual process companies for faster
deployments treat infrastructure like
software as code that can be managed
with the devops tools and processes
these tools basically let you make
infrastructure changes more easily
rapidly safely and reliably
moving on to the most important question
which among puppet chefs saw stack and
ansible is the best configuration
management tool and why this depends on
the organization's need so mention few
points on all of these tools puppet is
the oldest and most mature configuration
management tool puppet is a ruby based
configuration management tool but while
it has some free features much of what
makes puppet great is only available in
the paid version organizations that
don't need a lot of extras will find
puppet useful but those needing more
customization will probably need to
upgrade to the paid version on the other
hand Chef is written in Ruby so it can
be customized by those who know the
language it also includes free features
plus it can be upgraded from open source
to Enterprise level if necessary on top
of that it is a very flexible product
ansible is a very secure option since it
uses secure shell it is a simple tool to
use but it does offer a number of other
services in addition to configuration
management it is very easy to learn so
it is perfect for those who do not have
a dedicated IT staff but still need a
configuration management tool
salt stack is python-based open source
configuration management tool made for
larger businesses but its learning curve
is fairly low based on all of these
tools and which you're comfortable with
you can just mention which is the best
tool and why would you want to use it
the next question is what is puppet
puppet is a configuration management
tool which is used to automate
Administration tasks now you should
describe its architecture and how puppet
manages its agents puppet has a Master
Slave architecture in which the slave
has to First send a certificate signing
request to master and master has to sign
that certificate in order to establish a
secure connection between puppet master
and puppet slave as you can see in the
diagram puppet slave sends request to
puppet master and Puppet Master then
pushes configuration on slave
in the sixth question before a client
can authenticate with the Puppet Master
its certificates need to be signed and
accepted how will you automate this task
the easiest way to do this is to enable
auto signing in puppet.config it is
definitely a security risk but if you
still want to do this you can firewall
your puppet master that is restrict Port
TCP 8140 to only networks that you trust
create Puppet Masters for each trust
Zone and only include the trusted nodes
in that Puppet Masters manifest never
use a full wildcard such as asterisk
moving on to the seventh question which
open source of community tools do you
use to make puppet more powerful over
here you need to mention the tools and
how you have used these tools to make
puppet more powerful for example changes
and requests are ticketed through jira
and we manage requests through an
internal process then we use git and
puppets code Manager application to
manage puppet code in accordance with
the best practices additionally we run
all of our puppet changes through our
continuous integration pipeline in
Jenkins using the beaker testing
framework
the next important question is what are
puppet manifests it is a very important
question so make sure all of this is set
in the correct flow according to me you
should first Define manifests every node
or puppet agent has got its
configuration details in Puppet Master
written in the native puppet language
these details are written in the
language which puppet can understand and
are termed as manifests they are
composed of puppet code and the file
names use the pp extension Now with an
example you can write a manifest in
puppet master that creates a file and
installs Apache on all puppet agents or
slaves connected to the Puppet Master
moving on what is puppet module and how
it is different from Puppet manifest
a puppet module is a collection of
manifests and data such as fax files and
templates and they have a specific
directory structure modules are useful
for organizing your puppet code because
they allow you to split your code into
multiple manifests it is considered best
practice to use modules to organize
almost all of your puppet manifests
puppet programs are called manifests
which are composed of puppet code and
the file names use the pp extension
moving on to the 10th question what is
factor in puppet
Factor gathers basic information that is
facts about puppet agent such as
Hardware details network settings
operating system type and version IP
addresses Mac addresses SSH keys and
many more these facts are then made
available in Puppet Masters manifests as
variables
what is Chef
Chef is a very powerful automation
platform that transforms infrastructure
into code Chef is a tool for which you
write scripts that are used to automate
processes what processes pretty much
anything that is related to it now you
can explain the architecture of Chef it
consists of Chef server Chef node and
Chef workstation the chef server is the
central store of your infrastructures
configuration data the chef server
stores the data necessary to configure
your nodes and provide search a powerful
tool that allows you to dynamically
drive a node configuration Based on data
a node on the other hand is any host
that is configured using Chef client
Chef client runs on your nodes
contacting the chef server for the
information necessary to configure the
node since a node is a machine that runs
the chef client software nodes are
sometimes referred to as clients
finally a chef workstation is the host
you use to modify your cookbooks and
other configuration data
in the 12th question we're going to
discuss what is a resource in chef a
resource represents a piece of
infrastructure and its desired State
such as a package that should be
installed a service that should be
running or a file that should be
generated
the functions of resource are it
describes the desired state for a
configuration item it also declares the
steps needed to bring that item to the
desired state it specifies a resource
types just package template or service
it also lists additional details also
which are known as resource properties
whenever it's necessary they are also
grouped into recipes which describe
working configurations
what do you mean by recipe in Chef a
recipe is a collection of resources that
describes a particular configuration or
policy so a recipe describes everything
that is required to configure part of a
system
some of the functions of recipes are it
helps in installing and configuring
software components it manages files it
deploys applications and it also
executes other recipes
how does a cookbook differ from a recipe
in chef a recipe is a collection of
resources and primarily configures a
software package or some piece of
infrastructure
on the other hand a cookbook groups
together recipes and other information
in a way that is way more manageable
than having just recipes alone
what happens when you do not specify a
resource action in chef
here Chef really just applies the
default action you can explain this with
an example the first snippet is the same
as the below resource because create is
the file resource default action
what is ansible module modules are
considered to be the units of work in
ansible each module is mostly Standalone
and can be written in a standard
scripting language such as python Perl
Ruby bash Etc
one of the guiding properties of modules
is item potency which means that even if
an operation is repeated multiple times
example upon recovery from an outage it
will always place the system into the
same state
now what are playbooks and ansible
playbooks are ansible's configuration
deployment and orchestration language
they can describe a policy you want your
remote systems to enforce or a set of
steps in a general ID process playbooks
are designed to be human readable and
are developed in a basic text language
at a basic level playbooks can be used
to manage configurations of and
deployments to remote machines in the
picture on the screen you can see an
example of playbooks in ansible
how do I see a list of all the ansible
variables ansible by default gathers
facts about the machines under
management and these facts can be
accessed in playbooks and in templates
to see a list of all the facts that are
available about a machine you can run
the setup module as an ad hoc action
that is ansible hyphen M setup host name
this will print out a dictionary of all
the facts that are available for that
particular host
how do I turn the auto deployment
feature off
the auto deployment feature checks the
applications folder every three seconds
to determine whether there are any new
applications or changes to the existing
applications and then dynamically
deploys these changes the auto
deployment feature is enabled for
servers that run in development mode to
disable auto deployment feature use one
of the following methods to play servers
in production mode that is in the
administration control click the name of
the domain in the left pane then click
the production mode checkbox in the
right pane at the command line include
the following argument when starting the
domains Administration server that is
hyphen capital D weblogic dot production
mode enabled is equal to true
also the production mode is set for all
weblogic server instances in a given
domain
the last question in configuration
management is when should I use the
external stage option set external stage
using weblogic dot deployer if you want
to Stage the application yourself and
prefer to copy it to its Target by your
own means
now let's test your knowledge on
continuous monitoring
the first question is why is continuous
monitoring necessary
continuous monitoring allows timely
identification of problems or weaknesses
and quick corrective action that helps
reduce expenses of an organization
continuous monitoring provides solution
that addresses three operational
disciplines known as continuous audit
continuous controls monitoring and
continuous transaction inspection
the next really important question is
what is nagios
is one of the monitoring control tools
it is used for continuous monitoring of
systems applications services and
business processes Etc
it is used for continuous monitoring of
systems applications services and
business processes in a devops culture
in the event of a failure Legos can
alert technical staff of the problem
allowing them to begin remediation
processes before outages affect business
processes end users or customers with
this tool you do not have to explain why
an unseen infrastructure outage affect
your organization's bottom line now once
you've defined what is negios you can
mention the various things that you can
achieve using this tool you can plan for
infrastructure upgrades before outdated
systems cause failures they can also
respond to issues at the first sign of a
problem you can also automatically fix
problems when they are detected
coordinate technical team responses and
also ensure your organization's slas are
being met
how does negeus work the GEOS runs on a
server usually as a Daemon or service
the GEOS periodically runs plugins
residing on the same server the contact
hosts or servers on your network or on
the Internet one can view the status
information using the web interface you
can also receive email or SMS
notification if something happens the
Daemon behaves like a scheduler that
runs certain scripts at certain moments
it stores the results of the scripts and
will run other scripts if these results
change now expect a few question on a
gears components like plugins nrpe Etc
what are plugins in lagues they are
scripts like Perl script shell scripts
Etc that can run from a command line to
check the status of a host or service
nagios uses the results from plugins to
determine the current statuses of hosts
and services on your network once you
have defined plugins explain why you
need it nagios will execute a plugin
Whenever there is a need to check the
status of a host or service plugin will
perform the check and then simply
Returns the result to negios now GEOS
will process the results that it
receives from the plugin and take
necessary actions
the next important question is what is
nrpe interviews that is negus remote
plugin executor
the nrpe add-on is designed to allow you
to execute and reduce plugins on remote
Linux or Unix machines the main reason
for doing this is to allow no GEOS to
monitor local resources like CPU load
memory usage Etc on remote machines
since these public resources are not
usually exposed to external machines an
agent like nrpe must be installed on the
remote Linux or unox machines
I advise you to explain the nrpe
architecture basis on the diagram that
is given here the nrpe add-on consists
of two pieces the checked nrpe plugin
which resides on the local monitoring
machine and the nrpe Daemon which runs
on the remote Linux or unox machine
there is also a SSL connection that is a
secure socket layer between monitoring
host and remote hosts as seen in the
diagram
what do you mean by passive check in the
GEOS passive checks are usually
initiated and performed by external
applications or processes also the
results are submitted to nug use for
processing they are useful for
monitoring services that are
asynchronous in nature and cannot be
monitored effectively by polling their
status on a regularly scheduled basis
they can also be used for monitoring
services that are located behind a
firewall and cannot be checked actively
from the monitoring host
the next question is when does negios
check for external commands
negios checks for external commands
under the following conditions at
regular intervals specified by the
command check interval option in the
main configuration file or immediately
after event handlers are executed this
is in addition to the regular cycle of
external command checks and is done to
provide immediate action if any event
handler submits commands to negios
moving on what is the difference between
active and passive check in negios
for this answer first point out the
basic difference between active and
passive checks the major difference
between active and passive checks is
that active checks are initiated and
performed by nagios while the passive
checks are performed by external
applications if your interviewer is
looking unconvinced with the above
explanation then you can also mention
some key features of both active and
passive checks passive checks are useful
for monitoring services that are
asynchronous in nature and cannot be
monitored effectively by polling their
status on a regularly scheduled basis
they're usually located behind a
firewall and cannot be checked actively
from the monitoring host the main
features of active checks are that they
are initiated by the negus process
active checks are run on a regularly
scheduled basis
I hope the differences between active
and passive checks is clear and as you
can see on the screen these are the two
different pictures of how active and
passive checks really work
the ninth question is how does negios
help with distributed monitoring the
interviewer will be expecting an answer
related to the distributed architecture
of nagios so I suggest that you answer
it by explaining that first but in a
GEOS you can monitor your whole
Enterprise by using a distributed
monitoring scheme in which local slave
instances of negios perform monitoring
tasks and Report the results back to a
single Master you manage all
configuration notification and Reporting
from the master while the slaves do all
the work this design takes advantage of
niger's ability to utilize passive
checks that is external applications or
processes that send results back to
nucleos in a distributed configuration
these external applications are other
instances of nucleos
moving on explain main configuration
file of negios and its location the main
configuration file contains a number of
directives that affect how the negus
daemin operates this config file is read
by both the nagios Daemon and the cgis
it basically specifies the location of
your main configuration file now you can
tell where it is present and how it is
created a sample main configuration file
is created in the base directory of the
nagios distribution when you run the
configure script the default name of the
main configuration file is
nagios.cfg it is usually placed in the
ETC subdirectory of your nugios
installation
explain how flap detection works in the
GEOS
clapping occurs when a servicer hosts
changes state too frequently this causes
a lot of problem and Recovery
notifications once you have defined
flapping explain how nag use detects
flapping whenever an abuse checks the
status of a Hostess service it will
check to see if it has started to stop
flapping the GEOS follows a procedure to
do that firstly it stores the results of
the last 21 checks of the hostess
service analyzing the historical check
results and determines where State
changes or transitions occur using the
state transitions to determine a
person's State change value for the host
service finally it will compare the
percent State change value against slow
and high flapping thresholds a host of
services determined to have started
flapping when its person State change
first exceeds a high flapping threshold
a host of service is determined to have
stopped flapping when its person State
goes below a low flapping threshold
now what are the three main variables
that affect recursion and inheritance in
the GEOS
name user and register name is a
placeholder that is used by other
objects use defines the parent object
whose property should be used register
can have a value of 0 indicating it's
only a template and one which is an
actual object it's important to know
that the register value is never
inherited I hope the three main
variables that affect inheritance and
recursion in a GEOS is clear moving on
let us understand what is meant by
saying reduce is object oriented
one of the features of nagios is object
configuration format in that you can
create object definitions that inherit
properties from other object definitions
and hence the name this basically
simplifies and clarifies relationships
between various components
now the last question in continuous
monitoring is what is State stalking in
a GEOS
State stalking is used for logging
purposes when stalking is enabled for a
particular host or service nagios will
watch that Hostess service very
carefully and log any changes it sees in
the output of check results
it can be very helpful in later analysis
of the log files under normal
circumstances the result of a Hostess
service check is only locked if the
hostess service has changed State since
it was last checked
now we're moving on to the last topic of
today's session and let's see how much
you know about containers and virtual
machines
so what are containers containers are
used to provide consistent Computing
environment from a developer's laptop to
a test environment from a staging
environment into production
a container consists of an entire
runtime environment and application plus
all its dependencies libraries and other
binaries and configuration files needed
to run it bundled into one package
containerizing the application platform
and its dependencies removes the
differences in operating system
distributions and underlying
infrastructure
now let's move on to the second question
and discuss what are the advantages that
containerization provides over
virtualization
continuous provide real-time
provisioning and scalability but virtual
machines provide slow provisioning
containers are also lightweight when
compared to Virtual machines virtual
machines have limited performance when
compared to Containers containers have
better resource utilization compared to
Virtual machines these are some of the
advantages that containerization
provides over virtualization
moving on how exactly are containers
that is Docker in our case different
from hypervisor virtualization what are
the benefits
hypervisor virtualization in default
security support are to a great degree
but containers have a slightly less
degree over here in a hypervisor
virtualization the complete operating
system plus applications are in memory
on disk whereas in containers only the
application requirements are on disk
time taken to Startup is substantially
longer in hypervisor virtualized
