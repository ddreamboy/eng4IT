[Music]
welcome back so now we're going to talk
about how you can use the singular value
decomposition to compute the principal
component analysis or PCA PCA is the
bedrock dimensionality reduction
technique for probability and statistics
and it's still very very commonly used
in data science and machine learning
applications when you have big data that
might have some statistical distribution
and you want to uncover the low
dimensional patterns to build models off
of it
PCA has been around for a long time
since 1901 pearson paper okay so it's a
really really old method very
well-established ton of theory about the
statistics of this and I'm going to
essentially pigeonhole this as what
we're gonna call the statistical
interpretation interpretation of the SVD
and principal component analysis is in
particular going to provide us with a
data-driven hierarchical coordinate
system so it gives us a hierarchical
high or that's not how you spell
hierarchical a hierarchical coordinate
system hierarchical coordinate system
based on data to represent the
statistical variations in your data sets
okay so it's a coordinate system based
in terms of directions in your data that
captured the maximum amounts of the
variance in your data okay and so I'm
going to the notation here is gonna be a
little different than what we're used to
and that's because the PCA literature
and the SVD literature how kind of
different conventions about what the
matrix looks like so in the PCA
literature we still have a data matrix X
and it still has a bunch of measurements
from experiments independent experiments
but here we're going to represent those
independent experiments as big row
factors x1 x2 and so on and so forth
okay so each each row vector X are
essentially measurements from a single
experiment measurements from a single
experiment and what we're hoping is that
these are kind of individual experiments
so this measurement might be the
demographic information from a specific
human you know age weight sex race etc
etc etc and then X 1 would be person 1 X
2 is person 2 and so on and so forth so
the same basic idea as before with our
data matrix X except now instead of
columns having the information for a
specific individual kind of that that
measurement from a single experiment now
we're gonna have those B rows and that's
because that's consistent with the PCA
literature so I just want to show you
how it looks this way okay and the idea
here is that we're going to try to find
and we're going to assume that this data
X has some statistical distribution it's
not deterministic there are some
statistical variability to this
information and we're going to try to
uncover the dominant kind of
combinations of features that that
describe as much of the data as possible
okay so we're gonna do it using the SVD
but we're gonna write it a little bit
differently okay so because this is the
statistical interpretation of the SVD
there's a few steps that are extra that
we don't normally do with SVD that we're
gonna do here so kind of step one in
this procedure is that we're gonna
compute the mean the row-wise mean the
average row so we're gonna compute the
mean row and we're just gonna call this
X bar equals one over I'm gonna say that
I still have n rows 1 over n sum of each
of these X J's from J equals 1 to n ok
so this is the average row I just
average all of the rows
the next thing I'm going to do is I'm
going to build an average matrix so the
average matrix is going to be obtained
by taking a vector of ones and
multiplying it by that X bar vector okay
so I just literally create n copies of X
bar and that's my X bar average matrix
and so step two is I'm going to subtract
the mean from my data matrix okay so I'm
going to subtract the mean so now I have
B equals X minus X bar and the way we
say this in in PCA language is that this
is the mean centered data so if I have
some distribution of data where there's
some you know average value to all of
this when I subtract that out it brings
everything down so that the center of my
distribution is at the origin okay so
we're going to be modeling this data
matrix X assuming that it is a zero mean
Gaussian and so this is where we we
subtract off the mean so that it's zero
mean okay now what we're gonna do is
we're going to compute the covariance
matrix of this mean Center data so the
covariance matrix again this is kind of
just the correlation matrix from the SVD
but we're calling it a covariance matrix
in this context of the covariance matrix
of the rows of B and we're going to call
that matrix C C is equal to B transpose
B okay good so at this point all we've
done is essentially take our data matrix
we've written it in a transpose from how
we normally do it we've subtracted off
the mean and we've computed this
correlation matrix or this covariance
matrix so this looks a lot like the X
transpose X from before but we've
subtracted off the mean now what we're
going to do is we're going to compute
the eigenvectors the leading
eigenvectors of this correlation matrix
and that's going to be related
both to the singular vectors of X and
also to its principal components okay
and I'm gonna try my best to get the
notation and the terminology correct
again this is in section 1.5 in our book
so you can go there to refer for more
details good so now what we're going to
do is we're going to compute the eigen
decomposition we're going to compute the
eigenvalues and eigenvectors so the Ides
of C and in particular we're going to
compute for example let's call it V 1
transpose B transpose B V 1 that would
be the biggest eigenvector of this
matrix B transpose B is V 1 then I would
compute V 2 then V 3 then V 4 and so on
and so forth just like in the SVD and
there's corresponding eigenvalues just
like in the SVD and essentially what
we're gonna get is this matrix C times V
equals V times D where these are my
eigen values and these are my eigen
vectors okay good so all we've done is
we've computed the singular sorry the
eigen value decomposition of this
covariance matrix that you could
actually compute it using the SVD I'll
show you that in a minute
and you get these eigen values and eigen
vectors and here's where the principal
components come in so if I take this
matrix T which is equal to my my mean
subtracted data B times these eigen
vectors V these are called my principal
components these are my principal
components okay principal components and
this vector V of these eigen vectors are
called the loadings so essentially what
you do is you decompose this matrix into
kind of directions of maximal variance
just like in the singular value
decomposition called the principal
components
and the loadings are kind of how much of
each of those principle components each
of each of the experiments has the
loadings in a particular experiment of
those principal components columns okay
and oftentimes in terms of the singular
value decomposition language so let's
say X was equal to u Sigma V transpose
here then what we would say is that T is
simply equal to u times Sigma okay
because and if I write B as sorry I
should be a little bit more careful if I
took the singular value decomposition of
the mean subtracted data B equals u
Sigma V transpose then T would be either
B times V and B times V is simply u
times Sigma okay so these are also a
representation of the principal
components okay so you can get the
principal components and the loadings
directly from the SVD of the mean
subtracted data I guess that's the
headline here is that this very
important statistical representation of
your data can be achieved just by
computing the SVD of your mean
subtracted data it's the same is finding
these eigen vectors of the covariance
matrix which is what you would kind of
do in the classical principal component
analysis good ok now what's also
important is these eigen values here or
the singular values in Sigma give you an
indication of the amount of the variance
of this data set that these principal
components capture or these loadings
capture so if I only want to describe
this high dimensional data in terms of
the first two principal components or
the and the first two vectors of
loadings I would be able to compute how
much of the variance is captured by
computing kind of how much energy or
variances in those first two eigen
values of this this D matrix and so what
I could literally do I want to make sure
I'm being careful here these eigen
values lambda are equal to the square of
the singular values
it's literally equal to the variance of
that principal component in the data and
so if I want to know how much variance
is being captured in the first let's say
four modes I would take the sum from k
equals 1 to R of lambda K divided by the
sum of all n of my lambdas so I would
basically see how what's the fraction of
variance captured by my first are
lambdas divided by the total of all of
my eigen values all of the variants in
the data and so for example I might
decide to keep only as many principal
components as are needed to explain 95%
of the variance and so that would give
you a criterion for how many principal
components to keep okay again we're
going to code this up I'm actually going
to create a couple of data matrices real
data matrices that have distributions
one of them will be a random data matrix
another one will be a data matrix
consisting of genetic markers for people
with and without ovarian cancer and
we'll compute this principal component
analysis and look at the results so I'll
point out that again in MATLAB it's
pretty easy to compute this so it's
something very simple like V score and
then some extra variable s 2 equals PCA
of the B matrix okay so really really
easy to compute in MATLAB also easy to
compute in R and Python and so we'll do
examples of that the last thing I want
to do is just show you a picture so
again if I have data that is you know
some high dimensional data and it has
some distribution you're hoping that it
has some Gaussian kind of white noise
distribution or some some normal
distribution not drawing a great normal
distribution here then what the
principal component analysis is going to
do is it's essentially going to find
these ellipsoids of maximal variance in
terms of 1 & 2 & 3 standard deviations
so you can actually quantify if I have a
new point how
is it given the distribution of the old
points and not only that but that prints
the singular value decomposition in the
principal component analysis will tell
you exactly what directions are account
for the most variance the second most
variants the third most variance in the
data and so on and so forth okay so a
very useful statistical technique to
build models statistical models from
your data okay thank you
