hi in this video let's look at how to
use various programming languages or
frameworks in Hadoop applications in
Hadoop can be written in a wide range of
languages depending on the use case the
developer can choose an appropriate
processing engine as well as an
appropriate programming language some
problems may require a procedural
approach or a functional approach
scripts in a high-level languages such
as speak and hi are automatically
combined or compiled into a set of
MapReduce Java classes by the respective
engines which are then executed on the
yarn framework across a cluster yarn is
yet another resource negotiator which
we'll look in the next video pig is a
simple procedural scripting language in
which the developer defines how data is
transformed from one step to the next
hive uses high query language or in
short hql which is similar to the SQL we
will talk about more on Pig and hi in a
various different videos as the
developer can choose an appropriate
processing engine this gives the
developer a greater flexibility in
dealing with complex problems tools such
as storm and spark are often used for
processing data intensive applications
which may involve interactive or
analytical approaches these engines
provides support for multiple
programming languages now let's quickly
look at different types of processing
frameworks we have in Hadoop starting
with batch processing so batch
processing is used to process data in
batches and it reads data input
processes it and writes it to the output
Apache Hadoop is a most well-known and
popular open source implementation of
batch processing and a distributed
system
using the MapReduce paradigm the data is
stored in a shared and distributed file
system called her HDFS or Hadoop
distributed file system where the day of
data is divided into multiple splits
which are the logical data divisions for
the MapReduce processing to process
these splits using the MapReduce
paradigm the map task that reads the
splits and passes all of its key value
pairs the map function and writes the
results to the intermediate files after
the map phase is completed the reducer
reads intermediate files and passes it
to the reduce function finally the
reduce task writes results to the final
output files after a blink the relevant
logical the advantage of the MapReduce
model include making distributed
programming easier non linear speed-up
good scalability as well as fault
tolerance the disadvantage of this batch
processing model is being unable to
execute recursive or iterative jobs in
addition the obvious batch behavior is
that all inputs must be ready by map
before the reduced job starts which
makes MapReduce unsuitable for online
and stream processing use cases moving
on to the stream processing so seam
stream processing is to continuously
process and act on the live stream data
to get a specific result in stream
processing there are two popular
frameworks one is the storm which you
can find it on storm rat Apache dot
o-r-g it's from Twitter and s4 from
Yahoo which can be found on incubated
Apache dot o-r-g /s for both the
frameworks run on the Java Virtual
Machine and process heat streams in
terms of the programming model s4 is a
program defined as a graph of processing
elements small sub programs and s for
instant jets instantiates a PE
or a processing element perky in short
storm gives the basic tools to build
this dream book while s4 gives you a
bell different framework moving on to
the real-time processing real-time
processing is to process data and get
the result almost immediately
this concept in the area of real-time
ad-hoc queries over Big Data was first
implemented in dremel by Google it uses
a novel columnar storage format for
nested structures with fast index and
scalable aggregation algorithms for
computing query results in parallel
instead of bad sequences these two
techniques are the major characteristics
for real-time processing and are used by
similar implementations such as cloud
era Impala Facebook presto Apache drill
and hye-won tests powered by stinger
whose effort is to make a hundred times
performance improvement or Apache hype
on the other hand in memory computing no
doubt offers other solutions for
real-time processing in memory computing
offers very high bandwidth which is more
than 10 gigabytes per second compared to
hard disks 200 megabytes per second also
the latency is comparatively lower
nanosecond versus milliseconds compared
to the hard disks with the price of RAM
going lower and lower each day in memory
computing is more affordable as
real-time solutions such as Apache spark
which is a popular open source
implementation of in-memory computing
spark can easily be integrated with
Hadoop and the resilient distributed
data set or rdd's can be generated from
the data sources such as HDFS and HBase
for efficient caching in summary
real-time frameworks provide near
real-time processing for data in the
Hadoop ecosystem they can be built on
top of generic frameworks such as spark
streaming or spark or as a standalone or
a special-purpose framework such as
apache spark offers unique in memory
capabilities and is well suited to a
bright wide variety of data processing
workloads including machine learning and
micro batch processing moving on to the
grabb frameworks so here we have giraffe
graphics and graphlab which are popular
graph processing frameworks Apache
giraffe is a library that runs on top of
Map Reduce giraffe originated as
open-source counterpart to briegel the
graph processing architecture developed
at Google and which was described in
2010 graphics is a library for graph
processing on SPARC whereas graph lab
was standalone special-purpose graph
processing framework that can now also
handle tabular data moving on to the
machine learning frameworks we have
mahute ml Lib Oryx and h2o which are
commonly used machine learning
frameworks at a high level mahute is a
library on top of MapReduce although
there are plans to make mewho to work on
SPARC ml lib is a machine learning
library for SPARC
similarly Oryx and h2o are standalone
special purpose machine learning engines
the way in which these different
frameworks or languages are integrated
into Hadoop is shown in this picture big
hive scripts are compiled into MapReduce
jobs behind the scenes in the latest
versions these can be compiled into test
job then these MapReduce are tests jobs
will run on yarn framework of the Hadoop
cluster we will have a deep
understanding of Veon in a separate
video applications can be written in
scala storm spark and these engines can
plug straight into yarn framework to
provide greater flexibility to the
developer or to the analyst to provide
different ways of crossing for different
applications as we've seen already many
applications can run at the same time in
the same cluster even if they are
written in different line engines we
have seen how to use different languages
or frameworks in the Hadoop because
in the next video we'll discuss
primarily more on the Hadoop
architecture and the most important
demons are processed that form part of
the architecture if you enjoy this video
please subscribe to the channel for more
upcoming videos
