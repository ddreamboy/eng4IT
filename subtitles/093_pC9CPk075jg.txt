welcome to today's webinar on teaching
predictive modeling with jump Pro the
presenter today is Ross matusalem he's a
academic Ambassador a statistical
scientist that we have the pleasure of
having on our team at John
Ross are you available and ready to go
indeed I am Kevin thanks uh everybody
just a couple announcements for those
who are just joining now before we get
going
um this is the third webinar in our
spring semester webinar series which is
occurring every other Tuesday at 1 pm
Eastern Time U.S we have a couple more
webinars coming up here so next up two
weeks from today we're starting a
two-part series on multivariate methods
so these are things like principal
component analysis factor analysis
structural equation modeling and then
we'll finish up on April 4th with uh
genomics research methods in Jump Pro 17
so jump Pro 17 which just came out this
past fall has some new Tools in it
specifically for analyzing genomics data
so we'll take a look at that to register
for these webinars head on over to
jump.com
academic webinars
if you need more help learning jump
after today please don't hesitate to
reach out to academic jump.com we can
arrange one-on-one support you know you
can hop on a quick Zoom with me or Kevin
or another one of our colleagues to
answer some questions and show you how
some of the tools work or we can do
um formal or informal group workshops
for you your colleagues your students if
you want to arrange something like that
it's totally complementary just reach
out to academic jump.com
foreign
jump in a course or thinking about using
jump in a course please head on over to
jump.com teach for our library of free
teaching materials which includes a wide
range of materials including case
studies sample data sets and video
tutorials and a number of other things
as well and actually today towards the
end we'll take a look at some of the
materials specific to today's topic
and uh finally as Kevin mentioned please
if you could use the Q a window for any
questions that you have today so we do
have the chat window it's just kind of
hard to keep track of all the questions
and which ones haven't haven't been
answered yet when they're in the chat so
if you could look for that little
overlapping speech bubble icon and pull
up your q a panel in Zoom type any
questions you have in there throughout
that would be great Kevin will answer as
many questions as he can as we go along
and I'll stop a couple times to field
questions live as well
so thanks everybody for coming and let's
move on over to jump to talk about
teaching predictive modeling with jump
Pro uh our goal today is to show you
everything you need to know so that you
can start using jump Pro to teach
predictive modeling in your course uh
jump Pro as we're going to see it's an
end-to-end predictive modeling tool so
in addition to all the predictive
modeling algorithms you might expect you
can at the early stages get your data in
prepare it explore it maybe even do some
feature engineering all the way through
model validation and model deployment so
it's really a complete package that it
could be used for an entire semester
long or even a longer sequence of
courses in predictive modeling I should
mention it's also used pretty
extensively in Industry from Financial
Services to Market Research to retail
also high-tech research and development
and Manufacturing so this is a tool that
students can absolutely use once they
get out there into the real world I do
want to just issue a couple quick
caveats before we get started one in one
hour we're not going to have enough time
to go too deep into any specific
modeling technique that we are going to
see at least I think four different ones
um in uh in the examples that we have
queued up but of course as we've
mentioned we're here to help you with
specific needs and specific questions
just email academic jump.com
and then uh finally I am in Jump pro
version
17.0 right now which came out this past
October and I do want to highlight that
um predictive modeling while in standard
jump there are a number of features that
enable basic predictive modeling
applications this really is an area
where jump Pro is the appropriate tool
so if you uh are at a university that
doesn't have a license for jump Pro you
might be able to use the jump student
subscription to teach some predictive
modeling Basics but really to to do the
topic Justice uh jump Pro is is the tool
you want and as I go along today I might
say jump you know jump does this jump
has this feature but really what I'm
going to be showing you is is Jump Pro
so please forgive me if I don't use the
pro uh every time there
all right so let's actually get started
we're going to give you an idea of uh
how predictive modeling Works in Jump
Pro and in fact of course you always
notice right when you get started
misspellings there we go
um and we're going to take a look at how
to build and validate models how to
compare multiple models and how to
deploy and then also some of these
upfront things like screening your data
and doing some feature engineering but
before we get into all those things I
want to boost your intuition for just
what type of predictive modeling tool
jump Pro is
So today we're going to take a look at
these data this is a version of the
equity.jump sample data set which you
can find under help and Sample index or
in previous versions of jump uh sample
Library
here we have
5960 home equity loan applicants
including things like the amount of loan
that they requested the amount that they
had due on their mortgage and so forth
and our key outcome variable is whether
or not that person ultimately defaulted
and our goal at this data set is to
create a predictive model that can
accurately assign default probabilities
to new applicants so we can use that to
make decisions about whether we should
actually issue the loan or not
so let's just build a quick model here
just to show you how predictive modeling
in Jump Pro works
so let's say I wanted to build a neural
network model using all of these factors
to the right to predict that default
variable
so I'll go to analyze predictive
modeling and choose my tool of choice
say that I want to predict default using
these predictors right here
I'll click ok to get into the neural net
configuration let's say uh to validate
our model and we'll talk about model
validation a fair bit today I want to
use a k-fold validation procedure
I want to do a multi-layer neural
network
and I will click go with the remaining
defaults
and here's our output
so in just a couple clicks I fit a
multi-layer neural net we can actually
if I go to the diagram under this red
triangle menu and if you're new to jump
you're going to see me click on a lot of
red triangles they're just context
sensitive menus that provide more
options
we can actually see a quick diagram of
the neural net that I just fit here
up above we can see that We performed
our k-fold validation automatically and
I have a number of Statistics that tell
me how well my model is doing today
we're going to be looking mostly at
entropy R square which is just a value
that ranges from zero to one where one
is uh representative of perfect
predictions and uh zero means no better
than just guessing the base rate
so let's take a look actually at
our model visually here we have a tool
that we're going to look at a fair bit
called the prediction profiler so in
this profiler and here let me get off
the bottom of the screen for us
we have on the y-axis the predicted
probability of default on the top and
then of not defaulting on the bottom and
then we have all the different
predictors that we entered into the
model and we can just change the value
of any one of these predictors to see
how the prediction out of the model
changes for example as I increase the
value of the home we can see that we
actually observe an increased predicted
probability of default
or same goes for looking at the number
of delinquent lines of credit somebody
has so that is the number of lines of
credit where they've failed to meet make
the minimum monthly payment
and so if any of us are at all familiar
with neural networks we know that
they're often considered Black Box
models hard to actually understand the
nature of the relationships between the
factors and the response but here we can
actually see a nice visual tool that
gives us some idea of the shape of those
relationships and uh how they change
depending on the values of other factors
all right so in a few clicks I've built
a multi-layer neural network I've done
k-fold cross-validation on it I've
pulled up an interactive visual
representation of the model now let's
say I actually want to use this model we
ultimately don't want to forget the last
mile here where we have a new set of
loan applications so here's 1 000 new
applications with all of the applicant
information but
um we don't yet know if they'll ever
default or not
so in a few more clicks
we can publish our model
and choose to
score those new loan applications
and so now in these new applications we
have a predicted probability of default
for all of them save for the ones that
we're actually missing some of our
application information here
and so if I just click in this
interactive histogram for example I can
find the approximately 100 individuals
who are most likely to default
so I'm hoping that working through this
very quick example of building and
validating and deploying a neural net
model gives you an idea for how
predictive modeling Works in Jump it's
all through a point-and-click interface
it involves
um interactive Graphics it allows for
automatic model validation and tuning
and it makes actually working with the
models after they're built pretty easy
uh I really do think that jump Pro is a
great tool for introducing students to
predictive modeling Concepts and
techniques without the software getting
in the way too much I mean certainly if
in your course or your curriculum coding
is maybe a secondary learning objective
or maybe it occurs later in a curriculum
or perhaps not at all a tool like jump
Pro can make really powerful predictive
modeling techniques accessible to uh to
those students so those students that
might not necessarily know how to code
or not be prepared to learn coding at
the same time that they're learning all
of these techniques
so let me close all this down and then
we're going to get into the details of
how all of this works in Jump Pro here
so we'll go ahead and save our
journal and data set and we'll close
everything else
so let's talk about the predictive
modeling capabilities in Jump Pro in
general I should mention that we will be
posting this recording to our jump
academic webinar Library I think
Kevin if you haven't already if you
could post that in the chat just the
link to that that would be helpful for
everyone and you'll be able to download
This Journal as well which will have
some helpful information in links
one of those will take you to jump.com
where under the capabilities menu you're
going to find predictive modeling and
machine learning and that's really the
capabilities that we're focusing on
today and you'll find a pretty full list
of all of the different predictive
modeling algorithms and additional tools
and capabilities for example uh cross
validation techniques
um the you know model validation
statistics and graphs and so forth so go
ahead and have a look at that at your
leisure what I want to do is actually
give you just a quick condensed list of
the different types of models and other
capabilities
so jump Pro includes all of your linear
models uh and in particular uh relevant
here the regularized or penalized
regression techniques like lasso we just
saw neural Nets we also have common
algorithms naive Bayes K nearest
neighbors support Vector machines and
then a variety of algorithms based on
decision trees so your classification
and regression trees uh what we call
bootstrap Force you may also know it as
random Forest boosted trees and then
even via an add-in which you can click
here to download uh XG boost so stream
gradient boosting you'll see I'm not
going to show that today but you can
install just a simple add-in it's like a
macro and jump to get the XG boost
capabilities as well
and so in addition to these algorithms
we have a number of tools for automated
automated model validation and tuning
tools from model uh comparisons when you
use multiple techniques and want to see
which one performs the best we'll see
that that's pretty easy to do
also we're going to take a look at a
tool called the decision threshold
Explorer which allows us when we have a
categorical outcome or that is for
classification models where should we
set our decision threshold for
classification
then we already saw a little bit about
scoring new observations and we'll see a
little bit more of that as well and then
some of the upfront tools for data
screening and feature engineering so I
just want to do at the outset make very
clear that while we're not going to see
really everything today that jump again
it really is a full featured and
end-to-end predictive modeling tool
so now let's actually get into building
and validating models
and when you want to get started
building a predictive model and jump Pro
you're going to go to really one of two
places if you want to use a regression
technique maybe regularized regression
again like a lasso model you'll want to
go to analyze fit model
and in particular and we'll see this in
a moment there's a particular way to set
that up so that you get to the um the
type of regression tool that will allow
you to fit those models
what we're going to start though is an
analyze predictive modeling
so you'll find that right under fit
model and here's where you're going to
see all the I mean traditionally
referred to as you know machine learning
or maybe data mining tools so your
neural networks
partition is our classification
regression tree tool and so forth all
the way down as well as some additional
tools uh that are really specific to a
predictive modeling context
so let's actually get started with one
of these I'm going to choose for us
um a classification tree here and use
this the you know the process of going
through and building one as an
opportunity to teach you a little bit
about just how predictive modeling
platforms uh work in general
in Jump
so I've launched the predictive modeling
platform partition here
and you see on the left list of columns
you see on the right a list of roles
that these columns or variables can play
in the analysis now the very first thing
that I want to point out is that if you
launch a tool for the very first time in
jump you may of course Wonder well what
are these options maybe you know what is
this buy down here I see something about
informative missing what does that mean
you always have these questions
initially and the easiest way to get
them answered right when you need to
is to click help in the model launch
Windows so this button can sneak by
folks especially newer users there's
always going to be a help button inside
these tools that you can click on to
take you right to the relevant
documentation to learn how you should
set your analysis up in the first place
so if you're ever feeling Unsure how to
get started just click that help button
now most of the predicted modeling
Platforms in this launch window work the
same way you would select the thing that
you want to predict so that's loan
defaults and put it in the Y response
role
and then all of your predictors and put
them into the X Factor role you can see
we have a little scroll bar there but we
have all 12 of our predictors here
now uh of course in uh predictive
modeling we either can build a
classification model or a regression
model and when you launch these you
don't have to tell jump which one it's
supposed to be doing jump will know
based on what we call the modeling type
of the data if you look at default here
you're going to see these three red bars
and these indicate that jump is treating
this as a nominal variable so unordered
categories if you look at say something
like loan you can see the blue triangle
this means that jump is considering that
to be a continuous variable and So based
on whether the modeling type of your
data is set to nominal or continuous or
ordinal or there are some other modeling
types as well jump will know the
appropriate type of model to build and
if you ever need to change that you can
go into your data table in the columns
list on the left here and just click
right on one of the icons to set the
model and type them it's actually one of
a number of different pieces of metadata
that you can associate with a column and
you can always change those by by
clicking on your column header and just
going to column info but I'll spare us
that for now I just wanted to point that
out before we get rolling here
okay so I have set up the most basic
what I want to predict and what I want
to use to predict it
so I'm just going to click OK for now to
highlight the next thing to know about
predictive modeling and jump and that is
when you launch one of these tools
instead of just producing the final
model output for you it is going to
provide you with uh some kind of control
window for you to build the model
yourself though we're about to see ways
in which it can be fully automated as
well so uh without getting too much into
decision trees I do just want to give a
quick idea of what's how they work for
those who don't know about them
uh right here we're looking at a
representation of all of our data we can
see that in red about 20 percent of our
uh nearly 6 000 rows of data were
defaulted loans meaning about 80 percent
were not we actually just see the 80 20
split blue and red represented up here
graphically
now when I click split what the
algorithm does is finds a cut point
along one of our predictors that uh
maximally explains the probability of
default so if we look here we can see
that it has split on debt to income
ratio where if your debt to income ratio
is greater than or equal to 43.68 or in
some cases we have missing data we're
going to address that a little bit
uh your default probability is quite
high it's actually at about 63 percent
whereas if you are lower than that 43.68
on your debt to income ratio you have a
pretty low default probability about
point zero seven so seven percent
and so that's how this algorithm works
it builds out a tree where you
uh iteratively split or this is where we
get the name partition you recursively
partition the data into groups based on
these different cut points and then you
develop predictions by just following
the tree down so if somebody has a low
debt to income ratio you might end up
over here and then you might ask
yourself well how many delinquent lines
of credit do they have let's say they
have none so here we have delinquent
lines of credit less than two and we
would derive our prediction right here
and say that the predicted probability
of default is oh just over five percent
that's just a little bit about how
decision trees work and if you teach
predictive modeling this is uh likely to
be one of the topics in your course
now uh if you know much about predictive
modeling you know that one of the
biggest things is that all these
algorithms they have some way of either
learning incrementally or you have
multiple different settings that you
need to specify and you often want to be
able to tune your model in order to
figure out the best settings to get the
most accurate predictions and so the
next thing to highlight here is that all
of the predictive modeling Platforms in
Jump have some kind of built-in
capability for model tuning and model
validation so for our purposes here we
actually want to know what's the best
number of splits in the tree without
getting into the details if you're at
all again familiar with predicted model
we know that if we just kept splitting
the tree indefinitely until we had maybe
tens or even hundreds of splits we'd
probably end up over fitting our data
we'd have a tree that's fit to the noise
not just the signal
so let's see how in the context of this
tool we would use one of these built-in
capabilities uh to determine the number
of splits in the tree
so I'll go to analyze predictive
modeling and partition again
this case I'm going to click recall
which is going to populate defaults in
our predictors back into the response
and Factor roles
and in this case we have a little box
here that says uh the proportion of data
that we want to hold back as a
validation set and so I'm going to say
that I want to take 30 percent of our
data and withhold it so every time we
make a split we're going to use just the
4159 observations
uh in the training set to decide where
that split should be and then we'll test
how good the model is at predicting data
that it hasn't seen yet on the remaining
30 so about 1800.
so you can see if I request a single
split this
box up here populates with some
information telling us in this R square
column where this is stands for entropy
R square it's a version of R square
that's useful for categorical data uh
how well we're doing where again higher
values are better and so now I have this
go button and I can click go
and it looks like jump in this case has
landed on a 15 split tree
the split history graph is actually just
showing across all the numbers of splits
that it tested which was all the way out
to 25 splits
um in blue how well did we do in
predicting the training data and in red
the validation data and so if I zoom in
a little bit here we can see a slight
difference but not a particularly large
one I don't think we're dealing with
much overfitting here we can see that
where performs on the training data
always Rises as it always will as we
create a model more complexity we seemed
to Peak in our ability to predict the
evalidation data at 15 splits
so we're going to actually see well
really we already did if you remember
when I launched that neural network
platform at the beginning and selected
k-fold validation there was a way of
using that technique to
um perform automatic model validation in
this case to stop the fitting algorithm
for the neural net here we see another
example of one of these built-in tools
where we can ask jump to withhold a
random set of our data and validate it
and stop the algorithm automatically
so um
sometimes we want to do uh use those
tools that is if we're just working
inside partition and we're just trying
to teach our students the basic idea of
withholding some data and what that does
for us this might be valuable as we move
on maybe further in a course or in a
curriculum we may get towards fitting
multiple models and uh needing to
compare them their performance all
against the same test set of data or we
may start talking about the need to
impose certain constraints on how we
randomize the data into the training and
validation sets and so forth
and so if that describes your situation
you should be aware that there's a
specific tool in Jump Pro for making
these you know training and validation
data splits
so let's see how that works first I'm
going to point out I actually have a
column like that already in my data set
here so this validation column is
dividing data into a training set a
validation set and this is actually also
a third set the test set where in the
context of uh partition we're about to
see you would make splits using the
training set decide when to stop
splitting based on performance on the
validation set and then use this third
set the test as a final test of how well
the model generalizes to unseen data
so let's actually create a column like
that I'm going to go to analyze
predictive modeling down here I'll find
the make validation column utility
when I launched that I have some options
for restricting the randomization of
rows in our data table to the different
groups for example stratification just
means to do stratified random sampling
so for example I could say stratify on
default so that the training validation
and test sets have the same proportion
of defaulted loans in them
grouping columns are particularly good
when you have blocking structures or
repeated measures so for example if you
have multiple measures per individual
what you want to do is randomly assign
all the rows for a particular individual
to one set or another you don't want the
same individual appearing in the
training and validation sets or else you
get leakage
so you could Group by that individual
and then cut point is good for uh
oftentimes time ordered data so if you
just want to withhold the most recent
two months of data and train on
everything before that you could have a
Time column is a coupling
so I'm going to put default and
stratification you also notice that we
have an option down here for making a
validation column which is your basic
holdout validation as we've kind of been
working with so far if you want to do
k-fold validation you can make a k-fold
validation column as well I'm going to
stick with just hold out validation for
our purposes here
um when you eventually get your hands on
this journal you can click this link
right here that says validation methods
documentation to learn about how hold
out and k-fold validation are
implemented in all of Jump's various
tools it's going to drop you right at
this page where it says validation and
jump modeling and talk about a few
different modeling or excuse me
validation schemes and if you click
forward you'll go through the different
pages here to see which schemes are
supported and which modeling platform
and then also clicking forward you can
see
just very explicitly which platforms
support hold back versus k-fold
validation
so I'm not going to get too deep into
that for now these are great resources
if you're wondering which validation
methods are available depending on which
uh modeling technique you're using
so just to get back to where we were I'm
creating a three-way holdout validation
column I'm going to stratify on default
so I'll click ok
next I specify how much data goes into
each group so in my training set let's
say I want 0.6 so 60 and then 20 percent
each in my validation in test sets so it
looks like nearly 3 600 in the training
set and then both the validation and
tests they'll have nearly 1200.
if you want to reproduce the exact same
splits a subsequent time enter a random
seed here and then enter the same random
seed every time and the splits will be
the same and I know that that can be
important in teaching because oftentimes
when you are producing some kind of
example for the students you need your
numerical results to exactly match
what's in a textbook or what's in an
example that you showed them previously
and here in other places in Jump setting
a random seat is the is the best way to
do that
so I'll click go and now you can see in
our table we have this training
validation test split it's different
from the one before you can see when I
click on training and validation two
those are distributed across all three
groups and validation one
but one thing that's nice and uh in
particular uh helping students
understand some of the the reasons that
we want to be careful and how we
randomly assign data to these validation
groups is by using one of these columns
you can use all jumps other tools to
actually explore how the randomization
was done and if it was uh I suppose
we'll use the term effective here so for
example we stratified on default let's
actually take a look at the breakdown of
that variable across the three groups
that we just created if you've never
seen this before this is graph Builder
it's just a drag and drop graphing tool
so I'm going to drag default to the
y-axis the validation 2 to the X and
then change up here the graph element to
a mosaic plot
where if you're not familiar it's just
showing us here the breakdown across all
three groups almost like a stacked bar
chart of uh the defaults and then the
width is showing the size of each group
so the training group is 60 of our data
so it's width is 60 of the total
and we can see quite clearly we have
that nice 80 20 split uh across all
three groups
now oftentimes uh if we have a large
enough data we would expect let's say
that the the mean of some of these
predictors for example debt to income
ratio should be approximately equal
across the validation groups again
assuming random assignment and a um you
know a large enough sample so we could
check something like that too to help
boost our students intuitions maybe I'll
go to analyze tabulate so this is a drag
and drop tool much like graph Builder
but for creating uh crosstabs or summary
tables
so let's go ahead and say I want to look
at
all of my continuous predictors
and I don't want to look at their sum
value I want to look at their mean value
and I'm going to break that out by this
validation column that we
just created and so now I can just look
in a quick tabular way to see that all
of the means of all these different
groups are approximately equal and it
looks like that they are
any difference can probably be just
attributed to that randomization
so that's just to highlight some other
junk tools that you may find useful when
you know just using jump in general in
your course but also when working with
these validation columns and helping
students actually understand what just
happened and what we would expect to see
when our training validation and test
splits have been done appropriately
foreign
for now though and use this additional
or this uh excuse me original validation
column
and I'm going to relaunch the partition
platform
and now we'll put our default in y our
predictors in X as we did before but now
going to take the validation column and
put it into this validation rule which
you're going to see all over jump
so let me click OK you'll notice now we
have all three of our groups in this
table up here
and again I have this go button which
will allow me to continue splitting the
tree until performance on the validation
set levels off
I ended up with a 21 split tree here
blue represents the training data red
represents the validation data and
orange now represents the final test
data and so we stop splitting again more
validation performance peaked and then
we did a final test right here and it
looks like our entropy R squares about
0.458 which should take some domain
knowledge and expectations about what
counts as quote unquote good here but in
this context we might say that that's
not too bad
so now let's uh work with this model a
little bit more one of the nice things
about
um jump uh specifically in the classroom
is that students don't necessarily need
to know everything that they might want
to request or get out of their analysis
up front these red triangles that I
mentioned before contain all the
additional options so you can always go
to the red triangle to request more
information so for example if I wanted
to look at some further fit statistics I
might go to show fit details
and here I have multiple measures of
model performance so here right at the
top is the one that's actually depicted
in that graph but we might also look at
something like generalized our square or
the misclassification rate here assuming
a threshold of 0.5
down below we have confusion Matrix we
can see you know where the model is
doing better or worse it is it is it
committing more false positives or false
negatives
just to highlight a few additional
options
graphically to assess classification
performance at least we might want to
look at an Roc curve so here on the
bottom right we have the ROC curve for
the test set now if you're not familiar
with these without getting into the
details the diagonal here represents
essentially random guessing and the
farther that a curve gets up into the
corner here the better the model is at
distinguishing between the two
categories we often use an area under
the curve where to capture the overall
accuracy where one would be perfect and
we see about 0.91 which is pretty good
now of course uh as we saw with the
neural net we might want to use that
interactive profiler to visualize the
model so I can request that as well
so again we have the predicted
probability of default on the y-axis and
then all the values of our predictors on
the X and so I can see for example
there's a giant jump in default
probability right we get to a number of
delinquent uh credit lines of six or
more I can see actually as we cross that
threshold how other effects kind of
enter or leave based on what branch of
the decision tree that we're in so for
example as I go up to six we can see if
I scroll down here the number of recent
inquiries
doesn't seem to matter but once we're at
a lower number of delinquent credit
lines it does look like that number of
inquiries matters a bit
so I do want to highlight that this
prediction profiler is actually a full
featured tool for exploring your model
it has a red triangle of its own with a
number of uh different options for
example optimizing for particular
um response uh profile
um you might use it to actually simulate
outcomes or one thing I actually like to
do for predictive modeling especially
with kind of more black box like models
is actually go to this assess variable
importance where without getting into
the details of all these you can get
what we call marginal model plots
which are at the bottom here and a
marginal model plot actually essentially
depicts the average effect of a
particular Factor across many randomly
resampled values of the other factors so
we can see this General spike in default
probability for a number of delinquent
credit lines it's six like we saw before
we can see for the number of recent
inquiries there's a general uptick here
at around four but with these um these
confidence bands around here we can see
that once we're into this range where on
average it's flat we can see a big
effect or
um or not depending on maybe some of the
values of the other factors so these
marginal model plots I like they they
can be somewhat of a peak into a the
average effect in an otherwise maybe
opaque model
so that's just um one example of some of
the additional things you can find under
the profiler here
last thing I want to point out here
for your categorical responses I
mentioned this before we have a tool for
exploring decision thresholds so that is
now that I have this model
if I'm ultimately going to use it and I
need to set a cutoff point you know
predicted probability of default of 0.7
and now I will no longer issue a loan
where should I actually set that
threshold
so as of jump Pro 17 all of the
um predictive modeling tools if you use
a categorical response have this this is
a decision threshold tool in them
actually if you're still on 16 or before
I will show you where this tool lives
there but here you can find it right
under the red triangle
so we have a number of different things
let me try to focus us here actually I
want to get down let's go to the test
data instead and I'm going to get off
the bottom of the screen I'm going to
close a couple of these things down and
talk about what we're seeing here
so in this graph that I just made bigger
we have all of the data in the test set
up top here are all the people who did
not default and below that are the
people who did default
so what we're looking at here in the
blue
if we were to set the uh the decision
Threshold at just the middle of the
scale 0.5 here are all the people who
would be specified as not defaulting and
we are correct that they didn't default
to actually realize now we're specifying
probability in terms of not defaulting
so just to make that clear here
so that means then all these people who
did default if they were over that
threshold we would have predicted that
they didn't we would have been wrong
that would have been a false negative
and so that's why it's in red here
however here we would have had the
predicted probability below that
threshold and it turns out the people
did default so we would have been
correct
now we have all the corresponding
statistics if you're familiar with
classification models you might be used
to seeing so these are the actual counts
and rates of the different decisions so
the true positives true negatives false
positives and false negatives and what's
nice about this tool is I can actually
just change the probability threshold
and see how those values change so for
example is I increase the threshold
to about let's say 0.9 I can see that
when people actually would have
defaulted I would
believe that they wouldn't and in fact
issue them alone only seven and a half
percent of the time so I'm being nice
and conservative here and not giving
loans out to people who uh might default
and what would that do uh for the people
who ultimately didn't default well
unfortunately I would deny nearly 25
percent of their applications but 75
would still make it through
and so I can adjust the decision
threshold and actually see how these
statistics change
in order to help better understand the
consequences of setting the Threshold at
different values
now without getting into the details we
have a number of additional graphs and
metrics that might be useful and all of
them update in real time for example in
this graph on the right I have the
misclassification rates for both
categories yes and no so if I wanted to
find a point at which the two rates are
approximately equal I could just find
the point where they cross over and it
looks like that's a
decision threshold of about 0.88
and if I were to do that I would achieve
the following you know true positives
and false negatives and sensitivity and
specificity and so forth
so this decision threshold tool is a
great uh way for making visual the
consequences of setting the thresholds
when we're doing these classification
models
now I mentioned that uh you know we we
have we're going to see a number of
different
um predictive modeling tools today I do
just want to show a couple more before
I'll pause and take a couple questions
here
um just to give you an idea of how
everything that we just went over with
respect to that one partition tool
actually applies broadly to a number of
other tools so let's actually fit a
linear model here so let's go to this
fit model tool that I mentioned before
so just like before you get a launch
window
we'll say that we want to predict
default now because we're in a linear
model we need to build out our model
effects so for example I could just
click the add button to add 12 main
effects to our model or select any two
and click cross to add the two-way
interaction
I'm actually going to delete those and
use a helpful model Macro for example
factorial to degree where our degree was
set to two meaning that we're going to
get all the main effects and two-way
interactions here so that's a that's a
lot of possible effects a great use for
something like regularized regression
as I mentioned up here you want to use
the generalized regression personality
so I'm going to go to generalized
regression
can specify maybe that I want
probabilities stated in probability of
uh yes defaulting and click run
and just like we did before we get a
control window
where I could select maybe a lasso model
or an elastic net model
and validation so I could do hold back
or k-fold or had I put our validation
column into the launch dialog have done
that so maybe we'll do hold back
validation and use a third of our data
and click go
and now jump has run a lasso model where
We've Ended up with just 63 of those
parameters in the model
a number of statistical outputs and
graphs as you might expect we won't get
into the details and of course going to
the red triangle gives you all the other
things you might want for example
pulling up the profiler
or pulling up an Roc curve and so forth
so that's just a quick primer on linear
modeling we already saw
um neural net modeling briefly let's
also take a look at one more
and that's going to be
let's go with a boosted tree here so
this is a more advanced technique based
off of
um the decision tree technique
the launch looks very similar so I'll
put my default as my response my
predictors as my X
and we will use let's say our validation
column for validation I'll click ok now
one thing you're going to notice here is
our control window looks a little
different and that's because this
particular algorithm has a lot of
settings or what we call hyper
parameters that need to be set and when
that's the case you'll often get an
option to either just manually specify
what you want them to be or if I could
just back out of this
you can use what we call a tuning table
to
run the algorithm multiple times at
multiple settings and then return the
one that performs the best this table
without getting into it was actually put
together just using a doe tool yeah full
factorial design and I'm doing a grid
search over three of our hyper
parameters here
so now if I go to analyze
bootstrap forest and I'll click recall
right here
I can click use tuning design table
foreign
oh I must have selected something
incorrect there let's try that one more
time
we go boost a tree of course here we go
make sure to start on the table that you
actually want to model
so we'll click recall here
and we'll use our tuning design table
your tuning table there we go
and so what jump is doing is now running
I believe it's 18 uh different instances
the boosted tree algorithm and will
return
the model that performed the best
according to the validation statistic
and in this case here we can see the
hyper parameter values for that model
and as we might expect going to the red
triangle things like our Roc curve or
our profiler or our decision threshold
tool so I just wanted to point out the
existence of these tuning design tables
so I know for some especially in data
science doing something like a grid
search is a pretty core
component of the curriculum there
so uh with that Kevin I see we
've spent a lot of time just going over
all the different uh ins and outs of the
predictive modeling platforms I'm gonna
go ahead and pause and see if there are
just a couple questions we might take
live before we talk about a couple more
things okay we just had a question come
in so there's no questions the whole
time and either we interpret that as
you're doing such a phenomenal job that
no one has any questions or you're doing
a horrible job and you're confusing
everybody I'm gonna go with the with the
former
um there was a question that popped up
is somebody's asking if you can force a
split
in Partition
um yes you can so uh I'm uh for the sake
of time I'm not going to show it because
it's uh relatively straightforward when
you uh are in the partition platform
every single node in the tree has a red
triangle and if you go to the red
triangle you will find the options to
force a split is it can you can you
um I know you can choose the variable to
split by but can you pick the this
person is wondering like I want to
actually pick the split location not
just the variable to split
oops we are there let's find out very
briefly here
so we will stay on decision tree click
OK so you can go to split specific uh
and here you will choose the value and
then you can go with the optimal value
perfect perfect thank you
all right let's see well I see a couple
other questions Kevin that are in the
open we have uh five open questions
there
um one of them I see can we see how to
save the model to python code I think
that's possible that is one of the
places we are going next so in this for
the interest of uh time here
um Kevin I'll let you get to those in
the Q a while I keep
one thing I want to point out briefly
before we leave just the using the
modeling platforms behind is the
existence of a tool that allows you to
run multiple algorithms simultaneously
if you're not sure which one is actually
the most promising for a particular data
set
this is called Model screening and it
actually lives under the predictive
modeling menu
the setup is kind of the same in that I
will specify what I want to predict and
the predictors that I have and then if I
have it I could do a validation column
or do k-fold cross-validation what's
nice is down here I can select which
methods I want to try and jump will
automatically call out to all of these
different methods and I'm going to
deselect some just so that things run
nice and fast here
it will call out to all of them with
default settings and then return as we
are about to see
a table that will allow us to compare
these models to choose at least you know
one or two candidate techniques that
seem like were we to go in and actually
fully tune those models would perform
the best and so here on our test set we
can see the winner was boosted tree
followed by the boosted neural net and
the actual plane decision tree down at
the bottom
and under the details all of those
models are actually living here I'm not
going to get into them now but what I
would typically do is actually then
start working with one of these models
or even just launch it directly in its
own tool
the model screening tool especially
let's say for a student project if they
want to they have multiple techniques
they've learned and want to identify the
one that they might want to use they
could go to model screening first to
very easily uh just basically throw
their data into multiple algorithms and
see which one deserves their their
attention
and if you are in uh jump uh Pro 16 this
is where you're going to find the
decision threshold tool living and it
had not yet been implemented at all the
individual modeling platforms so instead
you will find it
um living right here under the red
triangle
so I just want to point out the
existence that tool sometimes it's very
handy to be able to just run multiple of
these algorithms at the same time
briefly about model comparison and
deployment so that is once you fit maybe
multiple models what tools are available
to you to continue working with them
so let's actually pull up I'm going to
pull up our original let's see 21 Split
Decision tree here I did this briefly at
the outset there are multiple ways for
exporting and working with the model if
you're using jump in the classroom I
highly recommend using this formula
Depot tool that we saw before and you're
going to be able to write a model
directly to it by using
the publish prediction formula option
that you're going to find Under the Red
Triangle sometimes it's under save
columns other times when they're shorter
red triangle menus it'll actually be a
top level option but when I do it
automatically puts the formula for that
model into this formula Depot window
which you can actually save so now the
model is independent of any one
particular data table you can always
actually just create a new formula Depot
by going to
um the new menu as well actually it's
somewhere may have been moved
anyway we have our formula Depot now we
have one model living in it let's say
that we want to compare the performance
of that model versus a couple others so
let's take this boosted tree model so
I'll just run that again quickly
and I will go to save columns publish
prediction formula and that's been added
to our formula Depot and let's do one
more I'm going to do the neural net
model that we were working with before
and I will
under the appropriate red triangle here
go to publish prediction formula
all right I have multiple models that
I've built and tuned and they're living
this formula Depot
if I want to know which model performed
with the best overall I can find out by
going to the red triangle and going to
model comparison and I'll say I want to
compare all three of these models
jump hopefully says hey do you want to
use that validation column when
Computing all the fit statistics and
I'll say yes please
uh yes I forgot to change something on
one of these uh
these particular formulas here
all right we may need to let's see just
so it doesn't wait because it's trying
to turn on something let me pull this
back up
and we'll actually just take a look
directly at
a formula Depot that I have available
right here
so let's see let me bring up our
Journal again as well
there we go
so if we go to the red triangle and go
to model comparison
once we get our table opened up there it
is get back to where we were
we'll say that we want to compare these
three models
with our validation column
and now we have our table out so for the
training validation test sets we have a
variety of fit measures
we're looking at our entropy R square
and maybe even down here in the
validation or excuse me the test set and
see that the boosted tree did in fact
outperform the uh boosted neural net
that we tried and the bootstrap for us
in use case
and then of course other ways in which
we might want to compare these models
for example looking at the ROC curves
or perhaps maybe a lift curve or even
taking a look at the profiler and I'll
also point out that here is where you
can create a average model across the
three and actually compare that to The
Originals as well to see if maybe an
ensemble of these three models
outperforms uh any model on its own
now finally oh I closed that down too
early we did have a question about this
so I'll bring it back quickly here
finally for model deployment we already
saw that you can for any given model
just go to run script and run it on a
table and it will automatically score
the observations in that table but you
can also go to something like generate
python code or generate SAS code and it
will generate the code that implements
that model and so we actually do have a
number of people who whether in teaching
or research or out there in Industry use
jump alongside something like Python and
may actually want to take this model and
import it into a python library to
continue working with it here's all the
code that you would need to do that
so I see we're starting to run up on
time
so I'm just going to highlight a couple
things in preparing for modeling uh
that's worth being aware of you know I
mentioned before and I think a lot of us
looking at the poll results have a fair
bit of jump experience so you probably
know about a lot of other tools in Jump
when it comes to predictive modeling you
might particularly uh be interested in
the tools for exploring outliers or
exploring missing values at the outset
of a modeling exercise
data Integrity of course is kind of the
one of the first considerations before
actual model building if you'd like to
take a closer look at those tools we're
happy to show them to you again you can
always email academicatjump.com
so now finally we will take a quick
quick look at the formula editor here
which is great for as it says feature
engineering
so this is another commonly taught thing
in data science and predictive modeling
the fact that we might actually want to
use some of the raw values we have to
derive new variables that might be
better predictors than the raw variables
themselves let's take one example here
we have the mortgage due on somebody's
home and we have the currently assessed
value of their home so the difference
between these two let's say so the value
of their home minus the mortgage that
they have due would be the equity that
they have in their home and maybe we
want to enter Equity as a potential
predictor
so I'm going to just ask junk to create
that for me quickly here I'm going to
right click after having selected these
two columns and when you right click on
any column you're going to find this new
formula column option where you can
perform Transformations for example or
in this case I'm going to take the
difference between the two so now I have
value minus mortgage due and this
represents the equity that somebody has
now if I right click and go to formula
you can see what happened is Jump just
wrote a column formula just a simple one
value minus mortgage due to compute this
for us
over on the left we see a really large
range of different operations or
functions that we can put into a formula
for example here number of different
transformations
or maybe we want to do comparisons
or conditionals
or compute different statistics for
example do something minus the uh the
the mean value of another column uh
maybe in this case what I'm going to do
actually is just while this is
highlighted I'm going to take the value
minus mortgage due
and put that over the value and so this
is actually going to be the percent
equity that they have in their home so
I'll click ok
and now we have those values here
so if you're teaching things like screen
your data or
um feature engineering up front just
know that jump has tools for you to do
that easily as well
now finally to wrap up and then we'll
hang out and take what remaining
questions we have I do want to highlight
the teaching resources that you might
find helpful for integrating jump Pro
into your course to maybe complement
what you're doing or even just lighten
your your load a little bit
so let's actually just pull up a browser
here
and go to jump.com teach
where you're going to find all of our
free teaching materials they're broken
out by course topic
so uh in this case you might want to as
this little link implies down here go to
the analytics and data science materials
and you're going to find a number of
different categories for example books
that you might use in your course
I do want to highlight the step-by-step
guides so if you click on recommended
guides and videos you're going to see
links where each one takes you to a
quick instructional video and one page
guide on how to do that analysis in job
so for example if I click on
classification trees
we are going to go to a page in what's
known as our learning library that will
show us in just uh you know a short I
think about three minute video how to do
classification trees
so here's that video I could play it
right here
or also click view guide to go to a
one-page PDF summary of that video with
Click by click instructions and
screenshots
these are really handy things to give to
your students in advance to avoid
getting a lot of questions about just
you know how do I do X in Jump
if you want to go to the complete
library of these videos just go to
jump.com learn
now finally if we back up just a little
bit you'll notice another particularly
uh well utilized resource that's our
case study Library
so let's take a look at the recommended
case studies
in fact I'm going to look at this one
here sticking with classification trees
on credit card marketing
so if we look down to the credit card
marketing case study we're going to find
a PDF that takes us to a real world
predictive modeling problem with a
worked example and exercises for
students to do so it begins with just
the problem background and the data
progresses through a number of
screenshots with instructions for how to
actually perform these analyzes and
little bits of additional interpretation
just the basics of how to interpret the
results
though we try to be relatively light on
that so that you have the freedom in
your course to explain things how you
want
at the end of each one of these we get
all the way to the bottom you will
actually find uh one or more exercises
for students to apply some of the
techniques they just learned in a new
context here
back in the case study Library you'll
notice that each one comes bundled with
the data set where you can just click
download the data
in this page that we're looking at with
all of these case studies on it is our
case study library and you can find it
at jump.com cases
so that's all that I have planned for
today so if you take away nothing else
it's just that jump Pro is it's really a
full-featured end-to-end predictive
modeling tool what makes it special is
that it provides point-and-click you
know Visual and interactive interface to
really powerful predictive modeling
methods and fits very nicely in uh
course or curriculum where uh maybe
students are not as comfortable with
coding or coding isn't necessarily one
of the primary learning objectives so it
takes these powerful techniques and
makes them accessible to that kind of
student population
