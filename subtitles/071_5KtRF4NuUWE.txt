hey everyone i welcome you all to this
devops full course by simply learn in
this complete course we will learn
everything that we need to know to
excellent devops
we have matthews chidanand and anuj to
guide us through this journey
we shall begin with having a basic
understanding of devops with a short
animated video
after that we shall dive deeper and
understand what devops is all about
we shall see what is its relevance in
the present scenario
we shall learn different tools in a
learning journey the tools that we shall
see are git
gradle maven selenium docker chef
ansible puppet nagios and jenkins
after we have a fair amount of knowledge
regarding devops and devops tools
we will look at the interview questions
that will increase our chances to crack
any devops interview
so before we begin make sure you have
subscribed to our youtube channel and
don't forget to click on the bell icon
so you never miss an update from simply
learn
now without any further ado let's begin
with our animated introduction
meet tim
tim builds a robot in his lab
a climate controlled and pollution-free
environment
once he's done he drops the robot off at
his project partner mia's house mia
takes it out to her backyard to ensure
that the robot meets the requirements
but here is where the problem arises the
change in the environment causes the
robot to malfunction
mia is now really annoyed and she has a
lot to correct and it seems to her as so
tim didn't really do much
this wall between them leaves a poor
robot to bite the dust
well what if we broke this wall tim and
miana work together in a common space
tim develops each block of functionality
of the robot which is then immediately
checked by mia both are now working
simultaneously instead of waiting on the
other to finish their task as and when a
feature is ready for use they are put
together to build the final product they
develop a common mindset and share ideas
to further speed up the process they use
several tools which can automate every
stage this means that the robot is now
ready sooner with less iterations and
manual work from an organization
perspective tim would be the developer
while mia the operations their union is
the core of the devops approach devops
has several stages and set of tools to
automate each of these stages let's have
a look at these tim first puts down a
plan
in terms of software this could mean
deciding on the modules and the
algorithms to use
once he has the plan he now codes the
plan with tools such as git tim has a
repository for storing all the codes and
their different versions this is called
version control
next this code is fetched and made
executable this is the build stage tools
such as gradle and maven will sort this
out now before deployment the product is
tested to catch any bugs the most
popular tool automating testing is
selenium once the products are tested
mia must deploy it the deployed product
is then continuously configured to the
desired state
ansible puppet and docker are some of
the most common tools used that automate
these stages
now every product is continuously
monitored in its working environment
nagios is one such tool that automates
this space and the feedback is fed back
to the planning stage
and finally we have the core of the
devops lifecycle the integration stage
tools such as jenkins is responsible for
sending the code for build and test if
the code passes the tests it's further
sent for deployment this is called
continuous integration
let's now have a look at an organization
that has adopted the devops approach
which of the below sequence of steps
would they follow to develop a software
leave your answers in the comment
section
keep an eye out for the right answer on
the comment section or our youtube
community
giants such as amazon netflix target xc
and walmart have all adopted devops and
seen a considerable increase in delivery
and quality
in 2014 an hour of downtime for netflix
would cost it two hundred thousand
dollars
it became absolutely crucial that
netflix prepared themselves for any sort
of failure and so they took to the
devops approach and implemented it in
the most unique way they developed a
tool called simeon army this tool
created failures and automatically
deployed them in an environment that did
not affect the users
the team would troubleshoot these
failures and this gave them enough
experience to deal with any degree of
collapse with everything being automated
and happening simultaneously
organizations can now deliver at a much
faster pace so considering the benefits
of devops and its divergence from the
traditional methods would devops be the
future uh we're going to go through what
we need to be able to do to go to devops
what the arguments are and why you need
to do devops and they will actually go
through all the individual tools you
need to be able to successfully
implement devops within your
organization in addition to that we're
also going to take time and go through
each of those tools so you get a good
understanding of a step-by-step
instructions on how to do basic setup of
each of those tools so let's get started
so what was devops before so what was
the process that we took for doing
delivery before devops well it was a
model called waterfall and waterfall was
a very traditional approach to actually
building out solutions and the reason
why it's called waterfall is that you
bring out all the individual
requirements and individual sections of
a project and they cascade off each
other so if we look at the breakdown we
have requirements design we have
implementation we have verification we
have maintenance you'll have user
acceptance testing and this is all based
on the software development uh lifecycle
model or sdlc and it's been around for
quite some time and is still used by a
lot of companies today the challenge you
had with the waterfall model is that it
really is a very long drawn out model
for actually building and delivering
solutions so it took a very long time to
actually um write code and then deploy
the code and it was very difficult to
actually identify problems within the
code and provide feedback to the
development team on what to fix um and
this really was a very time consuming
we're talking about months sometimes
years for projects to actually go
through a waterfall model process so
along came a new method of being able to
do delivery and it's called agile and
the agile approach is a way of being
able to take the actual
work that's done in a waterfall model
and compress it down into small
iterations and what we would do is a
fundamental change is that you would
actually take uh teams that were
disparate and as part of the individual
cascades within a waterfall project and
you actually bring them together so you
have your requirements team in person
design developer and release management
team all together in one group working
on an iteration the great thing about
agile is that you took a process that
was weeks or months or even years in
length as it was with waterfall and you
reduce it down to two or four week
sprints depending on the cadence for
your team uh typically you have a
two-week sprint and then the goal is is
that at the end of each sprint or
sometimes every other sprint you would
do a software release and so that
customers were getting the software much
faster the problem that we still ran
into though with um agile is
fundamentally similar to what we were
having with waterfall uh you have your
devops person working on code on their
system and you'll be working great on
their computer and then you have the
operations person who's migrating the
code from the developers environment the
test environment to the production
environment and you would run into
issues where the code simply doesn't
work and there's a lot of reasons why
that would happen uh the actual
developer environment would often be
very different or would have different
dependencies in it so the the hardware
the the software there may be additional
uh applications that were installed on
the operating system that simply hadn't
been transferred over to the operations
environment and so what you would have
is a disconnect between the developer
environment and the operations
environment making it difficult um to
actually roll out code so you'd run into
a program where when you rolled out code
you'd have to have a rollback plan in
case the code wouldn't work in
production and so each release became an
event where everybody got very stressed
about the actual event of releasing code
because you didn't know whether it was
going to work so devops really looks to
address and solve a lot of these
problems so the key word that you'll
often hear with
devops is continuous integration and
what that means is essentially that as a
developer is working on their code the
code is constantly being tested against
not just the actual code itself with
unit testing but the environment with
which it's going to be released in and
the goal from a devops model is that the
breakdown of communication that happens
with waterfall and agile where dev
developers and operations teams aren't
working in the same environment is being
removed and you're able to provide a
continuous and contiguous environment
between the developer and the apple
actual operating model so the reality is
when the developer is working on their
code they're actually working in an
environment that is identical to the
production environment and so when the
actual operations person comes to
actually do releases for the code and
you can see some teams are doing as many
as 20 to even up to 50 releases to
production environments every single day
you're able to guarantee that the actual
code itself will work and releases go
from being a stressful event to a
byproduct of good testing and good setup
and structure for how you actually build
out your solutions so what we're seeing
here so the goal is that as a developer
and as an operations person that the
code is working continuously in both
environments you have continuous
integration and continuous delivery so
simply put what we're able to do is
we're able to eliminate the problem of
the operation environment not being in
sync with the development environment
and this is a improvement on agile this
is not to say that waterfall or agile
are wrong as delivery models what it is
is just a maturity of the ability to
deliver solutions and devops is just
another rung in that maturity curve
using tools that are available to us now
that five ten years ago simply weren't
available so the goal is for you as a
team to move to a devops model where you
can implement continuous releases on
your software as long as you're using
the tools that are available and the
good news is those tools are open source
tools so let's go through some of the
benefits of why you'd want to go and use
devops so you know essentially what's in
it for you so let's over the next few
slides we're going to go through what is
devops we're going to go through the
benefits of devops so in the last few
slides you've actually seen you know
what is devops and the benefits of
devops along with the life cycle but
we're also going to start digging into
the tools you have that are useful for
devops and we're going to focus in on
seven tools that can provide an
end-to-end infrastructure for delivering
devops solutions but there are
significantly more tools available on
the market but these are seven of the
most popular uh for each of their
categories so devops really is an
essential collaboration between the
development team and the operations team
these are teams that have in past been
somewhat at conflict with each other and
what you have now is an opportunity
where those teams can now work
continuously with each other the
expectation with devops is that it will
continue to mature indeed you're
actually even seeing some groups which
are now called devsecops where they're
integrating security as part of the
delivery between the development team
and the operations team the bottom line
is a devops engineer is highly in demand
the demand for devops engineer is
literally going through the roof with
salaries going up exponentially around
that so let's dig into some of the
benefits of devops it's not just a new
catchphrase it's actually got
significant value and how you can speed
up delivery of your software so the
benefits of devops can really broken up
into a number of key areas first of all
we have continuous delivery of software
which allows you to continuously release
new features with the security and
understanding that the software going
out is of high quality it allows the
teams that are working on the software
delivery within your organization to
more effectively collaborate with each
other so that you're all talking from
the same page and understanding of what
needs to be delivered the deployment
process itself moves from being an event
where there's a lot of stress and
there's a lot of contingency plans to
being a much easier deployment the
efficiency within the actual code that
you're writing the ability to scale up
using the different tools are available
allows you to be able to bring in and
scale up and reduce the teams you have
running the software as needed errors
can be fixed much earlier and more
quickly and can be caught before
anything gets pushed out to the
production environment and fundamentally
what we're looking for is improving the
security of the actual releases so the
actual concept of security is center to
all the work you're doing and then
finally what really allows you to reduce
the number of errors is that there is
much less manual intervention you there
is a greater reliance on scripted
environments that you can actually test
and validate for their
security reliability and uptime
efficiency so let's talk a little bit
about the life cycle of a devops so the
very first step that you'll take is to
actually build out a build and test
environment and this is a continuous
building test branch and this is managed
with the first step of your source code
once you move through that and you're
looking at continuous integration which
means that every time somebody checks in
their code they're validating that the
code actually can run in the production
environment once you've actually then
passed the end continuous integration
and the testing that you have with your
code you're looking at continuous
deployment if the code works and is
available to be released into the
production environment let's go ahead
and release it and once you actually
have release code then you want to be
able to validate that your environment
is working efficiently you may release
code that is a new feature within your
application and customers may then
gravitate immediately to that new
feature if they do you want to be able
to ensure that the code is working and
more importantly that the infrastructure
is there to support and then finally
you're looking at software released as a
continuous event and then you go back to
the beginning you start working on more
code you run it through your build
environment and continuous integration
deployment continuous monitoring and
keep that cycle moving so let's dig into
the tools that you as a devops engineer
would need to learn if we break down the
environment that we have all the way
from source code management to software
release and there are a number of key
tools that you want to be able to use so
for instance source code management and
get is an open source tool that you
would want to use for managing your code
the continuous build and test
environment will be managed with maven
and selenium integration with the
environments that you're working on is
managed through jenkins the actual
deployment to your production
environments will be managed with
products such as ansible and docker and
then the monitoring of your network
would be used with tools like negios the
thing that you have to remember with all
these tools is that they're open source
tools there is no licensing that you
have to uh purchase uh some of the tools
will have a pro level licensing that you
can choose to select but to get started
all of these are open source tools you
can actually start using for free right
now so we'll start by downloading and
installing git on our system we'll then
have a look at the git bash interface
we'll type in some basic git commands
next we'll create a local repository
that is we'll create a repository on a
local machine we'll then connect to a
remote repository and finally we'll push
the file onto github first things first
we need to download and install git so
download git for windows and i'll select
the second link
so 2.19
which is the most latest version of kit
that's the one we want for windows
system choose your version so mine is a
64-bit system and it's downloading so
this will take a while
so git is finally downloaded now we need
to install this on our system
click here
run
so go to next we don't have to change
this path
uh just so just click on in quick launch
and on desktop
next
next again next nothing to change here
either
and install
so now git is getting installed on our
system so we don't need to view the
release notes we just want to launch the
git bash
so let's just tick that and then click
on finish
and your git bash interface appears here
so we are on the master branch the first
thing we do is we'll check the version
for our git so the command is git dash
dash version
and as you can see version 2.19.1
on our windows system which is exactly
what we just downloaded we'll now
explore the help command so let's just
type get help config so config is
another command and as i hit enter the
manual page for the second command
opened up which is config so what help
command does is that it provides the
manual or the help page for the command
just following it
so in case you have any doubts regarding
how a command is used what a command is
used for or the various syntax of the
command you can always use the help
command now there's another syntax for
using the help command itself which is
git config dash dash help enter this
does the exact same thing as you can see
now that we looked at the help command
let's begin by creating a local
directory so mkdir
test
now test is my new directory i'll move
into this directory so cd test
great so now that we are inside our test
directory let's initialize this
directory so git
init is the command for initializing the
directory and as you can see as you can
see the path here this is the local path
where a directory is created so i'll
just show you the directory test and
it's completely empty
what we do now is we'll create a text
file within this new directory that we
created so new text document and i'll
just name this demo i'll open this and
just put in some dummy content
so hello simply done
save this file and go back to your bash
interface
let's just check the status now so git
status and as you can see
our file has appeared it's visible but
nothing is committed yet so this means
that we have not made any change to our
file through the git tool itself so the
next thing that we are going to do is we
will be adding demo to our current
directory the next command that we'll be
applying is the commit command and when
you add certain files to the current
directory the commit command is applied
on all the above directories so git
commit minus m and a message that will
appear once the file is committed
so as you can see one file is changed
and one insertion
i'll just clear the screen next thing we
need to do is we need to link our git to
our github account so the command for
doing that is git config
global user dot username
and this will be followed by our
username so let me just show you my
github account
so this is my github profile and my
username is
simplylearn.github so guys before you
begin this procedure just make a github
account type in my username here simply
learn github and enter and there you go
our git is successfully linked with
github next thing we do is we'll just
open our github
and we'll create a new repository give a
repository name so i'll name it test
underscore demo and create repository
great so our repository is created this
is our remote repository what we do next
is just copy the link and then go back
to your bash interface
now we need to link our remote and our
local repository
so git remote origin and then paste
the http link and now that our local
repository and our remote repository are
linked we can push our local file onto
our remote repository so the command for
doing that is git
push origin master as we are on the
master branch
and that's done so now let's move back
to github i'll just click on test demo
and as you can see here our local file
has been pushed to our remote repository
with that we have successfully completed
our demo so we're going to introduce the
concept of version control that you will
use within your devops environment then
we'll talk about the different tools are
available in a distributed version
control system we'll highlight a product
called git which is typically used for
version control today and you'll also go
through what are the differences between
get and github you may have used github
in the past or other products like get
lab and we'll explain what are the
differences between git and git and
services such as github and git lab will
break out the architecture of what a get
process looks like and how do you go
through and create forks and clones how
do you have collaborators being added
into your projects how do you go through
the process of branching merging and
rebasing your project and what are the
list of commands that are available to
you in get finally i'll take you through
a demo on how you can actually run git
yourself and in this instance use the
software of git against a public service
such as github all right let's talk a
little bit about version control systems
so you may have already been using a
virtual control system within your
environment today you may have used
tools such as microsoft's team
foundation services but essentially the
use of a version control system allows
people to be able to have files that are
all stored in a single repository so if
you're working on developing a new
program that such as a website or an
application you would store all of your
version control software in a single
repository now what happens is that if
somebody wants to make changes to the
code they would check out all of the
code in the repository to make the
changes and then there would be an
addendum added to that so um there will
be the version one changes that you had
then the person would then
later on check out that code and then be
a version 2 and add it to that code and
so you keep adding on versions of that
code the bottom line is that eventually
you'll have people being able to use
your code and that your code will be
stored in a centralized location however
the challenge you're running is that
it's very difficult for large groups to
work simultaneously within a project the
benefits of a vcs system a version
control system
demonstrates that you're able to store
multiple versions of a solution in a
single repository now let's take a step
at some of the challenges that you have
with traditional version control systems
and see how they can be addressed with
distributed version control so in a
distributed version control environment
what we're looking at is being able to
have the code shared across a team of
developers so if there are two or more
people working on a software package
they need to be able to effectively
share that code amongst themselves so
that they constantly are working on the
latest piece of code so a key part of a
distributed version control system
that's different to just a traditional
version control system is that all
developers have the entire code on their
local systems and they try and keep it
updated all the time it is the role of
the distributed vcs server to ensure
that each client and we have a developer
here and developer here and developer
here and each of those our clients have
the latest version of the software and
then that each person can then share the
software in a peer-to-peer like approach
so that as changes are being made into
the server of changes to the code then
those changes are then being
redistributed to all of the development
team the tool to be able to do an
effective distributed vcs
environment is get now you may remember
that we actually covered git in a
previous video and we'll uh reference
that video for you so we start off with
our remote git repository and people are
making updates to the copy of their code
into a local environment that local
environment can be updated manually and
then periodically pushed out to the git
repository so you're always pushing out
the latest code that you've code changes
you made into the repository and then
from the repository you're able to pull
back the latest updates and so your git
repository becomes the kind of the
center of the universe for you and then
updates are able to be pushed up and
pulled back from there what this allows
you to be able to accomplish is that
each person will always have the latest
version of the code so what is get
get is a distributed version control
tool used for source code management so
github is the remote server for that
source code management and your
development team can connect their get
client to that
remote hub server git is used to track
the changes of the source code and
allows large teams to work
simultaneously with each other it
supports a non-linear development
because of thousands of parallel
branches and has the ability to handle
large projects efficiently so let's talk
a little bit about git versus github so
get is a software tool whereas github is
a service and i'll show you how those
two look in the moment you install the
software tool for git locally on your
system whereas github because it is a
service is actually hosted on a website
get is actually the software that used
to manage different versions of source
code whereas github is used to have a
copy of the local repository stored on
the service on the website itself get
provides command line tools that allow
you to interact with your files whereas
git help has a graphical interface that
allows you to check in and check out
files so let me just show you the two
tools here so here i am at the git
website and this is the website you
would go to to download the latest
version of git and again git is a
software package that you install on
your computer that allows you to be able
to do version control in a peer-to-peer
environment for that peer-to-peer
environment to be successful however you
need to be able to store your files in a
server somewhere and typically a lot of
companies will use a service such as git
hub as a way to be able to store your
files so git can communicate effectively
with github there are actually many
different companies that provide similar
service to github git lab is another
popular service but you also find that
development tools such as microsoft
visual studio are also incorporating git
commands into their tools so the latest
version of visual studio team services
also provides this same ability but
github it has to be remembered is a
place where we actually store our files
and can very easily create public and
shareable is a place where we can store
our files and create public shareable
projects you can come to github and you
can do a search on projects you can see
at the moment i'm doing a lot of work on
blockchain but you can actually search
on the many hundreds of projects here in
fact i think there's something like over
a hundred thousand projects being
managed on github at the moment that
number's probably actually much larger
than that and so if you are working on a
project i would certainly encourage you
to start at github to see if somebody's
already maybe done a prototype that
they're sharing or they have an open
source project that they want to share
that's already available and in github
certainly if you're doing anything with
them azure you'll find that there are
thousands forty five thousand azure
projects currently being worked on
interestingly enough github was recently
acquired by microsoft and microsoft is
fully embracing open source technologies
so that's essentially the difference
between git and github one is a piece of
software and that's git and one is a
service that supports the ability of
using the software and that's github so
let's dig deeper into the actual git
architecture itself so the working
directory is the folder where you are
currently working on your git project
and we'll do a demo later on where you
can actually see how we can actually
simulate each of these steps so you
start off with your working directory
where you store your files and then you
add your files to a staging area where
you are getting ready to commit your
files back to the main branch on your
git project you will want to push out
all your changes to a local repository
after you've made your changes and these
will commit those files and get them
ready for synchronization with the
service and will then push your services
out to the remote repository an example
of a remote repository would be github
later when you want to update your code
before you write any more code you would
pull the latest changes from the remote
repository so that your copy of your
local software is always the latest
version of the software that the rest of
the team is working on one of the things
that you can do is as you're working on
new features within your project you can
create branches you can merge your
branches with the mainline code you can
do lots of really creative things that
ensure that that a the code remains at
very high quality and b that you're able
to seamlessly add in new features
without breaking the core code so let's
step through some of the concepts that
we have available and get so let's talk
about forking and cloning and get so
both of these terms are quite old terms
when it comes to development but forking
is certainly a term that goes way way
way back um long before we had
distributed cvs systems such as the ones
that were using with git to fork a piece
of software is a particular open source
project you would take the project and
create a copy of that project and but
then you would then associate a new team
and new people around that project so it
becomes a separate project in entirety a
clone and this is important when it
comes to working with git a clone is
identical with the same teams and same
structuring as the main project itself
so when you download the code you're
downloading exact copy of that code with
all the same security and access rights
as the main code and then you can then
check that code back in and potentially
your code because it is identical could
potentially become the main line code in
the future now that typically doesn't
happen your changes are the ones that
merge into the main branch but also but
you do have that potential where your
code could become the main code with git
you can also add collaborators that can
work on the project which is essential
for projects where particularly you have
large teams this works really well when
you have product teams where the teams
themselves are self-empowered you can do
a concept what's called branching and
get and so say for instance you are
working on a new feature that new
feature and the main version of the
project have to still work
simultaneously so what you can do is you
can create a branch of your code so you
can actually work on the new feature
whereas the rest of the team continue to
work on the main branch of the the
project itself and then later you can
merge the two together pull from remote
is the concept of being able to pull in
services software the team is working on
from a remote server and get rebase is
the concept of being able to take a
project and re-establish a new start
from the project so you may be working a
project where there have been many
branches and the team has been working
for quite some time on different areas
and maybe you kind of losing control of
what the true main branch is you may
choose to rebase your project and what
that means though is that anybody is
working on a separate branch will not be
able to branch their code back into the
mainline branch so going through the
process of a get rebase essentially
allows you to create a new start for
where you're working on your project so
let's go through forks and clones so you
want to go through the process so you
want to go ahead and fork the code that
you're working on so let's use this
scenario that one of your team wants to
go ahead and add a new change to the
project the team member may say yeah go
ahead and you know create a separate
fork of the actual project so what does
that look like so when you actually go
ahead and create a fork of the
repository you actually go and you can
take the version of the mainline branch
but then you take it completely offline
into a local repository for you to be
able to work from and you can take the
mainline code and you can then work on a
local version of the code separate from
the mainland branch is now a separate
fork collaborators is the ability to
have team members working on a project
together so if you know someone is
working on a piece of code and they see
some errors in the code that you've
created none of us are perfect at
writing code i know i've certainly made
errors in my code it's great to have
other team members that have your back
and can come in and check and see what
they can do to improve the code so to do
that you have to then add them as a
collaborator now you do that in github
you can give them permission within
github itself that's really easy to do
super visual interface that allows you
to do the work quickly and easily and
depending on the type of permissions you
want to give them sometimes it could be
very limited permissions it may be just
to be able to read the files sometimes
it's being able to go in and make all
the changes you can go through all the
different permission settings on github
to actually see what you can do but
you'll be able to make changes so that
people can actually have access to your
repository and then you as a team can
then start working together on the same
code let's step through branching and
get so suppose you're working on an
application but you want to add in a new
feature and this is very typical within
a devops environment so to do that you
can create a new branch and build a new
feature on that branch so here you have
your main application on what's known as
the master branch and then you can then
create a sub branch that runs in
parallel which has your feature you can
then develop your feature and then merge
it back into the master branch at a
later point in time now the benefit you
have here is that by default we're all
working on the master branch so we
always have the latest code the circles
that we have here on the screen show
various different commits that have been
made so we can keep track of the master
branch and then the branches that have
come off which have the new features and
they can be many branches and get so get
keeps you the new features you're
working on in separate branches until
you're ready to merge them back in with
the main branch so let's talk a little
bit about that merge process so you're
starting with the master branch which is
the blue line here and then here we have
a separate parallel branch which has the
new features so if we're to look at this
process the base commit of feature b is
the branch f is what's going to merge
back into the master branch and it has
to be said there can be so many
divergent branches but eventually you
want to have everything merged back into
the master branch let's step through get
rebase so again we have a similar
situation where we have a branch that's
being worked in parallel to the master
branch and we want to do a get rebase so
we're at stage c and what we've decided
is that we want to reset the project so
that everything from here on out with
along the master branch is the standard
product however this means that any work
that's been done in parallel as a
separate branch will be adding in new
features along this new rebased
environment now the benefit you have by
going through the rebase process
is that you're reducing the amount of
storage space that's required for when
you have so many branches it's a great
way to just reduce your total footprint
for your entire project so get rebase is
the process of combining a sequence of
commits to form a new base commit and
the prime reason for rebasing is to
maintain a linear project history when
you rebase you unplug a branch and
replug it in on the tip of another
branch and usually you do that on the
master branch and that will then become
the new master branch the goal of
rebasing is to take all the commits from
a feature branch and put it together in
a single master branch it makes it the
project itself much easier to manage
let's talk a little bit about pull from
remote suppose there are two developers
working together on an application the
concept of having a remote repository
allows the code to the two developers
will be actually then checking in their
code into a remote repository that
becomes a centralized location for them
to be able to store their code it
enables them to stay updated on the
recent changes to the repository
because they'll be able to pull the
latest changes from that remote
repository so that they are ensuring
that as developers they're always
working on the latest code so you can
pull any changes that you have made to
your forked remote repository to your
local repository the command to be able
to do that is written here and we'll go
through a demo of how to actually do
that command in a little bit good news
is if there are no changes you'll get a
notification saying that you're already
up to date and if there is a change it
will merge those changes to your local
repository and you get a list of the
changes that have been made remotely so
let's step through some of the commands
that we have in get so git and it
initializes a local git repository on
your hard drive get ad adds one or more
files to your staging area get commit
dash m commit message is a commit
changes the get command commits changes
to head up so the git command commits
changes to your local staging area get
status checks the status of your current
repository and lists the files you have
changed get locked provides a list of
all the commits made on your current
branch get diff use the changes that
you've made to the file so you can
actually have files next to each other
you can actually see
the differences between the two files a
git push origin branch name so the name
of your branch command will push the
branch to the remote repository so that
others can use it and this is what you
do at the end of your project git config
dash global username will tell get who
you are by configuring the author name
and we'll go through that in a moment
git config global user email will tell
get the author of by the email id get
cloned creates a get repository copy
from a remote source and get remote add
origin server connects the local
repository to the remote server and adds
the server to be able to push to it get
branch and then the branch name will
create a new branch for you to create a
new feature that you may be working on
git checkout and then the branch name
will allow you to switch from one branch
to another branch
git merge branch name or merge a branch
into the active branch so if you're
working on a new feature you're gonna
merge that into the main branch i get
rebase will reapply commits on top of
another base tip and get rebase will
reapply commits on top of another base
tip and these are just some of the
popular get commands there are some more
but you can certainly dig into those as
you're working through using get so
let's go ahead and run a demo using get
so now we are going to do a demo using
get on our local machine and github as
the remote repository for this to work
i'm going to be using a couple of tools
first i'll have the deck open as we've
been using up to this point the second
is i'm going to have my terminal window
also available and let me bring that
over so you can actually see this and
the terminal window is actually running
git bash as the software in the
background which you'll need to download
and install you can also run git batch
locally on your windows computer as well
and in addition i'll also have the
github repository that we're using for
simply learn i already set up and ready
to go all right so let's get started so
the first thing we want to do is create
a local repository so let's go ahead and
do exactly that so the local repository
is going to reside in my development
folder that i have on my local computer
and for me to be able to do that i need
to create a drive in that folder so i'm
going to go ahead and change the
directory so i'm actually going to be in
that folder before i actually create
make the new folder so i'm going to go
ahead and change directory
and now i'm in the development directory
i'm going to go ahead and create a new
folder
and that's gone ahead and created a new
folder called hello world
i'm going to move my cursor so that i'm
actually in the hello world folder
and now that i'm in the helloworld
folder i can now initialize this folder
as a git repository
so i'm going to use the git command init
to initialize and let's go ahead and
initialize that folder so let's see
what's happened so here i have my hello
world folder that i've created and
you'll now see that we have a hidden
folder in there which is called dot get
and we expand that we can actually see
all of the different subfolders that git
repository will create so let's just
move that over a little bit so we can
see the rest of the work
and now if we check on our folder here
we actually see this is users matthew
development hello world dot get and that
matches up with hidden folder here
so we're going to go ahead and create a
file called readme.txt
in our folder so here is our hello world
folder and i'm going to go ahead and
using my text editor which happens to be
sublime
i'm going to create a file and it's
going to have in there text hello world
and i'm going to call this one readme
dot txt
if i go to my hello world folder you'll
see that we have the readme.txt
file actually in the folder what's
interesting is if i select the git
status command what it'll actually show
me is that this file has not yet been
added to the commits yet for this
project so even though the file is
actually in the folder it doesn't mean
that it's actually part of the project
for us to do that we actually have to go
and
select
for us to actually commit the file we
have to go into our terminal window and
we can use the git status to actually
read the files that we have there so
let's go ahead and use the get status
command and it's going to tell us that
this file has not been committed you can
use this with any folder to see which
files and subfolders have been committed
and what we can now do is we can
go and actually add the readme file so
let's go ahead and select at git add so
the git command is add readme.txt
so that then adds that file into our
main
project and we want to then commit those
files into the main repositories history
and so to that do that we'll hit the the
get command commit and we'll do a
message
in that commit and this one will be
first commit
and it has committed that project what's
interesting is we can now go back into
readme file and i can change this so we
can go
hello get
git is a very popular
version control
solution
and we'll
we'll save that
now what we can do is we can actually go
and see if we have made differences to
the readme text so to do that we'll use
the death command forget so we do get
def
and it gives us two releases the first
is what the original text was which is
hello world and then what we have
afterwards is what is now the new text
in green which has replaced the original
text
so what we're going to do now is you
want to go ahead and create an account
on github we already have one and so
what we're going to do is we're going to
match the account from github with our
local account so to do that we're going
to go ahead and say get config
and we're going to do dash and it's
going to be a global user dot name and
we'll put in our username that we use
for
github in this instance we're using the
simply learn
dash
github account name
and under the github account you can go
ahead and create a new repository name
in this instance we call the repository
hello dash world
and what we want to do is connect the
local github account with the remote
helloworld.gov
and we do that by using this command
from get which is our remote connection
and so let's go ahead and type that in
open this up so we can see the whole
thing so we can type in git
remote add
origin
https
backslash backslash
github
dot com
slash
simply learn
dash github and you have to get this
typed in correctly when you're typing in
the location hello dash world dot get
that creates the connection to your
hello world account
and now we want to do is we want to push
the files to the remote location using
the get push command commit get push
origin
master
so we're going to go ahead and connect
to our local remote github so i'm just
going to bring up my terminal window
again and so let's select git remote add
origin
and we'll connect to the remote location
github.com
simply
learn
dash
github
slash
hello
dash world dot get
oh we actually have already connected so
we're uh connected to that successfully
and now we're going to push the master
gesh so get
push origin
master
and everything is connected and
successful
and if we go out to github now
we can actually see that our file was
updated just a few minutes ago
so what we can actually do now is we can
go and fork a project from github and
clone it locally so we're going to use
the
fork tool that's actually available on
github let me show you where that is
located
and
here is our branching tool it's actually
changed more recently with a new ui
interface
and once complete we'll be able to then
pull a copy of that to our account using
the fork's new http url address
so let's go ahead and do that
so we're going to go ahead and create a
fork of our project now to do that you
would normally go in when you go into
your project you'll see that there are
fork options in the top right hand
corner of the screen now right now i'm
actually logged in with the default
primary account for this project so i
can't actually fork the project as i'm
working on the main branch however if i
come in with a separate id and here i am
i have a different id and so i'm
actually pretending i'm somebody else i
can actually come in and select the fork
option and create a fork of this project
and this will take just a few seconds to
actually create the fork
and there we are we have gone ahead and
uh created the fork
so you want to say clone or download
with this and so this is the
i select
they'll actually give me the web address
i can actually show you what that looks
like i'll open up my text editor
just doesn't correct
i guess that is correct
so i'm going to copy that
and
i can fork the project locally and clone
it locally i can change the directory so
i can create a new directory that i'm
going to put my files in and then post
in that content into that file so i can
now actually have multiple versions of
the same code running on my computer
i can then go into default content
and use the patchwork command
to actually
so i can create a copy of
that code that we've just created and we
call it that's a clone and we can create
a new folder that we're actually putting
the work in and we could for whatever
reason we wanted to we could call this
folder patchwork and that would be maybe
a new feature and then we can then paste
in the url of the new directory that has
the forked work in it and now at this
point we've now pulled in and created a
clone of the original content
and so this allows us to go ahead and
fork out all of the work for our project
onto our computer so we can then develop
our work separately
so now what we can actually do is we can
actually create a branch of the fork
that we've actually pulled in onto our
computer so we can actually then create
our own code that runs in that separate
branch
and so we want to check out um the the
branch and then push the origin branch
uh down to our computer
this will give us the opportunity to
then add our collaborators so we can
actually then go over to
github and we can actually come in and
add in our collaborators
and we'll do that under settings and
select collaborators and here we can
actually see we have different
collaborators that have been added into
the project and you can actually then
request people to be added via their
github name or by email address
or by their full name
one of the things that you want to be
able to do is ensure that you're always
keeping the code that you're working on
fully up to date by pulling in all the
changes from your collaborators
you can create a new branch and then
make changes emerged into the master
branch now to do that you would create a
folder and then that folder in this
instance would be called test we would
then move our cursor into the folder
called test and then initialize that
folder so let's go ahead and do that so
let's call create a new folder and we're
going to first of all change our root
folder and we're going to go to
development
and we're going to create a new folder
we'll call it test and we're going to
move into the test folder and we will
initialize
that folder
and we're going to move some files into
that test folder
call this one test one
and then we're gonna do file
save as and this one's gonna be test
two
and now we're gonna commit those files
okay add
that kit add and then we'll use the
dot to pull in all files
and then git
commit
m
files
committed
make sure i'm in the right folder here i
don't think i was
and now that i'm in the correct folder
let's go ahead and
and
commit
and it's gone ahead and edit those files
and so we can see the two files that
were created have been added into the
master
and we can now go ahead and create a new
branch we call this one git
branch
test underscore
branch
and let's go ahead and create a third
file to go into that folder
this is
file three
into file save as
we'll call this one
test3.txt
and we'll go ahead and add
that file
needs to get
add
test3.txt
and
we're going to move from the master
branch to the test branch
git
check
out
test underscore
branch
i switched to the test branch
and we'll be able to list out all of the
files that are in the
branch now
and we want to go through and merge the
files into one area so let's go ahead
and we'll do git
merge test underscore
branch
and it's well we've already updated
everything so that's good so otherwise
it would tell us what we would be
merging
and now all the files are merged
successfully into the master branch
there we go all merged together
fantastic
and so what we're going to do now is
move from master branch to test branch
so get
check out
test underscore
branch
and we can modify the files the test
three file that we took out
and
pull that file up
and we can
now
modify
and we can then
commit
that file
back
in
and we've actually been able to then
commit the file with one changes and
obviously it's the text free change that
was made
now we can now go through a process of
checking the file back in switching back
to the master branch and ensuring that
everything is in sync correctly
we may at one point want to rebase all
of the work is kind of a hard thing you
want to do but it will allow you to
allow for managing for changes in the
future
so let's just switch to it back to our
test branch
which i think we're actually on we're
going to create two more files
let's go to our folder here and let's go
copy those
and that's created
we'll rename those tests
four
and
five
and so we now have additional files
and we're going to add those into
our branch that we're working on
so we're going to go and select
git add
dash
a
and we're going to commit those files
get
commit
dash
a
dash m
adding
to new
files
and it's added in the two new files
so we have all of our files now we can
actually list them out and we have all
the files that are in the branch
and we'll switch then to our master
branch
we want to rebase the master
so we do get rebase
master
and that will then give us the command
that everything is now completely up to
date
we can go
get
checkout
master to switch to the master account
this will allow us to then
continue through and rebase the test
branch and then list all the files that
are all in the same area
so let's go
get rebase
test
underscore branch
and now we can list and there we have
all of our files listed incorrectly
now we will be talking about that how
exactly we can make use of maven here in
performing various kind of operations
whether it's a build compilation various
kind of stuff there so let's talk about
it why exactly we use maven here now me1
is something which
is a kind of a build tool which is there
for most of the job-based projects here
so this tool also helps us in building
up the source code by downloading some
couple of dependencies these
dependencies are something which is
being configured by the developers which
a developer feels that the project is
dependent on and he requires those
dependencies to be downloaded so uh this
tool is specifically used to build and
manage any kind of a job-based project
whatever the complexity of the topic or
the project is there so this tool will
be able to handle that particular
requirement or that project so it
simplifies the uh day-to-day work of a
java developer and helps them in their
projects for performing daily to daily
activities
right now meven also helps us in getting
the uh specific jar files for each and
every project so if you are going you
can pretty much make a jar file var file
or er file any kind of a package
mechanism you can follow for your maven
there so to download the dependencies we
don't have to actually go to the
official website of each and every
software you can easily get it uh the
dependencies of each and every third
party band race on the mvnrepository.com
this is a website which is present on
which if you visit you will be able to
download the dependencies which is
required by your project or by your
specific
project over here so it totally depends
on how exactly you want to download and
you want to get together
a specific repository over here so
mvnrepository.com is the one if you
visit you will be able to download and
go through all the jar files dependent
jar files as independences which you
require for your current project
right so now what exactly is a maven all
about so let's see on that part
so
maven is a popular open source build
tool which is uh developed by the apache
group and the primary ownership of this
tool is to build publish and deploy
several projects at once movement is
written in java and it can be used to
build projects written in c shaft scala
ruby etc so apart from java these are
the different programming languages or
the tools which is supported by the
maven here for performing the build
activity so the tool is used for build
and management of any kind of a
job-based project it simplifies the
day-to-day
work of a java developer and helps them
to automate most of the compilation or
build uh perspective tasks on the
projects
right now maven is a kind of a bomb
based project so it also known as an
project object model and it focuses on
of simplification and standardization of
the building process in the process it
take cares of all the followings here
builds dependencies reports
distribution releases mailing list so
all these uh things are being taken care
as in this particular process here and
ultimately it's an availability inside
the maven that how the overall build
process needs to be automated and
standardized here so all these
particular mechanisms is can be followed
as such over here so that we can have a
good
standards and the simplifications
implemented as such here now in this one
we are going to talk about that how we
can work on maven installation on
windows and ubuntu platform here so
first of all we'll do it on the windows
platform here and we'll see that how the
installation really works so for this
one i will be doing the installation
into my local system now before even
going for maven maven is in kind of a
build tool which is available there
primary for performing any kind of build
automation for the java based projects
now since it's used for java we also
require the jdk installation onto our
system so for that both of the
installations we will be covering over
here and we'll see that how the
installation really works as such over
here right so before even going for
maven we have to install the jdk onto
our windows machine so that we can
proceed with maven because maven
requires the jdec installation to be
there now there are two ways of jdk
installation either you go for the exe
file or you extract the binaries of jdk
to a directory and then set up the java
underscore home path for that but if you
are using the executable that will
perform the path configurations for you
also so in that case you don't have to
reconfigure it again and again so that's
what we are going to see here so first
of all we will try to do the
installation of jdk so for that we'll go
to the website of jdk and then we have
to click on this one to access the maven
installations part here so that's what
we are going to do so let's proceed and
let's open the chrome and try to go
through these two urls so that we can do
the installations one by one
so here i have to just search for jdk8
download so uh it will show me the url
for oracle.com so here you will get the
jdk for the different platforms for
different operating systems now here
what you can see here that uh you have
the configurations in which you have the
jdk installation so we have to so here
i'll go for the windows 64 version
because that is something i will be
using to do the installation so this is
the complete executable link so here you
can download so we are downloading the
sdk a
u251 so
just search for windows 64 yeah this is
the one so let me click on this one
let me quickly log into the web page so
once the login is done you will be able
to see that it's trying to download the
exe file so that was for the jdk for the
java now for the me one i am going to go
for the
maven download over here so in this one
the maven repo link is there maven dot
apache dot orj is the link for this one
now here we have the different options
you can see here that the binder is like
archive is there then sources archive is
there so depending on the installation
which you want to perform you should be
able to download that file and it works
there so let's first install the jdk
portion and then we'll go with this one
so we'll click on this one also anyways
because uh it's something we have to
anyways do it so we'll click on this so
that this maven is also getting started
install or downloading over here so once
both of them are downloaded we will
proceed further with the setup so let's
wait for another one minute to do this
one
so the apache maven is kind of
downloaded over here so we wait for the
jdk download also over here
so next five ten seconds only left out
so first jdk will be there and then even
installation configurations will be
there so for me when we just have to
extract into a directory and give the
envian home path into the uh properties
uh into the variables so that it can
pick it up over there okay so we have to
extract the zip file for maven give the
path and then we will check with the
maven command whether it's working fine
or not so let's try to install the
executive so we'll open the directory
where the executables are there so first
of all we'll install the jdk so we'll
double click the installer for jdk it's
off around 200 mb so the complete jdk
will be installed
so we have to just go for next now when
you go with the installer so usually the
java home parts and all that stuff is
already configured so you don't really
have to change the passwords or java
home any kind of variable you have to
put it up all those things will be
automatically cleaned up
so it's running the installation
close
so then we have to extract the maven so
we have to extract this directory
because here the maven complete
executables and libraries are there so
we just have to extract it and place
into a directory so that we can further
on use it and access the path so once
the axis is done
yep so this is done there so i can
rename it to like maven so i can rename
it like to ask me one and i can cut it
down and put it into the seat drive here
now this is the maven home path which is
going to be there right so all i have to
do is that i have to close this one and
this one and this one and this
so what we have to do is that we have to
really go to the system properties so
that we can extract it so i'm going to
open the system properties over here
properties now here the advanced system
settings now in this one we will be
setting up the paths the environment
variable will go now here if you scroll
down you will see all the different
parts which is available there as such
over here
now if we want to see that if the java
is fine or we require for jdk also so
all we have to do is that we have to
open the cmd prompt window r we are
providing so using the window r i'm
opening the run window where i can say
like cmd now in this cmd i'm going to
say like java
hyphen version so java is there so which
means that we don't have any problem as
such
now i'll just go here and set up a new
one called mvn underscore home so here
the same path so you can browse for
directory also but i am using the mvn
home over here in this one
now i have just added that and in the
path variable in the last we have to
actually
add the
bin directory also so that whenever we
run the mvn command so that should also
be resolved so mvn underscore home is
not the only variable which you will be
configuring in fact you will be
confirming the path variable also that
you can go ahead and put up a new
directory entry for the cmd here so cmd
till the cmd directory you will go there
and whatever the executable is there you
should be able to resolve that
so for that again you have to open the
window r cmd prompt
java hyphen version
and then you can run mvn iphone version
here
when you run the mvn hyphen version it
will let you know that which particular
java runtime you are using
1.8.0 underscore 251 that's the same one
which we have configured
and it also let us know that where
exactly the executable is there and we
can now proceed further with the
execution of the maven pail so you just
have to check out your source code or go
to the directory where source code is
there and you can run the mvn clean
install so that will ultimately go for
the build process so this is the way
that how we can do the installation of
maven on the windows platform now since
this is done now we will go ahead with
the second installation where we are
going to do the installation of maven
and jdk onto the open to machine so for
that we will log into the particular
cloud lab of simply learn lms so let's
log into the lms here so now we are
going to do the installation of jdk and
maven on the ubuntu virtual machine here
so for that we will log into the lms so
let's log into that so this is the uh
elements over here here i have already
started the lab here so all i have to do
is that i have to just click on this
link here so it will open the lab into a
new tab it's a gui mode of ubuntu server
so which we will be using to configure
it
so uh here uh first of all i can open
the chrome browser also so let me open
the browser here so that i can download
the maven executables as such over here
so jdk we can do it using the package
installation so uh using the app
executable we will be doing the
installation
so i have to just open the terminal here
so once the terminal is opened you have
to log in through the root id now why we
require root id so that we should be
able to do the package installations and
other configurations so that requires
root access
now once that is done you have to run
the
particular
app update command
next you have to install the
installer so the oracle installer we
have to install now uh once the setup is
done so let's down install the latest
package over here for eco oracle iphone
java 14
hyphen installation so this will install
the jdk 14 over here so for that
configurations now uh at the same time
we'll go for maven download also
so here uh
we'll go for download maven so this is
the tar file for executables so i'm
going to copy this url here
and
over here we'll say yes
yes
yes
so now uh the java should be configured
and could should be set up over here in
this one so what we really need to do is
that we need to run the java command to
see that if the installation is done
properly or not so uh let's open the
terminal again another terminal so that
i can perform the steps related to maven
so i have got the url so i'm going to
the opt directory which is always an
optional directory where we can do the
setup and all those configurations
so i'm going to run the w gate command
and then the url which i have downloaded
so this should uh download the tar file
over here of maven now once that is like
done so i have to extract it
so uh tar hyphen xv zf and the entire
file name this is the targenzip file so
i have to use tar hyphen xp zf parameter
over here to extract it now what exactly
i have got i have got the apache maven
directory over here so i am going to
rename it in order to rename it what we
need to do we need to actually go for
the particular move command so move
whatever the existing folder you have
and then i'm going to rename it with the
maven directory now once that is done
i'm going to the maven directory and
this is the path or maven underscore
home path which is available as such
over here in this one so here all the
bin directories executables all this
stuff is there so now i can use it like
mv underscore home and the same
configuration just like we did in the
window so here also it the same process
we have to follow so java hyphen version
should be let me know that which version
is deployed so 14.01 version is deployed
over here so for java i do not don't
have to set up the path again here
because i am viewing for the installer
so i don't have to set up the path
right so for maven now what exactly i
have to do i have to go to the edc
profile d now here i'm going to put up a
particular file now here you can see
that we have the jdk
file here which means that the path
variables and everything is configured
as such over here so you can see that
java underscore home path all these
variables are copied here so i'm going
to also create a same file similar to
this one here but in this case what i'm
going to use it that i'm going to remove
some of the values so i'm going to have
some of the parameters added so in order
to do it effectively i can do the echo
command
so that i can copy the things from here
and i can simply put up into the file
here so i'm going to have the path
variable added up over here now why i'm
putting up the paths because it's the
same configuration which we did over
there also so opt slash maven slash pin
so all the pin directory should be added
to the
specific path variable so i'm just
trying to create a file called maven dot
ss just like a jdk dot sh over here i'm
also creating similar kind of file over
here called maven dot sh now i'm not
going uh in the vi mode or other editors
because i just want to make you
understand that how i'm copying it the
content i'm taking the jdk dot sh as in
reference and putting up the details
over here in this one and then i'm going
to export mvn underscore home over here
which is related to like opt
slash
maven so this is what we are going to
put up over here and then wavin in the
end me when this is the file which you
should be having now why we are using it
because of the fact that it's being used
or it will be utilized in such a way
that you can perform the changes or
whatever configurations you want to
perform this will help you to set up the
certain parts like a path and ambient
home so that you can run these scripts
as such over here so i'm going to give
the executable permissions to this one
and that's it so now what exactly i have
to do is that i have to do the source
here
profile
dot d
maven
once that is done i will be able to run
the mvn hyphen version over here and the
jdk part is already open so it's going
to refer to that part so this is the way
that how we configure the mi1 and the
jdk specifically on the ubuntu virtual
machine
so pretty much that's it for this demo
in this demo we primarily talked about
that how we can do the installation of
jdk and maven on windows and how we can
do the similar task on the maven on the
ubuntu virtual machine also here hello
everyone welcome to this demo in this
demo we are going to talk about it how
exactly we can do the installation of
maven on the mac operating system so
here we are going to talk about it how
we can do for a specific setup where we
can do for the installation of maven in
a mac operating system so let's see so
there are two ways of doing the
installation here first is through the
brew executable on the mac now this brew
utility on mac is just like apt or yum
utility which is present in most of the
environments or most of the platforms
here but here we are using this brew to
do the installation of maven on the mac
operating system and again
next one is just in a particular
mechanism where you should be able to
have these setup like you can download
the binaries from the apache website
and then you can export the empty one
discord home and the path variable so
that you can start using the maven
executable so we'll talk about these
things and we'll see that how exactly we
can go for this setup so let's login uh
to a particular
machine or mac operating system so where
we will be able to do all this activity
so this is the mac system which we have
here now in this one what we are going
to do is that we are going to open the
terminal so we are going to open a
terminal here so this is my terminal so
usually in mac you don't require any
other mechanism so the inbuilt terminal
is more than sufficient for doing this
stuff because it's similar to a linux
based platform here so i'm going to run
the command here so first of all i'll
just double check that if my brew
executable is working fine so i'll just
run the brew help command to see that uh
if my brew is working fine as such over
here in this one now i have the command
called brew install so that is the
command which we need to use now in this
one we have to actually give the
specific package which we want to
install so we will say like brew install
maven
so it will download the uh specific
executables on the system
from the website it will download the
maven executables so it will take some
time but ultimately it's trying to
download it under the particular tar
file so you can see here that it's
downloading the 3.6.3 dar file over here
so uh i can see that the installation is
done so the 3.6.3 version should be
installed over here so when i do like
maven mvn under hyphen iphone version
over here so you can see that the
3.6.3 version is deployed as such over
here now this is something which is done
with the purely command over here called
brew which really helps us to do any
kind of package installations not only
maven other packages also can be
installed pretty much with the help of
this executable
now in order to see the manual stuff
what we really need to do is that we
need to actually clean this uh specific
installation so that i should i will be
able to show you the manual installation
also over here so what i'm going to do
is that i'm going to run the command
called
brew
uninstall maven so that the maven
should be removed from your system so we
are just trying to remove the maven from
here and you can see that the maven
executable is not working as such over
here so which means that the maven is
uninstalled in this case but why i have
uninstalled here so that i should be
able to
go ahead and download some custom maven
over here in this one so i have opened
the browser here so i'll go for the
maven executable so we'll go for maven
download so that
we should be able to download a specific
the executable here so i'm going for
this directory so here uh 3.6.3 version
same is available here so i'm going to
download the
bin.gz which was
the file we have so this is also a kind
of a linux platform right so that's the
reason why uh here the tar file should
work perfectly fine so here i'm going to
the opt directory where we can have all
the optionals
related packages so if it is not there
so i can create a directory called slash
opt
so i can
log in with
root
so if you don't have the root access so
you can go for the applications in the
applications you can actually create a
directory called maven so here i can go
for maven
yeah
so here we will be downloading the var
file here so the uh dot gang file we are
downloading over here so i'll give the
path here so once the
var file or w gate command i am using to
do the installation but sometimes the
wkate is not available so i can go for
the brew install wget also if that
package is available on video the
installation will be done so we'll try
to do the installation using grab locate
so that we will be able to get a tar
file generated or downloaded there are
other ways also you can download it from
the browser also yeah you can go to the
browser and here you can actually click
and download it and then you can
transfer it locally to this uh specific
directory also so both the possibilities
are there but i'm just trying to
download the double gate command so that
it will be easy for me to do the setup
and to do the download over here so i'm
going to run the same command again
so it will download the tar file so this
is the tar file which is available over
here so i'm going to
extract it over here
so once the extraction is done so i'm
going to rename it like to apache maven
so that i should not have any kind of
versioning or these kind of parameters
over here and the apache data
maven hyphen
this star file i'm trying to remove as
such over here so this is what i have
got as an end result over here now this
is the current directory in which i have
these uh applications and these setups
going forward over here so if i run the
mvn hyphen f1 version command
so that will not be giving me any kind
of output over here because
uh still i have extracted the tar file
but that is not the activity is not
complete i have to exactly set up some
environment variables so that i should
be able to run this mvn command so what
i really need to do over here is that we
have to
export certain variables
application slash me1
apache m1 so this is the home directory
which i'm trying to configure over here
and then next thing is that path
variable so i'm going to have we are
going to carry the path variable
with the folder so i'm going to have the
folder structure here
so
the bin directory we have to provide so
i'm going to copy here
and
paste and then i'm concatenated with the
bin here so now my environment variables
are established so my mvn hyphen iphone
version command as you can see here is
that uh you know it's being picked up
over here but it's saying that the
particular java runtime is not available
so i can go for java reinstall java so
that should be enough there to install a
java package there so you can see that
it says that it's already installed so
and up to date so that's the reason why
it's not able to do the installation so
i'm going to reinstall
open jdk
because what happens that when we do the
installation some environment variables
related to java should be uh formatted
or should be extracted over here right
so as you can see that uh here it's
saying that if you want to really use
the executable of java you have to
concatenate the path like this so let's
run that and uh then we can see that the
particular
executable of java will also be
find out over here so java hyphen
version showing that yes it's available
there and now my command
should be able to show me that yes it's
working fine the gdk was already
installed but the problem with that is
that since it's installed into a
particular location so you have to
actually override your path value so
that you should be able to see in that
directory and find out the java
executable so that i have got by doing
the installation i got the command over
there in the output saying that this is
the installation which is done and now
you just have to extract or you have to
just you know put that bar directory
into the specified uh path variable and
that's how you got the maven and java
both command lines working over here so
this is how you will be doing the maven
installations all together how the
overall installation of maven and the
jdk really works on mac operating system
now what exactly is a maven repository
all about
so maven repository is uh something
which uh is a
kind of a directory or it's kind of a
location where we have all the jar files
packaged all together in a single
location so depending on the software
which you want to download the dependent
java files can be downloaded from there
so the metadata refers to the palm files
relevant to each project here this
metadata is what allows maven to
download the dependencies now there are
three type of repositories from where we
can download the dependencies the first
one is the local repository second one
is the remote repository and the third
one is the central repository so these
are the primary three type of
repositories which is available there
for our specific maven over here
so whatever repository you want to
follow you want to access you can pretty
much access it and you can go with that
right
now the local repository will be primary
present onto your pc that's the way that
how it's present there and the
particular remote repository is
something which you can store on any
remote machine or remote server and the
central repository is something which
you can have it onto the internet so
usually the central repositories is
something in which we can actually host
our different kind of dependencies and
the jar files over there
so local repository on the machine of
the developer where the project material
is saved so all the particular
dependencies jar file will be available
on your local machine remote machine is
the location where you can actually
store all your particular dependencies
from where you can download these
dependencies whenever you require it so
this and repositories work similar to
the central repository whenever anything
is needed from the remote repository
it's first downloaded if into your local
repository and then it will be utilized
so if you feel that the dependency is
something which is not there in the
local it will be first downloaded from
the remote repository and then it will
be utilized or will be referenced onto
the local machine there
and central repository is refers to the
maven community repository there where
you will be able to see each and every
dependency present on that repository so
maven downloads the repositories from
here in the local repository whenever
they're needed or whenever required all
these dependencies are cached locally
from where it will be referenced in the
next or you know whenever we want to
refer that we will be able to do that
particular reference easily on that part
so this is the way that how the three uh
particular repositories or movement
repositories really helps us for
performing various kind of automations
as such here
right now what are the basic concepts of
maven here so some of the couple of
basic concepts which is present in maven
is like for the first time we have the
project object model so project bomb
reference to the xml file which has all
the information regarding the project
how this particular project should be
built up how the different build process
should be automated there it also has a
description of the project details
regarding the versioning versioning
information is also stored as such in
this xml file in this
particular form reference file as such
over here the xml file is which is there
in the project home directory so maven
is
something which search for the palm file
in the current directory if it's able to
find that particular pom file so it will
execute or it will proceed with the
build step otherwise they it will halt
there itself so this pawn file is very
important because it will act as an kind
of a built script and primarily the
maven tool will be able to process each
and everything which is configured as
such in this
specific xml file there and according to
that only it will function or it will
proceed further
right now second thing is the
dependencies and repositories so
dependencies are usually refers to the
java files so the java libraries which
we need in our into our particular
project so while uh working on the
project we may be having the dependency
on couple of dependencies so these
dependencies we have to actually
download or we have to actually refer
and once these a particular information
or these details are referred so what
will happen that uh we need to download
these specific dependencies from the
particular maven repositories so if
their dependencies are not present in
your local repository then maven will
try to download it from the central
repository and cache it into your local
repository but first of all it will look
on into the local repository and if it
is not present in local then only it
will go to the remote or the central
repository
then we have build lifecycles and phase
and goals there so build lifecycles
consists of the sequence of build phases
and each field phase consists of
sequence of goals here now each goal is
responsible for performing a specific
task when a phase is uh running all the
goals related to that phase and its
plugins are also used to following the
compilation preparing the artifacts
downloading the dependencies all
different things being done by the
plugins here
then we have the build profile so build
profiles refers to a set of
configuration values now generally we
have the generic build process for all
the environments all the particular code
base there but sometimes we do have some
kind of a differences there so that is
where we can go for the profiles because
a specific profile can have its own
configuration and uh whatever it's
required for the build process we can
have different different configurations
stored inside the maven build script
here
then with different build profiles are
added to the pom files and which enables
the different uh build as such here so
depending on these profiles we can
actually decide that how we need to
proceed with the build process and how
the different kind of build automations
needs to be performed here
then a build profile helps in
customizing the build for the different
environments so that also we can perform
as such while workforming or while
working on the different components and
the different management here so a build
profile is something which you can
configure you can utilize and depending
on that build profile we will be
deciding whether to proceed further or
how to decide on that particular factor
so these are something which we need to
take care or we need to work according
to see which we can decide that how the
specific configurations and specific
automation needs to be performed here
now
usually when we go for the
maven there so we have to use some
couple of maven plugins now the goal of
using these specific plugins is to
automate some of the basic stuff like
compilation creating the var file
creating the java file so when we are
using the plugins we don't have to
configure the steps or write the steps
that how these uh specific plugins
should be installed or should be
referenced it's ultimately the internal
configurations which we are looking
forward or which we are using while
working on these specific plugins so you
can download some couple of plugins
using which we will be able to decide
that how a specific goals needs to be
achieved here so maven has its own
standard plugins that can be utilized so
if uh you know want to go for a custom
one so you can do that but pretty much
you can go for the default one and uh
you can pretty much have some uh
specific goals executed with the help of
these build plugins
now next thing is the maven architecture
so uh in maven what happens that uh
let's talk about the basic architecture
of maven here so
the maven executable when it's deployed
so it usually uh reads the palm.xml file
which is the build script here now once
the specific found xml file is being
read out so it will uh process the
components it will uh you know download
or it will process the dependencies
plugins life cycles phases and goals and
even the build profiles and if any kind
of interaction is required so it will
try to connect to the central repository
or remote repository and according to
that it will provide the information and
provide the details as such so minimum
repository is something which is very
important because it's ultimately trying
to connect to the maven repository and
try to achieve the various kind of
automation here so it's very important
because ultimately it's trying to help
us to go for the
complete end-to-end build automation
when we'll go for a specific bomb based
build automation
so movement executable read the palm.xml
file then download the dependencies
defined into the power xml file and that
dependencies are cached locally from the
remote or the central repository now
then uh create and generate a remote uh
report according to the requirement and
execute the life cycles phases goals
plugins etc so it will process the power
xml file then and whatever the plugins
and goals or the life cycles which is
configured there step by step each of
them will be executed into a sequence
here
now let's talk about the overall build
life cycle of the maven here so what
happens that uh the maven build life
cycle is something which is a kind of a
collection of different steps there and
this is something which is followed to
perform a build automation for a
specific project so they are pretty much
three steps which is being done so first
one is the default so it handles the
project deployment there second handles
the project cleaning which is a clean
there and then we have site so it
handles the creation of the project
sites documentation there
so these are the three built-in
particular life cycles which is
available there at the paper level which
you can utilize to perform a various
kind of particular steps execution for
your project
all right so the build cycle has the
different build phases or stages there
because when we are performing a build a
variety of things needs to be performed
over here so the first one is the
compile then test compile is there so
compilation is there for the source file
and test compile is there for the test
cases test will execute the test cases
package will package the bundle then we
have the integration test to run the
integration test cases then we have the
verify then install goal is there and
the last one is the deploy which is
there to deploy the artifacts to the
artifactory so all these things are the
different phases and the stages which is
being followed as per a specific build
here so during these builds these are
the different phases and these stages
which we normally follow for performing
the build automation
now what is the exactly advantage of a
specific maven here so that is something
which we need to talk about that how the
particular automation needs to be done
or how we can actually go for the
configurations where we feel that yes we
want to perform or we want to take
certain benefits out of the
implementation of maven here so for that
what happens that we have to see that
the various kind of benefits the first
one is that apache maven helps manage
all the processes there such as building
documentation releasing and distribution
in project management here so the tool
simplifies the process of project
building it increases the performance of
the project and the building process all
together and the task of downloading jar
files and other dependencies is also
done automatically we don't have to
indulge in any kind of download part as
such manually over here
right and um provides easy access to all
the required information so may 1 makes
it easy for a specific developer to
build a project in the different
environments so we don't have to worry
about the environment we don't have to
worry about the infrastructure nothing
is required everything is available
there inside this package and you know
depending on the particular you can
write the source code on one machine and
you can pretty much do the build on
another machine and this is the reason
how the build automation really happens
on the first place because the developer
is writing the source code in one
location and the same code is deployed
onto the particular or built onto this
jenkins or any kind of build tool there
right in maven you can easily add new
dependencies so you must write the
particular dependencies in the form file
so if you feel that you want to download
some particular dependencies so all you
have to do is that you have to put that
dependency into the
specific form.xml file there and
depending on that the execution will be
really performed and executed as a chair
right so let's talk about the demo now
so let's see that how exactly we can go
for this demo and we can perform the
various kind of automations
so this is the virtual machine which we
have here on which the maven is already
installed so we can run like mvn
so maven will be available as in three
point six point three here now i'm going
to run a particular command called uh
mvin arc type generate here
let me create a directory here
our temp directory
and to perform this activity over there
so mvn
arc type
generate now once we run that so what
will happen that it will uh download
some of the binaries there because uh
ultimately what we are trying to do is
that we are trying to generate a new
project like a maven project so a couple
of uh particular plugins will be
downloaded by the maven executable so
that it can achieve that particular
execution so we just have to wait for
downloading all these values now here
it's trying to give us a particular
attributes like it's asking the
different attributes over here so what
exactly we want to configure so if you
want to configure you can provide that
details otherwise you can perform or
whatever the setup you want to perform
now here it's asking for the version so
uh which kind of version we want to
follow so i'm going to follow like five
here so i'll press five
then a group id which is there so uh
it's basically a kind of a group
mechanism so i can say like com
dot simply learn so that's the value
which i'm providing here artifate id i
can make it like a
sample project or something like that i
can do
so that will be the artifact uh
id which is there so
version i am keeping the same only so
and uh yeah so package same here
so i just want to create so i'll just
provide the value called yes and enter
so with this what will happen that a
sample project will be created here
right so whatever the artifact id you
provided so according to that the
project is created in this directory so
you have to go into this directory and
see that what exactly the files are
created there so you have the power.xml
file now this form.xml file when i open
so you can see here that there are some
attributes like you can have the values
uh related to what version group id you
want to follow so this is a group id
this is the artifact id so this is jar
file by default you can change it
according to your requirement and this
is the version and if you feel that you
want to do the changes to the name that
also you can perform here so by default
the junit dependency is added but if you
want to keep on adding your own custom
dependencies you will be able to do that
now in this case if you run like mv and
clean install so it will be considered
as in particular maven project a parlor
xml file is already there present in the
local directory so according to that the
execution of the steps will be performed
and according to that you will be able
to get some desired values here
so ultimately in the target directory
you will be able to see that some couple
of java file or a specific jar file is
generated here so you can see that in
the target directory this jar file or
this artifact is generated here so this
is a way that how we can actually go for
a generic one like a new uh particular
project and later on you can depending
on your uh understanding you can keep on
adding or you can keep on modifying the
dependencies and that's how you can get
the
final result there
so that's it for this demo uh in which
we have find out that how exactly we can
go for a particular project preparation
with the help of mvn executable
welcome everyone to this topic in which
we are going to talk about that what
exactly is the different maven interview
questions here now in this one we are
going to talk about what are the
different questions some couple of
questions we are going to go through and
we will try to understand that what
exactly the answers are
now uh let's talk about the first
question over here so what exactly is in
maven here so maven is nothing but a
kind of a popular open tool uh open
source build tool which is available
there now before maven there were a
couple of build tools which was present
like and and you know a lot of other
legacy tools was present there but
after that maven is something which was
uh released as an open source tool and
it really helps the organization to
automate some couple of build processes
and you know have some uh particular
mechanisms like build publish and deploy
of different different projects at once
itself so it's a very powerful tool
which can really help us to do the build
automations we can integrate them with
the other tools like jenkins and you
know we can automate them we can
schedule the builds so a lot of various
advantages we can get with the help of
this tool here it's primarily written in
java and it can be used to build up
various other kind of projects also like
c sharp scalar ruby etc so all these
other typical tools can also be built up
with the help of this tool so this tool
is primarily uh used to do the uh
particular development and management of
the artifacts in the java based projects
so uh for most of the job-based projects
nowadays this is the default tool and
it's already integrated with the eclipse
also so when you go for a new women
project automatically
it will be created for a java project
you can use it for other languages also
but yes
default choice of java of of the java
programming language is maven build tool
only now let's talk about the next
question so what does the maven help
with so maven apache maven helps to
manage all the processes such as build
process documentation release process
distribution deployment preparing the
artifact so all these tasks is being
primarily taken care by the apache maven
so this tool simplifies the process of
project building it also increases the
performance of the project and the
overall building process so all these
things are something which is being
taken care by the specific maven tool
here so it also uses the particular uh
you know it downloads the jar files of
the different dependencies for example
if your
source code is dependent on some of the
apache web service uh jar files or some
of the other third-party jar files in
that case you don't have to download
those jar files and keep in some
repository or keep it in some lib
directory you just have to mention that
dependency in the maven and that jar
file will be downloaded during the build
process and will be cached locally so
that's the biggest advantage which we
get with maven that you don't have to
take care of all these dependencies
anywhere into your source code system so
meven provides easy access to all the
required information it uh helps the
developer to build the projects and uh
without you know even worrying about the
dependencies processes or different
environments or different because it's
an uh kind of a tool which can be used
in any platform linux or windows so they
don't have to do any kind of conversions
so all they have to do is that they have
to just add new dependencies and that
should be updated into the pom file and
depending on that dependencies the
source code will be built up and they
don't have to refer any kind of
third-party jar files so they don't have
to play with the class bars during the
build process so no customizations is
actually required with this one now the
next question is that what are the
different elements that may one take
cares of so there are different kind of
elements which is being taken care by
maven so uh the these particular
parameters are elements are bills
dependencies reports distribution
releases and mailing list so these are
the typical different different uh
elements which is being taken care by
the maven during the build process and
during the preparation of the builds
here so all these things you can they
can explore they can extract on that
part and they can fully understand that
how they can work on all these different
different processes now next question is
that what is the difference primary
difference between the and and maven
first of all both of them are primarily
used for the java based project so and
is the older version and maven is
something which was launched after the
ant here so and has no formal
conventions like so which can be uh you
know coded into the build.xml file there
but yes the maven has convention so
information is not required as such in
the palm.xml file there so ant is
procedural whereas maven is declarative
over here so and does not have any kind
of life cycle so it depends on you that
how you program the and there but
maven is having a lot of life cycles
there which we can configure we can
utilize so the uh and related scripts
are not reusable because you cannot
reuse it and you have to do some kind of
customizations in order to work on that
but yes maven is not having much of the
project related any kind of dependencies
they can be easily reusable there
because there is nothing about the
palm.xml file it's just the artifact
name and the dependencies which is uh
something we can
override or we can change and then the
same palm.xml file we can reuse as such
for the new project also so that is
where the reusability comes into the
picture now and is a very specific build
tool so we don't have to there is no
plugins as such which is available there
you just have to code everything that
what build process you want to prepare
whereas in case of maven we have the
concept of plugins which can really help
us to understand that how we can make or
use of these plugins so that we can have
the reusability implemented so these are
some of the differences which is
available there between the ant and
maven here now next thing is that what
exactly isn't palm file all about so
palm file is nothing but it's kind of a
xml file which is available there and
it's have having all the information
regarding the project and the
configuration details so it primarily
used over here that how the
configuration needs to be done and how
the setup should be performed as such
here so palm technical file is the build
script which we prepare
you can prepare it uh using a particular
component or you can have a particular
mechanisms or if you feel that you want
to have some kind of setup so all these
things typically can be implemented can
be done with the help of build tools so
build tools can be really helpful for us
to do the automations and it can really
help us to understand that how some
build processes we can automate with
simply with the help of palm.xml file
here so the developers usually put up
everything inside these uh dependencies
in the palm.xml file here so this is the
file which is uh usually present in the
home directory it's in the current
directory so that once the build is
triggered it will be picked up from that
directory and according to the steps
according to the content of the palm.xml
file the build will be processed or will
be created here now what i'll
incorporate into the palm file here so
the different components which is
included into the palm dot xml file here
is the dependencies uh developers and
contributors plugins plugin
configuration and resources so these are
the typical components which is a part
of a permanent xml file which can be uh
same form a lot of projects you can do
some customization and then the same
prompt file can be reused for the other
projects also now one of the minimum
requirement of the elements which is
there for a pump for uh palmer xml file
so without which the pound xml file will
not be validated and we will be getting
a kind of validation errors so the
minimum required elements are project
root
model version so it should be 4.0.0 the
group id of the project the artifact id
of the project and the version of the
artifact these are the minimum things
which we want to define so that we can
understand that what kind of artifact we
are trying to prepare or we are trying
to create here so these are the minimum
required elements which is required in
the palm.xml file without which the
validation of the palm file will fail
and the build will also fail here
now what exactly is the mint uh with the
term called build tool so build tool is
an essential tool isn't kind of a
process for building or compiling the
source code here so it's needed or it's
required for the below for generated
processes if you want to generate the
source code if you want to compile the
source code you want to generate the
source code you want to generate some
documentation from the source code you
want to compile the source code or you
want to package the source code whether
it's a jar file it's a war file or it's
a er file so whatever the packaging mode
you want to select you will be able to
do it with the help of the particular
build tool here and if you feel that you
want to upload these particular
artifacts to the artifact tree whether
it's on remote machine or locally there
so that also you can do it with the help
of this particular build tools here so
build tools can be helpful in doing a
lot of activities for the developers
now one of the different steps which is
involved to install maven on windows now
all you have to do is that you have to
just first of all download the uh tar
file from the maven apache maven
repository there once that is done so
what happens that you have to set up
some couple of environment variables now
if you download the java jdk using the
exe file in that case the java
underscore home will be configured
automatically but if it is not available
and you are not able to run the java
command line in that case you have to
set up the java underscore home and then
similarly for maven you have to go for
the maven underscore home that
particular variable you have to
configure now once that is done all you
have to do is that you have to edit the
path variable so the bin directory of
the maven extracted a folder you have to
put it up into the path variable and
once that is done what will happen that
you will be able to
check the latest version the version of
the maven over there if it is like some
old version again you have to extract
the latest version and do the steps all
together again so these are some of the
ways that in which you can actually go
for the installation or the
configurations of maven on the windows
platform
now what are the different steps which
is involved for the installation of
maven and ubuntu so ubuntu it's fine you
just download the package of java jdk
there once the jdk is installed over
there what you can do you can simply go
and say that yes i want to search for a
particular maven package which is
available there so once the jdk is
installed all you have to do is that you
have to configure the java underscore
home
m3 underscore home main and disco home
and the path variable once all the path
variables are something which is
configured then we will be able to check
the latest version like whether it's a
the version is correct or we are getting
the standard version over here or not
over here so that's the main mechanism
that how we will be able to you know
configure the maven on ubuntu here now
what exactly is the command to install
to char into the locals repository now
sometimes what happens that we are not
able to fetch like uh some dependencies
not present on the
particular central repository minimum
repository or your artifactory
repository in that case you have some
third party jar which we want to install
locally onto your repository so in that
case we can go for the uh but we can
download the java file there and then we
can run the command called mvn install
install hyphen file and then we are
giving the path like hyphen d file where
the path of the file should be provided
now once that is done so what will
happen that in the local dot m2
directory this specific artifact will be
downloaded and will be installed there
so this is a mechanism where you will be
able to configure or you will be able to
set up the artifacts locally the java
file locally here in the local
repository so next question is that how
do you know that the version of the
maven being used here so the inversion
of the maven is pretty easy to calculate
so all you have to do is that you have
to just go for mvn and space hyphen
iphone version the moment you do that it
will let you know that what jdk or what
java version you are using and it will
also show you that what particular maven
version you're going to use here so all
that details you will be able to get
with that particular command here
now what exactly is the clean default
and frighten variable here so these are
the build cycles which is available
there in within maven so these are the
built-in build cycles so for clean what
happens that this life cycle will help
you to perform the project cleaning so
usually during the build there are some
files which is created into the target
directory so the clean lifecycle is
essentially helping us to clean up all
that directory all that particular
target directory and when we talk about
the specific default so default
lifecycle handles the projector
deployment that is the default lifecycle
and site is something which is uh you
know helpful for creating the site
documentation
you know it's kind of a life cycle which
is available there so clean default and
site are the different life cycles which
can perform different different kind of
uh attributes or different tasks here
next question
what exactly is a maven repository so
maven repository refers to the
directories of the package jar files
that contain metadata now the metadata
refers to the palm files relevant to
each project so here you can able to get
your artifacts uh stored there you can
download these artifacts also during the
maven build if you put up that
dependency you will there are different
kind of repositories which is available
there one is the local repository one is
the remote repository and one is the
central repository so these are the
different typical type of repositories
which we have where we can store the
artifacts also and from where we can
download the artifacts also whenever
required now the first one is the local
repository so local repository refers to
the machine of the developers itself
where all the project related files are
stored there now whenever we work on the
uh particular maven so there is an in
the home directory dot m2 folder is
created now usually whatever the
artifacts downloaded from artifactory or
from the
maven repository it gets cached locally
there and once it is downloaded next
time it will not download the same
artifacts or the same dependency all
together again so this local repository
is something which is available locally
on the developers machine only so it
contains all the dependent jars which a
particular developer is downloading
during the maven build now remote
repositories refers to the repository
which is present on the server and uh
with from where we will be downloading
the uh particular dependencies so the
when when we are running the maven build
on a fresh machine so usually over there
the local repository does not exist so
in that case what happens that the dot
m2 directory is empty but the moment you
run the build what will happen that the
artifacts or the dependencies will be
downloaded from the remote repository
and once it is done once it's uh
downloaded it will be added or it will
be downloaded cached locally there and
it will be uh helpful in the future run
so that will be
considered as a local repository because
all the artifacts all the dependencies
are downloaded there and central
repository is something which is known
as the maven community where all the
artifacts is available there so usually
we cache or we mirror these central
repositories as our particular remote
repositories because it could be a
possibility that these remote
repositories are something which we are
hosting into our organization and
central repository is something which is
available centrally for everyone to use
it so
these are something you know some kind
of repositories where each and every
artifacts will be stored and anyone will
be able to have the access to these
particular artifacts here so these
artifacts are every artifacts every open
source uh artifacts is something which
is available over there to these central
repository now how does the maven
architecture really work here so the
mimon architecture really works in the
three steps the very first step is that
it reads the palm.xml file here that's
the very first step second it downloads
the dependencies uh defined in the power
xml file into the local repository from
the central or the remote repository
here once that is done so it will uh you
know create or generate the reports
according to the life cycles which you
have configured whether it's a clean
install
site
deploy package or whatever the life
cycle you want to trigger you will be
triggering that particular life cycle
and corresponding to that the build or a
specific task will be performed so these
are the three steps in which the overall
build or any kind of uh execution of
palmer xml file really happens here now
what exactly is the maven build
lifecycle so maven lifecycle isn't
nothing but collection of steps here
that needs to be uh followed for doing a
proper uh build of a project here so
there are primary three built-in cycles
which is available there default which
handles the project deployment clean
which handles the project uh cleaning
there and site which handles the
creation of the project sites
documentation so these are the three
primary built-in build cycles life
cycles which is available as such
now so build lifecycle has you know
different kind of phases or the stages
here because in the previous uh
particular slide we were talking about
what what are the different uh
particular build life cycles which is
available there but these are the
different phases like what are the
different step-by-step executions like
for the deep down which is available
there inside a specific maven build life
cycle so here you can see that it's
compiling then the test compile test
execution is there then package
integration test to verify install and
the lastly deploy here so these are the
different build phases which is
available as such over here so what
exactly is the command to use to do a
particular maven site so mvn site is
something which is used to create a
maven site here now usually whatever the
artifacts is prepared that will be
prepared in the target directory so here
also you will be able to see a site
directory which is available there in
the target directory which you can refer
for the site documentations what is the
different conventions used while naming
a project in may 1
so usually
it involves three components so the full
name of a project in maven includes
first of all the group id uh for example
com. apache
com.example so these are some of the uh
particular way that where you can
provide the group id artifact id can be
exact project name like maven project or
whatever the project you are creating so
sample project example project so these
kind of things will be there in the
artifact id and lastly is the version
like which version of your artifact you
want to prepare like 1.0.0 hyphen
snapshot
2.0.0 so like this information you are
providing that what particular version
you are trying to configure here now
let's move on to the intermediate level
where we will be having a little bit
more complex questions related to the
maven here now what exactly is a maven
artifact now usually what happens that
when we do a build process as an end of
result of the build process we will get
some artifacts for example when we build
a.net project so there we will be able
to have a exe or dll files as an
artifacts
similarly in case of maven when we do a
build process there we get the different
kind of artifacts like depending on the
packaging mode like jar file var files
or the er files here so these are
something which is you know getting
generated during the build process
during the main process and you can
store them into your local repository or
you want to push them to the remote
repository it's something that totally
depends on you so maven is a tool which
can help you to create all the artifacts
whether it's a jar file whether it's a
var files or whether it's a er file here
and every artifact is having three
attributes the first one is the group id
the artifact id and a particular version
and that's how you will be able to
identify a full-fledged artifact as such
in maven so maven is not about only the
name of the jar file it's actually
referring to the attributes like crop id
the artifact id and the version of the
artifact here now what are the different
phases of the clean life cycle here so
clean is something which is being used
to clean the target directory so that a
fresh build can be triggered there so
there are three steps pretty clean clean
and post clean here so if you wish that
you want to override the particular life
cycle configurations and you want to run
some particular steps before the clean
activity so you can do it into the
pre-clean and if you want to do it like
some steps after clean then pour screen
can be utilized now what are the
different phases of the site life cycle
so pre-site
site post site and site deploy so these
are the different phases which is
available there in the site life cycle
what is exactly we meant by the maven
plug-in now this is the huge difference
between the ant and maven here because
in and we were not having this that much
support of the plugins and that's the
reason why we had to deal with all the
build configurations
so we have to simply put the overall
build process that how the build should
be triggered but that is not something
which is there in case of maven in maven
we have a lot of flexibility because we
can actually put up what exactly build
configurations we want to put here we
can put some features like important
features over here in maven and uh these
plugins we can utilize for example i
want to perform a compilation now i
don't really want to do any kind of
configurations in this one so what i can
do is that i can simply use the
compilation plugin in maven and that can
really help me because i don't have to
unnecessarily write or rewrite the
configuration that how the compilation
should be done
it's something which is pre-configured
or pre-written in this plugins that i
can simply import the plugin and i can
do the build process or the compilation
process in a pretty standard mode so i
don't really have to do any kind of
workarounds with that and simply with a
small automations i will be able to
reach that how these maven plugins can
be integrated into my palmer xml file
and i can desire or i can have some
particular procedures and some steps
executed there so that's the biggest
benefit which we really get with the
help of maven plugins now why exactly
the maven plugins are utilized so to
create a jar files to create the war
files to compile the code files to
perform the unit testing to create the
project documentation and to create the
project reports so there are variety of
things in which we can actually use
these maven plugins through the
integrations within the palm direct xml
file there so it's all about the plugins
you just import the plugin and that
desired activity will be performed there
now what are the different type of
plugins which is there so
you can have either a build projects uh
for performing the build activities you
can have some build plugins for
reporting plugins also there which can
be only generated or utilized to
generate the reports to process the
reports and do any kind of formatting or
any kind of processing on the reports
here so that is where the reporting
plugins are utilized now what is exactly
the difference between the convention
and the configuration in maven so
convention is in particular process when
the developers are not required to
create the build processes so
configuration is when you know the
developers are supposed to create the
build processes
so the users do not have to rectify the
configuration in detail and once the
project is created it will automatically
create a structure so they must specify
every in case of configuration you have
to provide each and every details so
that's how the configurations really
happens because um you have to put every
detail into the power xml file and
that's how the particular configurations
really work as such so this is the huge
difference between the conventions and
the configurations here
now
so why exactly said that maven uses
conventions over the configurations so
maven pretty much does not puts any
efforts like on the particular
developers that they have to put each
and every configuration so there are
some ready-made
plugins which is available there and
pretty much we are making use of that so
that in such a case we don't have to
worry about the executions and we will
be able to pretty much work on that so
conventions like maven uses the
conventions incident of the
configurations so the developers just
you know they don't just have to create
the movement project the rest of the
particular structure will be compared
automatically so they are not uh you
know expecting that the developers
should be doing the configuration work
and everything should be taken care in
such a way that you just have to start
the things and rest of the things should
be taken care by the maven itself so
maven will be
responsible because they do the plugins
it will be responsible to set up the
default architecture
the default folder structures and all
you have to do is that you have to just
place the source code in the desired
folder structure here so that's
something which you need to do as in
particular developers so what exactly is
the maven order of inheritance here so
the order of inheritance is the settings
cli parameters
parent pom and the project bomb which
means that if you have some
configuration and settings that will be
the highest value then the cli
parameters are there then the parent
palm is there and then the project palm
so this is the way that how the
particular parameters or the
configurations will be picked up by the
may one so that's the order so what does
the build life cycles and the phases
imply in the basic concept of may 1. so
build life cycles consist of a sequence
of build phases and each build phase
consists of a sequence of goals when a
phase is run
all the goals related to that phase and
its plugins are also compiled so you
will be able to have a lot of particular
goals which is residing inside of phase
there and similarly life cycle is
nothing but a kind of a sequence of the
different phase so life cycle comes in
the top then it comes on the phases and
then it comes on the goals here now what
is the terminology called goal in case
of maven the term a terminology goal
refers to the specific task that makes
it possible for the the project to be
built and organized so it's something
which we can run so it's an actual
implementation which is going on there
for example in the build process in the
lib build phase i have a different goals
like clean install package deploy these
are the different typical goals which is
available there which i can execute into
the main one here so these are the
different goals like uh which we can
execute and which we can run during a
maven build next question is what is
exactly meant by the term dependencies
and the repositories in maven here so
dependencies refer to the java libraries
which we usually put up into the
bound.xml file there now what happens
that sometimes our source code is
requiring some java files like a
secondary java files for performing the
build process so instead of downloading
it and uh storing it into the class path
for during the build process we just
have to specify the dependency of that
artifact what dependency we need to put
and once that dependency is put up there
we will be able to have that jar file
downloaded and cache into the local
repository during the maven build
project
now if the dependencies are not present
in your local repository then maven will
try to download it from the central
repository and again if it is not uh uh
you know
it's something which is available which
is downloaded from the central
repository then it will be cached
locally into the local repository so
that's the cycle which is being
implemented and utilized during this
process
now what exactly is in snapshot in maven
so snapshot refers to the version
already available in the movement report
repository it signifies the latest
development copy that's what we do with
the case of snapshot here so meven
checks for a new version of snapshot in
the remote repository for every new
build so during the build process like
you know a new snapshot version is being
downloaded and the snapshot is updated
by the data service team which uh with
updated source code every time to the
repository for each maven build so snap
shot is something which we will be using
like very frequently we will be updating
to that and frequently we will be
updating the version to that and we will
try to explore and we will try to do the
modifications now what are the different
type of projects available in maven so
there are thousands of java projects
which you know uh can be utilized or we
can be uh implemented by my one here so
this helps the as the user that they as
they no longer have to remember every
configurations to set a particular
project for example spring boot spring
mvc spring boot etc these are the
different projects which is already
available in maven so most of the we
have already discussed that
for the job-based projects maven is
something which is you know considered
as by default so a lot of organizations
are actually using it for you know
storing or utilizing it for the
particular maven project now what
exactly is the maven uh archetype over
here so maven octave refers to a maven
plugin that is uh entitled to create a
for project structure as per its
template these archetypes are just
project templates that are generated by
maven when any new project is created
there so this is something which we are
using so that we will be able to create
a fresh new projects right so let's go
on to the advanced level of this maven
questions now what exactly is the
command to create a new project based on
an archive type so mvn archetype
generate is used to create a new java
project based on the archetype now this
will take up some parameters from
as an end user from you and depending on
that parameters it will create the
palm.xml file it will create the source
directories uh inside that main java
test all these different couple of
directories directly structures will be
automatically created now why we require
this command so that if you are going to
create a project from scratch from the
from the day one this command will help
you to have all the folder structures
created and then further on you can put
up your source code and those files as
such in this folder structure so that's
how is the mechanism that we will be
able to see that how the setup can be
performed really over here now what does
maven clean implies now movement clean
is a plugin that suggests that it's
going to clean the files and directories
there so whenever we do a build process
usually in the target directory we have
some class files some jar files or
whatever the generated source code which
is available that will be present in the
target directory so the maven clean is
something which is available which is
going to clean all these directories and
why we are doing this directory
structure cleanup so that we will be
able to do a fresh uh build process and
there should not be any kind of
issues as such over here so that's the
main reason why we are looking forward
for this
particular mechanism or for this uh
particular changes as such here now so
what exactly isn't build profile all
about so build profiles refers to the
set of configurations uh where we can
have like typically two different kind
of build processes there so if you feel
that the same form.xml file you can use
you want to run for different different
uh particular configurations so that you
will be able to do pretty much with the
help of this component so build profile
is used to do a customization processes
so that you will be able to have the
process and you will be able to
perform the configurations and the
setups all together there so that's a
very important aspect to be considered
that which we need to
discuss when we talk about the build
profile so build profile whenever you
feel that you want to do some
customizations and you want to proceed
with the setup so that's where it's
utilized next thing is that what are the
different type of build profiles which
is available there so the build profiles
can be done on for a particular project
like per project you can do you can uh
even do the build profiles in the
settings.xml file also and if you feel
that you want to do it into the
globalsettings.xml file so that also you
can do as such over here so there are
different ways in which you can do the
customization and once the customization
is done you will be able to have the
different uh ways of doing the setups
and the configurations over there so
what exactly is meant by the uh
particular system dependencies here so
let's talk about that also so system
dependencies refers to the uh particular
mechanisms where we feel that how the
dependencies should be uh you know
present there so that is something which
is having a scope of system there so
these dependencies are commonly used to
help maven know the dependencies that is
being provided by the jdk system
dependencies are mostly used to resolve
the dependencies on the artifacts that
are provided by the jdk so these
dependencies are somewhat which is being
utilized and used over here so that we
will be able to implement and go ahead
through the system dependencies what is
the reason for using an optional
dependency here so optional dependencies
are used to decrease the transitive
burden of some libraries so what happens
that when you download an artifact when
you put up a dependency so it could be a
possibility that some dependencies as in
particular optional can also be
downloaded now these are not always
required but yes sometimes what happens
that these are downloaded so that you
don't have to put uh each and every uh
artifact or dependency into the powder
xml file for example you're trying to
download some apache tool and with that
some like three four jar files or three
four another dependencies are also
getting downloaded now if you are using
that dependencies that totally create
because you don't have to put that uh
list or that entry in the dependency
list in the pound or xml file and that
can really save your time but if you
feel that you don't want to have them
and you these are the optional ones and
you really want to
get rid of that so that also you can
exclude while downloading any kind of
dependencies so these are the optional
ones which depending on your requirement
you can utilize you can uh
process and if you feel that you don't
want to get it you won't want to process
it you can certainly ignore it and you
can get rid of that also now what is a
dependency scope and how many type of
dependencies scope are there so there
are different type of dependency scores
which is there which is used on each and
every stage of the build here so compile
provided runtime test system import
these are the different kind of
dependency scopes which we have using
which we can define that when exactly we
want to go ahead for a specific build
process so depending on your requirement
you can explore all these build scopes
and you can get benefits out of that
what is exactly an intransitive
dependency in may one so maven avoids
the need to find out and specify
libraries that our own dependencies
require by including the transitive
dependencies automatically so transitive
dependencies says that if he depends if
x depends on y and y depends on z then x
depends on y and both there
so which means that you are not
dependent on one artifact you also need
the z artifact there with the y artifact
so that is what you need to do so that
you will get both the dependencies there
because this is normal that if you are
trying to
download some particular artifacts or
download some dependency and that
dependency is also dependent on some
other artifact or some other jar file
then you have to include both of them so
this is something which you will be able
to get so that you
can easily download all these dependent
jar files also and the maven build can
be success how can a maven build profile
can be activated so even build profile
can be activated through a different
ways so
explicitly using the command
command line you can talk about that
which profile you want to execute
through maven settings you can do uh
based on environment parameters os
settings and present and missing files
so these are the different ways in which
you can actually activate that which
particular profile you want to have so
profiles configurations can also be
saved in various situations and various
files and from there you will be able to
refer that which file you want to refer
as such now what is meant by the
dependency exclusion the exclusion is
used to exclude any transitive
dependency because you never know that
if you are trying to put up a dependency
uh entry in the power xml file that
artifact is also further dependent on
another artifact so in order to feel in
order to see that you want to exclude
that dependent artifact which is being
automatically downloaded that also we
can exclude with the help of exclusion
so you can uh avoid the transitive
dependency with the help of dependency
exclusions here so what exactly is in
mojo so mojo is nothing but maven plain
old java object here so it's an
executable goal in maven and a plug-in
refer to the distribution of such mozos
so moses enable the maven to extend its
functionality that already is not
founded in so it's kind of an extension
which is there and using this we can get
some additional benefits and some
executions over there so what is the
command to create a new project based on
a hard drive
so again archetype is something which we
normally use to create the new projects
now you can give the parameters in the
command itself or you want it to in in
kind of an interactive mode where it
will take the parameters from the end
user and according to that the project
will be created onto hard drive or onto
server wherever you wish you want to
create you can create a new project
so explain about the maven settings.xml
file so maven settings.xml file contains
the elements that are used to define
that how the human execution should be
there so there are different uh settings
like local remote center all these
different repositories are configured as
such over here now in this case what
happens that uh usually the
configurations are done in such a way
that it can you know go for the
executions it can go for the build
process and the complete executions can
be uh involved and can be achieved as
such here so all these executions are
something which we can really perform
and uh here we can put some credentials
how to connect to the remote repository
how to connect to remote repository all
that stuff is something which we talk
about over here what exactly is that
meant by term superbomb here so
superform refers to the default bomb of
maven so the moms of maven can't derive
from so it's nothing but a reference to
a parent bomb which is available there
that is a super pump so if you define
some dependencies in that super pump
automatically the uh child bomb will
also be able to inherit all those
dependencies so we can put some uh
executions like we can put some
configuration in the super pump so that
if multiple uh projects are going to
refer that they should be able to refer
that easily so that's the reason why we
primarily use the super pump so that we
can have the execution some processes
put up over there and all the other
projects should be effort to refer or
inherit from there so where exactly the
dependencies are stored so dependencies
are stored like in different locations
like you have the local repository
remote repository is there local
repositories on the local developers
machine and remote repository something
which is available on a server in form
of artifactory
now let's talk about the gradle
installation because this is a very
important aspect to be done because when
we are doing the installation
we have to download the cradle
executables right so let's see that what
are the different steps is involved in
the process of the cradle installation
so when we talk about the gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
java is installed now if the uh java is
not installed so you can go to the open
jdk
or you can go for the oracle java so you
can do the installation of the jdk on
your system
so jdk8 is something you can most
commonly use nowadays so you can install
that
once the java is downloaded and
installed then you have to do the gradle
uh download cradle there now once the
gradle binaries are executable uh or the
zip file gets downloaded so you can add
the environment variables and then you
can validate if the gradle installation
is working fine as expected or not so we
will be doing the gradle installation
into our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we will see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the gradle installation so this is the
website of the jdk or for java or recall
java now here you have different jdk so
from there you can do whatever the
option you want to select you can go
with that so jdk8 is something which is
most commonly used nowadays like its
most comfortable or compatible version
which is available so um in case you
want to see that if the jdk is installed
into your system all you have to do is
that you have to just say like java
hyphen version and that will give you
the output that whether the java is
installing to your system or not so in
case my system the java is installed but
if you really want to do the
installation you have to download the
jdk installer from this website from
this oracle website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the cradle installation because
cradle is something though which will be
performing the build automations and all
that stuff so you have to download the
binaries like the zip file probably in
which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the
system modified over there so right now
we have got like the prerequests as in
java version installed now the next
thing is that we have to install or
download the executables
so uh in order to download the latest uh
gradle distribution so you have to click
on this one right now over here there
are different options like uh you want
to go for 6.7 now it's they're having
like binary only or complete we'll go
for the binary only because we don't
want to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 mb of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the gradle
executables will be running and it will
give the
complete output to us over here in this
case so it may take some time and once
the particular modifications and the
download is done then we have to extract
it and once the extraction is done so we
will be able to go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is
downloaded we have to extract it now
extraction is required so that we can
have the setup like we can set up this
path into our environment variables and
once the path is configured and
established we will be able to start
further on that part on the execution so
meanwhile these files are getting
started let's see so we already got the
folder structure over here and we will
see that we will give this path here
there is two environment variables we
have to configure one is the cradle
underscore home and one is the
in the path variable so we'll copy this
path here
so meanwhile this is getting uh
extracted we can save our time and we
can go to the environment variable so we
can right click on this one properties
in there we have to go for the advanced
systems settings
then environment variables
now here we have to give it like gradle
underscore home now in this one we will
not be going giving it till the bin
directory so that only needs to be there
where the gradle is extracted so we'll
say okay
and uh then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the gradle executables should be there
when i am running the gradle command so
these two variables i have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all those things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like cmd
java iphone version to check the version
of the
java and then the gradle
underscore version is what you're going
to see that you see check the version of
the gradle which is installed and now
you can see that it says that 6.7
version is being installed over here in
this case so that's the way that how we
are going to have the cradle
installation performed into our
particular system
and in this one uh we will be also
working on some demos and some hands-on
to understand that how we can make use
of gradle for performing the build
activity so let's begin with the first
understanding that what exactly is
incredible all about
now griddle isn't kind of a build tool
which can be used for the
build automation performance and it can
be used for various programming
languages primarily it's being used for
the
java based applications it's in kind of
build tool which can help you to see
that how exactly automatically you can
prepare the builds you can perform the
automations earlier we used to do the
build activity from the eclipse and we
used to do it manually right but with
the help of this build tool we are going
to do it like automatically without any
uh manual efforts as such here there are
like lot of activities which we will be
doing during the build process primarily
there are different activities like
compilations
linkage packaging these are the
different tasks which we perform during
the build process so that we can
understand that how the build can be
done and we can perform the automations
uh this uh process also it's kind of a
standardized because again if you want
to automate something standards or a
standard process is something which we
require for that before we been going
ahead with that part so that's the
reason why we are getting this well tool
because this build tool helps us to do a
standardization process to see that how
the standards can be met and how we can
proceed further with that part
also it's something which can be used a
variety of languages programming
languages java is the primary language
for which we use the gradle but again
other languages like scala android cc
plus plus ruby these are some of the
languages for which we can use the same
tool
now it's actually using like it's
referring to as an trophy based domain
specific language rather than xml
because ant and maven these are the xml
based build tools but this one is not
that dependent on xml it's using the
groupie based domain specific language
dsl language is being used here
right now um again it's something which
can be used to do the build it can
further only use to perform the test
cases automations also there and then
further on you can deploy to the
artifactory also that okay i want to
push the artifacts to the artifactory so
that also that part also you can get it
done over here so
primary this tool is known for doing the
build automations for the big and large
projects the projects in which the
source got the amount of source code and
the
efforts is more so in that case this
particular tool makes sense now gradle
includes both the pros of maven and uh
and but it removes the drawbacks or
whatever the uh issues which we face
during these two build tools so it's
helping us to remove all the cons which
we faced during the implementation of
ant and maven and again again all the
pros of and maven is implemented with
this cradle tool
now let's see that why exactly this
gradle is used because that's a very
valid question that what is the activity
like what is the reason why we use the
gradle because the first one is that it
resolves issues faced on other build
tools that's a primary reason because we
are already having the tools like maven
and and which is available there but
primary this griddle duel is something
which is removing all the issues which
we are facing with the implementation of
other tools so these issues are getting
uh removed as such second one is that it
focuses on maintainability performance
and
flexibility so it's giving the focus on
that how exactly we can manage the big
large projects and uh we can have
flexibility that what different kind of
approaches i want to build today i want
to build in different ways tomorrow the
source code modifies gets added up so i
have the flexibility that i can change
this build script so i can perform the
automations so a lot of flexibility is
available which is being supported by
this tool
and then the last one is like
it provides a lot of features a lot of
plugins now this is one of the benefit
which we get in the case of maven also
that we get lot of features but again
when we talk about cradle then it
provides a lot of plugins like let's say
that normally in a build process we do
the compilation of the source code but
sometimes let's say that we want to
build an angular or a node.js
application now in that case we may be
involved in running some command line
executions some command line commands
just to make sure that yes we are
running the commands and we are getting
the output so there are a lot of
features which we can use like there are
a lot of plugins which is available
there and we will be using those uh
plugins in order to go ahead and in
order to execute those build process and
doing the automations now let's talk
about the cradle and move on because
again when we talk about me when like it
was like something which was primarily
used by the java but again when we are
talking about cradle so again it's just
uh being used primarily for the java
here but what is the reason that
we prefer gradle over the create maven
so what are the different
reason for that let's talk about that
part because this is very important we
need to understand that what is the
reason that gradle is preferred as an
better tool for the java as compared to
maven when we talk about for the build
automation here
now the first one is that the gradle
using the groupie dsl language domain
specific language whereas the maven is
considered as an project management tool
which is uh creating the palms or xml
file format files so it's being used for
the java project but xml format is being
used here and on the other hand gradle
is something which is not using the xml
formats and uh whatever the build
scripts you are creating that is
something which is there in the group
based
dsl language and on the other hand in
the pump we have to create the xmls
dependencies whatever the attributes you
are putting up in the main one that's
something which is available there in
the format of xml the overall goal of
the gradle is to add functionality to a
project whereas the goal of the maven is
to you know to complete a project phase
like to work on different different
project phase like compilation test
executions uh then uh packaging so uh
then deploying to artifactory so these
are all different phases which is
available there into the maven but on
the other hand gradle is all about
adding the functionality that how you
want to have some particular features
added up into the build scripts in
gradle there are like we usually specify
that what are the different tasks we
want to manage so different different
tasks we can add up into the case of
griddle and we can override those tasks
also in case of maven it's all about the
different phases which has been
happening over here and it's in sequence
manner so these phases happens in the
sync with order that how exactly you can
build up the sequence there but in case
of gradle you can have your own tasks
custom tasks also and you can disrupt
the sequence and you can see that how
the different steps can be executed in a
different order
so maven is something which is a phase
mechanism there but gradle is something
which is according to the features or
the flexibilities now gradle works on
the tasks whatever the task you want to
perform you uh it works directly on the
stars there on the other hand uh maven
is something does not have any kind of
inbuilt cash so every time you're
running the build so separate uh
things or the plugins and all these
information gets loaded up which takes
definitely a lot of time on the other
hand gradle is something which is using
its own internal cache so that it can
make the uh builds a little bit faster
because it's not something which is
doing the things from the scratch
whatever the uh
things is already being available in the
cache so it's just pick that part and
from there it will proceed further on
the build automation and that's a reason
why cradle performance is much faster as
compared to maven because it uses some
kind of a cache in there and then helps
to improve the overall performance now
let's talk about the gradle installation
because this is a very important aspect
to be done because when we are doing the
installation
we have to download the cradle
executables right so let's see that what
are the different steps is involved in
the process of the critical installation
so when we talk about the gradle
installation so there are primary four
steps which is available the very first
one is that you have to check if the
java is installed now if the
java is not installed so you can go to
the open jdk
or you can go for the oracle java so you
can do the installation of the jdk on
your system so jdk8 is something you can
most commonly use nowadays so you can
install that
once the java is downloaded and
installed then you have to do the gradle
uh download cradle there now once the
gradle binaries are executable uh or the
zip file gets downloaded so you can add
the environment variables and then you
can validate if the gradle installation
is working fine as expected not so we
will be doing the gradle installation
into our local systems and uh into the
windows platform and we'll see that how
exactly we can go for the installation
of cradle and we'll see that what are
the different version we are going to
install here so let's go back to the
system and see that how we can go for
the gradle installation so this is the
website of the jdk or for java oracle
java now here you have different jdk so
from there you can do whatever the
option you want to select you can go
with that so jdk8 is something which is
most commonly used nowadays like it's
most comfortable or compatible version
which is available so um in case you
want to see that if the jdk is installed
into your system all you have to do is
that you have to just say like java
hyphen version and that will give you
the uh output that whether the java is
installed into your system or not so in
case my system the java is installed but
if you really want to do the
installation you have to download the
jdk installer from this website from
this oracle website and then you can
proceed further on that part now once
the jdk is installed so you have to go
for the cradle installation because
cradle is something though which will be
performing the build automations and all
that stuff so you have to download the
binaries like the zip file probably in
which we have the executables and all
and then we have to have have some
particular environment variables
configured so that we will be able to
have the
system modified over there so right now
we have got like the prerequests as in
java version install now the next thing
is that we have to install or download
the executables so uh in order to
download the latest uh gradle
distribution so you have to click on
this one right now over here there are
different options like uh you want to go
for six point seven now it's they having
like binary only or complete we'll go
for the binary only because we don't
want to have the source we just want the
binaries and the executables now it's
getting downloaded it's around close to
100 mb of the installer which is there
now we have to just extract into a
directory and then the same path we need
to configure into the environment
variable so that in that way we will be
able to see that how the gradle
executables will be running and it will
give the
complete output to us over here in this
case so it may take some time and once
the particular modifications and the
download is done then we have to extract
it and once the extraction is done so we
will be able to
go back and
have some particular version or have the
configurations established over there so
that let's just wait for some time and
then we will be continuing with the
environment variables like this one so
once the installation and the extraction
is done now we just have to go to the
downloads where this one is
downloaded we have to extract it now
extraction is required so that we can
have the setup like we can set up this
path into our environment variables and
once the path is configured and
established we will be able to start
further on that part on the execution so
meanwhile these the files are getting
started let's see so we already got the
folder structure over here and we will
see that we will give this path here
there is two environment variables we
have to configure one is the cradle
underscore home and one is the um in the
path variable so we'll copy this path
here
so meanwhile this is getting uh
extracted we can save our time and we
can go to the environment variable so we
can right click on this one properties
in there we have to go for the advanced
systems settings
then environment variables
now here we have to give it like gradle
and let's go home now in this one we
will not be going giving it till the bin
directory so that only needs to be there
where the gradle is extracted so we'll
say okay
and uh then we have to go for the path
variable where we will be adding up a
new entry in this one we will be putting
up till the pin directory here because
the gradle executables should be there
when i'm running the gradle command so
these two variables i have to configure
then okay okay
and okay
so this one is done so now you have to
just open the command prompt and see
that whether the execution or the
commands which you're running is is
completely successful or not so
meanwhile it's extracting all the
executables and all these things it will
help us to understand that how the whole
build process or how the build tools can
be integrated over there now once the
extraction is done so you have to run
like cmd
java iphone version to check the version
of the
java and then the gradle
underscore version is what you're going
to see that you see check the version of
the gradle which is installed and now
you can see that it says that 6.7
version is being installed over here in
this case so that's the way that how we
are going to have the cradle
installation performed into our
particular system so let's go back to
the content let's talk about the cradle
core concepts here now
in this one we are going to talk about
what are the different core concepts of
cradle are all about the very first one
is the projects here now a project
represents a item to be performed over
here to be done like deploying an
application to a staging environment
performing some build so gradle is
something which is required uh the
projects um the gradle project which you
prepare is not having multiple tasks
which is available there which is
configured and all these tasks all these
different tasks needs to be executed
into a sequence now sequence is again is
a very important part because again if
the sequence is not meant properly then
the execution will not be done in a
proper order so that's the very
important aspect here
tasks is the one in which is a kind of
identity in which we will be performing
a series of steps these tasks may be
like compilation of a source code
preparing a jar file preparing a web
application archive file or er file also
we can have like
in some tasks we can even publish our
artifacts to the artifactory so that we
can store those artifacts into a shared
location so there are different ways in
which we can have this uh particular
tasks executed
now build scripts is the one in which we
will be storing all this information
what are the dependencies what are the
different tasks we want to refer it's
all going to be present in the
build.gradle file there build.greater
file will be having the information
related to what are the different
dependencies you want to download and
you want to store this all these things
will be a part of the build scripts
now let's talk about the features of
cradle what are the different features
which we can
use in case of cradle here
there are different type type of
features which is available there so
let's talk about them one by one so the
very first one over here is the high
performance then uh high performance is
something which we can see that we
already discussed that in case you are
using a large projects so gradle is
something which is in better approach as
compared to maven because of the high
performance which we are getting it uses
an internal cache which makes sure that
you are using like you are doing the
builds faster and that can give you a
higher performance over there
second one is the support it provides
the support so it yes definitely
provides a lot of support on how you can
perform the builds and it's being the
latest tool which is available there so
the support is also quite good in terms
of how you want to prepare the build how
you want to download the plugins
different plug-in supports and the
dependencies uh information also there
next one is multi-project build software
so using this one you can have multiple
projects in case in your repository you
have multiple projects say so all of
them can be easily built up with the
help of this particular tool so it
supports multiple project to be built up
using the same gradle project and gradle
scripts so that support is also
available with this gradle build tool
uh incremental builds are also something
which you can do with the help of cradle
so if you have done only the incremental
changes and you want to perform only the
incremental build so that can also be
possible with the help of a griddle here
the uh build scans so we can also
perform the build scans so we can use
some
integrations with sonarcube and all
where we can have the uh scans done to
the source code on
understand on how the build happens or
how the source code really happens over
there so that code scan or the build
scans can also be performed with this
one
and then uh it's a familiarity with java
so for java it's something which is uh
considered as in by default not even
java in fact android which is also using
the java programming language is using
the uh particular cradle over here so
that the build can be done and it can
gain uh benefits out of that so in in
all the manners in all the different
ways it's basically helping us to see
that how uh we can make sure that this
tool can help us in providing a lot of
features and that can help us to make a
reliable build tool for our java based
projects or any other programming based
project here right now let's see that
how we can convert a java project with
the cradle here and uh for that we have
to go back and created something which
is already installed we just have to
create a directory where we can have
like how we can perform some executions
we can prepare some build scripts and we
can have a particular execution of a
gradle build happened over there so
let's go back to the machine okay so we
are going to open the terminal here and
we will see that how we can create it so
first of all i have to create a
directory structure let's say that we'll
say like gradle
hyphen project now once the project is
created so we can go inside this
directory so to
create some critical related projects
and preparing the files now uh in this
one let's first create a particular one
so we will be saying like vi
build
dot gradle
so in this one we are going to put like
uh
two plugins we are going to use so we
are going to say like apply
plugin
java
and uh then we are going to say like
apply
plugin
application
so these two plugins we are going to use
and when we got this file over here in
this one so it shows like build.gradle
which is available there in this case
two these files are available now if you
want to learn like you know what are the
different tasks so you can run like
gradle tasks command over there so
gradle task will help you know that what
are the different tasks which is
available over here by processing the
build scripts and all so um
this will definitely help you to
understand on giving you the output so
here all the different tasks are being
given and it will help you to understand
that what are the different tasks you
can configure and you can work over here
just like jar files clean and all that
stuff build compile then it is there
then all these different executions
assemble then javadoc then build then
check test all these different tasks are
there and if you really want to run the
gradle build so you can run like cradle
clean to perform the clean activity
because right now you are doing like a
build so before that you can have a
clean and then you can run a
specific command or you can run the
griddle clean build which will perform
the cleanup also and it will at the same
time will have the build process also
performed over there so build and clean
up both will be executed over here
and what is the status whether it's a
success or a failure that will be given
back to you now in this case in the
previous one if you see that when you
run the clean the cradle clean it was
only running one task but when you go
for the
build uh process when you run the gradle
clean build it's going to give you a
much more information in fact you can
also give me uh further information like
you can have the hyphen iphone info flag
also there so that if you want to get
the details about the uh different uh
tasks which we which is being executed
over here so that also you're going to
get over here in this one so you just
have to put like hyphen iphone info and
then all these steps will be given back
to you that how these uh tasks will be
executed and the response will be there
so that's a way that how you can create
a pretty much simple straightforward
project in form of gradle which can
definitely help you to run some couple
of gradle commands and then you can
understand that what are the basic
commands you can run and how the
configurations really works on there
right let's go back to the main content
right now let's move on to the next one
so in the next one we are going to see
that how we can prepare a griddle build
project in case of eclipse now we are
not using the a local system we are not
directly creating the folders and files
here we are actually using the eclipse
for performing the creating a new cradle
project over here so let's move on that
part
okay so now the eclipse is open and i
have opened in this one the very first
thing is that we have to do the gradle
plugin installation so that we can
create new projects on cradle
and then we have to
configure the path that's how the gradle
plugin can be configured on the previous
uh preferences and all that stuff and
then we will be doing the build process
so the very first thing is that we have
to go to the eclipse marketplace
in there we have to search for griddle
so once the search is
done will show us the plugins related to
gradle so we have to go for
build ship
gradle integration so we'll click on the
install
it will proceed with installation it
will download it in some cases maybe
it's part of the eclipse as an in the id
so you can go to the installed tab and
you can see that also that if this
plug-in is already installed or not but
in this case we are installing it and uh
once the installation is done we just
have to restart the
specific ones we have to restart this
eclipse so that the changes can be
reflected
so it's downloading
it's downloading the cradle here and
once that is installed we will be able
to use it over here in this case in this
scenario so we have to just wait for
that part so still downloading the jar
files
so once the jar file is done it's now
over the areas and downloaded so after
that we will be able to proceed further
on that download password so it's going
to take some time to download it and
once it's done we will be able to
proceed further now once the progress is
done so it's asking us for the restart
now
so uh before that uh we just have to
click on restart now and then the
eclipse will be restarted all together
again here so you can do it manually or
you can go for that options it just
require a restart so that the new
changes can be reflected over here so
the plugins can be activated and can be
referenced here now we have to just uh
put up like the
you know the configuration where we can
have the system so we can go for the
gradle configuration so we can go for
windows and then preferences
now in this case we have to go for the
uh for the ones in which the cradle
option is available there so cradle is
what we are going to select now user
home the gradle user home is what we
need to use right so you want to go for
the gradle you want to go for local
installation so so all these options you
can use you can if you go for the
griddle wrapper then it will be
downloading the cradle locally and it is
going to use the cradle w or
griddlew.bat file but if you already
have an installation locally so you can
prefer that also right now in the
previous demo we have already got the
griddle uh extracted so we just have to
go for the downloads and the downloads
already gradle is available so we are
going to select that part here so this
is what we are going to select
right so this represents that this is
the directory structure in which we are
having the uh mechanism so you can
either go for the pill scan so you can
select the build scan also so once this
is enabled then all the projects will be
scanned and will be you know published
and uh
it's in kind of additional option which
is available if you really want to
disable it you can disable it also and
you can go with this configuration
so uh this is where the particular
gradle folder is being put over here in
this case
and then we have to just click on apply
and
we just have to click on apply and close
so with this one the
particular execution is done now we will
be going for the project creation so you
can right click over here or you can go
to the file also so here we are going to
go for the job project and in this we
are going to have a gradle project so
cradle project is what we are going to
create here
and next
so we are going to say like cradle
project
and then
next
so once that is done so finish
so with this one when you create the
project so what will happen that
automatically there will be a folder
structure will be available there right
and uh there are some
gradle scripts which will also be
created there so we will be doing the
modifications there and we'll see that
how the uh particular gradle build
script looks like and how we can we will
be adding some couple of uh selenium
related dependencies and we'll see that
how we can have more and more
dependencies added and what will be the
impact of those dependencies on the
overall project so that also it's very
important aspect to be considered so let
this processing be happen over there
it's just creating and uh some plugins
and binaries are getting installed and
getting downloaded so we'll see that
once the project is uh imported
completely executed over here and got
created we can extract that now if you
see here the particular option is
available about the cradle tasks so you
can extract it also and you will be able
to know that what are the different
tasks which is available there let's see
that in the build they are running like
build these are the different tasks
which is happening inside the build
process so
gradle executions will be also available
over here in this case and greater tasks
will be different it will be represented
over here in this one so you just have
to extract on the gradle project okay
this is the library which is available
now uh what happens that you will be
able to have like settings.gradle in
this one you will be able to have like
okay gradle hyphen project is something
which is available there in this one so
that's what i'm being referring
then we have over here as in these
folder structures which is created like
source main java
this is the one source test java is the
one which is available as in the folder
structure
and so test resources are also available
here so the mean source main resources
are also available
now in this case what happens that these
are the dependencies project and
external these are the different
dependencies are available there so
let's see let's add another dependency
over here in this one in the
build.gradle script and see that how we
can do that if we open build.gradle file
so you can see that these dependencies
are there like test implementation junit
is available there right and then we
have a implementations of this one which
is available now these jar files which
you put up it will automatically be
added up as in part of this one as in
part of the
particular
dependencies over here and uh which
means that you don't have to store them
as an within the repository and
automatically they can be happened over
there so let's open a dependency page so
we will be going to mvn repository where
we will be opening a dependency link
so this is the dependency link here so
selenium iphone java is available and it
can give you the dependency for all the
different options now we have for maven
this is the one and for gradle this is
the one here so we have to just copy
this one and uh we have to use it as an
dependency so this is the group and this
is the name and the version which we are
using here now we have copied this one
so we will go back to the
eclipse so here we have to just put that
dependency
and uh we have to just save it so uh
this is something which is providing
like selenium dependencies which is
available so now we have to just refresh
the project so right click over here
then you will be able to see the options
in the gradle saying that refresh gradle
project now once the moment you do that
so you will be able to do like for the
first time maybe it will take some time
to download all the dependencies which
is related to selenium but after that
you will be able to see like the
dependencies will be simply added up
over here in this case so you can see
that all the selenium related
dependencies are added up for any reason
if you comment these ones
and you say like
synchronize again
so you will see that all the
dependencies which you are adding up
from this selenium represent uh from the
selenium perspective will be gone back
again
so this is the way that how you can keep
on adding the dependencies which is
required for preparing your build for
your source code and from there you will
be able to proceed further on the
execution part so that's the best part
about this uh cradle here so that's a
way that how we are going to prepare a
greater project within the eclipse and
now you can keep on adding like the
source code in this one and that's the
way that how the code base will be added
up over here right so that's the way
that how the particular
executions or this gradle project is
being prepared in case of eclipse
selenium installation is a three-step
process so it has certain prereqs the
first prerequisite you need to have java
in your system so we will be installing
java first and then we will be working
with eclipse id so we will be installing
eclipse and then we will install
selenium for java we will install the
version java 8 and for eclipse we have a
version 4.10 this was the last stable
version which was released in december
last year so i'll be using that version
and selenium we will download the latest
3.14 version okay so let's get started
with our first step which is the java
installation so to install java let's go
to the browser and simply just search
for java 8 download
so now you will see that there is an
oracle site which is listed there and
that is where you would be downloading
all your java package so go ahead and
click on that and for you to download
any jdk package from the oracle site you
need to create an account so if you
already have one you just need to log in
using that account and then you can
download any of the jdks and if you do
not have one please go ahead create a
new account on the oracle login to that
account and then you can just download
the java it so since i already have an
account and i have already downloaded
the package but i'll show you how and
where to download it from so in this
page if you scroll down so you will see
this java development kit 8211. so this
is the version we'll be downloading it
so click on the accept license agreement
and then since we are working on the
windows system today so we will be
downloading this the windows package so
just click on that and it will get
downloaded in your downloaded folder and
as i said i've already downloaded the
packages so here it is what i've done is
i've just created a directory called
installers and i'm going to be keeping
all my installables here so here i have
a folder called java installer and this
is where my installable is so now that
we have this file so we will just go
ahead double click on it and launch this
installer the installer is launched and
just click on run so this will take a
few minutes to install java the
installer is launched now just click on
the next button here so here for the
installation directory you can
change the directory to the choice of
whatever drive and the folder structure
you want to i would like to leave it as
default here and we'll just go and click
on next and then the java installation
is in progress so let's wait until this
is completed it really shouldn't take
too much time maybe just a few more
minutes here
okay accept the license term just click
on next we leave the destination folder
as it is
so jdk8 is successfully installed on
your system so close the installer now
and let's go ahead and check whether the
installation is done properly so for
that what i'll do is i'll go to my
command prompt and i'll send just say
java minus version so it says java
version 1.8 and this tells us that the
java is installed successfully now after
this installation there are couple of
configurations which we need to do and
what are those configuration one is you
need to set the path variable and then
we are also going to set a java home
directory so for that first let's go
ahead and check where is the java
installed actually let's figure out the
directory first so if you remember the
directory structure where the java got
installed was in program files java i
have there are certain previous versions
which had installed and then uninstalled
it so that is why you see some residuals
here sitting here let's not worry too
much about that instead let me go to the
latest one what i have installed which
is this okay and there is a bin folder
here and this is the path which we need
to set in our path variable so what i
will do is i will just copy this path
and then
go to your control panel here go to your
where is my system yeah so click on the
system go to advanced system setting and
here in the environment variables find
the path variable okay and then say edit
now what are we doing here in the path
variable is we are going to add the java
bin directory to the path be very
careful whenever you are editing your
path variable do not overwrite anything
always go into the edit mode go towards
the end here and then just say control v
paste the path which you have just
copied from the explorer window that's
it now just say okay done so your path
setting is done so what's the next one
we need to do we need to add a new
environment variable called the java now
what i'll do for that is i just say new
i just type java home here
and what is the value of this we need to
set we need to set the same path but
without the bin directory so we just
need to set the path till your java
directory that is this so we'll just
copy the path again and paste it here
that is all just say okay click on ok
click on ok here and we are done so
again let's go to our command prompt and
just say java minus version
so everything seems to be fine so now
successfully we have installed java on
the system so what is our next
installation step what we have now we
need to install the eclipse so let's go
back to the browser again so to download
eclipse we will be downloading the
package from the eclipse.org so when you
go here to eclipse.r you can see the
latest version which is available and
the latest version available when this
video was made was 2090 06. so
especially with eclipse since it's an
open source i prefer to work with the
last stable version and so does most of
the developers do and hence that is the
reason why i have picked up the version
which is like last year's version which
is
4.10 which was released in last december
so you can always choose to go with the
latest version but then if there are any
issues and if you are like first time
working with the eclipse you're going to
get confused as where these issues are
coming from right so i would still
recommend that you use the last stable
version which is available with your
eclipse so now to get the last table
version what you need to do is go and
click on this download packages and here
if you scroll down this page you can see
here more downloads so there is a list
of all the previous releases of eclipse
which is available and this is what we
need to download so just click on that
4.10 version and then click on the os on
which you want to install eclipse for us
it is windows so i'll just click here on
the 64-bit windows and then click on the
download and you will be downloading the
complete package so once you download
this is what it will look like so let's
go back to our directory of installers
so this is the installer for the eclipse
which i got now what's the next step i
need to do just launch this installer
and install eclipse so i'll just say
double click on this i'll say run
so here you'll see multiple options here
for eclipse installation so depending on
your requirement you can go ahead and
install any of these packages so for us
we just need an eclipse id for java
developer so i'll select this and i'll
say install so again you'll have a
choice of directory where you want to
install so i have chosen d drive here
this is the default directory name it
takes which is ok we can leave it as it
is and then also you have an option to
create a start menu entry and desktop
shortcut so just leave the default
selection as it is and go ahead and
click on install so this will take a
while to install the eclipse
this is select all you can close this
window this is select all and accept it
okay so the installation has been
completed successfully so let's go and
click on this launch and let's see the
first window what opens
when you launch the eclipse you need to
specify a workspace directory now what
is this workspace directory so this is a
directory or a folder wherein all the
java files or any programs or any
artifacts which you are going to create
through eclipse will be stored in this
particular folder so this could be any
location on your system so this is you
can go ahead browse the location and
change it so for in our case what we
will do is i'll go to the d drive and
i already have a directory so here i'll
create i'll just clear select this
folder and then create a folder called
workspace i'll say my workspace and then
i'll say launch so every time i open the
eclipse right so this is going to take
as my default workspace and all my
programs all my java scripts or my
automation scripts are getting are going
to be stored in this particular location
so we'll say launch
so this is a welcome window which opens
we can just close this and there we go
the eclipse is open with a certain
perspective so there are certain windows
here which we do not need let's just
close them so now the first thing what
you do after launching the eclipse is go
ahead and create a new project so i'll
say file new and since i'm going to be
using java with selenium i'll say create
java project so give a project name
let's say my first project now you have
an option here to select the jre which
you want to use so we just installed
this jdk 1.8 okay so i'm going to click
on use default gre otherwise you also
have an option to use a project specific
gr for example i could have two
different projects where one project i'm
going to be working with gre 1.8 and
there is another project which i want to
work with the latest java maybe java 12
and i can have more than one java
installed on the machine so this gives
me an option to select whichever java i
want to work with so if you have another
java installed here it will show up in
this list and you can just go ahead and
select that now since we have only one
java installed on our machine which is
java 1.8 i will say use default gre
which is 1.8 and i will click on finish
now if you observe this folder structure
the project which is created see all the
reference libraries to this particular
java have been created here now we are
ready to create
any kind of java programs in this
project so now we have successfully done
the second step of our installation
which is the eclipse installation after
this we need to install the selenium so
again let's go back to the browser and
see what files we need to download to
install selling so let me go to my
browser and here i will be going to the
seleniumhq.org so if you are working
with selenium this particular website
the seleniumhq.org is going to be a
bible everything and anything related to
selenium is available in this website
whether you want to download the files
whether you want to refer to the
documentation anything regarding to
selenium is available here so what we
want now is the installables for
selenium so here go to the download tab
now for you to install selenium and
start working with selenium there are
three things which are required for you
to download one is a stand alone
selenium server so this is not required
immediately when you get started with
selenium however when you start working
with remote selenium webdriver you would
be requiring this when you have a grid
setup you will be requiring the
standalone server so for that what you
can do is you can just download the
latest version available here so when
you click on that it will download the
file into your download folder so this
is one particular file which you need to
keep next selenium client and web driver
language bindings now in today's demo we
will be looking at selenium with java so
that means my client package of java is
what i need to download so whatever
programming language selenium supports
we have respective downloadables
available with that say if you're
working with python then you need to
download your client library for python
and since we are working with java you
need to download this package so simply
what you need to do click on this link
and it will download the java package
for you which are basically the jar
files so we have client libraries now
and then there is another component what
we need now with selenium you are going
to be automating your web browser
applications correct and you also want
your applications to run on multiple
browsers so that means your scripts the
automation scripts which you create
should be able to run on any browser
selenium works with multiple browsers
like edge safari chrome firefox and
other browsers even it has a support for
headless browser now every browser which
it supports comes with its own driver
files now say for example we want to say
work with firefox driver so that means
for us to start working with firefox
browser we need to download something
called as a gecko driver here and if you
want to work with chrome browser you
need to install the chrome driver so
depending on what browsers you'll be
testing with go ahead click on each of
this link and download the latest driver
files now since we are going to be
working with firefox in this demo what i
need to do is i just need to click here
on the latest link so when i click on
the latest link it is going to take me
to this driver files so driver files are
specific to each of the operating system
so if you go down here you will see
there is a separate driver file
available for linux for mac and for
windows so depending on which operating
system where you'll be running your test
download that particular driver file and
this is the driver file i need because
we are working on windows machine so
these are the three different packages
which we need to download from the
selenium hq dot org for us to install
selling so let me show you the folder
where i've already downloaded all this
so if you see here selenium java
3.141.59 okay this is nothing but our
client library which we saw here let's
go back to the main page here that is
this so once i download this this is a
zip file after i unzip the file this is
the folder structure i see and let's see
what is there at this folder structure
so there are two jar files here and then
in the lips there are multiple jar files
and we will need all this to work with
selenium and then we also downloaded the
driver files so what i did was after
downloading those driver files for the
browser i created a directory here
called drivers and i've kept all my
browser drivers here so i have a driver
file downloaded for chrome i want to
work with firefox so i have a gecko
driver here and then for internet
explorer that's it so this is all we
need so once we have all this what you
need to do is go to your eclipse in the
eclipse right click on the project which
you have created and then go to the
build path and say configure build path
go to the libraries tab here now do you
see this jre libraries here this is what
got installed first and now similarly we
are going to add the selenium jars to
this library and how do we add that on
your right you can see this add external
jars click on add external jars go to
your folder where you have downloaded
your selenium which is this select all
the jar files which is available so i
have two jar files here i'll just say
click open again i will click on add
external jar now from the libs folder i
will select all this file so select all
the five jars and click on open so you
should see all the seven jar files here
so once you have this just say apply and
close now if you look into your project
directory here you will see some a
folder called referenced library and
this is where you will see all the
selenium charts here this is a very
simple installation in eclipse when you
want to install selenium you just need
to export all the jars of the selenium
into eclipse and now your system is
ready to start working with selenium
scripts all right so now let's just test
our installation by writing a small
selenium test script so for that what i
will do is i'll go to the source folder
right click new and i'll say java class
so let's name this as say first selenium
test and i will select this public
static void main and i will click on
finish all right so now let's create a
use case say we want to launch a firefox
browser and then we want to launch the
amazon site so this will be just two
simple things which we will be doing in
this test script so for me to do that
what i usually do is i create a method
for any functionality which i want to
create here so now i want to do a launch
browser so i'll create a method here
called launch browser now whenever you
start writing your selenium scripts the
first line what you need to do is you
need to declare an object of webdriver
class so here i'll say web driver driver
so now if you hover over this error what
it is showing it says import web driver
from or dot open qa dot selling it so if
you remember when we installed the
selenium we imported all these jars
right so that means so what whenever we
want to use a web driver we need to
import this class from these packages so
just go ahead and click on this import
state done now next step now for us to
launch a firefox browser it is a two
steps process which is involved here one
is you need to set the system property
and then you need to launch the driver
so let's do that i'll say system dot set
property so use this method set property
so this takes two arguments the key and
the value now what is the key i'm going
to mention here i'm going to be
mentioning the gecko driver and the path
for the gecko driver okay because since
i'm working with the firefox so in
double quotes i'll say
webdriver.geco.driver this is my key and
the value is going to be sorry the fully
qualified path for your driver files and
you know where we have kept our driver
first let's go to that driver files in d
colon i have selenium tutorial in
installers i have driver folder okay so
i'm just going to copy the complete path
from here ctrl c and
i paste it here control v along with
this i need to provide the file name for
the gecko driver which is
geckodriver.exe and let's complete this
step next so once i've set the property
i need to provide a command for
launching my firefox driver and how do i
do that i simply use this driver object
which i have created driver equal to
new firefox driver again similarly the
way we imported packages for web driver
we also need to import the package for
firefox driver so just hover over the
mouse over that and select import
firefox drive with these two statements
we will be able to launch the firefox
browser and as i said in our use case
what is the next thing we want to do we
want to launch say amazon dot in website
for that there is a command in selenium
which says driver.get
and you pass the url here so for me to
write the url what i usually do is i go
to my browser i open the website which i
want to work with in our case it's
amazon.in and i just simply copy this
fully formed url go to my eclipse and
just paste it here now this ensures that
i don't make any mistakes in typing out
the url let's complete the statement and
we are done and now in the main function
i'll just create an object of this and
we will call this method so i'll copy
this class for selenium test say obj
equal to new for selenium test and now
i'll say obj dot this is a function
launch browser so let's save this and
execute this control c right click run
as
java application okay so the mozilla
firefox has been launched
now it should launch your amazon.com
bingo so there goes our first test
script which runs successfully before
you start understanding any automation
tool it's good to look back into what
manual testing is all about what are its
challenges and how automation tool
overcomes these challenges challenges
are always overcome by inventing
something new so let's see how selenium
came into existence and how did it
evolve to become one of the most popular
web application automation tool selenium
suite of tools selenium is not a single
tool it has multiple components so we
will look into each of them and as you
know every automation tool has its own
advantages and limitations so we will be
looking at what the advantages are and
the limitations of selenium and how do
we work around those limitations all
right so let's get started manual
testing a definition if you can say a
manual testing involves the physical
execution of test cases against various
applications and to do what to detect
bugs and errors in your product it is
one of the primitive methods of testing
a software this was the only method
which we knew of earlier it is execution
of test cases without using any
automation tools it does not require the
knowledge of a testing tool obviously
because everything is done manually also
you can practically test any application
since you are doing a manual testing so
let's take an example so say we have a
use case you are testing say a facebook
application and in facebook application
let's let's open the facebook
application and say create an account
this is your web page which is under
test now now as a tester what would you
do you would write multiple test cases
to test each of the functionalities on
this page you will use multiple data
sets to test each of these fields like
the first name the surname mobile number
or the new password and you will also
test multiple links what are the
different links on this page like say
forgotten account or create a new page
so these are the multiple links
available on the web pages also you look
at each and every element of the web
page like your radio buttons like your
drop down list apart from this you would
do an accessibility testing you would do
a performance testing for this page or
say a response time after you say click
on the login button literally you can do
any type of tests manually once you have
these test cases ready what do you do
you start executing this test cases one
by one you will find bugs your
developers are going to fix them and you
will need to rerun all these test cases
one by one again until all the bugs are
fixed and your application is ready to
ship now if one has to run test cases
with hundreds of transactions or the
data sets and repeat them can you
imagine the amount of effort required in
that now that brings us to the first
demerit of the manual testing manual
testing is a very time consuming process
and it is very boring also it is very
highly error prone why because it is
done manually and human mistakes are
bound to happen since it's a manual
executions testers presence is required
all the time one is to keep doing manual
steps step by step again all the time he
also has to create manual reports group
them format them so that we get good
looking reports also send this reports
manually to all stakeholders then
collection of logs from various machines
where you have run your test consoliding
all of them creating repositories and
maintaining them and again since it's
all as a manual process there is a high
chance of creating manual errors there
scope of manual testing is limited for
example
let's say regression testing ideally you
would want to run all the test cases
which you have written but since it's a
manual process you would not have the
luxury of time to execute all of them
and hence you will pick and choose your
test cases to execute that way you are
limiting the scope of testing also
working with large amount of data
manually is impractical which could be
the need of your application what about
performance testing you want to collect
metrics on various performance measures
as a part of your performance testing
you want to simulate multiple loads on
application under test and hence
manually performing these kind of tests
is not feasible and to top it all up say
if you are working in an agile model
where code is being churned out by
developers testers are building their
test and they are executing them as and
when the bills are available for testing
and this happens iteratively and hence
you will need to run this test multiple
times during your development cycle and
doing this manually definitely becomes
very tedious and burning and is this the
effective way of doing it not at all so
what do we do we automate it so this
tells us why we automate one for faster
execution two to be less error prone and
three the main reason is to help
frequent execution of our test so there
are many tools available in the market
today for automation one such tool is
selenium birth of selenium much before
selenium there were various tools in the
market like say rft and qtp just to name
a few popular ones selenium was
introduced by gentleman called jason
huggins way back in 2004. he was an
engineer at thoughtworks and he was
working on a web application which
needed frequent testing he realized the
inefficiency in manually testing this
web application repeatedly so what he
did was he wrote a javascript program
that automatically controlled the
browser actions and he named it as
javascript testrunner later he made this
open source and this was renamed as the
selenium core and this is how selenium
came into existence and since then
selenium has become one of the most
powerful tool for testing web
applications so how does selenium help
so we saw all the demerits of manual
testing so we can say by automation of
test cases one selenium helps in speedy
execution of test cases since manual
execution is avoided the results are
more accurate no human errors since your
test cases are automated human resources
required to execute automated test cases
is far less than manual testing because
of that there is a lesser investment in
human resources it saves time and you
know time is money it's cost effective
as selenium is an open source it is
available free of cost early time to
market since you save effort and time on
manual execution your clients will be
merrier as you would be able to ship
your product pretty fast lastly since
your test cases are automated you can
rerun them any point of time and as many
times as required so if this tool offers
so many benefits we definitely want to
know more detail about what selenium is
selenium enables us to test web
applications on all kind of browsers
like internet explorer chrome firefox
safari edge opera and even the headless
browser selenium is an open source and
it is platform independent the biggest
reason why people are preferring this
tool is because it is free of cost and
the qtp and the rft which we talked
about are chargeable selenium is a set
of tools and libraries to facilitate the
automation of web application as i said
it is not a single tool it has multiple
components which we'll be seeing in
detail in some time and all these tools
together help us test the web
application you can run selenium scripts
on any platform it is platform
independent why because it is primarily
developed in javascript it's very common
for manual testers not to have in-depth
programming knowledge so selenium has
this record and replay back tool called
the selenium id which can be used to
create a set of actions as a script and
you can replay the script back however
this is mainly used for demo purposes
only because selenium is such a powerful
tool that you should be able to take
full advantage of all its features
selenium provides support for different
programming languages like java python
c-sharp ruby so you can write your test
scripts in any language you like one
need not know in-depth or advanced
knowledge of these languages also
selenium supports different operating
systems it has supports for windows macs
linux even ubuntu as well so you can run
your selenium test on any platform of
your choice and hence selenium is the
most popular and widely used automation
tools for automating your web
applications selenium set of tools so
let's go a little more deeper into
selenium as i said selenium is not a
single tool it is a suite of tools so
let's look at some of the major
components or the tools in selenium and
what they have to offer so selenium has
four major components one selenium id
it's the most simplest tool in the suite
of selenium it is integrated development
environment earlier selenium ide was
available only as a firefox plugin and
it offered a simple record and playback
functionality it is a very simple to use
tool but it's mainly used for
prototyping and not used for creating
automation in the real-time projects
because it has its own limitations like
any other record and replay tool
selenium rc this is nothing but selenium
remote control it is used to write web
application test in different
programming language what it does it it
basically interacts with the browser
with the help of something called as rc
server and how it interacts its uses a
simple http post get request for
communication this was also called as
selenium 1.0 version but it got
deprecated in selenium 2.0 version and
was completely removed in 3.0 and it was
replaced by webdriver and we will see in
detail as why this happened selenium
webdriver this is the most important
component in the selenium suite it is a
programming interface to create and
execute text test cases it is obviously
the successor of the selenium rc which
we talked about because of certain
drawbacks which rc had so what webdriver
does is it interacts with the browsers
directly unlike rc where the rc required
a server to interact with the browser
and the last component is the selenium
grid so selenium grid is used to run
multiple test scripts on multiple
machines at the same time so it helps
you in achieving parallel execution
since the selenium web driver with you
can only do sequential execution grid is
what comes into picture where you can do
your parallel execution and why is
parallel execution important because in
real time environment you always have
the need to run test cases in a
distributed environment and that is what
grid helps you to achieve so all this
together helps us to create robust web
application test automation and we will
go in detail about each of these
components so before that let's look at
the history of selenium version so what
is selenium version comprised of it had
an ide rc and grid and as i said earlier
there were some disadvantages of using
rc so rc was on its path of deprecation
and web driver was taking its path so if
you look at selenium-2 version it had an
earlier version of webdriver and also
the rc so they coexisted from 3. onwards
rc was completely removed and webdriver
took its place there is also a 4 dot
version around the corner and it has
more features and enhancement some some
of the features which are talked about
are w3c webdriver standardization
improved ide and improved grid now let's
look at each of the components in the
selenium suite selenium ide is the most
simplest tool in the suite of selenium
it is nothing but an integrated
development environment for creating
your automation scripts it has a record
and playback functionality and is a very
simple and easy to use too it is
available as a firefox plugin and a
chrome extension so you can use either
of this browser to record your test
scripts it's a very simple user
interface using which you can create
your scripts that interact with your
browser the commands created in the
scripts are called selenes commands and
they can be exported to the supported
programming language and hence this code
can be reused however this is mainly
used for prototyping and not used for
creating automation for your real-time
projects why because of its own
limitation which any other record and
replay tool has so a bit history of
selenium id so earlier selenium id was
only a firefox extension so we saw that
ide was available since the selenium
version one selenium id died with the
firefox version 55 that was id was
stopped supporting from 55 version
onwards and this was around 2017 time
frame however very recently all new
brand selenium id has been launched by
apply tools and also they have made it
across browser so you can install the
extension on chrome as well as as an
add-on on firefox browser so they
completely revamped this ide code and
now they have made it available on the
github under the apache 2.0 license and
for the demos today we'll be looking at
the new id now with this new ide also
comes a good amount of features
reusability of test cases better
debugger and most importantly it
supports parallel test case execution so
they have introduced a utility called
selenium side runner that allows you to
run your test cases on any browser so
you can create your automation using
idec on chrome or firefox but through
command prompt using your site runner
you can execute this test cases on any
browser thus by achieving your cross
browser testing control flow statement
so initially in the previous versions of
idea there were control flow statements
available however one had to install a
plugin to use them but now it is made
available out of box and what are these
control flow statements these are
nothing but your if else conditions the
while loops the switch cases so on it
also has an improved locator
functionality that means it provides a
failover mechanism for locating elements
on your web page so let's look at how
this id looks and how do we install it
and start working on that so for that
let me take you to my browser so say
let's go to the firefox browser so on
this browser i already have the id
installed so when you already have an id
installed you will see an icon here that
says selenium id and how do you install
this you simply need to go to your
firefox add-ons here where it says find
more extension so just type in
selenium id and search for this
extension so in the search results you
see the selenium id just click on that
and now since i've already installed
here it says remove otherwise for you it
is going to give you an add button here
just click on the add button it will
install this extension once it is
installed you should be able to see this
selenium ide icon here okay so now let's
go ahead and launch this id so when i
click on that it is going to show me a
welcome page where it's going to give me
few options the first option is it says
record a new test case in a new project
so straight away if you choose this
option you can start recording a test
case in which case it's going to just
create a default project for you which
you can save it later then open an
existing project so you can open if you
already have a saved project create a
new project and close so i already have
an existing project with me for the demo
purpose so i'll go ahead and open that
so i'll say open existing project and i
have created a simple script what the
script does is it logs me into the
facebook using a dummy user mail your
sorry username and password that's all
it's very simple script with few lines
and this is what it's going to do so
what we will simply do is we'll just run
the script and see how it works for that
i am just going to reduce the test
execution speed so that you should be
able to see every step of execution here
all right so what i'll do now here is
i'll just adjust this window and i'll
just simply say run current test all
right so i'll just get this side by side
so that you should be able to see what
exactly the script is doing okay so now
you are able to see both the windows
okay so now it's going to type in your
user email here there you go and now
it'll enter the password and it has
clicked on the login button so it's
going to take a while to say login and
since these are the dummy ids it is you
are not able to log in here and you are
going to see this error window fine that
is what is the expected output here now
on the id if you look here after i
execute the test case every statement or
every command which i have used here is
colored coded in green so that means
this particular step was executed
successfully and then here in the log
window it will give you a complete log
of this test case right from the first
step till the end and your end result is
it says fb login which is my test case
name completed successfully let's look
at few components of this id the first
one is the menu bar so let's go to our
id all right so the menu bar is right
here on the top so here is your project
name so either you can add a new project
here or rename your project so since we
already have this project which is named
as facebook and then on the right you
have options to create a new project
open an existing project or save the
current project and then comes on
toolbar so using the options in this
toolbar you can control the execution of
your test cases so first one here is the
recording button so this is what you use
when you start recording your script and
then on the left you have two options
here to run your test cases the first
one is run all tests so in case you have
multiple test cases written here you can
execute them one by one sequentially by
using this run all test else what you
can do is if you just want to run your
current test this is what you would use
then id has this debugger option which
you can use to do a step execution so
say for example now whenever i run the
script it's going to execute each and
every command here sequentially so
instead if i just select the first
command and say do step execution all
right so what it does is the moment it
finishes the first command which is
opening of facebook
right i think which is already done here
yeah all right so once this is done it
is going to wait immediately on the
second command and it says pause the
debugger so from here you can do
whatever you would like to do in case
you want to change the command here you
can do that you can pause your execution
you can resume your execution here right
you can even completely stop your test
execution or you can just select this to
run the rest of the test case so if we
say run the test case what it is going
to do is it's just going to simply go
ahead and complete the com complete the
test case now there is another option
here which is you see the timer there
which is test execution speed so to
execute your test cases in the speed you
want say whenever you are developing an
automation script right and say you want
to give a demo so you need to control
the speed sometimes so that the viewer
is able to exactly see all the steps
which is being performed and this gives
you an option to control that complete
execution right so do you see the
grading here so we have somewhere from
fast to completely slow execution so the
previous demo which i showed was i
control the speed and then i executed it
so that we could see every command how
it has been executed all right so what's
the next this is called as an address
bar so whichever wherever whenever you
enter an url here that is where you want
to conduct your test and another thing
what it does is it keeps a history of
all the urls which have used for running
your test then here is where your script
is recorded so each and every
instruction is displayed here in the
order in which you have recorded the
script and then if you look here you
have something called as login reference
so now log is an area where it records
each and every step of your command as
in when they get executed right so if
you see here it says open https
facebook.com and ok so that means this
command was executed successfully and
after the complete test case is done it
gives you whether the test case passed
or failed so in case there is a failure
you will immediately see this test case
is filled in red color also there is
something called as reference here for
example say if i click on any of this
command the reference tab what it is
going to show me is the details of this
command which i have used in the script
it gives you the details of the command
as well as what the arguments have been
used or how how is that you need to be
using this particular command okay so
now what we'll do is let's go ahead and
write a simple script using this id so
with this you'll get an idea how do we
actually record scripts in id so for
that i have a use case here a very very
simple use case so what we will do is we
will open amazon amazon.in then we'll
search simply search for say product
iphone and once we get that search page
where all your iphones are displayed we
will just do an asset on the title of
the page simple all right so let's do
that so first thing what i need is an
url okay so first let me go to my
firefox browser here and say amazon.com
so why i'm doing this just to simply get
the right url absolute url path here and
so that i don't make any mistakes while
typing in the url okay so i got this so
let me close all this windows i don't
need any of this let's minimize this
all right so here what i'll do in the
tests tab i'll say add a new test and
name this test as
amazon search
done i'll say add now i'll enter this
url which i just copied it from my
browser okay and then i'll just say
start recording so what it
did was since i've entered the url in
this address box it just opened the
dot in url now let's do the test case so
in my test case what i said was i want
to search for iphone once i have this
i'm just going to click on my search
button so now this gives me a list of
all iphones and then i said i want to
add an assertion on the title of this
page so for me to do that what id gives
me an option is i have to just right
click anywhere on this page and you'll
see the selenium id options here so in
this i will select assert title and then
i will close this browser so that kind
of completes my test case so now take a
look at all the steps which is created
for me so it says open slash because i
have already provided the url here so
either you can replace it with your
regular url or you can just leave it as
it is so what i will do since this is
going to be a proper script and i might
be using this to run it from my command
prompt also so i'll just replace this
target with the actual url and then what
it is doing it is setting a window size
then there are whatever i did on that
particular url on that website it has
recorded all the steps for me so this is
where it says type into this particular
text box which is my search box and what
did it type iphone this was the value
which i entered now there was one more
feature which i told you in this new id
which had which i said it has a failover
mechanism for your locating techniques
now that is what this is now if you look
here this id is equal to tab search
textbook this is nothing but that search
box where we entered the text iphone and
it has certain identification through
which this ide identifies that web
element and that has multiple options to
select that particular search box so
right now what it has used is id is
equal to two tab search box however if
you know the different locating
techniques you will be able to see here
that it has other techniques also which
it has identified like the name and the
css and the xpath so how does this help
in failovers say tomorrow if amazon.in
website changes the id of this element
right you are not going to come and
rewrite the scripts again instead by
using the same script what it will do is
if this particular id fails if it is
unable to find the element using the
first locator which is the id it simply
moves to the next available ones and it
tries to search for that element until
one of this becomes true that is what
was the failure mechanism which has got
added it's a very brilliant feature
because most of our test cases break
because of element location techniques
well let's come back to this so then we
added an assert title right so what is
us our title here it simply captures the
title of that particular page and it
checks this is all a very simple test
case so what we will do now is we will
stop the recording and then i have also
given a close browser so right now what
i'll do is i'll just comment this out
why because if i just run this test case
it's going to be very fast and you might
not be able to catch the exact command
execution what has happened all right so
right now i'll just disable it so that
it will just do all these test cases and
it just stays there without closing the
browser so now i'll just say run the
current test date so your amazon end is
launched okay it is typed in the iphone
it's also clicked on the search so it is
done so now if you look here since we
are in the reference tab it is not able
to show so let's go to the log and now
let's see the log so it's going to be a
running log so if you notice here the
previous examples which we have run for
facebook is also in the same lock so we
will have to see the log from running
amazon search because that's our test
case so if you see here every command
line right was executed successfully
assert title was also done and your test
case was executed successfully so it
passed now what we will do is on this
assert title i'll just modify this
and let's say just add some text i'll
just add double s here now this by
intentionally i'm going to fail this
test case just to show you that whenever
there is a test case failure how does
the id behaves and how do you get to
know the failures all right so i'll just
run the disk test case again so before
that let's close the previous
window all right done and now
here i'll also uncomment the close
because anyway it's a failure which i'm
going to see which i should be able to
see it in the logs so i'll close the
browser after the execution of test case
okay so let's simply go and run the test
case okay amazon.com is launched it
should search for iphone now yeah there
you go all right
now it should also close the browser yes
it has closed the browser and it has
failed now see here now this is the line
where our command filled why because the
expected title was not there and if you
look in the logs it says your assert
title on amazon.in failed actual result
was something different and it did not
match with what we had asked it for so
this is how simple it is to use your id
to create your automation scripts
so we saw all the components of id we
saw the record button then i showed you
the toolbar i showed you the editor box
and also the test execution lock so now
let's come to what are the limitations
of this id with ide you cannot export
your scripts your test scripts to web
driver scripts this support is not yet
added but it is in the works data driven
testing like using your excel files or
reading data from the csv files and
passing it to the script this capability
is still not available also you cannot
connect to database for reading your
test data or perform any kind of
database testing with selenium webdriver
yes you can also unlike selenium
webdriver you do not have a good
reporting mechanism with the ide like
say for example test ng or repotengi so
that brings us to the next component of
the suite which is selenium rc selenium
remote control so selenium rc was
developed by paul hammond he refactored
the code which was developed by jason
and was credited with json as a
co-creator of selenium selenium server
is written in java it is used to write
web application test in different
programming languages as it supports
multiple programming languages like your
java c perl python and ruby it interacts
with a browser with the help of an rc
server so this rfsave server uses a
simple http get and post request for
communication and as i said earlier also
selenium rc was called as selenium 1.0
version but it got deplicated in
selenium 2.0 and was completely removed
in 3.0 and it got replaced by what
webdriver and we'll see why this
happened and what was that issue which
we had with the rc server so this is the
architecture of selenium remote control
at a very high level so when jason
huggins introduced selenium you know the
tool was called as javascript program
and then that was also called as a
selenium core so every html has a
javascript statements which are executed
by web browser and there is a javascript
engine which helps in executing this
command now this rca had one major issue
now what was that issue say for example
you have a test script say
test.javascript here which you are
trying to access elements from anywhere
from the google.com domain so what used
to happen is every element which is
accessible e are the elements which can
belong only to google.com domain like
say for example mail the search or the
drive so any elements from this can be
accessible through your test scripts
however nothing outside the domain of
say google.com in this case was
accessible say for example if your test
scripts wanted to access something from
yahoo.com this was not possible and this
is due to the security reasons obviously
now to overcome that the testers what
they had to do was they had to install
the selenium core and the web server
which contained your web application
which is under test on the same machine
and imagine if you have to do this for
every machine which is under test this
is not going to be feasible or even
effective all the time and this issue is
called as a same origin policy now what
the same origin policy issue says is it
prohibits a javascript from accessing
elements or interacting with scripts
from a domain different from where it is
launched and this is purely for the
security measure so if you have written
a scripts which can access your
google.com or anything related to
google.com these scripts cannot access
any elements outside the domain like as
we said in the example yahoo.com this
was the same origin policy now to
overcome this what this gentleman did
was he created something called as
selenium remote control server to trick
the browser in believing that your core
your selenium core and your web
application under test are from the same
domain and this is what was the selenium
remote control so if you look at again a
high level architecture or how did this
actually work first you write your test
scripts which is here right in any of
the supported language like your php or
your java or python and before we start
testing we need to launch this rc server
which is a separate application so this
selenium server is responsible for
receiving the cellini's commands and
these selenius commands are the ones
which you have written in your script it
interprets them and reports the result
back to your test so all that is done to
your rc server the browser interaction
which happens through rc server right
from here to your browser so this
happens through a simple http
post and get request and that is how
your rc server and your browser
communicate and how exactly this
communication happens this rc server it
acts like a proxy so say your test
scripts ask to launch a browser so what
happens is this commands goes to your
server and then your rc server launches
the browser it injects the javascript
into the browser once this is done all
the subsequent calls from your test
script right from your test scripts to
your browser goes through your rc and
now upon upon receiving these
instruction your selenium core executes
this actual commands as javascript
commands on the browser and then the
test results are displayed back from
your browser to your rc to your test
scripts so the same cycle gets repeated
right until the complete test case
execution is over so for every command
what you write in your javascript here
or your test script here goes through a
complete cycle of going through the rc
server to the browser collecting the
results again from the rc server back to
your test scripts so this cycle gets
repeated for every command until your
complete test execution is done so rc
had definitely lot of shortcomings and
what are those so rc server needs to be
installed before running any test
scripts which we just saw so that was an
additional setup since it acts as a
mediator between your commands which is
your salinist commas and your browser
the architecture of rc is complicated
why because of its intermediate rc
server which is required to communicate
with the browser the execution of
commands takes very long it is lower we
know why because every command in this
takes a full trip from the test script
to your rc server to the core engine to
the browser and then back to the same
route which makes your overall test
execution very slow lastly the apis
supported by rc are very redundant and
confusing so rc does have a good number
of aps however it is less object
oriented so they are redundant and
confusing say for example say if you
want to write into a text box how and
when to use a type key command or just a
type command is always confusing another
example is some of the mouse commands
using a click or a mouse dot both kind
of you know almost providing a similar
functionality so that is a kind of
confusion which developers used to
create
hence
selenium rc got deprecated and is no
more available in latest selenium
versions it is obsolete now now to
overcome this shortfalls webdriver was
introduced so while rc was introduced in
2004 web driver was introduced by simon
stevert in 2006. it's a cross-platform
testing platform so webdriver can run on
any platform like say linux windows mac
or even if you have a ubuntu machine you
can run your selenium scripts on this
machine it is a programming interface to
run test cases it is not an ide and how
does this work actually so test cases
are created and executed using web
elements or objects using the object
locator and the web driver method so
when i do a demo you will understand
what this web driver methods are and how
do we locate the web elements on the web
page it does not require a core engine
like rc so it is pretty fast why because
webdriver interacts directly with the
browser and it does not have that
intermediate server like the rc hat so
each browser in this case what happens
is each browser has its own driver on
which the application runs
and this driver is responsible to make
the browser understand the commands
which you will be passing from the
script like say for example click of a
button or you want to enter some text so
through your script you tell which
browser you want to work with say chrome
and then the chrome driver is
responsible for interpreting your
instructions and to execute it on the
web application launched on the chrome
browser so like rc webdriver also
supports multiple programming languages
in which you can write your test scripts
so another advantage of web driver is it
supports various frameworks like test ng
junit n unit and report nj
so when we talk about the limitations of
webdriver you will appreciate how this
support for various frameworks and tool
help in making the selenium a complete
automation solution for web application
so let's look at the architecture of
webdriver at a high level what is in web
drivers a web driver consists of four
major components
the first one is we have got client
libraries right or what we also call it
as language bindings so since selenium
supports multiple language and you are
free to use any of the supported
languages to create your automation
script these libraries are made
available on your selenium website which
you need to download and then write your
scripts accordingly so let's go and see
from where do we download this so if i
go to my browser
so
seleniumhq.org right so if you're
working with selenium this website is
your bible so anything and everything
you need to know about selenium right
you need to come here and use all these
tabs here in this website so right now
what we are going to look at is what are
those language binding so for that i'll
have to go to this download tab here
okay and if you scroll down here you
will see something like selenium client
and web driver language bindings and for
each of the supported language of
selenium you have a download link
right so say for example if you're
working with java here what you need to
do is you need to download your java
language binding so let's go back to the
presentation so this is where your
language bindings are available next so
selenium provides lots of aps for us to
interact with the browser
and when we do the demo i'll be showing
you some of this aps right and these are
nothing but the rest apis
and everything whatever we do through
the script happens through the rest
calls then we have a json wire protocol
what is json javascript object notation
it is nothing but a standard for
exchanging data over the web so for
example you want to say launch a web
application through your script
so what selenium does it it creates a
json payload and posts the request to
the browser driver
that is here and then we have this
browser drivers themselves and as i said
there is a specific driver for each
browser
as you know every tool has its own
limitation
so does selenium
so let's look at what these limitations
are and if there are any workarounds for
them
cannot test mobile applications requires
framework like apm
selenium is for automating web
application it cannot handle mobile
applications mobile applications are
little different and they need its own
set of automation tool
however what selenium provides is a
support for integrating this apm tool
which is nothing but a mobile
application automation tool
and using apm and selenium you can still
achieve mobile application automation
and when do you usually need this when
your application under test is also
supported on mobile devices you would
want a mechanism to run the same test
cases on web browser as well as your
mobile process right so this is how you
achieve it the next limitation so when
we talked about the components of
selenium i said that with webdriver we
can achieve only sequential execution
however in real time scenario we cannot
just live with this we need to have a
mechanism to run our test cases
parallelly on multiple machines as well
as on multiple browsers so though this
is a limitation of webdriver but what
selenium offers is something called as
grid which helps us achieve this and we
will see in shortly what the selenium
grid is all about also if you want to
know more details as how do we work with
the grid how do you want to install that
grid so do check out our video
on simply learn website on selenium grid
third limitations so limited reporting
capability so selenium webdriver has a
limited reporting capability it can
create basic reports but what we
definitely need is a more
so it does support some tools like say
test ng report ng and even extent
reports which you can integrate with
selenium and generate beautiful reports
powerful isn't it
also there are other challenges
with selenium like selenium is not very
good with image testing
especially for the ones which are
designed for web application automation
but then we have other tools which can
be used along with selenium like auto it
and seculi so if you look at all this
selenium still provides a complete
solution for your automation needs and
that's the beauty of selenium and that
is why it makes the most popular tool of
today for automation
okay let's do a quick comparison between
the selenium rc and the web driver
so rc has a very complex architecture
you know why because of the additional
rc server whereas due to direct
interaction with the browser webdriver
architecture is pretty simple
execution speed
it is lower in rc and much faster than
webdriver why because in web driver we
have eliminated the complete layer of
selenium server
right that the rc server and we
established a direct communication with
the browser through browser drivers
it requires an rc server to interact
with the browsers we just talked about
it and whereas webdriver can directly
interact with the browser
so rc again we talked about this as one
of the limitations that we have lot of
redundant abs which kept developers
guessing as which api to use for what
functionality
however webdriver offers pretty clean
apis to work with
rc did not offer any support for
headless browser whereas in web driver
you do have a support for using headless
browsers
let's see the web driver in action now
now for the demo we will use this
particular use case and what this use
case says is navigate to the official
simply learn website
then type the selenium in search bar and
click on it and click on the selenium
3.0 training so we are basically
searching for selenium 3.0 training on
the simply learn website first let's do
the steps manually and then we will go
ahead and write the automation script so
let's go to my browser on my browser
what i'll do is i'll let me first launch
the simply learn website
okay and here what my use case step says
is i need to search for
selenium and click on the search button
so once i do that it is going to give me
a complete list of all kind of selenium
trainings which is available with
simpler and what i'm interested in is
the selenium 3.0 training here once i
find this on the web page i need to go
and click on that all right so this is
all the steps which we are going to
perform in this use case okay now so for
writing the test cases i'll be using an
id which is eclipse i've already
installed my eclipse and also i have
installed selenium in this instance of
my eclipse all right so if you can see
the reference library folder here you
will see all the jars which are required
for the selenium to work next another
prereq which is required for selenium
and that is your driver files now every
browser which you want to work with has
its own driver file to execute your
selenium scripts and since for this demo
i'll be working with the firefox browser
i will need a driver file for firefox
now driver file for firefox is the
geckodriver which i've already
downloaded and placed in my folder
called drivers
now where did i download this from let's
go ahead and see that so if i go back to
my browser and if you go to your
selenium hq dot website you have to go
to this download tab here in the
download tab when you scroll down you
will see something like third-party
drivers bindings and plugins in this you
will see the list of all the browsers
which is supported by selenium and
against each of this browser you will
find a link which has the driver files
now since we'll be using the gecko
driver
this is the link where you need to go to
and depending on which operating system
which you're working on you need to
download that particular file now since
i am working on mac this is the file
which i am using if you are a windows
user you need to download this zip file
and unzip it so once you unzip that you
would
get a file called gecko driver for your
firefox or a chrome driver for your
chrome browser and then what you do is
you just create a directory called
drivers under your project
and just place the driver files here so
these are the two prereqs for your
selenium one is importing your jar files
like this and then having your drivers
downloaded and keep them under a folder
where you can reference to okay so now
we'll go ahead and create a class
i already have a package created in this
project so i'll use this project and
create a new class
so i'll say create new java class and
let's call this as
search training i'll be using a public
static void main and i'll click on
finish
so let's remove this auto generated
lines as we do not need them all right
now the first statement which you need
to write before even you start writing
the rest of your quotas what you need to
do is you need to define or declare your
driver variable using your class web
driver so what i would do is i'll say
web driver
driver
done all right
now you will see that this id is going
to flash some errors for you that means
it is going to ask you to import certain
libraries which is required by the web
driver so simply just go ahead and say
import webdriver from
org.opensq dot selenium this is the
package which we will need all right so
you have a driver created which is of
the class web driver and now after this
i'm going to create three methods all
right so first method i will have for
launching the firefox browser okay and
then i will write a simple method for
searching
selenium training
and clicking on it
this is the actual use case what we'll
be doing and then third method i'm going
to write is just to close the browser
which i'm going to be opening right so
these are the different methods which
i'll be creating and from the public
static void main i will just call these
methods one after the other okay so
let's go ahead and write the first
method now my first method is launching
the firefox browser so i'll say public
void since my return type is null or
there is no return type for this let's
call it as launch browser
okay all right now in this for launching
any browser i need to mention two steps
now the first step is where i need to do
a system.set property okay let's do that
first and then i'll explain what this
does i'll just say system dot set
property so this accepts a key and a
value pair so what is my key here my key
here is web driver dot
gecko dot driver and i need to provide a
value so value is nothing but the path
to the gecko driver and we know that
this gecko driver which i'm going to use
here is right here in the same project
path under the drivers folder correct
and that is what the path which i am
going to provide here so here simply i
need to say
drivers slash
gecko driver
g e c k o all right done and let me
close this sentence all right now since
i'm a mac user my gecko driver
installable is just the name gecko
driver if you're a windows user and if
you are running your selenium scripts on
the windows machine you need to provide
a complete path to this including dot
exe because driver executable on your
machines is going to be geckodriver.exe
alright so just make sure that your path
which you mentioned here in the
system.set property is the correct path
okay then the next thing what we need to
do is i need to just say driver is equal
to new
firefox driver okay so this command new
firefox driver creates an instance of
the firefox browser now this is also
flagging me error why because again it's
going to ask me to import the packages
where the firefox driver classes present
okay we did that now these two lines are
responsible for launching the firefox
browser firm so this is done so what's
my next step in the use case now i need
to launch the website simply learn so
for that we have a command called
driver.get
driver.get what it does is whatever url
you're going to give it here in this
double quotes as an argument it is going
to launch that particular website and
for us it's a simply learn website so
what i do as a best practices instead of
typing out the url i go to my browser
launch that url which i want to test and
i simply copy it come back to your
eclipse and just simply paste it so this
ensures that i do not make any mistakes
in the url okay so done so our first
method is ready where we are launching
the browser which is our firefox browser
and then launching the simplylearn
website now the next method what is my
next method in my next method i need to
give the search string to search
selenium training on this particular
website now for that we need to do few
things what are those few things let's
go to the website again all right so let
me relaunch this let's close this okay
let me remove all this and let's go to
the home page first okay this is my home
page so as you saw when i did a manual
testing of this i entered the text here
so now since i have to write a script
for this first i need to identify what
this element is for that what i'm going
to do is i'm just going to say right
click here and i'll say inspect element
all right now this element let's see
what attribute it has which i can use
for finding this element so i i see that
there is an id present so what i'm going
to do is i'm just going to simply use
this id
and then i'll just copy this id from
here go back to eclipse let's write a
method first so i'll say public void
and what do we give the method name say
search training
or just search all right now in this i
need to use a command called driver dot
find element
by id is what i'm going to use as a
locating technique and in double quotes
the id which i copied from the website
is what i'm going to paste here okay and
then what am i going to do on this
element is i need to send that text the
text which i'm going to search for which
is selenium so i'll just say send keys
and whatever text i want to send i need
to give it in double quotes so for that
selenium so this is done so now i've
entered the text here and after entering
the text i need to click on this button
so for that i need to first know what
that button is so let's inspect that
search button okay now if you look at
the search button other than the tag
which is span and the class name i do
not have anything here all right so what
i can do is i can either use the class
name or i can write an x path since this
is a demo which we have already used id
locating technique i would go ahead and
use the x path here so for me to
construct an x path uh i will copy this
class first okay
and then i already have a crow path
installed on my firefox so i'll use the
crow path and first test my x bar so
i'll just say double slash let's see
what was that element it has a span tag
okay so i'll have to use span and at
class equal to and i'll just copy the
class name here and let's see if it can
identify that element yeah so it is able
to identify so i'll just use this x path
in my code so i'll go back to eclipse
and i'll say driver dot find element by
dot x path and the x path which i just
copied from crow path is what i'm going
to paste here and what is the action i
need to do here i need to say click
done so i have reached a stage where i
have entered this selenium okay and then
i have clicked on the search button once
i do this i know that expected result is
i should be able to find this particular
link here selenium 3.0 training okay and
i should be able to click on that so for
that again i need to inspect this so
let's inspect this selenium 3.2 all
right so now what are the elements this
has now this particular element has
attributes like it has a tag h2 then it
has got some class name and some other
attributes so i would again would like
to use a xpath here now this time while
using the x path i am going to make use
of a text functionality so that i can
search for this particular text so i'll
simply copy this i'll go to my crow path
the tag is h2 so i'll say simply h2
okay and here i'll say text equal to and
this is the text which i copied i missed
out that yes there so i'm just going to
add an s okay so let's first test here
whether it is able to identify that
element yeah so it is able to identify
so can you see a blue dotted line it is
able to show us which element it is
identified so i'll copy this x path now
and let's go to my ide eclipse so now
here what i need to do is i'll have to
again simply say driver dot find element
by dot x path
and paste the x path which we just did
and then again i have to do a click
operation done all right so technically
we have taken all the steps of the use
case and we have written the commands
for that
now let's add an additional thing here
say after coming to this page after
finding this we want to uh say print the
title of this page now what is the title
of this page if you just hover your
mouse on this it says online and
classroom training for professional
certification courses simply now so what
i will do is after doing all these
operations i will just print out this
page title on our console so for that i
have to just do this driver dot uh so
let's do a sysop so i'll say sis out
system dot out dot println okay and here
i would say
let's add a text here the page title is
and then let's append it with
driver dot
get title so this is the command which
will be using to fetch the page title
done now what is the last method i need
to add just to close the browser all
right so let me add a method here i'll
say public void close browser
and this is one single command which i
need to call i'll say driver dot quit
okay and then i need to call all these
methods from my public static void main
so i let me use my class name which is
this so i'm going to create an object
obj is equal to
new class name and then using this
object first is i need to call the
method launch browser and then i'll call
the method search
right and then i'll call the method
close process done so technically our
script is ready with all the
functionality which we wanted to cover
from our use case now there are few
other tweaks which i need to do this and
i'll tell you why i need to do this now
for example after we click here right
after we click on the search if you
observed on your website it took a
little while before it listed out all
the selenium trainings for us and
visually when you're actually doing it
you wait for the selenium 3.0 training
to be available and then you click on
that now same thing you also need to
tell your scripts to do that you need to
tell your scripts to wait for a while
until you start seeing the selenium 3.0
training or it appears on your web page
there are multiple ways to do that in
your script and it is a part of overall
synchronization what we call where we
use kind of implicit and explicit kind
of favorites now since this is a demo
for demo purpose what i am going to do
is i am going to use a command called
thread.sleep and i'm just going to give
an explicit weight of say three seconds
so you can use this mainly for the demo
purposes you can use a thread.sleep
command now this thread.sleep command
needs us to handle some exceptions so
i'm just going to click on add throws
declaration and say interrupted
exception now same thing i'll have to do
it in my main function also okay so
let's do that and complete it
all right so this is done so by doing
this what am i doing i'm ensuring that
before i click on the selenium three dot
training we are giving enough time for
the script to wait until the webpage
shows this link to the selenium 3.0
training that's one thing i'm doing all
right and also now since you're going to
be seeing this demo through the video
recording the script when it starts
running it is going to be very fast so
you might just miss out saying how it
does the send keys and how did it click
on the search button for us to enable us
to see it properly i'll just add some
explicit weights here just for a demo
purpose so after entering the keys right
so what i'll do is i'll just give a
simple thread dot sleep here
okay so probably a three seconds or two
seconds wait should be good enough okay
three seconds wait should be good enough
here so that we should be able to see
how exactly this works on your browser
when we execute this
okay now our complete script is ready so
what i'll do is i'll just save the
script and then we will simply run the
script so to run the script i just say
right click run as java application okay
it says ask me to select and save i have
saved the script now so let's observe
how it runs okay the
simplylearn.com the website is launched
so the selenium text has been entered in
the search box it has clicked on the
search okay all right so now it did
everything whatever we wanted it to do
all right so since we are closing the
browser you are unable to see whether
the selenium three dot training was
selected or not however what i have
given here is to fetch the title after
all these operations were complete and
if you see here the complete operations
was done and we were able to see the
page title here okay so now what i'll do
since we are unable to see whether it
clicked on the selenium 3.0 training or
not i'll just comment our the close
browser the command okay so we will not
call the close browser so that the
browser remains open and we get to see
whether did it really find the training
link or not okay so let me close this
window we don't need this firefox window
close all tabs and then i'll just
exactly execute the script so i'll say
run as java application so save the file
okay simplylearn.com is launched so
search text is entered now it's going to
click on the search button yes all right
so we've got the search results it
should click on selenium theodore
training and yes it is successfully able
to click on that all right so now it's
not going to close the browser because
we have commented on that line however
it did print us the title
here all right so this is a simple way
of using the selenium scripts
selenium grid so grid is used to run
multiple test scripts on multiple
machines at the same time with webdriver
you can only do sequential execution but
in real time environment you always have
the need to run test cases in
distributed environment and that is
where selenium grid comes into picture
so grid was conceptualized and developed
by patrick the main objective is to
minimize test execution type and how by
running your test parallelly so design
is in such a way that commands are
distributed on multiple machines where
you want to run test and all these are
executed simultaneously what do you
achieve by this methodology of course
the parallel execution on different
browsers and operating system grid is
pretty flexible and can integrate with
many tools like say you want a reporting
tool integrated to pull all the reports
from the multiple machines where you're
running your test cases and you want to
present that report in a good looking
format so you have an option to
integrate such report okay so how does
this grid work so grid has a hub and
node concept which helps in achieving
the parallel execution let's take an
example say your application supports
all browsers and most of the operating
systems like as in this picture you
could say one of them is a windows
machine one of them is a mac machine and
another one is say a linux machine so
your requirement is to run the test on
all supported browsers and operating
system like the one which is depicted in
this picture so what you have to do is
first thing is you configure a master
machine or what you also call it as a
hub by running something called a
selenium standalone server and this
talent standalone server can be
downloaded from the selenium hq website
using the server you create a hub
configuration that is this node and then
you create nodes specific to your
machine requirement and how are these
nodes created you again use the same
server which is your standalone selenium
server to create the node configuration
so i'll show you where the selenium
server can be downloaded so if we go
back to our
selenium hq website so you can see here
right on the top which says selenium
standalone server welcome everyone to
our one another demo on which we are
going to see that how exactly we can do
the installation of docker on the
windows platform specifically on windows
10. now docker is something which is
available for most of the operating
systems different different platforms so
it supports both the unix and the
windows platform as such so um linux
through various commands we can do the
installation but in the case of windows
you have to download the exe file and a
particular installer from the docker hub
websites you can simply google it and
you will get a kind of a link from where
you will be able to download the package
so let's go to the chrome and try to
search on for the windows string
particular installer you will get a link
from docker hub you download it you get
the stable version you get the edge
version whichever version you want you
wish to download you can download it so
let's go back to the chrome so here you
have the docker next off for windows so
you can go for this table or you can go
for the edge right so you also have the
comparison that what is the difference
between these two versions right
so um the particular edge version is
something which is getting releases
every month and uh the
stable version is getting the releases
every quarter so they are not doing much
of the changes to the stable version as
compared to the edge there so you just
have to double click on the installer
and that will help you to do the
installation of the process so let's get
started so you just click on the get
instable version so when you do that the
particular installer is going to install
now it's going to take like around 300
mb there so that's the kind of installed
which is available so once the installer
is downloaded so what you can do is that
you can actually go ahead and you can uh
proceed with the doing the double click
on this installer when you double click
on that you have to proceed with some of
the steps like you know from the gui
itself you are going to proceed with
these steps so we'll wait for 10 to 20
seconds more and then the installer will
be done and then we can do the double
click and the installation will proceed
so another thing is that uh there is a
huge difference between the installer
like for example in case of unix the
installer is a little bit less but in
case of windows it's a gui is also
involved and there are a lot of binaries
which is available there so that's the
reason why you know the huge size is
there now it's available for free that's
for sure and it also requires the
windows 10 professional or enterprise
64-bit there so um if you are working on
some previous version of operating
systems like windows 7 and all you have
the older version called docker toolbox
so they used to call it as like docker
toolbox earlier but now they are calling
it as a docker desktop with the new
docker windows 10 support as such here
so another couple of seconds and then
the installer will be done and then we
will be able to proceed with the
installation
so let's see that how much progress is
there to the download so we'll click on
the downloads and here still we have
some particular installations or some
download going on so we'll wait for some
time and uh once the installation is
done then we'll go back and we'll
proceed with installation
so
couple of seconds
so it's almost done so i'll just click
on this one you can go to the directory
to the downloads and you can double
click on that also but if you want to do
the installation you can click on this
one also and it will ask for the
approval yes or no you have to provide
now once that is done so um a desktop a
kind of a gui component will open there
so it will start proceeding with
installation so it's asking whether you
want to add the desktop the shortcut to
desktop so you can say okay i'm going to
click on ok so it will unpack the files
all the files which is required for
docker to successfully install that is
getting unpacked over here so it will
take some time to do the installation
because it's doing a lot of work here so
you can just wait for till the execution
of the installer to be completed and
once the installer is done you can open
your command line and start working on
the docker
so taking some time to extract the files
now it's asking us to you know do the
close and do the restart so once that is
done you will be able to proceed further
and you can just you know run the
command line and any docker command if
you can run so that will give you the
response whether the docker is installed
or not so you can see here that docker
is you know something which is installed
so you can run like docker version you
will be able to get a version of the
client when you do the restart of the
machine then at that moment of time the
docker server will also be started and
then this particular error message will
go off right now the docker daemon is
not up and running because the
installation requires a restart and when
you close on this one and go for the
restart the machine will be restarted
here so this is the way that how exactly
we can go for a docker installation and
we can go on that part so now let's
begin with the demo we'll be installing
docker on an ubuntu system so this is my
system i'll just open the terminal so
the first thing you can start with is
removing any docker installation that
you probably already have present in
your system if you want to start from
scratch so this is the command to do so
sudo apt get remove
docker
docker engine docker dot io enter your
password
and docker is removed so now we'll start
from scratch and we'll install docker
once again
before that i'll just clear my screen
okay so before i install docker let me
just ensure that all these softwares on
my system currently is in its latest
date
so sudo apt get update
great so that's done next thing we'll
actually install our docker
so type in sudo apt
get
install
docker
now as you can see here there's an error
that's occurred so sometimes it's
possible that due to the environment of
the machine that you're working in this
particular command does not work
in which case there's always another
command that you can start with
just type docker install
and that by itself will give you the
command you can use to install docker
so as it says here sudo apt install
docker dot io is a command that we will
need to execute to install docker
and after that we will execute the sudo
snap install docker so sudo apt install
docker dot io first
and this will install your docker
after that's done we will have
sudo snap install docker so snap install
docker installs a newly created snap
package
there are basically some other
dependencies for docker that you'll have
to install
of course since this is the installation
process for the entire docker io
it will take some time
great so our docker is installed the
next thing we do as i mentioned earlier
is that we need to install all the
dependency packages so the command for
that is
sudo snap install docker
enter your password
so with that we have completed the
installation process for docker but
we'll perform a few more stages where we
will test if the installation has been
done right
so before we move on with the testing
for docker let's once again just check
the version that we have installed
so for that the command is docker
version
and as you can see docker version
17.12.1
ce has been installed
next thing we do is we pull an image
from the docker hub
so docker run
hello
world
now hello world is a darker image which
is present on the docker hub docker hub
is basically a repository that you can
find online so with this command the
docker image hello world has been pulled
onto your system
so let's see if it's actually present on
your system now the command to check
this
is sudo docker images
and as you can see here hello world
repository
this is present on our system currently
so the image has been successfully
pulled onto the system
and this means that our docker is
working now we'll try out another
command
sudo docker ps minus a
this displays all the containers that
you have pulled so far so as you can see
here there are three hello world images
displayed and all of them are in exited
state so i did this demo previously too
which is why the two hello worlds which
is created two minutes ago is also
displayed here and the first hello world
which has been created a minute ago is
the one we just did for this demo now as
you have probably noticed that all the
hello world images over here all these
containers are in the exited state so
when you give the option for docker ps
minus a where minus a stands for all it
displays all the containers whether they
are in exited or running state if you
want to see only those containers which
are in their running state you can
simply execute sudo docker ps
sudo
docker
yes
and as you can see no container is
visible here because none of them are in
running state in this presentation we're
going to go through a number of key
things we're going to compare docker
versus traditional virtual machines and
what are the differences and why you
want to choose docker over a virtual
environment we'll go through the
advantages of working with docker and
the structure and how you would build
out a docker environment and during that
structure we'll dig through the
components and the advanced components
within docker at the end of the
presentation we'll go through some basic
commands and then show you how those
basic commands can be used in a live
demo so with all that said let's get
started so let's first of all compare
docker with a traditional virtual
machine so here we have the architecture
on the left and right of a traditional
virtual machine versus a darker
environment and there are some things
that you'll probably see immediately
that are big differences one is that the
virtual environment has hypervisor layer
whereas the docker environment has a
docker engine layer and then in addition
to that there are additional layers
within the virtual machine each of these
really start compounding and creating
very significant differences between a
docker environment and a virtual machine
environment so with a virtual machine
the actual memory usage is very high
whereas with the docker environment the
memory usage is very low if we look at
performance virtual machines when you
start building out particularly more
than one virtual machine on a server the
performance starts degrading and starts
getting poorer whereas with docker the
performance always stays really good
this is largely due to the lightweight
architecture used to construct the
docker containers themselves
if we look at portability virtual
machines just are terrible for
portability they're still dependent on
the host operating system and there's
just a lot of problems that happen when
you are using virtual machines for
portability in contrast docker was
designed for portability so you can
actually build solutions in a docker
container environment and have the
guarantee that the solution will work as
you have built it no matter where it's
hosted finally boot up time now the bit
up time for a virtual machine is fairly
slow in comparison to the boot up time
for a docker environment which is almost
instantaneous so we look at these in a
little bit more detail one of the other
challenges that you have with a virtual
machine is that if you have unused
memory within the environment you cannot
reallocate that memory so if you set up
an environment that has nine gigs of
memory that's being used we have six
gigs that are free you can't do anything
with it though that whole nine gig has
been allocated to that virtual machine
in contrast with docker if you have nine
gigs and six gigs becomes free that free
memory can then be reallocated and
reused across other containers used
within that docker environment another
challenge is running multiple virtual
machines in a single environment that
can lead to instability and performance
issues whereas docker is designed to run
multiple containers
in the same environment and actually
gets better the more containers you run
in that hosted single docker engine
portability issues with a virtual
machine is the software can work on one
machine but then when you move that vm
to another machine suddenly some of the
software won't work because there are
some dependencies that haven't been
inherited correctly whereas docker
itself is designed specifically to be
able to run across multiple environments
and to be deployed very easily across
the systems and again the actual boot up
time for a vm it just takes a long time
you took about minutes in contrast to
the milliseconds that it takes for a
docker environment to boot up so let's
dig into what docker actually is and
what allows for these great performance
improvements over a traditional vm
environment so docker itself is an os
virtualized software platform and it
allows it organizations to really easily
create deploy and run applications as
what are called docker containers that
have all the dependencies within that
container very easily
and the container itself is really just
a very lightweight package that has all
the instructions and dependencies such
as frameworks libraries bins etc all
within that container and that container
itself can then be moved from
environment to environment very easily
if we to look in our devops lifecycle
the place where docker really shines is
in deployment because when you're
actually at the point of deploying your
solution you want to be able to
guarantee that the code that has been
tested will actually work in the
production environment but in addition
to that what we often find is that when
you're actually building the code and
you're actually testing the code having
a container running the solution at
those stages is also a really good plus
because what happens is that the people
building the code and testing the code
are able to validate their work in the
same environment that would be used for
the production environment so really
you can use docker in multiple stages
within your devops cycle but it becomes
really valuable in the deployment stage
so let's look at some of the key
advantages that you have with docker
some things that we've already covered
is that you can do
rapid deployment and you can do it
really fast the environment itself is
highly portable and was designed for
that in mind the efficiencies that
you'll see will allow you to run
multiple docker containers in a single
environment as compared to more
traditional vm environments the
configuration itself can be scripted
through a language called yaml which
allows you to be able to write out and
describe the docker environment that you
want to create this in turn allows you
to be able to scale your environment
very very quickly but with all of these
advantages probably the one that is most
critical to the type of work that we're
doing today is security you have to
ensure that the environment you are
running is a highly secure but highly
scalable environment and i'm very
pleased to say that docker takes
security very seriously so you'll see it
as one of the key talents for the actual
architecture of the system that you're
implementing so let's look at how docker
actually works within your environment
so docker works there is a what's called
a docker engine the docker engine is
really comprised of two key elements you
have a server and a client and the
communication via the two is via a rest
api the server as you can imagine has
the instructions that are communicated
out to the client and instructs the
client on what to do the connection
between the client and the server the
communication is via a rest api
on older systems you can take advantage
of the docker toolbox which allows you
to go ahead and control the docker
engine the docker machine docker compose
and kitematic so let's now go into what
the actual root components though of
docker are so let's have a look at those
key components there are four components
that we're going to go through we have
the docker clients and server we have
docker images we have the docker
registry and the docker container we're
going to step through each of these one
by one so let's look at the docker
clients and server first so the docker
client and server is a command line
instructed solution where you would use
terminal on your mac or command line on
your pc or linux system to be able to
issue commands from the docker daemon
the communication between the docker
client and the docker host and is via a
rest api
so you can do some communication such as
a docker pull command which would send
an instruction to the daemon which would
then form the interaction of pulling in
the correct components such as an image
or container or registry to the docker
client the docker daemon itself is
actually a service which actually
performs all sorts of operating and
performance services and as you'd
imagine the docker damon is constantly
listening across the rest api to see if
it needs to perform any specific
requests if you want to trigger and
start the whole process you what you
want to do is use the command dockered
within your docker daemon and that will
start all of your performances and then
you have a docker host which actually
runs the docker daemon and registry
itself so now let's look into the actual
structure of a docker image so a docker
image itself is a template which
contains instructions for the docker
container and that template is written
with a language called yaml and yaml
stands for yet another markup language
it's very easy to learn the docker image
itself is built within that yaml file
and then hosted as a file in the docket
registry the image is really comprised
of several key layers and you start with
your base layer which will typically
have your base image and this instance
is your base operating system such as
ubuntu and then you then have layer of
dependencies above that this would then
comprise the instructions in a read-only
file that would become your docker file
so let's let you go through and look at
what one of those in sets of
instructions would look like so here we
have four layers of instructions we have
a from pull run and then command so what
does that actually look like in our
layers so to break this down the from
creates a layer which is based on ubuntu
and then what we're doing is we're
adding in files from the docker
repository onto that base command that
base layer and then what we want to be
able to do is then say okay what are the
wrong commands so we can actually then
build the container within the
environment and then we want to be able
to then have a command line that
actually executes something within that
container and in this instance the
command is to run python so one of
things that we will see is that as we
set up multiple containers each new
container is a new layer with new images
within the docker environment each
container is completely separate from
the other containers within your docker
environment so you're able to create
your own separate read write
instructions within each layer what's
interesting is that if you delete a
layer then the layer above it will also
get deleted so what happens when you
pull in a layer but something has
changed in the decor image what's
interesting then is that the actual main
image of itself cannot be modified once
you've copied the image you can then
modify it locally but you can never
modify the actual base image itself so
here are some callouts for the
components within a docket image so the
base layer are in read-only format the
layers can be combined in a union file
system to create a single image the
union file system saves memory space by
avoiding duplication of files and this
allows a file system to appear as a
writable but without modifying the file
which is known as a copy on write the
actual base layers themselves are read
only so to be able to get around this
structure within a docker container the
docker environment itself uses what's
known as a copy and write strategy
within the images and the containers
themselves and so what this allows you
to do is you can actually copy the files
for better efficiency across your entire
container environment the copy and write
strategy does make docker super
efficient and what you're able to do all
the time is keep reducing the amount of
disk space you're using and the amount
of performance that you're taking from
the server and that's really again a key
element for docker is just this constant
ability to be able to keep improving the
efficiency within the actual system
itself all right so let's go on to item
number three which is the docker
registry so the docker registry itself
is the place where you would host and
distribute the different types of images
that you have created or you want to be
used within your environment the actual
repository itself is just a collection
of docker images and those docker images
are built on instructions that you would
write with yaml and are very easily
stored and shared and what you can
actually do is you can actually
associate specific name tags to the
actual docker images themselves so it's
easy for people to be able to find and
share that image within the docker
registry itself one of the things you
actually see as we go through the demos
you actually see us actually using the
the tag name and you'll see how it is an
alpha numeric identifying how we
actually use it to actually create the
actual container itself one of the
things you can do to as start off how
you would manage a registry is you can
actually use the publicly accessible
docker hub registry which is available
to anybody but you can also create your
own registry for your own use internally
the actual registry that you create
internally can have both public and
private images that you create and this
may be for various reasons of how you
structure your environment the actual
commands you would use to actually
connect to the registry are both push
and pull push is to actually push a new
container environment that you've
created from your local manager node to
the remote registry and a pool allows
you to pull a new client that has been
created and is being shared so again
full command and it pulls and retrieves
a docket image from the docker registry
and makes it very easy for people to
share different images consistently
across teams and a push command allows
you to take a new command that you've
created a new container that you created
and push it to the registry whether it's
docker hub or whether it's your own
private registry and allow it to be
shared across your teams some key did
you know in docker registry deleting a
repository is not a reversible action so
if you delete a repository it's gone so
let's go into the final stage here which
is the actual docker container itself so
the docker container itself um is an
executable package of applications and
its dependencies bundled together so it
gives all the instructions that you
would have for the solution that you're
looking to run it's actually really
lightweight and again this is because of
the redundancy that's built into how you
structure the container and the
container itself is then inherently also
extremely portable
what's really good about running a
container though is that it does run
completely in isolation so you're able
to share it again very easily from
grouped group and you are guaranteed
that even if you are running a container
it's not going to be impacted by any
host os peculiarities or unique setups
as you would have in a vm or a
non-containerized environment the actual
memory that you have on a docker
environment that can be shared across
multiple containers which is really
useful typically when you have a vm you
would have a defined amount of memory
for each vm environment the challenge
you start running into though is that
you can't share that memory whereas with
docker you can easily share the memory
um for a single environment across
multiple containers
the actual container is built using
docket images and the commands to
actually run those images is a run
command so this guy should go through a
basic structure of how you would run a
docket image so you go into terminal
window and you would write a docker run
redis
and then it would run a container called
redis so we're going to go in and if you
don't have the redis image locally
installed it will then pull it from the
registry then the new docker container
redis will be then available within your
environment so you can actually start
using it so let's look at why containers
are so lightweight they're so
lightweight because they really have
been able to get away from some of the
additional layers that you have in
virtualization within vms and the
biggest one is the hypervisor and the
need to run on a host operating system
those are two big big elements so if you
can get rid of those then you're doing
great so let's look at some of the more
advanced concepts within the docker
environment and we're going to look at
two advanced components one is docker
compose and the second is docker swamp
so let's look at docker compose docker
compose is really designed for
running multiple containers as a single
service and it does this by running each
container in isolation but allowing the
containers to interact with each other
as was stated earlier on you would
actually write the compose environment
using yaml as the language in the files
that you would create so where would you
use something like docker compose so an
example would be if you are running an
apache server with my sql database and
you need to create additional containers
to run additional services without the
need to start each one separately and
this is where you would write a set of
files using docker compose to be able to
help balance out that demand
so let's now look at docker swarm so
docker swarm is a service that allows
you to be able to control multiple
docker environments within a single
platform so what you actually are
looking at doing is within your docker
swamp is we're treating each node as a
docker daemon and we're actually having
an api that's interacting with each of
those nodes
there are two types of node that you're
going to be getting comfortable working
with one is the manager node and the
second is the worker node and as you'd
expect the manager node is the one
sending out the instructions to all of
the worker nodes but there is a two-way
communication that is happening the
communication allows for the imagine
node to be able to manage the
instructions and then listen to receive
updates from the working node so if
anything happens within this environment
the imagine node can react and adjust
the architecture of the worker node so
it's always in sync it was really great
for
large scaled environments so finally
let's go through what are some of the
basic commands you would use within
docker and once we've gone through all
these basic commands we'll actually show
you a demo of how you'd actually use
them as well so if we're going to go in
probably the first command is to install
docker and so if you have yam installed
you just do yum install docker and
you'll install docker onto your computer
to start the docker daemon as you want
to do a system ctl start docker the
command to remove a docker image is
docker rmi and then the image id itself
and that's not the image name that's the
actual alphanumeric id number that you
want to uh grab the command line to
download a new image is docker pull and
then the name of the image you'd want to
pull and by default you're going to be
pulling from the docker default registry
that will then connect to your docker
daemon and download the images from that
registry client the command line to run
an image is docker run and then the
image id
and then we have the if we wanted to
pull specifically from docker hub then
we would have a docker pull and then the
image name and colon its tag
to pull build an image from a docker
file you would do docker build dash t
and then the image name and colon tag to
shut down the container you do docker
stop container id the access for running
a container is docker exec it container
id bash
so we've gone through all the different
commands but let's actually see how they
would actually look and we're going to
go ahead and do a demo so welcome to
this demo where we're going to go ahead
and put together all of the different
commands that we have outlined in the
presentation for docker uh first is just
to list all of the docker images that we
have so we do
sudo docker images and we enter in our
password
and this will now list out the images
that we've created already and we have
three images there
so let's go ahead and pull a docker
image so to do that we'll we'll go ahead
and type sudo
docker
and actually we don't want to do image
we want to select pull
and then the name of the image that we
want to pull which is going to be my
sequel
and by default this is actually going to
go ahead and use the latest my sequel
command my sql image that we have so
it's now go ahead and pull this image
it's going to take a few minutes
depending on your internet connection
speed it's kind of a large file that has
to be downloaded so we'll just wait for
that to download
you can see the others have completed
just waiting for this last file to
download almost there
once that's done we're going to go ahead
and do is we'll actually
run the docking container and create a
new container using the image that we
just downloaded but we have to wait for
this to download first
all right so the image has been pulled
from docker hub
and let's go ahead and create the new
docker container so we're going to do
sudo docker
run
dash d
dash
p
0.0.0.0
[Music]
colon
80
colon 80.
and then put in my sequel
call on
latest so we have the latest version
and we have our new token
and that shows our new docker container
has been created
now let's go ahead and see if the
container is running
and we'll do
sudo docker
ps
to list all the running containers and
what we see is that the container is not
listed there which means it's probably
not running so let's go ahead and list
out all of the images that we have
within docker so we can see whether it's
actually listed there so we'll do
ps-a
and yes there we are we can see that we
do have our new container in my sql
latest
and it was created 36 seconds ago
but it's in the exit mode so what we
have to do is we have to change that
status so it's actually running
so let's change that to running state
we'll do sudo
docker
run
dash
dash dash
name
and we can name it
sl
sql
sequel
slash spin slash
flash
and that's now going to be in the route
and we'll exit out of that and now if we
list out the docking containers we
should see it is now
an active container
sudo docker
start
and then we'll start the scene
and then
and we should now see it
there we are it's now in the running
state excellent
and we can see that it was updated six
seconds ago
we're gonna go ahead and we're gonna
clear the screen
okay now what we want to do is remove
the docker container
so we're going to do is check list of
images that we have
and to sudo docker images
here are the images that we have and we
have my sql is listed
and what we want to do is delete my sql
and to do that we're going to type in
sudo docker rm-f
image
mysql
run that command and what we'll find is
the image there's no such image oh okay
so what we actually have to do is we
have to go and see that the image is
now gone it's been removed excellent
it's exactly what we wanted to see
and we can also delete an image by its
image id as well
however if an image is running
and active we have to kill that image
first so we'll go ahead and we're going
to select the image id we'll copy that
and it's going to replace that it won't
be able to actually run correctly
because the image is active so what we
have to do now is
stop the image and then we can kill it
so it's in the running state
so we have to do
so we do sudo darker kill
and kill sl and that will kill the
container
and now
we'll see that the container has gone
and now we can delete the image
and that's going to be the image going
with image id but
easy peasy
okay let's go ahead onto the next
exercise which is two
so here we are we've listed all of the
uh
containers and they're all gone so let's
go on to the next exercise final
exercise which is to actually
create a batch image
and we do a batch http image so let's go
ahead and write that out so it's going
to be docker
run
dash
did
dash dash
name
white is that's going to be the name of
this http service dash p
8080 colon 80 dash v
open quotes
dollar sign pwd
close quotes
colon
slash usr
slash local
slash
apache2
slash htdocs
slash
httpd
semicolon 2.4
run that
put in our password again
uh so what we see is the port is already
being used so let's go ahead and see
which ports let's go see if we can
change the port or see what ports are
running so let's do
sudo images and see which ports are
being used because it's either the the
port or the name
hasn't been put in correctly so sudo
docker images
ps
pseudo docker
ps-a
and
yep
let's port 80 there
so we'll clear the screen
so we're going to change the container
name because i think we actually have
the wrong container name here so let's
go and change that and we'll paste that
in and voila
here we go now working and we'll just
double check and make sure everything's
working correctly so to do that we'll go
into our web browser
and we'll type in
as soon as firefox opens up
type in localhost
colon 8080
which was the the port that we created
and there we are it's a list of all the
files which shows that the server is up
and running and today we'll be looking
at the installation for the tool chef as
you probably already know chef is a
configuration management tool so that
basically means that chef is a tool
which can automate the entire process of
configuring multiple systems it also
comes with a variety of other
functionalities which you can check out
in our video on what a chef and the chef
tutorial so before we move on to the
installation process let me just explain
to you in brief the architecture of
share so chef has three components
there's the workstation which is where
the system admin sits and he or she
writes the configuration files here your
second system is the server the server
is where all these configuration files
are stored and finally you have the
client or the node systems so these are
the systems that require the
configuration you can have any number of
clients but for a demo to keep it simple
we'll just have one client now i'm using
my oracle vm virtualbox manager as you
can see here i'll have two machines the
master and the node both of these are
centos 7 machines as of the server we'll
be using this as a service on the cloud
so let's begin let's have a look at our
master system first this is my master
system the terminals open over here and
the terminal color here it's black
background with green text
and this is my note system so the
terminal here has a black background
with white text so you can differentiate
between the both so we start at our
master system the first thing we need to
do is we need to download the chef dk so
you can write w get which is the command
for downloading and then go to your
browser and just type chef dk here
the first link
so here you have different versions of
chef dk depending on the operating
system that you're using you need to
select the appropriate one i'm using the
red hat enterprise version and that's
number seven so i'm using centos 7 so
this is my link for downloading chef dk
just copy this link and go back to your
terminal
and paste it here
so your chef dk is being downloaded this
will take a while right after we
download the chef dk our next step is to
install it on our system
swash chef dk is downloaded now let's
install it
so guys this is the version of chef dk
that you have downloaded so make sure
this is exactly what you type down here
to
so great our chef dk is installed so
basically our installation for the
workstation is done right now but just
so you understand how the flow is we'll
also write a sample recipe on our
workstation so before we do that let's
first create a folder
my folder name chef repo basically the
chef repository
and let's move into this folder
okay so we're in next what we need to do
is as i mentioned earlier all your
recipes will be within a cookbook so
let's create a folder which will hold
all our cookbooks
and let's move into this too
okay so our next stage is to create the
actual cookbook within which we'll have
our recipe so the command for creating
the cookbook is chef generate
cookbook
sample because so samples the name of my
cookbook so guys please notice here cook
books is the directory that i created
which will hold all our cookbooks and
here cookbook is the key word so sample
is that one cookbook that we are
creating under our folder cookbooks
and our cookbook is being created great
so that's done moving into our cookbook
okay so when our cookbook sample was
created automatically there's this
hierarchical structure associated with
it so let's have a look at this
hierarchical structure to understand
what our cookbook sample exactly is
before we move on so the command for
looking at a hierarchical structure is
tree so as you see here within our
cookbook we have a folder recipes and
under this this the default.rb recipe
this is where we'll be creating our
recipe so we'll just alter the content
of default.rb so let's move on to
finally writing our recipes so we'll
move into this recipes folder first
now we'll open our recipe default.rb in
g edit
so the recipe for this particular demo
is to install the httpd package on our
client node that is basically your
apache server and we'll also be hosting
a very simple web page so let's begin
so the recipes in chef is written in
ruby
so i'll explain you the recipe in a
while
okay so the first line is where you
install httpd the second line for
service is where you start or enable the
http service on the client node that's
our first task the second part where we
need to create our web page
so this is the part where your web page
will be stored
if you have written any html file
previously you know that this is
probably like a default path where our
web pages are created
yep that's it so this is the content
that will be displayed on your web page
if everything works right and i'm pretty
sure it will so now we can save our
recipe and that's done close your gra so
now that we have created the recipe all
our work at the workstation is completed
the next thing we do is we move on to
the server so as i mentioned earlier
we'll be using the server as a service
on the cloud
so go to your browser and here just type
manage dot chef dot io
so this is the home page of your chef
server
click here to get started we need to
first create an account for using the
chef server this is completely free we
just need to give our email id and a few
other details it's in fact a lot like
creating an account on facebook or
instagram
fill in all the details check the terms
of service box
so the next thing you need to do is go
back to your inbox and verify your email
id so i have my inbox opened here on my
windows machine so this is my inbox you
would have received a mail from chef
software just click on this link to
verify it and create your password
and that's done so let's continue this
on our workstation machine
so type in your username and password
so the first time you log into your chef
server you'll have this pop-up appear
where you need to create a new
organization so create your organization
so this organization is basically the
name that will be associated with a
collection of the client machines first
thing you do go to your administration
tab and download the starter kit so guys
when you're doing this part make sure
that you're on your workstation that is
you're opening your shep server on the
workstation because you need this folder
to be installed here
you save the file so this gets
downloaded so the shift starter kit is
the key to connecting your workstation
with the server and the server with the
node so basically it has a tool called
knife which we'll come across later in
our demo this knife is what takes care
of all the communication and the
transferring of cookbooks between the
three machines in our case the two
machines the workstation and the node
and the one server so let's go back to
our root directory
so our chef starter zip file is within
our downloads folder what we do first is
we'll move the sip folder into our
cookbooks folder and then we'll unzip it
there because our cookbooks folder is
the one that contains the recipe and
that is where we require knife tool
command to be present so we can send
this recipes over to the server
so we'll just check the contents of our
cookbooks right now to ensure that our
chef started.zip file is within the
cookbooks
yep so it's here so next thing we do is
we need to unzip this folder
great so that's unzipped
and this means that our workstation and
our server are now linked
so we just need to use the knife command
tool to transfer or to upload our
recipes which we created on the
workstation onto the server so before we
execute this command we need to move
into our cookbooks directory as you know
that is where we unzipped our chef
starter kit so that is where our knife
command is present to
and now let's execute the knife command
so it's knife
cookbook
upload and sample so as you probably
recall sample is the name of the
cookbook that we created and within
sample we created rsp which is
default.rb so we are uploading the
entire cookbook onto the server execute
the command
great so our cookbooks uploaded now
let's check this on our server so move
to your browser where you opened your
chef server and go to policy so here you
go this is the cookbook we uploaded
sample and it's the first time we
uploaded it so the version
0.1.0 the first version now what you
would notice is if you go to the notes
tab there are no notes present so if you
have no nodes you basically have no
machine to execute your cookbooks and
the nodes are not seen right now because
we have not configured them yet so
that's the next thing we need to do all
this so far was done on your master
machine now we'll move on to the node
machine
so before moving on let's just check the
ip of our node machine
so that's our ip note this down
somewhere and now we move back to our
workstation
as we already saw that we uploaded a
sample workbook next thing we need to
make sure that our server and node are
able to communicate with each other so
again we use the knife tool for this too
the command here is knife bootstrap and
enter the ip address of your node which
we just checked
we'll be logging in there so we'll be
using the node as the root user and then
we also need to specify our root
password for the node
and we give a name to this node so this
is the name by which we'll be
identifying our node at the server
so as you have probably noticed here we
are using the term ssh which is a secure
shell so it basically provides a channel
of secure communication between two
machines in an unsafe environment okay
so it's done so if your command has
executed right which in our case as we
can see has our chef's server and our
chef node must be able to communicate
with each other so if this is so we
should be able to send the cookbook that
we previously uploaded from our
workstation onto the server now from our
server to our node so to do that before
we move on to the node machine we need
to go back to our chef's server let's
refresh this page
and as you see here previously under the
notes tab we did not have any node
mentioned now we do chef node which is
the node we wanted to identify our node
by which is a centos platform and that's
our ip so it's active for to hours
that's the uptime last check-in the last
time we checked into our node was a
minute back and yeah that's pretty much
it so now we'll create a run list and
we'll add a sample to this run list
so just click on your node and you'll
see the small arrow here in the end
click on that
edit run list and under available
recipes we have a cookbook sample
present so drag and drop this to the
current run list and accept it okay so
now that we updated our run list our
recipe is sent to our node what we next
need to do is that we need to execute
this at our node so now we'll move on to
our node machine
shift client is the command to execute
your
so while this recipe is executing you
can see what exactly is happening our
recipe was to install http package first
which is your apache server so the first
line that's done and it's up to date the
second line it's enabled third line the
service is started and the fourth line
is where your contents created for the
web page at this very location so by the
look of this everything should work fine
so how do we check this we can just go
to our browser and the search bar just
type localhost and there you go so our
httpd package which is the patchy server
is installed and our sample webpage is
also hosted congratulations on
completing the chef demo today we'll
dive into a tutorial on the
configuration management tool chef so if
you look at the devops approach or the
devops life cycle you will see that chef
falls under operations and deployment so
before we begin let's have a brief look
at all that you'll learn today first
we'll get to know why should we use chef
and what exactly is the chef two of the
most common terms used with chef
configuration management and
infrastructure as code we'll have a
brief look at these we'll also have a
look at the components of chef and the
chef architecture quickly go through the
various flavors of chef and finally
we'll wrap it up with the demo a demo on
the installation of apache on our notes
so let's begin guys why should we use
chef well consider a large company now
this company caters to a large number of
clients and provides a number of
services or solutions of course to get
all of this done they need a huge number
of servers and a huge number of systems
basically they will have a huge
infrastructure now this infrastructure
needs to be continuously configured and
maintained in fact when you're dealing
with an infrastructure that size there's
a good chance systems may be failing and
in the long run as your company expands
new systems may even get added so what
do you do well you could say the company
has the best system administrator out
there but all by himself could he
possibly take care of an infrastructure
that size no he can't and that's where
chef comes in because chef automates
this entire process so what does chef
provide chef provides continuous
deployment so when you look at the
market space today you see how products
and their updates are coming out in a
matter of days so it's very important
that a company is able to deploy the
product the minute it's ready so that
once it's out it's not already obsolete
chef also provides increased system
robustness as we saw chef can automate
the infrastructure but in spite of this
automation there's a good possibility
that errors do creep in chef can detect
all these bugs and remove them before
deploying them into the real environment
not only this chef also adapts to the
cloud we all know how today the services
tools solutions everything is revolving
around the cloud so chef does really
play along by making itself easily
integratable with the cloud platform so
now that you know why to use chef let's
look at what exactly is chef chef is an
open source tool developed by ops code
of course there are paid versions of
chef such as chef enterprise but other
than that most of it is freely
accessible chef is written in ruby and
erlang if you would have gone through
any previous material on chef i'm sure
you would have come across ruby being
related to chef but not erlang so this
is why because ruby and erlang are both
used to build chef but when it comes to
actually writing the codes in chef it's
just ruby and these are the codes that's
deployed onto your multiple servers and
does the automatic configuration and
maintenance and this is why chef is a
configuration management tool so i have
used this term configuration management
a couple of times what exactly does this
mean let's start with the definition of
configuration management configuration
management is a collection of
engineering practices that provides a
systematic way to manage entities for
efficient deployment so let's break this
down configuration management basically
is a collection of practices and what
are these practices for these practices
are for managing your entities the
entities which are required for
efficient deployment so what are these
entities that you need for efficient
deployment they are code infrastructure
and people code is basically the code
the system administrators right for
configuring your various systems
infrastructure is the collection of your
systems and your servers and then
finally you have the teams that take
care of this infrastructure so codes
need to be updated whenever your
infrastructure needs a new configuration
or some sort of updation in the
operating system or the software
versions your code needs to be updated
at first and as the requirements of the
company change the infrastructure's
configuration needs to change and
finally of course the people need
coordination so if you have a team of
system administrators and say person a
makes some change to the code person b c
d and so on need to be well aware when
the change is made as to why it was made
what was the change made and where
exactly this change was made so there
are two types of configuration
management on our left we have the push
configuration here the server that holds
the files with instructions to configure
your notes pushes these files onto the
node so the complete control lies with
the server on your right side we have
the pull configuration in case of pull
configuration the nodes pole against the
server to first check if there's any
change in the configurations required if
there is the nodes themselves pull these
configuration files shelf follows pull
configuration and how it does this we'll
see further in our video another
important term often used with chef
infrastructure as code so let's
understand what this term infrastructure
as code means through this small story
so here's tim tim's a system
administrator at a large company now he
receives a task he has to set up a
server and he has to install 20 software
applications over it so he begins he
sets up the server but then it hits him
it would take him the entire night to
install 20 software applications
wouldn't things have been much simpler
if he just had a code to do so well of
course codes do make things much simpler
codes have a number of advantages
they're easily modifiable so if today
tim is told we need mysql installed on
20 systems tim simply writes a code to
do so and the very next datum is told we
changed our mind we don't need maya
skill i think we'll just use oracle this
does not bother him because now he just
opens the file he makes a few
corrections in his code and that should
work just fine code is also testable so
if tim had to write ten commands to do
something and at his tenth command he
realized the very first command he wrote
there was something not right there well
that would be quite tiresome wouldn't it
with codes however you can test it even
before running it and all the bugs can
be caught and corrected codes are also
deployable so they're easily deployable
and they're deployable multiple times so
now that we saw the various advantages
of having codes let's say what
infrastructure as code exactly is here's
the definition infrastructure as code is
a type of id infrastructure where the
operation team manages the code rather
than a manual procedure so
infrastructure as a code allows the
operations team to take care of a code
which automatically performs various
procedures rather than having to
manually do those procedures so with
this feature all your policies and your
configurations are written as code let's
now look at the various components of
chef so our first component is the
workstation the workstation is the
system where the system administrator
sits he or she creates the codes for
configuring your nodes now these codes
which in case of chef are written in
ruby are called the recipes and you'll
have multiple number of recipes so a
collection of recipes is called a
cookbook now these cookbooks are only
created at the workstation but they need
to be stored at the server so the knife
is a command line tool so it's basically
a command that you will see us executing
in one of our demos that shifts these
cookbooks from the workstation over to
the server a second component is the
server so servers like the middleman it
lies between your workstation and your
nodes and this is where all your
cookbooks are stored because as you saw
previously the knife sends these
cookbooks over from the workstation to
the server the server can be hosted
locally that's on your workstation
itself or it can be remote so you can
have your server at a different location
you can even have it on the cloud
platform and a final confidence the node
so nodes are the systems that require
the configuration in a chef architecture
you can have a number of nodes oh hi is
a service which is installed on your
node and it is responsible for
collecting all the information regarding
your current state of the node this
information is then sent over to the
server to be compared against the
configuration files and check if any new
configuration is required chef client is
another such service on your node which
is responsible for all the
communications with the server so
whenever the node has a demand for a
recipe the shift client is responsible
for communicating this demand to the
server since you have a number of nodes
in a chef architecture it's not
necessary that each node is identical so
of course every node can have a
different configuration let's now have a
look at the chef architecture so here we
have a workstation one server machine
and two nodes you can have any number of
nodes first things first the system
administrator must create a recipe so
the recipes that are mentioned in our
chef architecture are just dummy recipes
we look into actual functioning recipes
later in our demo so you have one recipe
two recipes three recipes and a
collection of recipes forms a cookbook
so guys if you look at the recipe in
source you have simply learn three dot
erb erb is the extension for your ruby
files so the cookbooks are only created
at the workstation they now need to be
sent over to the server where they are
stored and this is the task of the knife
knife is a command line tool which is
responsible for transferring all your
cookbooks onto the server from the
workstation here's the command for
running your knife knife upload simply
db where simply db is the name of the
cookbook we then move on to our node
machines at our node we run the ohi
service the ohai service will collect
all information regarding the current
state of your notes and send it over to
the chef client when you run the chef
client these informations are sent over
to the server and they are tested
against the cookbooks so if there is any
discrepancy between the current state of
your nodes and the cookbook that is if
one of the nodes does not match the
configurations required the cookbook is
then fetched from the server and
executed at the node this sets the note
to the right state there are various
flavors of chef we'll quickly go through
these first we have chef solo with chef
solo there's no separate server so your
cookbooks are located on the node itself
now this kind of configuration is used
only when you have just a single node to
take care of the next flavor is the
hosted chef with hosted chef you still
have your workstation and your node but
your server is now used as a service on
the cloud this really makes things
simple because you don't have to set up
a server yourself and it still performs
all the functioning of a typical chef
this is the configuration you will
notice that we'll be using in our demo
chef client server with chef client
server you have a workstation you have
server and you have n number of nodes
now this is the traditional chef
architecture this is the one we have
used for all the explanations previously
and finally we have private chef private
chef is also known as enterprise chef in
this case your workstation server and
node all are located within the
enterprise infrastructure this is the
main difference between chef client
server and private chef in case of shep
client server all these three machines
could be dispersed the enterprise
version of chef also provides the
liberty to add extra layers of security
and other features and we reach the
final part of our video where we'll have
the hands-on so before we dive into our
demo let me just quickly give you an
introduction to it we'll be using two
virtual boxes both sent to s7 one will
be used as workstation while the other
will be a node so we are just using one
node to make things simple the server
will be used as a service on the cloud
now these are the steps we'll be
performing during our demo we'll first
download and install the chef dk on our
workstation we then make an empty
cookbook file and we'll write a recipe
into it we need to then set up the
server so as i mentioned earlier server
will be a service on the cloud so you'll
have to create a profile but this will
be completely free we then link the
workstation to the server and will
upload the recipe to the server the
notes will now download the cookbooks
from the server and configure themselves
so now that you have some idea about
what we'll be doing let's move on to the
actual demo we begin our demo
here's my oracle vm virtualbox manager i
have two machines here i've already
created my workstation and node both of
these are sent to s7 machines
just for you to differentiate this is my
terminal and for my workstation it's a
black background with white text and as
of my note it's a black background with
green text
the first thing you do is you go to your
workstation box and open a web browser
search for chef dk installation
go to the first link which is your
chef's official page a very warm welcome
to all our viewers i'm anjali from
simply learn and today i'll be showing
you how you can install the
configuration management tool ansible so
let's have a brief about why one would
use ansible and what exactly is ansible
so if you consider the case of an
organization it has a very large
infrastructure which means it has more
than probably hundreds of systems and
giving one or even a small team of
people the responsibility to configure
all these systems makes their work
really tough repetitive and as you know
manual work is always prone to errors so
ansible is a tool which can automate the
configuration of all these systems with
ansible a small team of system
administrators can write simple code in
yaml and these codes are deployed onto
the hundreds and thousands of servers
which configures them to the desired
states so ansible automates
configuration management that is
configuring your systems it automates
orchestration which means it brings
together a number of applications and
decides an order in which these are
executed and it also automates
deployment of the applications now that
we know what ansible does let's move on
to the installation of ansible so here
is my oracle vm virtualbox manager i'll
be using two systems there's the node
system which is basically my client
system and there's the server system or
the master system so let's begin at our
server system
so this is my master system guys so the
first thing we do is we download our
ansible tool
so one thing we must remember with
ansible is that unlike
chef or puppet ansible is a push type of
configuration management tool so what
this means is that the entire control
here lies with your master or your
server system this is where you write
your configuration files and these are
also responsible for pushing these
configuration files onto your node or
client system as and when required
creates our ansible tool is installed
now we need to open an ansible host file
and there we'll specify the details of
our node or client machine
so this is a ansible host file as you
can see here the entire file is
commented but there's a certain syntax
that you'd observe for example here we
have a group name web servers under
which we have the ip addresses or
certain host name so this is about how
we'll be adding the details for our
client system first we need to give a
group name
under this group basically we add all
the clients which require a certain type
of configuration since we are using just
one node we'll give only the details for
that particular node first we need to
add the ip address of our client machine
so let's just go back to our client
machine
and this here is the ip address 192.168.
once you have typed in your ip address
give a space and then we'll specify the
user for our client machine
so all communications between the server
or the master system and the client or
the node system takes place through ssh
ssh basically provides a secure channel
for the transfer of information
follow this up with your password in my
case it's the root's password
and that's it we are done so now we save
this file and go back to our terminal
so now that our host file is written the
next thing we do is we write our
playbook the playbook is the technical
term used for all the configuration
files that we write in ansible now
playbooks are written in yaml yaml is
extremely simple to both write and
understand it's in fact very close to
english
so now we'll write our playbook the
playbook or any code in yaml first
starts with three dashes this indicates
the beginning of your file next thing we
need to give a name to our playbook
so name
and i'm going to name my playbook sample
book
we next need to specify our host systems
which is basically the systems at which
the configuration file or the playbook
in our case will be executed so we'll be
executing this at the client machines
mentioned under the group ansible
servers so we had just one client
machine under it we'll still mention the
group name
we next need to specify the username
with which we'll be logging into our
client machine
which is root in my case
and become true specifies that you need
to become the root to execute this
playbook so becoming the roots called a
privilege escalation next we need to
specify our tasks so these are basically
the actions that the playbook will be
performing so you would have noticed
everything so far is aligned that is
name host remote user become and task
because these are at one level now
whatever comes under task will be
shifted slightly towards the right
although yaml is extremely simple to
understand and read both it's a little
tricky while writing because you need to
be very careful about the indentations
and the spacing
so my first task is install httpd which
is basically a apache server
so now my command yum and this will be
installing the httpd package
and the latest date of it will be
installed so that's our first task
now our second task would be running our
apache service
so name
run httpd
and the action which is service
will be performed on httpd hence the
name httpd
and state
must be started
now we come to our third task
so here we'll create a very simple web
page that will be hosted
so create content is the name of our
task
and the content that we are creating
here will basically be copied to our
node system at a particular file
location that we'll provide
our content will be congrats
and then we'll provide the destination
at which this file will be copied
so this is the default location for all
our html files
and that's it we are done writing our
playbook
just save this and go back to your
terminal
before we execute the playbook or push
the playbook onto our note system let's
check the syntax of our playbook so the
command for doing so is
and if everything is fine with your
playbook the output would be just your
playbook name so our syntax is perfectly
fine now we can push on the playbook to
our node or the client machine
and that's the syntax for doing so now
as your playbook is being sent over to
the client machine you can see that
first the facts are gathered that is the
current state of your client machine is
first fetched to check what all is to be
changed and what is already present so
the first thing is installing httpd so
our system already had httpd so it says
okay because this does not need to be
changed our next task was running httpd
now although our system had the apache
service it was not running so that is
one thing that was changed the next was
there was no content available so the
content was also added so two tasks were
changed and four things were okay now
everything seems fine and before you
move any forward it is very important
that you check this one line of
documentation provided by ansible you
have all kind of information available
here regarding which all tasks were
executed if your client machine was
reachable or unreachable and so on so
now that everything is fine here we can
move on to our node system
and we'll just go to our browser so if
our playbook has been executed here what
should happen is that the httpd service
must be in the running state and the web
page that we created should be hosted so
let's just type localhost
and great everything's working fine so
our web page is displayed here so we
come to an end for our installation and
configuration video for the
configuration management tool ansible if
you have any doubts please post them in
the comment section below and we'll
definitely get back to you as soon as
possible thanks essentially now we have
matthew and anjali to take us through
how to work with ansible
ansible today as one of the key tools
that you would have within your devops
environment
so the things that we're going to go
through today is we're going to cover
why you would want to use a product like
ansible what ansible really is and how
it's of value to you and your
organization the differences between
ansible and other products that are
similar to it on the market and what
makes ansible a compelling product and
then we're going to dig into the
architecture for ansible we're going to
look at how you would create a playbook
how you'd manage your inventory of your
server environments and then what is the
actual workings of ansible as a little
extra we're going to also throw in
ansible tower one of the secret source
solutions that you can use for improving
the speed and performance of how you
create your ansible environments and
finally we're going to go through a use
case by looking at hootsuite social
media management company and how they
use ansible to really improve the
efficiency within their organizations so
let's jump into this so the big question
is why ansible so you have to think of
ansible as another tool that you have
within your devops environment for
helping manage the service and this
definitely falls on the operations side
of the dev ops equation so if we look
here we have a picture of sam and like
yourselves sam is a system administrator
and he is responsible for maintaining
the infrastructure for all the different
servers within his company so some of
the servers that he may have that he has
to maintain could be web servers running
apache they could be database servers
running uh mysql and if you only have a
few servers then that's fairly easy to
maintain i mean if you have three web
servers and two database servers and
let's face it would we all love just to
have one or two servers to manage it
would be really easy to maintain the
trick however is as we start increasing
the number of servers and this is a
reality of the environments that we live
and operate in it becomes increasingly
difficult to create consistent setup of
different infrastructures such as web
servers and databases for the simple
reason that we're all human as if we had
to update and maintain all of those
servers by hand there's a good chance
that we would not set up each server
identically now this is where anspo
really comes to the rescue and helps you
become an efficient operations team
ansible like other system solutions such
as chef and puppet uses code that you
can write and describe the installation
and setup of your servers so you can
actually repeat it and deploy those
servers consistently into multiple areas
so now you don't have to have one person
redoing and re-following setup
procedures you just write one script and
then each script can be executed and
have a consistent environment so we've
gone through why you'd want to use
ansible let's step through what ansible
really is
so you know this is all great but you
know how do we actually use these tools
in our environment so
ansible is a tool that really allows you
to create and control three key areas
that you'd have within your operations
environment first of all there's it
automation so you can actually write
instructions that automate the it setup
that you would typically do manually in
the past the second is the configuration
and having consistent configuration
imagine setting up hundreds of apache
servers and being able to guarantee with
precision that each of those apache
servers is set up identically and then
finally you want to be able to automate
the deployment so that as you scale up
your server environment you can just
push out instructions that can deploy
automatically different servers the
bottom line is you want to be able to
speed up and make your operations team
more efficient so let's talk a little
bit about pull configuration and how it
works with ansible so there are two
different ways of being able to set up
different
environments for server farms
one is to have a key server that has all
the instructions on and then on each of
the servers that connect to that main
master server you would have a piece of
software known as a client installed on
each of those servers that would
communicate to the main master server
and then would periodically either
update or change the configuration of
the slave server this is known as a pull
configuration an alternative is a push
configuration and the push configuration
is slightly different the main
difference is as with a pull
configuration you have a master server
where you actually put up the
instructions but unlike the pull
configuration where you have a client
installed on each of the services with a
push configuration you actually have no
client installed on the remote servers
you simply are pushing out the
configuration to those servers and
forcing a restructure or a fresh clean
installation in that environment so
ansible is one of those second
environments where it's a push
configuration server and this contrasts
with other popular products like chef
and puppet which have a master slave
architecture with a master server
connecting with a client on a remote
slave environment where you would then
be pushing out the updates with ansible
you're pushing out the service and the
structure of the server to remote
hardware and you are just putting it
onto the hardware irrelevant of the
structure that's out there and there are
some significant advantages that you
have in that in that you're not having
to have the extra overhead weight of a
client installed on those remote servers
having to constantly communicate back to
the master environment so let's step
through the architecture that you would
have for an ansible environment
so when you're setting up an ansible
environment the first thing you want to
do is have a local machine and the local
machine is where you're going to have
all of your instruction and really the
power of the control that you'd be
pushing out to the remote server so the
local machine is where you're going to
be starting and doing all of your work
connected from the local machine are all
the different nodes pushing out the
different configurations that you would
set up on the local machine the
configurations that you would write and
you would write those in code within a
module so you do this on your local
machine for creating these modules and
each of these modules is actually
consistent playbooks the local machine
also has a second job and that job is to
manage the inventory of the nodes that
you have in your environment the local
machine is able to connect to each of
the different nodes that you would have
in your hardware network through ssh
clients so a secure client
let's dig into some of the different
elements within that architecture and
we're going to take a first look at
playbooks that you would write and
create for the ansible environments so
the core of ansible is the playbook this
is where you create the instructions
that you write to define the
architecture of your hardware so the
playbook is really just a set of
instructions that configure the
different nodes that you have and each
of those set of instructions is written
in a language called yammer and this is
a standard language used for
configuration server environments did
you know that yaml actually stands for
yaml 8 markup language let's just a
little tidbit to hide behind your ear so
let's have a look or one of these play
books it looks like and here we have a
sample
yaml script that we've written so you
start off your jammer script with three
dashes and that indicates the start of a
script and then the script itself is
actually consistent of two distinct
plays at the top we have play one and
below that we have play two within each
of those plays we define which nodes are
we targeting so here we have a web
server in the top play and in the second
play we have a database server that
we're targeting and then within each of
those server environments we have the
specific tasks that we're looking to
execute so let's step through some of
these tasks we have an install apache
task we have a start apache task and we
have an install my sequel task and when
we do that we're going to execute a
specific set of instructions and those
instructions can include installing
apache and then setting the state of the
apache environment or starting the
apache environment and setting up and
running the mysql environment
so this really isn't too complicated and
that's the really good thing about
working with yaml is it's really
designed to make it easy for you as an
operations lead to be able to configure
the environments that you want to
consistently create so let's take a step
back though we have two hosts we have
web server and database server where do
these names come from well this takes us
into our next stage in the second part
of working with ansible which is the
inventory management part of ansible so
the inventory part of ansible is where
we maintain the structure of our network
environment so what we do here is part
of the structure in creating different
nodes is we've had to create two
different nodes here we have a web
server node and a database server net
and under web server node we actually
have the names that were actually
pointing to specific machines within
that environment so now when we actually
write our scripts all we have to do is
refer to either web server or database
server and the different servers will
have the instructions from the yammer
script executed on them this makes it
really easy for you to be able to just
point to new services without having to
write out complex instructions so let's
have a look at how ansible actually
works in real world so the real world
environment with is that you'd have the
ansible software installed on a local
machine and then it connects to
different nodes within your network on
the local machine you'll have your first
your playbook which is the set of
instructions for how to set up the
remote nodes and then to identify how
you're going to connect to those nodes
you'll have an inventory we use secure
ssh connections to each of the servers
so we are encrypting the communication
to those servers we're able to grab some
basic facts on each server so we
understand how we can then push out the
playbook to each server and configure
that server remotely the end goal is to
have an environment that is consistent
so let's ask you a simple question
what are the major opportunities the
ansible has over chef and puppet
really like to hear your answers in the
comments below pop them in there and
we'll get back to you and really want to
hear how you feel that ansible is a
stronger product or maybe you think it's
a weaker product as it compares to other
similar products in the market here's
the bonus we're going to talk a little
bit about ansible tower so ansible tower
is an extra product that red hat created
that really kind of puts the cherry on
the top of the ice cream or is the icing
on your cake ansible by itself is a
command line tool however as well tower
is a framework that was designed to
access ansible and through the ansible
tower framework we now have an easy to
use gui this really makes it easy for
non-developers to be able to create the
environment that they want to be able to
manage in their devops plan without
having to constantly work with the
command prompt window so instead of
opening up terminal window or command
window and writing out complex
instructions only in text you can now
use drag and drop and mouse click
actions to be able to create your
appropriate playbooks inventories and
pushes for your nerds alright so we
talked a lot about anspol let's take a
look at a specific company that's using
ansible today and in this example we're
going to look at hootsuite now hootsuite
if you've not already used their
products and they have a great product
hootsuite is a social media management
system they are able to help with you
managing your pushes of social media
content across all of the popular social
media platforms they're able to provide
the analytics they're able to provide
the tools that marketing and sales teams
can use to be able to assess a sentiment
analysis of the messages that are being
pushed out really grateful and very
popular but part of their popularity
drove a specific problem straight to
hootsuite the challenge they had at
hootsuite is that they had to constantly
go back and rebuild their server
environment and they couldn't do this
continuously and be consistent there was
no standard documentation and they had
to rely on your memory to be able to do
this consistently imagine how complex
this could get as you're scaling up with
a popular product that now has tens of
thousands to hundreds of thousands of
users this is where ansible what came in
and really helped the folks over at
hootsuite today the devops team at
hootsuite write out playbooks that have
specific instructions that define the
architecture and structure of their
hardware nodes and environments and are
able to do that as a standard product
instead of it being a problem in scaling
up their environment they now are able
to rebuild and create new servers in a
matter of seconds
the bottom line is ansible has been able
to provide hootsuite with it automation
consistent configuration and free up
time from the operations team so that
instead of managing servers they're able
to provide additional new value to the
company a very warm welcome to all our
viewers i'm anjali from simply learn and
today i'll be taking you through a
tutorial on ansible so ansible is
currently the most trending and popular
configuration management tool and it's
used mostly under the devops approach so
what will you be learning today you
learn why you should use ansible what
exactly is ansible the ansible
architecture how ansible works the
various benefits of ansible and finally
we'll have a demo on the installation of
apache or the http package on our client
systems we'll also be hosting a very
simple web page and during this demo
i'll also show you how you can write a
very simple playbook in yemen and your
inventory file
so let's begin why should you use
ansible let's consider a scenario of an
organization where sam is a system
administrator sam is responsible for the
company's infrastructure a company's
infrastructure basically consists of all
its systems this could include your web
servers your database servers the
various repositories and so on so as a
system administrator sam needs to ensure
that all the systems are running the
updated versions of the software now
when you consider a handful of systems
this seems like a pretty simple task sam
can simply go from system to system and
perform the configurations required but
that is not the case with an
organization is it an organization has a
very large infrastructure it could have
hundreds and thousands of systems so
here is where sam's work gets really
difficult not only does it get tougher
sam has to move from system to system
performing the same task over and over
again this makes sam bored not just that
repeating the same task leaves no space
for innovation and without any ideas or
innovation how does the system grow and
the worst of it all is manual labor is
prone to errors so what does sam do well
here is where ansible comes in use with
ansible sam can write simple codes that
are deployed onto all the systems and
configure them to the correct states
so now that we know why we should use
ansible let's look at what exactly is
ansible ansible is an i.t engine that
automates the following tasks so first
we have orchestration orchestration
basically means bringing together of
multiple applications and ensuring an
order in which these are executed so for
example if you consider a web page that
you require to host this webpage stores
all its values that it takes from the
user into a database so the first thing
you must do is ensure that the system
has a database manager and only then do
you host your web page so this kind of
an order is very crucial to ensure that
things work right
next ansible automates configuration
management so configuration management
simply means that all the systems are
maintained at a consistent desired state
other tools that automate configuration
management include puppet and chef
and finally ansible automates deployment
deployment simply means the deploying of
application onto your servers of
different environments so if you have to
deploy an application on 10 systems with
different environments you don't have to
manually do this anymore because ansible
automates it for you in fact ansible can
also ensure that these applications or
the code are deployed at a certain time
or after regular intervals
now that we know what exactly ansible is
let's look at ansible's architecture
ansible has two main components you have
the local machine and you have your node
or the client machine so the local
machine is where the system
administrator sits he or she installs
ansible here and on the other end you
have your node or the client systems so
in case of ansible there's no supporting
software installed here these are just
the systems that require to be
configured and they are completely
controlled by the local machine at your
local machine you also have a module a
module is a collection of your
configuration files and in case of
ansible these configuration files are
called playbooks playbooks are written
in yaml yaml stands for yaml ain't a
markup language and it is honestly the
easiest language to understand and learn
since it's so close to english we also
have the inventory the inventory is a
file where you have all your nodes that
require configuration mentioned and
based on the kind of configuration they
require they are also grouped together
so later in the demo we'll have a look
at how the playbook and the inventory is
written and that will probably make it
clearer so of course a local machine
needs to communicate with the client and
how is this done this is done through
ssh ssh is your secure shell which
basically provides a protected
communication in an unprotected
environment
okay so we saw the various components of
ansible now how does ansible exactly
work you have your local machine on one
end this is where you install ansible if
you have gone through any previous
material on ansible you would have come
across the term agentless often being
associated with the stool so this is
what agentless means you're installing
ansible only on your local machine and
there's no supporting software or plugin
being installed on your clients this
means that you have no agent on the
other end the local machine has complete
control and hence the term agentless
another term that you would come across
with ansible is push configuration so
since the local machine has complete
control here it pushes the playbooks on
to the nodes and thus it's called a push
configuration tool now the playbooks and
the inventory are written at the local
machine and the local machine connects
with the nodes through the ssh client
this step here is optional but it's
always recommended to do so it's where
the facts are collected so facts are
basically the current state of the node
now all this is collected from the node
and sent to the local machine so when
the playbook is executed the task
mentioned in the playbook is compared
against the current status of the note
and only the changes that are required
to be made further are made and once the
playbooks are executed your notes are
configured to the desired states so as i
mentioned before ansible is currently
the most trending tool in the market
under the configuration management
umbrella so let's have a look at the
various benefits of ansible which gives
it this position well ansible is
agentless it's efficient it's flexible
simple important and provides automated
reporting how does it do all this let's
have a look at that agentless as i
already mentioned before you require no
supporting software or plugin installed
on your node or the client system so the
master has complete control and
automatically this means that ansible is
more efficient because now we have more
space in our client and node systems for
other resources and we can get ansible
up and running real quick ansible is
also flexible so an infrastructure is
prone to change very often and ansible
takes no amount of time to adjust to
these changes ansible cannot get any
simpler with your playbooks written in a
language such as yaml which is as close
to english as you can possibly get
air important basically means that if
you have a playbook which needs to be
run n number of systems it would have
the same effect on all of these systems
without any side effect and finally we
have automated reporting so in case of
ansible your playbook has number of
tasks and all these tasks are named so
whenever you run or execute your
playbook it gives a report on which
tasks ran successfully which failed
which clients were not reachable and so
on all this information is very crucial
when you're dealing with a very large
infrastructure and finally we reach the
most exciting part of our tutorial the
hands-on before we move on to the actual
hands-on let me just brief you through
what exactly we'll be doing so i'll be
hosting two virtual boxes both centos 7
operating systems one would be my local
machine and other my node or the client
machine so on my local machine first
i'll install ansible we then write the
inventory and the playbook and then
simply deploy this playbook on the
client machine there's just one thing
that we need to do is that we need to
check if the configurations that we
mentioned in our playbook are made right
so we'll now begin our demo this is my
oracle virtualbox here i have my master
system which is the local machine and
this is the client machine so let's have
a look at these two machines this is my
client machine the terminals open right
now so the client machine terminal has a
black background with white text and the
master machine terminal has a white
background with black text just so you
can differentiate between the two so
we'll start at the master machine the
first thing to do is we need to install
our ansible so yum install
ansible hyphen y is the command to do so
so this might take some time
yeah so ansible is installed the next
step we go to our host file so a host
file here is basically the inventory
it's where you'll specify all your nodes
in our case we just have one node
that's the path to your host file
as you'll see everything here is
commented so just type in the group for
your client notes
so i'm going to name it ansible clients
[Music]
and here we need to type the ip address
of a client machine so my client
machine's ip address is 192.168.2.127
so before you come to this it's advised
that you check the ip address on your
client machine the simple command for
that is ifconfig now once you type the
ip address put a space and here we need
to mention the username and the password
for our client so i'll be logging in as
the root user
so this is the password
and then the user
which is root in my case
that's it now you can save this file
just clear the screen
next we move on to our playbook we need
to write the playbook
so the extension for our playbook is yml
which stands for yaman
and as you can see here i have already
written my playbook but i'll just
explain to you how this is done
so a yaml file always begins with three
dashes this indicates the start of your
yaml file
now the first thing is you need to give
a name to the entire playbook
so i have named it sample book
host is basically where this would be
executed
so as we saw earlier in our inventory i
mentioned clients group name as ansible
clients so we use the same name here
the remote user is the user you'll be
using at your client so in my case
that's root and become true is basically
to indicate that you need to set your
privileges at root so that's called a
privilege escalation
now a playbook consists of tasks
so we have here three tasks the first
task i've named it to install httpd so
what we are doing here is we are
installing our http package which is
basically the apache server and we are
installing the most latest version of it
hence the state value is latest the next
task is running httpd so for the service
the name is httpd because that's the
service we need to start running and the
state is started our next task is
creating content so this is the part
where we are creating our web page
so copy because this is the file that
will be created at the client
the content will be welcome
and the destination of the file will be
www
index.html as you know this is like a
default path that we use to store all
our html files
now as you can see here there's quite a
lot of indentation and when it comes to
yaml although it's very simple to write
and very easy to read
the indentation is very crucial
so the first dash here represents the
highest stage that is the name of the
playbook and all the dashes under task
are slightly shifted towards the right
so if you have two dashes at the same
location they basically mean that they
are siblings so the priority would be
the same so to ensure that all your
tasks are coming under the tasks label
make sure they are not directly under
name so yeah that's pretty much it so
when you write your yaml file the
language is pretty simple very readable
indentations absolutely necessary make
sure all your spaces are correctly
placed we can now save this file
next thing we need to check if the
syntax of our yaml file is absolutely
right because that's very crucial
so the command to check the syntax of
the yaml file is ansible
playbook
the name of your playbook
syntax check
so we have no syntax errors which is why
the only output you receive is
sample.yml which is the name of your
playbook
so our playbook is ready to be executed
the command to execute the playbook
is ansible playbook
and the name of your playbook
so our playbooks executed as you can see
here gathering facts that's where all
the facts of the note that the present
state of the note is collected and sent
to the local machine so it's basically
to check that if the configuration
changes that we are about to make is
already made
so it's not made we do not have the
httpd package installed on our node so
this is the first change that's made
also if it's not installed of course
it's not running that's the second
change that's made so it's put into the
running state
and a final task which is create content
is under the okay state this means that
the contents already present in the
client machine so i made it this way so
that you can actually see the different
states that's present
so over here we have okay four so four
things are all fine the facts are
gathered two things are changed and one
is already present two changes are made
zero clients are unreachable and zero
tasks have failed so this is the
documentation that i was referring to
previously that ansible provides
automatically
and is very useful as you can see
so our next step we need to just check
on our client machine if all the changes
that we desired are made so let's move
to our client
so this is my client machine so to check
this since we are installing the httpd
package and hosting a web page the best
way to do it
is open your browser
and type in
localhost
so there you go your apache server is
installed and your web page is hosted
today i'll be showing you the
installation procedure for the
configuration management tool puppet so
what exactly is the use of puppet if you
consider the scenario of an organization
which has a very large infrastructure
it's required that all the systems and
servers in this infrastructure is
continuously maintained at a desired
state this is where puppet comes in
puppet automates this entire procedure
thus reducing the manual work so before
we move on to the demo let me tell you
what the architecture of puppet looks
like so puppet has two main components
you have the puppet master and the
puppet client the puppet master is where
you write the configuration files and
store them and the puppet client are
basically those client machines which
require the configuration in case of
puppet these configuration files that
you write are called manifest so let's
move on to the demo so here are my two
machines the first is the server system
which is basically a master where you'll
write your configuration files and the
others the node or the client system so
let's have a look at both of these
machines this my node system the
terminals open here and the terminal has
a black background with white text
and as of my server or the master
machine it has a black background with
green text
so we started a server machine the first
thing that we need to do is we need to
remove the firewall so in a lot of cases
there are chances that the firewall
stops the connection between your server
and your node now since i'm doing a demo
and i'm just showing you how puppet
works between two virtual boxes i can
safely remove the firewall without any
worries but when you're implementing
puppet in an organization or a number of
systems on a local network be careful
about the consequences of doing so
so a firewall is disabled next thing
that we do is we'll change the hostname
of our server system now while using the
puppet tool it's always advisable that
you name your server's host as puppet
this because the puppet tool identifies
the hostname puppet by default as the
host name for the master or the server
system
let's just check if the host name is
changed successfully
yup so that's done so as you see still
localhost is appearing as the host name
so just close your terminal and start it
again
and you see here the hostname has been
changed to puppet okay so the next thing
that we have to do is we install our
puppet labs make sure your system is
connected to the net
right so puppet labs is installed next
we need to install the puppet server
service on our server system
now that our puppet server service is
installed we need to move into the
system configurations for our puppet
server so the path for that is atc
sysconfig puppet server
so this is a configuration file for the
puppet server now if you come down to
this line now this line here this is the
line which allocates memory for your
puppet server now you must remember that
puppet is a very resource extensive tool
so just in case to ensure that we do not
encounter any errors because of out of
memory we will reduce these sizes so as
of now we have 2 gb allocated by default
we'll change this to 512 mb
now in a lot of cases it may work
without doing so but just to be on the
safer side we make this change
save it
and go back to your terminal we are now
ready to start our puppet server service
the first time you start your puppet
server service
it may take a while next we need to
enable this
and if your puppet server service is
started and enabled successfully this is
the output that you would get in case
you're still not sure you can always
check the status at any point of time
and as you see here it's active so
everything's fine as of now next thing
we do is we'll move on to our agent
system or a client or node system
so here too we'll have to install puppet
labs but before we do so we need to make
a small change in our hosts file so
let's open the host file
yeah so this is a host file we need to
add a single line here which specifies
our puppet master so first we put our
puppet masters ip address followed by
the host name and then we'll add a dns
for our puppet server so let's just go
back
to our server system and find out its
ip address
and that's my ip address for the server
system
now the host name of our puppet server
and a dns for it
save this file
and return to your terminal
so now we can download our puppet labs
on the node system is the exact same
procedure that you followed for
downloading puppet labs on your server
system too
so in my note system the puppet labs is
already downloaded so the next thing is
we need to install a puppet agent
service
so puppet is a pull type of
configuration tool what this means is
that all your configuration files that
you'll be writing on your server is
pulled by the node system as and when it
requires it so this is the
co-functionality of the agent service
which is installed on your client node
or agent system so my puppet agent
service is installed so next i'll just
check if my puppet server is reachable
from this node system
so 8140 is a port number that the puppet
server must be listening on
and it's connected to puppet so that
guarantees that your server is reachable
from the node system
so now that everything is configured
right we can start our agent service
so guys you would have noticed that the
command for starting the agent service
is a little more complex in the command
for starting your server service this is
because when you start your agent
service you're not just starting a
service but you're also creating a
certificate this is the certificate that
will be sent over to your master system
now at the master system there's
something called the certificate
authority this gives the master the
rights to sign a certificate if it
agrees to share information with that
particular node so let's execute this
command which does both the function of
sending the certificate and starting
your agent service so as you can see
here our services started successfully
it's in a running state now we'll move
to our master system or the server
system
so first we'll have a look at the
certificates that we received
the certificate should be in this
location
so as you can see here this is the
certificate that we just received from
our agent service so this here within
quotes is the name of our certificate so
next when we are signing the certificate
this is the name we'll provide to
specify that this is the particular
certificate that we want to sign so the
minute we sign a certificate the note
that send the certificate gets a
notification that the master has
accepted your request so after this we
can begin sharing our manifest files now
here's the command for signing this
certificate
okay so our certificate is signed
which means that the node's request is
approved and the minute the certificate
is signed the request is removed from
this list so now if we execute the same
command as we did to check the list of
all the certificates we will not find
the certificate anymore let's just check
that so as you see now there are no more
requests pending because we have
accepted all the requests if you want to
have a look at all the certificates that
is signed or unsigned you can use the
same command with the addition of all
and all the certificates received so far
will be listed as you can see here the
plus sign indicates that the certificate
request has already been accepted so now
that our certificate is signed the next
thing we do is we'll create a sample
manifest file
this is the part that you create your
manifest files in
our file name is sample.pp and our files
created so right now we have no content
in this file we'll just check if the
agent is receiving it and once that's
confirmed we'll add some content to the
file so let's move to our agent system
now this is the command to execute at
the agent system to pull your
configuration files
so a catalog is applied in 0.02 seconds
so now that the communication between
our agent system and our master system
is working perfectly fine let's add some
content to the previous placeholder file
that we created on our master system
so now we open the same file
in an editor
okay so we are going to write a code for
installing the httpd package on our note
system which is basically your apache
service
node and then within quotes insert the
hostname of your node system
so my node systems hostname is client
the package you wish to install which in
our case is httpd
and the action to be performed
and that's it a very small and simple
code save this file
now let's go back to our node system
and let's pull the second version of the
same configuration file so every time
you execute this command as we did
previously to what happens is the agent
service so the agent service basically
checks on your master system if there's
any new configuration file added or if
there's any change to the previous
configuration file made if so then the
catalog is applied once again so now our
catalog is applied in 1.55 seconds so
now to check if our catalog served its
purpose let's just open our browser
just type localhost here
and as you can see if your httpd package
has been successfully installed the
apache testing page will appear here so
in this session what we're going to do
is we're going to cover what and why you
would use puppet what are the different
elements and components of puppet and
how does it actually work and then we'll
look into the companies that are
adopting puppet and what are the
advantages that they have now received
by having puppet within their
organization and finally we'll wrap
things up by reviewing how you can
actually write a manifest in puppet so
let's get started so why puppet so here
is a scenario that as an administrator
you may already be familiar with you as
an administrator have multiple servers
that you have to work with and manage so
what happens when a server goes down
it's not a problem you can jump onto
that server and you can fix it but what
if the scenario changes and you have
multiple servers going down so here is
where puppet shows its strap with
puppets all you have to do is write a
simple script that can be written with
ruby and write out and deploy to the
servers your settings for each of those
servers the code gets pushed out and to
the servers that are having problems and
then you can choose to either roll back
to those servers to their previous
working states or set them to a new
state and do all of this in a matter of
seconds and it doesn't matter how large
your server environment is you can reach
to all of these servers your environment
is secure you're able to deploy your
software and you're able to do this all
through infrastructure as code which is
the advanced devops model for building
out solutions so let's dig deeper into
what puppet actually is so puppet is a
configuration management tool maybe
similar tools like chef that you may
already be familiar with it ensures that
all your systems are configured to a
desired and predictable state public can
also be used as a deployment tool for
software automatically you can deploy
your software to all of your systems or
to specific systems and this is all done
with code this means you can test the
environment and you can have a guarantee
that the environment you want is written
and deployed accurately so let's go
through those components of puppet so
here we have a breakdown of the puppet
environment and on the top we have the
main server environment and then below
that we have the client environment that
would be installed on each of the
servers that would be running within
your network so if we look at the top
part of the screen we have here our
puppet master store which has and
contains our main configuration files
and those are comprised of manifests
that are actual codes for configuring
the clients we have templates that
combine our codes together to render a
final document and you have files that
will be deployed as content that could
be potentially downloaded by the clients
wrapping this all together is a module
of manifest templates and files you
would apply a certificate authority to
sign the actual document so that the
clients actually know that they're
receiving the appropriate and authorized
modules outside of the master server
where you'd create your manifest
templates and files you would have
public client is a piece of software
that is used to configure a specific
machine there are two parts to the
client one is the agent that constantly
interacts with the master server to
ensure that the certificates are being
updated appropriately and then you have
the fact of it the current state of the
client that is used and communicated
back to through the agent so let's step
through the workings of puppet so the
puppet environment is a master slave
architecture the clients themselves are
distributed across your network and they
are constantly communicating back to a
master server environment where you have
your puppet modules the client agent
sends a certificate with the id of that
server back to the master and then the
master will then sign that certificate
and send it back to the client and this
authentication allows for a secure and
verifiable communication between client
and master the factor then collects the
state of the client and sends that to
the master based on the facts sent back
the master then compiles manifests into
the catalogs and those catalogs are sent
back to the clients and an agent on the
client will then initiate the catalog a
report is generated by the client that
describes any changes that have been
made and sends that back to the master
with the goal here of ensuring that the
master has full understanding of the
hardware running software in your
network this process is repeated at
regular intervals ensuring all client
systems are up to date so let's have a
look at companies that are using puppet
today there are a number of companies
that have adopted puppet as a way to
manage their infrastructure so companies
that are using puppy today include
spotify google att so why are these
companies choosing to use puppet as
their main configuration management tool
the answer can be seen if we look at a
specific company staples so staples
chose to take and use puppet for their
configuration management tool and use it
within their own private cloud the
results were dramatic the amount of time
that the it organization was able to
save in deploying and managing their
infrastructure through using puppets
enabled them to open up time to allow to
experiment with other and new projects
and assignments a real tangible benefit
to a company so let's look at how you
write a manifest in puppet so so
manifests are designed for writing out
in code how you would configure a
specific node in your server environment
the manifests are compiled into catalogs
which are then executed on the client
each of the manifests are written in the
language of ruby with a dot pp extension
if we step through the five key steps
for writing a manifest they are one
create your manifest and that is written
by the system administrator two compile
your manifest and it's compiled into a
catalog three deploy the catalog is then
deployed onto the clients four execute
the catalogs are run on the client by
the agent and then five end clients are
configured to a specific and desired
state if we actually look into how
manifest is written it's written with a
very common syntax if you've done any
work with ruby or really configuration
of systems in the past this may look
very familiar to you so we break out the
work that we have here you start off
with a package file or service as your
resource type and then you give it a
name and then you look at the features
that need to be said such as ip address
then you're actually looking to have a
command written such as present or start
the manifest can contain multiple
resource types if we continue to write
our manifest and puppet the default
keyword applies a manifest to all
clients so an example would be to create
a file path that creates a folder called
sample in a main folder called etc the
specified content is written into a file
that is then posted into that folder and
then we're going to say we want to be
able to trigger an apache service and
then ensure that that apache service is
installed on a node so we write the
manifest and we deploy it to a client
machine on that client machine a new
folder will be created with a file in
that folder and an apache server will be
installed you can do this to any machine
and you'll have exactly the same results
on those machines we're going to decide
which is better for your operations
environment is it chef puppet antipal or
saltstack all four are going to go
head-to-head so let's go through the
scenario of why you'd want to use these
tools so let's meet tim he's our system
administrator and tim is a happy camper
putting and working on all of the
systems in his network but what happens
if a system fails if there's a fire a
server goes down well tim knows exactly
what to do he can fix that fire really
easily the problems become really
difficult for tim however if multiple
servers start failing particularly when
you have large and expanding networks so
this is why tim really needs to have a
configuration management tool and we
need to now decide what would be the
best tool for him because configuration
management tools can help make tim look
like a superstar all he has to do is
configure the right codes that allows
him to push out the instructions on how
to set up each of the servers quickly
effectively and at scale all right let's
go through the tools and see which ones
we can use the tools that we're going to
go through are chef puppet and support
and salt stacks and we have videos on
most of these software and services that
you can go and view to get an overview
or a deep dive in how those products
work so let's go and get to know our
contestants so our first contestant is
chef and chef is a tool that allows you
to configure very large environments it
allows you to scale very effectively
across the entire ecosystem and
infrastructure chef is by default an
open source code and one of the things
that you find is a consistent metaphor
for the tools that we recommend on
simplylearn is to use open source code
the code itself is actually written in
language of ruby and erlang and it's
really designed for heterogeneous
infrastructures that are looking for a
mature solution the way that chef works
is that you write recipes that are
compiled into cookbooks and those
cookbooks are the definition of how you
would set up a node and a node is a
selection of servers that you have
configured in a specific way so for
instance you may have apache linux
servers running or you may have a my sql
server running or you may have a python
server running and chef is able to
communicate back and forth between the
nodes to understand what nodes are being
impacted and need to have instructions
sent out to them to correct that impact
you can also send instructions from the
server to the nodes to make a
significant update or a minor update so
this great communication going back and
forth if we look at the pros and cons
the pros for chef is that there is a
significant following for chef and that
has resulted in a very large collection
of recipes that allow you to be able to
quickly stand up environments there's no
need for you to have to learn complex
recipes the first thing you should do is
go out and find the recipes that are
available it integrates with git really
well and provides for really good strong
version control some of the consoles are
really around the learning speed it
takes to go from beginner user with chef
to being an expert there is a
considerable amount of learning that has
to take place and it's compounded by
having to learn ruby as the programming
language and the main server itself
doesn't really have a whole lot of
control it's really dependent on the
communication throughout the whole
network all right let's look at our
second contender puppet and puppet is
actually in many ways very similar to
chef there are some differences but
again puppet is designed to be able to
support very large heterogeneous
organization it is also built with ruby
and uses dsl for writing manifests so
there are some strong similarities here
to chef as with a chef there is a master
slave infrastructure with puppet and you
have a master server that has the
manifests that you put together in a
single catalog and those catalogs are
then pushed out to the clients of an ssl
connection some of the pros with puppet
is that as with chef there is a really
strong community around puppies and
there's just a great amount of
information and support that you can get
right out of the gate it is a very well
developed reporting mechanism that makes
it easier for you as an administrator to
be able to understand your
infrastructure one of the cons is that
you have to really be good at learning
ruby again as with chef you know the
more advanced tasks really need to have
those ruby skills and as with chef the
server also doesn't have much control so
let's look at our third contender here
ansible and so ansible is slightly
different it is the way the ansible
works is that it actually just pushes
out the instructions to the server
environment there isn't a client server
or master slave environment where
ansible would be communicating backwards
and forwards with its infrastructure it
is merely going to push that
instructions out the good news is that
the instructions are written in yaml and
yowell stands for yaml a markup language
yaml is actually pretty easy to learn if
you know xml and xml is pretty easy if
you know xml you're going to get yammo
really well ansible does work very well
on environments where the focus is
getting servers up and running really
fast it's very very responsive and can
allow you to move quickly to get your
infrastructure up quick very fast and
we're talking seconds and minutes here
really really quick so again the way
that ansible works is that you put
together a playbook and an inventory or
you have a playbook so the way that
ansible works is that you have a
playbook and the playbook it then goes
against the inventory of servers and
will push out the instructions for that
playbook to those servers so some of the
pros that we have for ansible we don't
need to have an agent installed on the
remote nodes and servers it makes it
easier for the configuration yaml is
really easy to learn you can get up to
speed and get very proficient with yamo
quickly the actual performance once you
actually have your infrastructure up and
running is less than other tools that we
have on our list now i do have to add a
proviso this is a relative less it's
still very fast it's going to be a lot
faster than individuals manually
standing up servers but it's just not as
fast as some of the other tools that we
have on this list and gamble itself as a
language while it's easy to learn it's
not as powerful as ruby ruby will allow
you to do things that at an advanced
level that you can't do easily with the
ammo so let's look at our final
contender here salt stack so sort stack
is a cli based tool it means that you
will have to get your command line tools
out or your terminal window out so you
can actually manage the entire
environment via salt sac the
instructions themselves are based on
python but you can actually write them
in yammer or dsl which is really
convenient and as a product it's really
designed for environments that want to
scale quickly and be very resilient now
the way that sort snap works is that you
have a master environment that pushes
out the instructions to what they call
grains which is your network and so
let's step through some of the pros and
cons that we have here with saltstack so
subsequent is very easy to use once it's
up and running it has a really good
reporting mechanism that makes your job
as an operator in your devops
environment much much easier the actual
setup though is a little bit tougher
than some of the other tools and and
it's getting easier with the newer
releases but it's just a little bit
tougher and related to that is that
saltstack is fairly late in the game
when it comes to actually having a
graphical user interface for being able
to create and manage your environment
other tools such as ansible have
actually had a ui environment for quite
some time all right so we've gone
through all four tools let's see how
they all stack up next to each other so
let the race begin let's start with the
first stage architecture so the
architecture for most of our
environments is a server client
environment so for chef puppet and salt
snack so very similar architecture there
the one exception is ansible which is a
client only solution so you're pushing
out the instructions from a server and
pushing them out into your network and
there isn't a client environment that
there isn't a two-way communication back
to that main client for what's actually
happening in your network so let's talk
about the next stage ease of setup so we
look at the four tools there is one tool
that really stands out for ease of setup
and that is ansible it is going to be
the easiest tool for you to set up and
if you are new to having these types of
tools in your environment you may want
to start with ansible just to try out
and see how easy it is to create
automated configuration before looking
at other tools now and so with that said
chef puppet and socksec aren't that hard
to set up either and you'll find there's
actually some great instructions on how
to do that setup in the online community
let's talk about the languages that you
can use in your configuration so we have
two different types of language with
both chef and ansible being procedural
in that they actually specify at how you
actually supposed to do the task in your
instructions with puppet and sort stack
it's decorative where you specify only
what to do in the instructions let's
talk about scalability which tools scale
the most effectively and as you can
imagine all of these tools are designed
for scalability that is the driver for
these kind of tools you want them to be
able to scale to massive organizations
what do the management tools look like
for our four contenders so again we have
a two-way split with ansible and sort
stack the management tools are really
easy to use you're gonna love using them
they're just fantastic to use with
puppet and chef the management tools are
much harder to learn and they do require
that you learn some either the puppet
dsl or the ruby dsl to be able to be a
true master in that environment but what
does interoperability look like
again as you'd imagine with the similar
to scalability interoperability with
these products is very high in all four
cases
now let's talk about cloud availability
this is increasingly becoming more
important for organizations as they move
rapidly onto cloud services well both
ansible and softstack have a big fail
here neither of them are available in
the most popular cloud environments and
puppet and chef are actually available
in both amazon and azure now we've
actually just haven't had a chance to
update our chef link here but chef is
now available on azure as well as amazon
so what does communication look like
with all of our four tools so the
communication is slightly different with
them chef has its own knife tool and
whereas puppet uses ssl to secure
sockets layer and ansible and saltsec
use secure socket hashing and ssh as
their communication tool bottom line all
four tools are very secure in their
communication so who wins well here's
the reality all four tools are very good
and it's really dependent on your
capabilities and the type of environment
that you're looking to manage that will
determine which of these four tools you
should use the tools themselves are open
source so go out and experiment with
them there's a lot of videos our team
has done a ton of videos on these tools
and so feel free to find out other tools
that we have then covered so you can
learn very quickly how to use them but
consider the requirements that you have
and consider the capabilities of your
team if you have ruby developers or you
have someone on your team that knows
ruby your ability to choose a broader
set of tools becomes much more
interesting if however you're new to
coding then you may want to consider
yaml-based tools again the final answer
is going to be up to you and we'll be
really interested on what your decision
is
monitoring as the term says you're
monitoring you're watching you're
logging your production environment so
of course there are whole bunch of
monitoring tools so they become an
important part of your production
environment and a lot of these
monitoring tools are also i've seen them
also being used especially in your uac
environment
and
you can optionally have them for some
time even in your you know development
environment
no not not development different
development services are usually not
very
high-end configurations but you know
maybe a decent uh development slash
integration
server especially if you have uh long
running scripts and if you have uh
programs that use a lot of
servers uh you know maybe cpu or
processing power so then you can have
monitoring tools when you're writing
such scripts and you know
uh you are
doing the uh unit testing for those
scripts
so that you know to see uh what kind of
server utilization
happens when you run this script you
know if you put this in production will
it actually you know slow down your
production server and what kind of uh
impact that will have on the you know
your rest of your application
or other applications running on that
server
but the this particular chapter is more
in context with production environment
so these tools they basically monitor
your server they monitor your switches
of course they monitor your application
then any services that you have deployed
on your
servers
and they generate alerts when something
goes wrong that's the whole job of
monitoring it is continuously watching
continuously looking at what is running
what is happening what is going up what
is going down
when is the cpu peaking when is memory
peaking and all that so that you can
you typically send limits for these
all these different parameters and
anytime any of these parameters goes
outside of that limit you know even more
than that or less than that
these monitoring tools usually send out
an alert
and these alerts could again be sms
alerts or email alerts and there are
there are usually people monitoring
these monitoring tools
to look look out for any
issues reported
and they also generate alerts when the
problem has been dissolved so it will
work both ways
so nagios
is an open source monitoring tool and it
can
even monitor your network services
there's a little diagram here which
is too small
but here is nadia somewhere what i can
read
and
status
these are different devices
i think no no no yeah these are
different devices to which nigeria's is
sending the status there's a browser
there's an sms there's an email
and then there's a graph also
and these are different objects
that nigeria is basically monitoring
this is an smtp server i can read smtp
i don't know tcpip no i don't know some
database server okay this is a database
server and this is an application server
do you switch router okay okay i can
read that now so this is a different
kind of object these are different kind
of servers that nigeria's
monitors
and
these are the different kind of uh
devices or statuses that it can send
so it helps uh monitor your cpu usage
your disk usage and you know even your
system logs
and it uses a plug-in script that can be
written uh you know in
any scripting language actually
the naji is remote plugin executors are
basically agents that allow remote
scripts
to be executed as well and
these scripts are usually executed to
monitor
again
your cpu just use a number of users
logged in who is logged in who's logged
in at what time logged out at what time
and all these things
so all these monitoring tools work on
the concept of polling
so
polling is more like you know they
so the nrp agent is a program that will
continuously keep polling a machine
for certain parameters that are
configured
in nadio's to be monitored so this
program continuously keeps pinging the
server bringing the program
uh you know to keep checking for what it
has been asked to check so in case of
logged in users you keep checking uh at
a you know like maybe every 30 seconds
or every one minute
you keep pinging uh to see how many
users have logged in onto this server
and who are the users who have logged in
what time they're logged in what time
they logged out and things like that
so now just pull agents on remote
missions this is what basically it means
that you have agent programs that can
you know help you
call
or ping even remote machines
the nigeria's remote data processor is
an agent that allows uh you know
flexible data transport and you know it
uses uh http and
xml protocols to do that
and we're talking about the
essentially your databases and
data server usages like you know within
if you have an oracle database how many
database instances are there you know
how
your load balancing is set up on that
how
data is moving
between different uh
database servers within oracle
and how
data is moving within the load balancers
and
there's always a dip uh there's always a
backup with database so that's why you
see me mention trp as soon as i say the
word database
and
if there's a
backup plan you know
uh
how how's the data moving how much time
does did the backup take did it take too
much time
and why you know why did it take so it
helps you do all those kind of
monitoring
the ns client is basically mainly used
to monitor windows machines and
typically when we talk about servers
we end up
talking more about you know unix on
linux servers of course now with a lot
of microsoft
technologies being uh robust then they
were you know
like sharepoint or
things like that there are windows
servers too but
10 years ago if you
would talk about having a windows server
it was actually
kind of frowned upon especially for
production
and again you know this helps you
monitor as usual your cpu your this
usage and it pulls the plug-in and this
particular
agent listens to this particular port
always so that's a reserved port
and usually your system administrators
or server admin administrators know all
these things today let's get started
with jenkins jenkins in my opinion is
one of the most popular continuous
integration servers of recent times what
began as a hobby project by a developer
working for sun microsystems way back in
early or mid 2000s has gradually and
eventually evolved into very very
powerful and robust automation servers
it has a wide adoption since it is
released under mit license and is almost
free to use
jenkins has a vast developer community
that supports it by writing all kinds of
plugins plugins is the heart and soul of
jenkins because using plugins one can
connect jenkins to anything and
everything under the sun
with that introduction let's get into
what all will be covered as a part of
this tutorial i will get into some of
the prerequisites required for
installing jenkins post which i will go
ahead and install jenkins on a windows
box there are few first time
configuration that needs to be done and
i will be covering those as well so once
i have jenkins installed and configured
properly i will get into the user
administrative part i create few users
and i will use some plugins for setting
up various kinds of access permissions
for these users i will also put in some
freestyle jobs freestyle job is nothing
but a very very simple job and i will
also show you the powerfulness of
jenkins by scheduling this particular
job to run based upon time schedule i
will also connect jenkins with github
github is our source code where source
code repository where i've got some
repositories put up there so using
jenkins i will connect to github pull up
a repository that is existing on github
onto the jenkins box and run few
commands to build this particular
repository that is pulled from github
sending out emails is a very very
important configurations of jenkins or
any other continuous integration server
for that matter whenever there is any
notifications that has to be sent out as
a part of either a build going bad or
built being good or built being
propagated to some environment and all
these scenarios you would need the
contains integration servers to be
sending out notifications so i will get
into a little bit details of how to
configure jenkins for sending out emails
i will also get into a scenario where i
will have a web application a maven
based java web application which will be
pulled from a github repository and i
will deploy it onto a tomcat server the
tomcat server will be locally running on
my system eventually i will get into one
other very very important topic which is
the master slave configuration it's a
very very important and pretty
interesting topic where distributed
builds is achieved using a master slave
configuration so i will bring up a slave
i will connect the slave with the master
and i will also put in a job and kind of
delegate that particular job to the
slave configuration finally i will let
you know how to use some plugins to back
up your jenkins so jenkins has got a lot
of useful information set up on it in
terms of the build environments in terms
of workspace all this can be very very
easily backed up using a plugin so this
is what i'm going to be covering as a
part of this tutorial jenkins is a web
application that is written in java and
there are various ways in which you can
use and install jenkins i have listed
popular three mechanisms in which
jenkins is usually installed on in any
system the topmost one is as a windows
or a linux based services so if at all
you have windows like the way i have and
i'm going to use this mechanism for this
demo so i would download a msi installer
that is specific to jenkins and install
this service so whenever i install it as
a service it goes ahead and nicely
installs all that is required for my
jenkins and i have a service that can be
started or stopped based upon my need
any flavor of linux as well one other
way of running jenkins is downloading
this generic warp file and as long as
you have jdk installed you can launch
this war file by the command opening up
a command prompt or shell prompt
differently on linux box specifying java
hyphen jar and the name of this war file
it typically brings up your web
application and you know you can
continue with your installation the only
thing being if at all you want to stop
using jenkins you just go ahead and
close this prompt you either do a
control c and then bring down this
prompt and your jenkins server would be
down other older versions of jenkins
were run popularly using this way in
which you already have a java based web
server running up and running so you
kind of drop in this war file into the
root folder or the httpd root folder of
your web server so jenkins would explode
and kind of bring up your application
all user credentials or user
administration is all taken care of by
the apache or the tomcat server or the
web server on which jenkins is running
this was a very older way of running but
still some people use it because if they
don't want to maintain two servers if
they already have a java web server
which it's being nicely maintained and
backed up jenkins can run attached to it
all right so either ways it doesn't
matter however you're going to bring up
your jenkins instance the way we're
going to operate jenkins is all going to
be very very same or similar one with
the subtle changes in terms of user
administration if at all you're
launching it through any other web
server which will take care of the user
administration otherwise all the
commands or all the configuration or the
way in which i'm going to run this demo
it is going to be same across any of
these installations all right so the
prerequisites for running jenkins as i
mentioned earlier jenkins is nothing but
a simple web application that is written
in java so all that it needs is java
preferably jdk 1.7 or 1.8 2gb ram is the
recommended ram for running jenkins and
also like any other open source tool
sets when you install jdk ensure that
you set in the environment variable java
home to point to the right directory
this is something very specific to jdk
but for any other open source tools that
you've installed there's always a
preferred environment variable that you
got to set in which is specific to that
particular tool that you are going to
use this is a generic thing that is
there for you know for any other open
source projects because the way open
source projects discover themselves is
using this environment variables so as a
general practice or a good practice
always set these environment variables
accordingly so i already have jdk 1.8
installed on my system but in case you
do not what i would recommend is just
navigate on your browser to the oracle
homepage and just type in or search for
install
jdk 1.8
and navigate to the oracle homepage
you'll have to accept the license
agreement and there are a bunch of
installers that is okay that you can
pick up based upon the operating system
on which you're running so
i have this windows 64 installer that is
already installed and running on my
system so i will not get into the
details of downloading this or
installing it let me show you once i
install this what i've done with regard
to my path
so
if you get into this environment
variables
all right so i've just set in a java
home variable
if you see this sequel and program files
java jdk 1.8 this is where my my java is
located c program files
c program files java
okay so this is the home directory of my
jdk so that is what i've been
i've set it up here in my environment
variable
so if you see here this is my java home
all right one other thing to do is
ensure that in case you want to run java
or java c from a command prompt ensure
that you also add that path into this
path variable so if you see this
somewhere i will see yes there you go c
colon program files java jdk 1.8 pin so
with these two i'll ensure that my java
installation is nice and you know good
enough so to check that to double check
that or to verify that let me just open
up a simple command prompt
and if i type in java iphone version
all right
and java c
iphone version
so the compiler is on the path java is
on the path and if i told i do this
even the environment variable specific
to my java is
installed correctly so i'm good to go
ahead with my jenkins installation
now that i have my prerequisites
all set for installing jenkins let me
just go ahead and download jenkins
so let me open up a browser and say
download
jenkins
all right lts is nothing but the long
term support these are all stable
versions weeklies i would not recommend
that you try these unless until you have
a real need for that
long term support is good enough and as
i mentioned there are so many flavors of
jenkins that is available for download
you also have a docker container
wherein you know you can launch jenkins
as a container but i'll not get into
details of that in this tutorial
all right so what i want is
yes this is the war file which is a
generic war file that i was talking to
you earlier and this is the windows msi
installer so go ahead and download this
msi installer i already have that
downloaded so let me just open that up
all right so this is my downloaded
jenkins instance
or other installer this is a pretty
maybe a few months old but this is good
enough for me
before you start jenkins installation
just be aware of one fact that there is
a variable called jenkins home this is
where jenkins would store all this
configuration data jobs project
workspace and all that specific to
jenkins so by default if at all you
don't set this to any particular
directory if at all you install an msi
installer all your installation gets
into c colon program files 86 and
jenkins folder if at all you run a war
file depending upon the user id with
which you're running your war file the
jenkins folder there's a dot jenkins
folder that gets created inside the user
home directory so in case you have any
need wherein you want to backup your
jenkins or you want jenkins
installations to get into some specific
directories go ahead and set this
jenkins home variable accordingly before
you even begin your installation for now
i don't need to do any of these things
so i've already downloaded the installer
let me just go ahead with the default
installation
all right so this is my jenkins msi
installer i would just i don't want to
make any changes into the jenkins
configuration c colon program files is
good for me yeah this is where all my
destination folder and all the
configuration specific to it goes i'm
happy with this i don't want to change
this
i would just say go ahead and click
installation
okay so what typically happens once the
jenkins installation gets through is
it'll start installing itself and there
are some small checks that needs to be
done
so and by default jenkins launches on
the port 8080 so let me just open up
localhost 8080.
there's a small checking that will be
done as a part of the installation
process wherein i need to type in the
hash key all right so there's a very
very simple hash key that gets stored
out here so i'll have to just copy this
path
if i told you you're running as a war
file you would see that in your
logs all right so this is a simple hash
key that gets created every time when
you do a jenkins station so as a part of
the installation it just asks you to do
this
so
if that is not great it will crib about
it but this looks good
so it's going ahead
all right one important part during the
installation
so you would need to install some
recommended plugins
what happens is the plugins are all
related to each other so it's like the
typical rpm kind of a problem where you
try to install some plugin and it's got
a dependency which is not installed and
you get into all those issues in order
to get rid of that what jenkins
recommends there's a bunch of plugins
that is already recommended so just go
ahead and blindly click that
install recommended plugin so if you see
there is
a whole lot of plugins which are bare
essential plugins that is required for
jenkins in order to run properly so
jenkins as a part of the installation
would get all these plugins and then
install it for you
this is a good combination to kind of
begin with and mind you at this moment
jenkins needs
lots of bandwidth in terms of network so
in case your you know your network is
not so good few of these plugins would
kind of fail
and these plugins are all you know on
available on openly
or or mirrored sites and sometimes some
of them may be down so do not worry in
case some of these plugins kind of fail
to install you'll get an option to kind
of retry installing them but just ensure
that you know at least most or 1995 of
all these plugins are installed without
any problems let me pause the video here
for a minute and then get back once all
these plugins are installed
my plugin installation is all good there
was no failures in any of my plugins so
after that i get to create this first
admin user again this is one important
point that you got to remember
keen given any username and password but
ensure that you kind of remember that
because it's very hard to get back your
username and password in case you forget
it alright so i'm going to create a very
very simple username and password
something that i can remember
i will
that's my name and um an email id is
kind of optional but it doesn't allow me
to go ahead in case i don't so i just
given an admin and i got a password i've
got i remember my password this is my
full name
all right i say save and finish
all right that kind of completed my
jenkins installation
it was not that tough was it
now that i have my jenkins installed
correctly let me quickly walk you
through some bare minimal configurations
that is required these are kind of a
first time configuration that is
required so and also let me warn you the
ui is little hard for many people to
wrap their head around it specifically
the windows guys but if at all you're a
java guy you know how painful it is to
write ui in java you will kind of
appreciate you know all the effort that
has gone into the ui bottom line you are
a little hard to you know wrap your head
around it but once you start using it
possibly you'll start liking it
all right so let me get into something
called as managed jenkins
this can be viewed like a
main menu for all jenkins configuration
so i'll get into some of those important
ones
something called as configure system
configure system this is where you kind
of put in the configuration for your
complete jenkins instance few things to
kind of look out for this is a home
directory this is a java home where all
the configurations all the workspace
anything and everything regarding
jenkins is stored out here
system message you want to put in some
message on the system you just type in
whatever you want and it's probably show
up somewhere up here on the menu number
of executors very very important
configuration
this just lets jenkin know at any point
in time how many jobs or how many
threads can be run you can kind of
visualize it like a thread that can be
run on this particular instance as a
thumb rule if at all you're on a single
core system number of executors two
should be good enough
in case at any point in time there are
multiple jobs that kind of get triggered
at the same time in case the number of
executors are less compared to the
number of jobs that have that woken up
no need to panic because they will all
get queued up and eventually jenkins
will get to running those jobs just bear
in mind that whenever a new job kind of
you know gets triggered the cpu usage
and the memory usage in terms of the
disk right is very high on the jenkins
instance
so that's something that you got to kind
of keep in mind all right but number of
executors two for my system is kind of
good label for my jenkins i don't want
any of these things usage how do you
want to use your jenkins this is good
for me because i only have a primary uh
server that is running so i want to use
this note as much as possible quite
pretty each of these options have got
some pair minimal
help kind of a thing that is that is out
here by clicking on these question marks
you will get to know as to what are
these particular configurations
all right so this all look good what i
want to show you here is there's
something regarding the docker
timestamps git plugin svn
email notifications
i don't want that what i want the yes i
want this smtp server configuration
remember i mentioned earlier that i
would want jenkins to be sending out
some emails
and what i've done here is i've just
configured the smtp details of my
personal email id
in case you're in a in organization you
would have some sort of an email id that
is set up for a jenkins server so you
can specify the smtp server details of
your company so that you know you can
authorize jenkins to kind of send out
emails
but in case you want to try it out like
me i have configured my personal email
id which is on my gmail for sending out
notifications so the smtp server would
be
smtp.gmail.com i'm using the smtp
authentication i have provided my email
id and my password i'm using the smtp
port which is 465 and i'm you know reply
to address is the same as mine i can
just send out an email
and see if at all this configuration
works again gmail would not allow you to
allow anybody to send out notifications
on your behalf so you'll have to lower
the security level of your gmail id so
that you can allow a programmatically
somebody to send out email notifications
on your behalf so i've done already that
i'm just trying to see if i can send a
test email with the configuration that
i've set in
yes
all right so the email configuration
looks good
so this is how you configure your uh you
know your gmail account in case you want
to do that if not put in your
organization smtp server details which
are for the valid username and password
and it should all be set
all right so no other configurations
that i'm going to change here all of
these look good
all right so i come back to managing
kens
okay one other thing that i want to kind
of go over is the global tool
configuration
imagine this scenario look at it this
way jenkins is a is a continuous
integration server it doesn't know what
kind of a code base it's going to pull
in what kind of a tool set that is
required or what is the code that is
going to pull in and how is it going to
build so you will have to put in all the
tools that is required for building the
appropriate kind of code that you're
going to pull in from you know your
source code repositories so just to give
you an example in case your source code
is a java source code and assuming that
you know because in this demo this is my
laptop and i've put in all the
configurations jdk everything on my
laptop because i'm a developer i'm
working on the laptop but my continuous
integration server would be you know a
separate server without anything being
installed on it so in case i want
jenkins to you know run a java code i
would need to install jdk on it i need
to specify the jdk location of this out
here this way
okay since i already have the jdk
installed and i've already put in the
java home directory or rather the
environment variable correctly i don't
need to do it git if at all i want the
jenkins server to use git git is a you
know command
bash or the command prompt for for
running git and connecting to any other
git server so you would need git to be
you know installed on that particular
system and set the path accordingly
gradle and maven if at all you have some
mavens as well you want to do this any
other tool that you're going to install
on your system
which is your continuous integration
server you will have to come in here and
configure something
in case you don't configure it when
jenkins runs it will not be able to
find these tools for building your task
and it'll crib about it
that's good i don't want to save
anything
manage jenkins let me see what else is
required yes configure global security
all right the security is enabled and if
you see by default it's the security
access control is set to jenkins own
user database so what does this mean you
know jenkins by default it uses file
system where it stores all the user
names which hashes up these user names
and kind of stores them
so as of now it jenkins is configured to
use its own database assuming that you
are running in an organization you would
probably want to have a you know some
sort of an ad or an ldap server using
which you would want to control access
to your jenkins repository rather
jenkins tool
so you would specify your ldap server
details the root dn
password or the manager dn and the
manager password and all these details
in case you want to connect your jenkins
instance with your
ldap or ad or any of the authentication
servers that you have in your
organization
but for now since i don't have any of
these things i'm going to use this own
database that's good enough
all right so i will set up some
authorization methods and stuff like
that once i put in few jobs so for now
let me not get into any of these details
of this just be aware that jenkins can
be connected for authorization to an
ldap server or you can have jenkins
managing its own servers which is
happening as of now so i'm going to save
all this stuff
that's good for me so enough of all
these configurations let me put in a
very very simple job
all right so job new item you know a
little difficult to kind of figure out
but then that's the new item so i'll
just say you know first job this is good
for me i just give a name for my job i
would say it's a freestyle project
that's good enough for me i don't want
to choose any of that so unless until
you choose any of this this particular
button would not become active so choose
the freestyle project and say okay
at a very high level you would see
general source code management build
triggers build environment build and
post build in case you install more and
more plug-ins you will see a lot more
options but for now this is what you
would see so what am i doing at the
moment i'm just putting up a very very
simple job and the job could be anything
and everything so i don't want to put in
a very complicated job for now for the
demo let me just put in a very very
simple job i'll give a description
this is an optional thing this is my
first
jenkins job
all right i don't want to choose any of
these again there are some helps
available here
i don't want to choose any of this i
don't want to connect it into any source
code for now
i don't want any triggers for now i'll
come back to this in in a while build
the environment i don't want any build
environment as a part of this build step
you know i just want to you know run few
things so that i kind of complete this
particular job so since i'm on a windows
box i would say execute windows batch
command
all right so what do you want to do i
would let me just echo something echo
uh hello
this is my first jenkins job
and possibly i would want the date and
the time stamp
pertaining to the job i mean the date
and time in which this job was run all
right very very simple command that says
you know this is my first job it just
puts out something along with the date
and the time
all right i don't want to do anything
else i want to keep this job as simple
as this so let me
save this job
all right so once i save this job you
know the job names comes up here and
then i need to build this job and you
would see some buildings tray out here
nothing is there as of now because i've
just put in a job i have not run it yet
all right so let me try to build it now
you see a build number you would see a
date and a timestamp so if i click on
this you will see a console output if i
go here
okay as simple as that
and where is all the job details that is
getting into if you see this
if i navigate to this particular
directory
all right so this is the directory what
i was mentioning earlier regarding
jenkins home so all the job related
stuff that is specific to this
particular jenkins installation is all
here
all the plugins that is installed the
details of each of those plugins can be
found here
all right so the workspace is where all
the jobs that i've created whichever i'm
running would be wrong there will be
individual folders specific to the jobs
that has been put up here all right so
one job one quick run
that's what it looks like pretty simple
okay let me do one thing let me put up a
second job
i would say second job
i would say freestyle project
all right this is my second job
i just want to demonstrate the
powerfulness of the automation server
and how simple it is to automate a job
that is put up on jenkins
which will be triggered automatically
remember what i said earlier about
jenkins because at the core of jenkins
is a very very powerful automation
server alright so what i'm going to do i
will just keep everything else the same
i'm going to put in the build script
pretty much similar to
second job
that
gets triggered
automatically
every
minute all right let me do that
percentage date
and i'll put in the time
all right so i just put in another job
called second job and it pretty much
does the same thing as what i was doing
earlier in terms of printing the date
and the time but this time i'm just
going to demonstrate the powerfulness of
the automation server that is there if
you see here there's a build trigger
so a build can be triggered using
various triggers that is there so we'll
get into this github uh triggering or
hook or a web hook kind of a triggering
later on but for now what i want to do i
want to ensure that this job that i'm
going to put in would be automatically
triggered on its own let's say every
minute i want this job to be run on its
own
so build periodically is my setting
if you see here there's a bunch of
help that is available for me so for
those of you who have written cron jobs
on linux boxes you'll find it very very
simple but for others don't panic let me
just put up a very very simple regular
expression for scheduling this job every
minute
all right so that's
one
two
three four
five all right come up come up come up
all right so five stars is all that i'm
going to put in and jenkins got a little
worried and he's asking me do you really
mean every minute oh yeah i want to do
this every minute let me save this and
how do i check whether it gets triggered
every minute or not i just don't do
anything i'll just wait for a minute
and if at all everything goes well
jenkins would automatically trigger my
second job in a minutes time from now
this time around i'm not going to
trigger anything look there you see
it's automatically got trigger
if i go in here
yep second job that gets triggered
automatically you know it was triggered
at 42
1642 which is 442 my time that looks
good
and if everything goes well every one
minute onwards this jog would be
automatically triggered
now that i have
my jenkins up and running a few jobs
that has been put up here on my jenkins
instance i would need a way of
controlling access to my jenkins server
this is wherein i would use a plugin
called role based access plugin and
create few rules the rules are something
like a global rule and a project rule
project specific growth i can have
different roles and i can have users who
have signed up or the users who might
create kind of assigned to these roles
so that each of these users fall into
some category this is my way of kind of
controlling access to my jenkins
instance and
ensuring that people don't do something
unwarranted all right so first things
first let me go ahead and install a
plugin
for doing that so i get into manage
jenkins and manage plugin
a little bit of a confusing screen in my
opinion there is updates available
installed in advanced as of now we don't
have the role based plugin so let me go
to available it'll take some time for it
to get refreshed all right now these are
the available plugins these are the
install plugins
all right so let me come back to
available and i would want to search for
my role based access plugin so i would
just search for role and hit enter
okay role based authorization strategy
enables user authorization using a role
based strategy roles can be defined
globally or for particular jobs or nodes
and stuff like that so exactly this is
the plugin that i want
i would want to install it without a
restart
all right looks good so far
yes
go back to the top of the page yes
remember jenkins is running on a java
using a java instance so typically many
things would work the same way unless
and until you want to restart jenkins
once in a while but as a good practice
whenever you do some sort of a big
installations or big patches on your
jenkins instance just ensure that you
kind of restart it otherwise there would
be a difference in terms of what is
installed on the system and what is
there on the file system you will need
to flush out few of those settings later
on but for now these are all very small
plugins so this would run without any
problems but otherwise if at all there
are some plugins which would need a
restart you know kindly go ahead and
restart your jenkins instance but for
now i don't need that it looks good i've
installed the plugin so where do you see
my plugin i installed the plugin that is
specific to the
user control or the access control so
let me go into
yes
global security
and i would see this role-based strategy
showing up now
all right so this comes in because of my
installation of my role-based plugin so
this is what i would want to enable
because i already have my own database
set up and for the authorization part in
the sense that who can do what i'm going
to install i mean i've already installed
a role based strategy uh plugin and i'm
going to enable that strategy all right
i would say save
okay now i've installed the role based
access plugin i would need to just set
it up and check that you know i would go
ahead and create some roles and ensure
that i assign users as per these roles
all right so let me go to
manage jenkins
configure
all right let me see where is this
configure configure global security is
that where i create my roles
nope not here
yes manage and assign roles
okay again you would see these options
only after you install these plugins so
for now i've just enabled the plugin i
have enabled role based access control
and i would go ahead and create some
rules for this particular jenkins
instance so i would say first
manage rules
so i would need to create some roles
here and the rules are at a very high
level these are global rules and there
are some project rules and there are
some slave rules i'll not get into
details of all of this at a very very
high level which is a global role let me
just create a role a role can be kind of
visualized like a group so i would
create a role called developer typically
the jenkins instance or the ca instance
are kind of owned up or controlled by qa
guys so qr guys would need to provide
some sort of you know limited access to
developers so that's why i'm creating a
role called developer
and i'm adding this role at a global
role level so i would say add this here
and you would see this developer role
that is there and each of these options
if we hover over it you would see some
sort of a
help on what what are these you know
permissions specific to so what i want
is like you know it sounds a little
different but i would want to give very
very little permissions for the
developer so from an administration
perspective i would just want him to
have a read um kind of a role
credentials again i would just want to
view kind of a rule i don't want him to
create any agents and all that stuff
that looks good for me
for a job i would want him to just
possibly
read i don't want him to build i don't
want him to cancel any jobs i don't want
him to configure any job i don't even
want him to create any job i would just
want him to read few things i would not
give him possibly a role to the
workspace as well i mean i don't want
him to have access to the workspace i
would just want him to
read a job
or check you know have read-only access
to the job run um no i don't want him to
give him any any particular
access which will allow him to run any
jobs view configure
yeah possibly create yeah
delete i don't want to read yes
definitely
and this is the specific role so what
i'm doing i'm just creating a global
role called developer and i'm giving him
very very limited roles
in the sense that i don't want this
developer to be able to run any agents
nor create jobs or build jobs or cancel
jobs or configure jobs at the max i
would just want him to read a job that
is already put up there
okay
so i would save
now i created a role
i still don't have any users that is
there on the system so let me go ahead
and create some user on the system
that's not here i will say configure
manage and cans manage users
okay let me create a new user
i would call this user as yeah
developer1 sounds good
some password
some password that i can remember
okay his name is developer1
d at
d.com or something like that
okay so this is the admin with with
which i kind of configured or brought up
the system and developer1 is a user that
i have configured so still have not set
any rules for this particular user yet
so i would go to manage and gains
i would say
manage and assign roles i would say
assign rules
okay so if you see what i'm going to do
now is assign a role that is specific to
that particular i will find the
particular user and assign him the
developer role that i have already
configured
the role shows up here i would need to
find my user whoever created and then
assign him to that particular role so if
you remember the user that i created was
developer1 i would add this particular
user and now this particular user what
kind of a role i want him to have
because this is the global rule that had
created so developer i would assign this
developer 1 to this particular global
rule and i would go ahead and save my
changes
now let me check the permissions of this
particular user by logging out of my
admin account and logging back as
developer one
if you remember this role was created
with very less privileges
so
there you go i have jenkins but i don't
see a new item i can't trigger a new job
i can't do anything i see these jobs
however i don't think so i'll be able to
start this job i don't have the
permission set for that the maximum i
can do is look at the job see what was
there as a part of the console output
and stuff like that so this is a limited
role that was created and i added this
developer to that particular role which
was a developer role
so that the developers don't get to
configure any of the jobs because the
jenkins instance is owned by a qr person
he doesn't want to give developer any
administrative rights so the rights that
he set out by creating a developer role
and anybody who is tagged any user who
is tagged as a part of this developer
role would get the same kind of
permissions and these permissions can be
you know fine grain it can be a project
specific permissions as well but for now
i just demonstrated the high level
permission that i had set in
let me quickly log out of this user and
get back
as the admin user because i need to
continue with my demo with the developer
role that was created i have very very
less privileges one of the reasons for
jenkins being so popular as i mentioned
earlier is the bunch of plugins that is
provided by users or community users who
don't charge any money for these plugins
but it's got plugins for connecting
anything and everything so if you can
navigate to or if you can find jenkins
plugins
you would see index of over so many
plugins that is there all of these are
wonderful plugins whatever connectors
that you would need if you want to
connect jenkins to an aws instance or
you want to connect jenkins to a docker
instance or any of those containers you
would have a plugin you can go and
search up if i want to connect jenkins
to bitbucket
bitbucket is one of the git servers
there's so many plugins that is
available okay so bottom line jenkins
without plugins is nothing so plugins is
the heart of jenkins for you to connect
or for in order to connect jenkins with
any of the containers or any of the
other tool sets you would need the
plugins if you want to connect or you
want to build a repository which has got
java and maven you would need to install
maven and jdk on your jenkins instance
if at all you're looking for a net build
or a microsoft build you would need to
have ms build installed on your on your
jenkins instance and the plugins that
will trigger ms build if at all you want
to listen to some server side web hooks
from github you will need github
specific plugins if you want to connect
jenkins to aws you need those plugins if
you want to connect to a docker instance
that is running anywhere in the world as
long as you have the url which is
publicly reachable you just have a
docker plug-in that is installed on your
jenkins instance sonarcube is one of the
popular static code analyzers so you can
connect a jenkins build you can build a
job on jenkins and push it to sonar cube
and get sonar cube to run analysis on
that and get back the results in jenkins
all of these works very well because of
the plugins now with that let me connect
our jenkins instance to github i already
have very very simple java repository up
on my github instance so let me connect
jenkins to this particular github
instance and pull out a job that is put
up there all right so this is my very
very simple uh you know repository there
is there called hello java and this is
what is there in the repo there is a
hello hello.java application that is
here or a simple class file that is
there it's got just one line of system
dot out so this is already present on
github.com at this place and this would
be the url for this uh repository
if i pick up the https url this is my
steepest url so what i would do is i
would connect my jenkins instance to go
to github provide my credentials and
pull out this repository which is on the
cloudhostedgithub.com
and get it to my jenkins instance and
then build this particular java file
i'm keeping the source code very very
simple it's just a java file how do i
build my java file how do i compile my
java file i just say java c and the name
of my
class file which is hello java and how
do i run my java file i would say java
and hello
okay so remember i don't need to install
any plugins now because uh what it needs
is a git plugin so if you remember when
we were doing the installation there was
a bunch of recommended plugins so git is
already installed on my system so i
don't need to install it again so let me
put up a new job here it says get job
let it be a freestyle project that's
good for me i would say okay
all right so the source code management
remember in the earlier examples we did
not use any source code because we were
just putting up some echo kind of uh
jobs we did not need any integration
with any of the source code systems so
now let me connect this so i'm going to
put up a source code and git would show
up because the plugin is already there
svn performs any of those additional
source code management tools if at all
you would need just install those
plugins and jenkins connects wonderfully
well to all these particular source
control tools okay so i would copy the
https url from here i would say this is
the url that i'm supposed to
go and grab my source code from but all
right that sounds good but what is the
username and password so i'll have to
specify a username
and password all right so i would say
the username this is my username and
this is my https credential for my job
okay so this is my username and this is
my password i just save this i say add
and then i would say you know use this
credentials to go to github and then on
my behalf pull out a repository all
right if at all at this stage if there's
any error in terms of not able to
jenkins not able to find git or the git
dot exe or if my credentials are wrong
somewhere down here you would see a red
message saying that you know something
is not right you can just go ahead and
kind of fix that for now this looks good
for me i'm going to grab this
url what am i going to do
this step would pull the source code
from the github
and then what would be there as a part
of my build step
because this repository just has a java
file correct hello.java so in order to
for me to build this i would just say
execute windows batch command and i
would say java c
hello dot
java that is the way i would build my
java code and if i have to run it i
would just say java
hello pretty simple two steps and this
would run after the repository contents
are fetched from github
so java c java that sounds good i would
say save this
and let me try to run this
okay if you see
there's a lot of you know it executes
git on your behalf it goes out here it
provides my credentials and says you
know it pulls all my repository and by
default it will pull up the master
branch that is there on my repository
and it kind of builds this whole thing
java c hello java and it runs this
project java hello and there you see
this is the output that is there and if
at all you want to look at the contents
of the repository if you can go here
this is my
workspace
of my system hang on this is not right
okay good job if you see here this is my
hello.java this is the same program that
was there on my github repository
okay so this is a program that was there
on
github repository all right so this was
the same program that was here and
jenkins on our behalf went over all the
way to github pulled this repository
from there and then you know it brought
it down to my local system on my jenkins
instance it compiled it and it ran this
particular application
okay now that i've integrated jenkins
successfully with github for a simple
java application let me build a little
bit on top of it what i will do is i
have a maven based web application that
is up there as a repository in my
github so this is the repository that
i'm talking about it's called amv and
web app it's got it's a maven based
repository as you would know maven is a
very very simple java based
build tool that will allow you to run
various targets and it will compile it
will based upon the goals that you
specify it can compile it can run some
tests and you can you can build a war
file and even deploy it into some other
server for now what we're going to use
maven is just for building and creating
a package out of this particular web
application it contains a bunch of
things and what is important is just the
index.jsp it just contains an html file
that is there as a part of this web
application so from a perspective of
requirements now since i'm going to
connect jenkins with this particular
repository get we already have that set
we only need two other things one is
maven because jenkins will use maven so
in order to use maven jenkins would have
to have a maven installation that is
there on the jenkins box and in this
case the jenkins box is this laptop
and after i have my maven installed i
also need a tomcat server tomcat is a
very very simple web server that you can
freely download i'll let you know how to
quickly uh download and install the
tomcat all right so download maven first
the various ways in which you can kind
of download this maven there is zip
files
binary zip files and archive files so
what i've done is i've just already
downloaded maven and if you see i've
unzipped it here so this is the folder
with which i have unzipped my maven so
as you know maven again is is one open
source build tool so you'll have to set
in a few configurations and set up the
path so mvn hyphen iphone version if i
specify this after i set in my path my
one should work and if at all i echo m2
home which is nothing but the variable
environment variable specific to my
onenote it is already set here
so once you unzip my one just set this
m2 home variable to the directory where
you unzip your mi1 also just set the
path to this particular directory slash
bin because that is where your maven
executables are all found
all right so that's with maven and you
know since i've set the path and the
environment variable maven is running
perfectly fine on my system i just
verified it okay next one is a tomcat
server
download apache tomcat
server 8.5 is what i have on my system
so i'm just going to show you where to
download this from
this is where you download tomcat server
and um
i already have the server downloaded
again this doesn't need any installation
i just unzip it here and it kind of has
a bin and configuration i have made some
subtle changes in the configuration
first and foremost tomcat server also by
default runs on port 8080 since we
already have our jenkins server running
on port 8080 we cannot let tomcat run on
the same port there will be a port clash
so what i've done i have configured
tomcat to use a different port so if i
go to this configuration file here there
is a server.xml
let me open this up here
all right okay so this is the port by
default it will be 8080 i've just
modified it to 8081. so i've changed the
port on which my tomcat server would run
all right so that's one chain second
change when jenkins kind of tries to get
into my tomcat and deploy something for
someone he would need some
authentications so that he'll be alloyed
deployment by tomcat so for that i need
to create a user on tomcat and provide
this user credentials to my jenkins
instance
so i would go to
tomcatusers.xml file here
i've already created
a username called deployer and the
password is deployer and i've added a
role called manager hyphen script
manager hyphen script will allow
programmatic access to the tomcat server
so this is the role that is there so
using this credentials i will enable or
i'll empower jenkins to get into my
tomcat server and deploy my application
all right only these two things that is
required
let me just start my tomcat server first
so i get into my bin folder i open a
command prompt here and there's a
startup.bat
it's pretty fast it just takes a few
seconds yes there you go tomcat server
is up and running now this is running on
port 8081 so let me just check if that
looks good
so localhost
8081.
okay my tomcat server is up and running
that sounds good the user is already
configured on this that's also fine so
what i'll do as a part of my first job
my one is also installed on my system so
i'm good to use maven as a part of my
jenkins so i will put up a simple job
now
i will say job
mbn
web app i'll call this
freestyle job that's good
okay so this will be a git repository
what is the url of my git repository is
this guy https url
okay
that's this url i will use the
credentials the old credentials that i
set up will work well because it's the
same git user that i'm kind of
connecting into
all right so now the change happens here
where after i get this since i said this
is a simple maven repository i will have
some maven targets to run
so the simple target first is
let me run maven package this creates a
war file
okay so mvn package is the target
package is the target so when whenever i
run this package it kind of creates it
it builds it it tests it and then
creates a package so this is all that is
required maybe let me try to save this
and let me first run this and see
if it connects well if there's any
problem with my war file or the war file
gets created properly
okay
wonderful
so it built a war file and if you see it
all shows you
what is the location where this war file
was generated
so this will be the workspace if you see
this this war file was successfully
built now i need to grab this particular
war file and then i would need to deploy
it into tomcat server
again i would need a small plugin to do
this because i need to connect tomcat
with my jenkins server
let me go ahead and
install the plugin for the container
deployment so i will go to manage
plugins
available
type in container
container container
deploy to container okay so let's put
this the plugin that i would need i will
install it without a restart
right seems to be very fast
nope sorry it's still installing
okay
it installed the plugin so if at all you
see this
if you go to my workspace
okay in the target folder
i would see this web application war
file that is already built
so i would need to configure this plugin
to pull up this war file and deploy it
onto the tomcat server for deploying
onto the tomcat server i will use the
credentials of the user that i've
created
okay so let me go to configure this
particular project again
and um
okay all this is good
so the package is good i'm gonna just
create a package that's all fine
now
add a post build step
so after the war file is built as a part
of this package directive let me use
this
deployment to container now this will
show up after you install the plugin so
deploy this one to the container
now what is that you're supposed to
specify you're supposed to specify the
what is the location
okay so this is a global uh you know
configuration that is there that will
allow you to
from the root folder it will pick up the
war file that is there so star star
forward slash star dot war that's good
for me
okay what is the context path context
path is nothing but just the name of an
application that you know under which it
will get deployed into the tomcat server
i will just say mvn web app that's the
name of my thing now i need to specify
what kind of a container that i'm
talking about
all right so the deployment would be for
this tomcat 8.5 is what i need
okay because the server that we have is
a tomcat 8.5 server that i have so this
would be the url so the credentials yes
i need to add a credential for this
particular
server so if you remember i had created
a credential
for my web application so let me just
find that
my tomcat server yes configuration of
this
okay so deployer and deployer username
is deployer password is deployer
okay so let me use that credential
i would say i would say add a new
credential jenkins credential the
username is deployer and the password is
deployer
so i would use this deployer credentials
for that and what is the url of my
tomcat instance
so this is the url of my tomcat instance
so take the war file that is find found
in this particular
folder
and then you know context path is map
use the deployer deployment credentials
and get into this local host which is
there
8081 this is the tomcat server that is
running on my system and then go ahead
and deploy it
okay so that is all that is required so
i would say just save this
and let me run it now
okay it builds successfully built the
war file it is trying to deploy it and
uh looks like the deployment went ahead
perfectly well
so
the context path was mvn web app so if i
type in this
all right if at all i go ahead into my
tomcat server there would be a web apps
folder
you would see the
you know the date timestamp so this is
the file that get got recently copied
and this is the explorer version of our
application
so the application was built the source
code of this application was pulled from
the github server it was built locally
on the jenkins instance and then it was
pushed into a tomcat server which is
running on a different port which is
808.1 now for this demo i'm running
everything locally on my system but
assuming that you know this particular
tomcat instance was running on
some other server with some other
different ip address all that you've got
to go and change is the url of the
server
so this would be the server in case you
you already have that you know if you
have a tomcat server which is running on
some other machine that's all fine with
a different ipa that's all good enough
the whole bundle or the war file that
was built as a part of this jenkins job
gets transferred onto the other server
and gets deployed that's the beauty of
jenkins and automatic deployments or
other deployments using jenkins and
maven
distributed build or master slave
configuration in jenkins as you would
have seen you know we just have one
instance of jenkins server up and
running all the time and also i told you
that whenever any job that kind of you
know gets started on the jenkins server
it is little heavy on in terms of disk
space and the cpu utilization so which
kind of you know if at all you're in an
organization wherein you're heavily
reliant on the jenkins server you don't
want your jenkins server to go down so
that's wherein you kind of start
distributing the load that is there on
the jenkins server so you primarily have
a server which is just a placeholder or
like a master will take in all the kind
of jobs and what he'll do is based upon
trigger that has happened to the job or
whichever job needs to be built he if at
all he can delegate these jobs onto some
other machines or some other slaves you
know that's a wonderful thing to have
okay use case one use case two assuming
that you know if you have a jenkins
server that is running on a windows box
or on a linux one and if at all you have
a need where you need to build based
upon operating systems you have multiple
build configurations to support maybe
you need to build a windows you know
windows based dot net kind of a projects
where you would need a windows machine
to build this particular project you
also have a requirement where you want
to build linux linux based systems you
also have a mac you you support some
sort of apps or something that is built
on mac os you would need to build you
know mac based system as well so how are
you going to support all these needs so
that's where in a beautiful concept of
master slave or you know primary and
delegations or agent and master comes
into play so typically you would have
one jenkins server who will just you
know configure with all the proper
authorizations users configurations and
everything is set up on this jinkin
server his job is just delegations he
will listen to some sort of triggers or
based upon the job that is coming in he
will if there's a way nice way of
delegating these jobs to somebody else
and you know taking back the results he
can control a lot of other systems and
these systems may not have a complete or
there's no need to put in a complete
jenkins installation all that you got to
do is have a very very simple runner or
a slave that's a simple jar file that is
run as a low priority thread or a
process within these systems so with
that you can have a wonderful
distributed build server that can be set
up and in case one of the servers goes
down your master would know that what
went down and kind of delegate the task
to somebody else so this is the kind of
distributed build or the master slave
configuration so what i'll do in this
exercise or in this demo is i will set
up a simple slave but since i don't have
too many machines to kind of play around
what i'll do is i will set up a slave in
in one other folder within my hard drive
so i've got the c drive and d drive my
jenkins is on my c drive so what i do is
i would just use my e drive and set up a
very very simple slave out there i'll
just show you how to provision a slave
and how to connect to a slave and how to
delegate a job to that slave let me go
back to my jenkins master and configure
him to you know talk to an agent
so there are various ways in which this
client and server talk to each other
what i'm going to choose is something
called as jnlp java network launch
protocol so using this i would ensure
that you know the client and server talk
to each other so for that i need to
ensure that i kind of enable this jnlp
port so let me try to find out where is
that let me try this
okay yes
agents and by default this jlp agent's
thing would be disabled so if you see
here there's a small help on this
so i'm going to use this jnlp which is
nothing but java network launch protocol
and you know i'll configure the master
and server to talk to each other using
jlp so for that i need to enable this
guy so i enable this guy instead of
making the by default the configuration
was disabled so i make him random i make
him you know enabled and i say save this
configuration
all right so now configured or made a
setting for the master
so that the jlp
port is kind of opened up so let me go
ahead and
you know create an agent
so i'll go to manage nodes so if you see
here there's only one master here so let
me provision a new node here so this is
the way you know in which you bring up a
new node you have to configure it on the
server jenkins would put in some sort of
security around this particular
agent and let you know how to launch
this particular engine so that he can
connect to our jenkins master so i would
say new node i would give a name for my
node i would say windows
node because both of these are windows
only so that's fine i just give an
identifier saying that windows note i
would say this is a permanent agent i'll
say okay
so if you see the name let me just copy
this name here with the description
number of executors since it's a slave
node and both of these are running on my
system i'll keep the number of executors
as one that's fine remote root directory
now this is where let me just clarify
this since i have both my
my master is running on my c drive c
drive program files 86
on not 86
c column program files
it is indeed 86
all right jenkins so this is where my
master is running so i don't want the c
drive what i'll do is i'll use something
called as a drive i have another drive
in my system but please visualize this
like you know you're running this on a
separate system altogether
so i create a folder here called jenkins
node and this is where i'm going to
place my or i'm going to provision my
slave
and i'm going to run him from here so
this is the directory in which i'm going
to provision my slave node so i'm going
to copy this here and that is the remote
root directory of your particular agent
or slave so i just copied here the label
you know probably this is fine for me
and usage
how do you want to use this guy
so i would don't want him to run all
kinds of jobs i will only build jobs
with label expressions that match this
particular node and so this is the label
of this node so in order for somebody to
kind of delegate any task to them
they'll have to specify this particular
label
so imagine this way if i have a bunch of
windows system i name it as windows star
anything that says from windows i can
give a regular expression and say that
anything that matches windows run this
particular task there if i have some mac
machines i name all these mac agents as
a mac star or something like that and i
can delegate all tasks you know saying
that start with whatever starts with mac
and this node run the mac jobs there
so you identify a node using the label
and then delegate the task there
all right so launch method you know we
will use java web start because we're
going to
we we're going to use jnlp protocol okay
that sounds good directory i think
nothing else is required availability
yes we'll keep this agent yep online as
much as possible that sounds good all
right let me save this
all right i'm just provisioning this
particular node now
so if i click on this node i get a bunch
of commands
along with an agent.jar
so this is the agent.jar that has to be
taken down to the other machine or the
slave node and from there i need to run
this along with a small security
credential so let me copy this whole
text here in my notepad
not bad plus plus is good for me
okay i copy this whole
path there i also want to download this
agent.jar
i would say yes
this agent.jar is the one that is
configured by our server so all the
details that is required for launching
this agent.jar is found in this
sorry for launching this agent is found
is agent.jar so typically i need to
take this jar file onto the other system
and then
kind of run it from there so
i have this alien.jar i copy this or i
cut this i come back to my folder my
jenkins node i paste it here
okay so now with this provisionagen.jar
and i need to use this whole command
control a control c and then launch this
particular agent so let me
bring up a command prompt right here
and then launch that so i'm saying in
the same folder where there is agent.jar
i'm going to launch this a particular
agent java hyphen jar agent.jar jnlp
this is the url of my server in case the
server and client are on different
locations or different ips let us
specify the ipad address all this anyway
would show up
and then the secret and you know the
root folder of your jenkins or the slave
node
okay
so
something ran and then you know it says
it's connected very well it seems to
connected very well so let me come back
to my
jenkins instance
and see you know if you told me you see
earlier this was not connected now let
me refresh this guy
okay now these two guys are connected
provision are jenkins node and then i
copied all the credentials or the
slave.jar along with the launch code and
then took it to the other system and
kind of
ran it from there since i don't have
another system i've just got a separate
directory in another folder another
drive and i'm launching the agent from
here as long as this particular agent is
up and running or this command prompt is
up and running the agent would be
connected so once i close this the
connection goes down
all right so successfully you've
launched this particular agent now this
would be the home directory of this
jenkins node or the jenkins slave so any
task that i'm going to delegate to this
particular slave would all be run here
it'll create a workspace right here all
right so good
so let me just come back and let me kind
of put up a new task here i will say
that you know delegate job is good i say
freestyle project i'm going to create a
very very simple job here
i don't want it to connect to gate or
anything like that let me just create a
very very simple
echo
relegated to the
slave negative two i don't like the word
slave delegated to
agent
put this way
all right so delegated to agent sounds
good now how am i going to ensure that
this particular job
runs on the agent or on the slave that i
have configured
right you see this if at all you
remember how we provisioned our
particular slave we give a label
right so now
i'm going to put in a job that will only
match this particular label
so i'm going to say that whatever
matches this you know windows label run
this job on that particular node so we
have only one node that's matching this
in a windows node so this job will be
delegated out there so i save this
and let me build this
this is again a very very simple job
there's nothing in this i just want to
demonstrate how to kind of delegate it
to an agent so if you see this
it ran successfully
and where is the workspace
the workspace is right inside our
jenkins node it created a new workspace
delegated job it put in here so my old
or my primary master job is in sql
program files under jenkins and this is
the slave job that was successfully run
very very simple but very very powerful
concept of master slave configuration or
distributed build in jenkins
okay approaching the final section where
we've done all this hard work in
bringing up our jenkins server
configuring it putting up some jobs on
it creating users and all this stuff now
we don't want this configuration to kind
of go away we want a very nice way of
ensuring that we backup all this
configuration and in case there is any
failure hardware crash or a machine
crash we would want to kind of restore
from the existing configuration that we
kind of backed up
so one quick way to do that would be or
one dirty way to do that would be just
you know take a complete backup of our
colon program files colon jenkins
directory because that's where our whole
jenkins configuration is present but we
don't want to do that let's use some
plugins for taking up a backup so let me
go to manage jenkins
and click on available
and let me search for some back there
are a bunch of backup plugins so i would
recommend one of these plugins that i
specifically use
so this is the backup plugin so let me
go ahead and install this plugin
all right so we went ahead and installed
this plugin
so let me come back to my
manage plugins
stop this plugin is there
so
hang on backup manager so you will see
this option once you uninstall this
plugin so first time i can you know do a
setup i would say back up this
particular i'll give a folder uh this
folder is pertaining to the folder where
i want jkins to backup some data
and i would say the format should be zip
format is good enough let me give a name
or a template or a file name for my
you know backup this is good i want it
in verbose mode i don't want to shut on
my jenkins or should i shut it down no
okay one thing that you got to remember
is that whenever a backup happens if
there are too many jobs that is running
on the server
it can kind of slow down your jenkins
instance because it's it's in the
process of copying a few of those things
and the files are being changed at that
moment it's little bit problematic for
jenkins so typically you backup your
servers only when there is very less
load or typically try to you know bring
it to a shutdown kind of a state and
then take a backup
all right so i'm gonna backup all these
things you know i don't want to exclude
anything else i want the history i want
the maven artifacts
possibly i don't want this guy i would
just say save
and then i would say
back him up
so this would run a bunch of you know
steps and all the files that is required
as a part of this is pretty fast but
then if at all you have too many things
up on your server for now we didn't have
too many things up on our server but in
case you had too many things to kind of
backup this may take a while so let me
just pause this recording and get back
to you once the backup is complete so
there you go the backup was successful
created a backup of all the workspace
the configurations the users and you
know all that so all this is kind of
hidden down in this particular
zip file so at any instance if at all i
kind of crash my system for some
instance or say hard disk failure and i
bring up a new instance of jenkins i can
kind of use the backup plugin for
restoring this particular
configurations so how do i do that i
just come back to
my manager cans
come back to
backup manager and i will say restore
her that's an or jenkins configuration
so devops today is being implemented by
you know most of the major organizations
whether it's a financial organization
whether it's a kind of a service
organization every organization is
somehow looking forward for the
implementation and the adaptation of
devops because it totally redefines and
automate the whole development process
all together and whatever the manual
efforts you were putting earlier that is
simply or gets automated with the help
of these tools here so this is something
which get really implemented because of
some of the important uh feature like a
ci cd pipeline because cicd pipeline is
responsible for delivering your source
core into the production environment in
large duration of time so cicd pipeline
is ultimately the goal which really
helps us to deliver more into the
production environment when we talk
about from this perspective
now let's talk about that what exactly
is a cac pipeline now when we go into
that part when we go into that
understanding so cicd pipeline is a
basically continuous integration and
continuous delivery concept which is
used or which is considered as a
backbone of the overall devops approach
now it's one of the prime approach which
we implement when we are going for a
devops implementation for our project so
if i have to go for a devops
implementation the very first and the
minimum implementation and the
automation which i'm looking forward is
actually from the uh particular ca city
pipelines here so cacd pipelines is
really a wonderful option when we talk
about the devops here
so what exactly is the pipeline term
all about so pipeline is in series of
events that are connected together with
each other it's kind of a sequence of
the various steps like you know
typically when we talk about any kind of
deployment so we have like you know
build process like we compile the source
code which under the artifacts we do the
testing and then we deploy to a specific
environment all these various steps
which we use to do it like manually that
is something which we can do it into a
pipeline so pipeline is nothing but a
sequence of all these steps
interconnected with each other executed
one by one into a particular sequence
now
the pipelines is responsible for
performing a variety of tasks
like building up the source code running
the test cases
probably the deployment can also be
added up in when we go for the
continuous integration and continuous
delivery there so all these steps are
being done into a sequence definitely
because sequence is very important when
we talk about the pipeline so you need
to talk about the sequence the same way
in which you are working on the
development and in a typical world the
same thing you will be putting up into a
specific pipeline so that's a very
important aspect to be considered now
let's talk about what is the continuous
integration here now continuous
integration is also you know known as
the ci uh pretty much you can see that a
lot of uh tools are actually named as ci
but they are referring to the continuous
integration only so continuous
integration is a practice that
integrates the source code into a shared
repository and it used to
automate the verification of the source
code so it involves the build
automations test cases automation so it
also helps us to detect the issues and
the bugs quite easily and quite faster
that's a very early mechanism which we
can do as such if we want to resolve all
these problems
now continuous integrations does not
eliminate the bugs but yes it definitely
helps them uh you know easily to find
out because we we are talking about the
uh automated process we are talking
about the automatic test cases so
definitely that is something which can
help us to
find out the bugs and then you know the
development can help on that and they
can you know proceed with those bugs and
they can try to resolve those things one
by one so it's not a kind of automated
process which will eventually remove the
bugs bugs is something which you have to
recode and you have to fix it by
following the development practice but
yes it can really help us to find those
bugs quite easy and help them to remove
now what is the continuous delivery here
so continuous delivery also known as cd
is in kind of a phase in which the
changes are made uh into the code before
the deployment now in this case what
happens that uh it's um something which
we are discussing or we are validating
that what exactly we want to deliver it
to the customer so what exactly we are
going ahead or we are moving to the
customers so that's what we typically do
in case of continuous delivery and the
ultimate goal of the pipeline is to make
the deployments that's the end result
because
coding is not the only thing you code
the programs you do the development
after that it's all about the
deployments like how you're going to
that to perform the deployment so that
is a very important aspect you want to
go ahead with the deployments that's
right you can go there and that's a real
beauty about this because it it's in
kind of a way in which we can identify
that the how the deployments can be done
or can be executed as such here
right so the ultimate goal for the
pipeline is nothing but to do the
deployments and to proceed further on
that
right so when both these practices are
placed in together in an order so all
these steps could be referred as in
complete automated process and this
process is known as cicd so when we are
talking about like when you are working
on this automation so in that case what
happens that we are looking forward that
how the automation needs to be done and
since it's in kind of a ci cd automation
which we are talking about so it's
nothing but the uh end result would be
like build and deployment automation so
you will be taking care of both the
build and the test case executions and
the deployments as such when we talk
about as such the cicd here
the implementation of chcd also enables
the team to do the build and deploys
quite quickly and efficiently because
these are things which is you know
happening automatically so there is no
manual efforts involved and there is no
scope of human error also so we have
frequently seen that while doing the
deployments we may miss some binaries or
some miss can be there so that is
something which is you know completely
removed as such when we talk about this
the process makes the teams more agile
productive and the uh confident here
because um the automations definitely
gives a kind of a boost to the
confidence that yes things are going to
work perfectly fine and there is no
issues as such present
now why exactly jenkins like jenkins is
what we typically understand are we you
know uh here and there that's in ci2
it's a cd tool so what exactly is
jenkins all about so jenkins is also
known as a kind of orchestration tool
it's an automated tool which is there
and the best part is that it's
completely open source yes there are
some particular or paid or the
enterprise tools are there like cloud
b's and all but there is no as such of
offering difference between the
cloudbees and the jenkins here so
jenkins is an kind of open source tool
which a lot of organizations pretty much
implement as it is itself so even if
they don't want to go um we have seen in
a lot of big organizations where you
know they are not going for the
enterprise to like cloud bs and all and
they are going for the pretty much you
know core jenkins software as such here
so this tool uh makes it easy for the
developers to integrate the changes to
the project that is something which is
very important because it can really
help the teams to say that how the
things can be done and how it can be
performed over there so the tools is
really easy for the developers to
integrate and that's the biggest uh you
know benefit which we are getting when
we talk about these tools as such so
jenkins is a very important tool to be
considered when we talk about all these
automations now jenkins achieves
continuous integration with the help of
plugins that is also a kind of another
feature or benefit which we get because
there are so many plugins which is
available there as such which is being
used and uh for example you want to have
an integration with kubernetes docker
and all maybe by default those plugins
are not installed but yes you have the
provisioning that you can go for the
installation of those plugins and yes
those features will start embedded up
and integrated within your jenkins so
this is the reason this is the main
benefit which we get when we talk about
the jenkins implementation
so jenkins uh is you know one of the
best fit which is there for building a
ci cd pipeline because of its
flexibility uh open source nature
plug-in capabilities the support for
plugins and it's quite easy to use and
it's very simple straightforward gui
which is there which can definitely
helps us you can you know easily
understand and go through the jenkins
and you can grab the understanding and
as an end result you will be able to
have a very robust tool which using
which pretty much any kind of source
code or any kind of programming language
you can implement ci city whether it's
an android it's a not net it's a java
it's a node.js all the languages are
having the support for the jenkins
so let's talk about this acd pipeline
with the jenkins here now to automate
the entire development process a ci cd
pipeline is the ultimate you know
solution which we are looking forward to
build such a pipeline jenkins is our
best solution and best fit which is
available here
so there are pretty much six steps which
is involved when we look forward for any
kind of pipeline it's a generic pipeline
which we are looking forward now it may
have like
any other steps which is available there
probably some additional steps you're
doing like some other plugins you are
installing but these are the basic steps
which is there like a minimum pipeline
if you want to design these are the
steps which is available there now let's
see the first one is that we have the uh
required java jdk like a jdk to be
available on the system now most of the
operating systems are already available
with the gre like a java gre but the
problem with gre is that it's only for
the build process uh it will not be
doing the compilation you can run the
artifacts you can run the jar files you
can you know run the application run the
code base but the compilation requires
the java c or the java jdk kit to be
installed onto the system and that's the
reason why for this one we also require
the jdk and certain linux commands
execution understanding we need to have
because we are going to run some kind of
steps some installation steps and you
know process so that's pretty much
required now let's talk about how do ca
cd pipeline with jenkins now first of
all you have to download the jdk
and uh that is something which is
installed so after that you can go for
the jenkins download now jenkins dot io
slash download is the website is our
official websites of jenkins now the
best part is that there you have the
support for different operating systems
and platforms from there you can easily
say that if you want to go for a java uh
package like a bar file tucker ubuntu
deviant centos fedora red hat windows
open sushi uh freebsd ganto mac
operating system in fact whatever the
different kind of artifacts or different
environment or different uh
application you want to download you
will be able to do that so that's a very
first thing to start up on you download
the generic java package like a war file
then you have to execute it you have to
download that into a specific folder
structure let's say say that you have
you know created a folder called jenkins
now you have to go into that jenkins
folder with the help of cd command and
there you have to run the command called
java hyphen jar and the jenkins dot bar
there so uh these are the executables uh
artifacts
so uh war files can be easily executable
um jar files raw files can be easily
deployed so
just because with the java command you
can run them you don't require any kind
of web container or application
container as such so here also you can
see that we are running the java command
and it runs the applications as such and
once that is done so you can open the
web browser and uh you can open like
localhost colony t so jenkins uses atd
port just like a tomcat apache so
if you know once the deployment is done
installation is done so you can just
open the localhost colony
now if you want to get uh the jenkins up
and running in the browser probably you
can you know go through the
public ipad address also there so you
can put the public ipad is calling edt
and that can also help you to you know
start accessing the jenkins application
now in there you will be having an
option called create new jobs so you
need to click on that now once the
particular new job new item new job
that's a
different naming conventions which is
available there now all you're going to
do is that you're going to do like you
are proceeding with the creating the
pipeline job so you will be having an
option called pipeline job over there
just select that and provide your custom
name what pipeline name or job name you
want to uh refer or you want to process
there now once that is available so what
happens that it will be in easy task for
us to see that how exactly we can go
ahead and we can perform on that part so
this can really help us to see that how
a pipeline job can be created and you
know performed on this modifications as
such
now when the pipeline is selected and we
can give a particular name that this is
the name which is available and then we
can say okay as such over there now you
can scroll down and find the pipeline
section so uh there what happens that
when you go over there and say that okay
this is the way that how the pipelines
are managed and you know those kind of
things so you will scroll down and find
the pipeline section and go with that
pipeline script now when you select that
option there are different options which
is available like how you want to manage
these pipelines now you are you know
have the direct access also like if you
want to directly uh create the uh create
a pipeline skip you can do that if you
feel that you want to manage like you
want to retrieve the jenkins file so so
scored management tool also can be used
there so you can work on that also so
like this there are so many a variety of
things which is available like which you
can use to work around that how exactly
the pipeline job can be created so
either you can fetch it from the source
code management tool uh like gate
version or something like that or you
can directly put the pipeline code as
such over there right now so next thing
is that we can configure and execute a
pipeline job with the direct script so
uh we can once the pipeline is selected
so you can put the
particular script like jenkins file into
your uh particular github link so you
you may be having like already a github
link so that the where the jenkins file
is there so you can make use of that now
once you process the github link so what
we can do is that we can proceed with
that
and uh once the processing is done so
you can do the same and you know you can
keep the changes and you know uh it will
be picking up the pipelines you know the
pipeline script is added up into the
github and you know you have already
specified that uh let's just go ahead
with this jenkins file pipeline script
from the github repository and proceed
further now once that is done so what
next you can do is that you can go with
the build now process you click on the
bill now and once that is done so what
will happen that you will be able to see
that how the build process will be done
and how the build will be performed over
there so these are pretty much a kind of
a way so you can click on the console
output you will get all the logs that is
happening in the inside that whatever
the pipeline steps are getting executed
all of them you will be able to get or
you will be able to you know get on that
part there so these are the different
steps which is involved as such and the
sixth one is that you know uh yes
whatever the
particular
when you run the build now you will be
able to see that source code will be uh
you know will be checked out and will be
downloaded before the build and you can
proceed with that part now later on if
you want to change the url of this
github you can configure the job again
the existing job and you can change that
url github link url whenever you require
you can also clone this uh job whenever
you go ahead and you work on that and
that's also kind of you know the best
part which is available as such right
and uh
then you can have the advanced settings
over there so in there you can put like
uh your github repository you can say
like okay uh the github repository is
there so i'm just going to put this url
and you know with that what will happen
that the settings will be available
there and the jenkins file will be
downloaded as such and when you run the
build now you will be able to have a lot
of steps like a lot of configurations
going on so uh then the checkout sem so
we can have a declaration like check out
sm which is there so when the checkout
sm is there so it will check out a
specific source code after that you go
to the log and you will be able to see
that each and every stage which is being
built up and executed as such
okay so now we are going to talk about a
demo here so on the pipeline here so
this is the jenkins portal now you can
see here that there is an option called
create a job you can either click on the
new item or you can click on the new
uh create a job here now here i'm going
to say like a pipeline
and uh then you know you can select the
pipeline uh job type here now you have
the freestyle pipeline github
organization multiple branch pipeline
these are the different options which is
available there but i'm going to
continue with the pipeline here as such
so when i selected the pipeline and say
okay so what will happen that i will be
able to see a configuration page which
is related to the pipeline
now here the very important part is that
you have all the uh general build
trigger
you know options which is similar to the
freestyle but the build step and the
post build step is completely removed
because of the pipeline introduction now
here you either have the option to put
the pipeline script altogether you can
also have some particular example for
example let's talk about some github
maven uh particular uh tool here so you
can see that
we have you know got some steps as such
over here and you know it's pretty much
running over there now you run it it
will work smoothly it will check out
some source code but how we are going to
integrate like the version the jenkins
file into the version control system
because that's the ideal approach we
should be following when we create a
pipeline of a cicd now i'm going to
select a particular pipeline from sim
here then go with the git here now in
there the jenkins file is the name of
the file of the pipeline script and i'm
going to put my
repository over here in this one now
this repository is of my gate which is
like having um even build pipeline which
is available there it's having some
steps related to ci with for the build
and deployments and that's what we can
follow as such over here now in this one
the uh if it is a private repository
definitely you can add on your
credentials but this is a public
repository a personal repository so i
don't have to put any kind of
credentials but you can always add the
credentials with the help of ad here and
that can help you to you know set up
whatever the credentials the private
repositories you want to configure now
once you save the configuration here
now what it's going to do is that your
it's going to give you a particular page
related to build now uh if you want to
run if you want to delete the pipeline
if you want to reconfigure the pipeline
all these different options are
available there so we are going to click
on the build now here and when i do that
immediately the pipeline will be
downloaded and will be processed now you
may not be able to get the complete
stage view as of now because it's still
running so yeah you can see that the
checkout code is done then it's going on
to the build okay that's one of the
steps which is there now once the build
will be done so it will continue with
the next steps with the next further
steps there so you can also go to the
console output log here like you can
click on this or you can click on the
console output to check the complete log
which is happening there or in fact you
can also see the stage wise logs also
because that is also very important when
you go for the complete logs uh it may
you know have a lot of steps involved
and you know a lot of logs will be
available there but if you want to see a
specific log of a specific stage that's
where this comes into the picture and as
you can see that all the different uh
steps like test cases executions the
scenar cube analysis the archive
artifacts deployment and in fact the
notifications so all this is a part of a
complete pipeline this whole pipeline is
done here and you know you get a kind of
a stage view it's success over here and
the artifacts is also available to
download so you can download this war
file is a web applications as such over
here so this is what our typical
pipeline looks like that how the
automation the complete automations
really looks like as such over here now
this is a very important aspect because
it really helps us to understand that
how the pipelines can be configured can
be done and pretty much with the same
steps you will be able to automate any
kind of pipelines as such so that was
the demo to build a simple pipeline as
such with the jenkins and uh pretty much
in this one we understood that how
exactly the ci cd pipelines can be
configured and we can use them and we
can get hold on that part so there are
approximately seven sections that we
cover in devops we go from general
devops questions source code management
with tools such as git continuous
integration and here we'll focus on
jenkins continuous testing with tools
such as selenium and then you also have
the operation side of devops which is
your configuration management with tools
such as chef puppet enhanceable
containerization with docker and then
continuous marching with tools such as
nagios so let's just get into those
general devops questions so one of the
questions that you're going to be asked
is how is devops different from agile
and the reality is is that devops is a
cultural way of being able to deliver
solutions that's different from agile if
we look at the evolution of delivery
over the last five to ten years we've
gone from waterfall based delivery to
agile delivery which is on
sprints to where we are with continuous
integration and continuous delivery
around devops the whole concept of
devops is culturally very very different
from agile and the difference is is that
you're looking at being able to do
continuous releases what does that mean
the difference is is that you want to be
able to send out code continuously to
your production environment that means
the operations team the development team
have to be working together that means
that any code that gets created has to
be able to go to production very quickly
which means you need to be testing your
code continuously and then that
production environment must also be able
to be tested continuously and any
changes or any errors that come up have
to be communicated effectively and
efficiently back to the dev and op team
another area in which i see that devops
is different is really the response that
we have for how we engage with the
customer so the customer is coming to
your website to your mobile app to your
chat bot or any digital solution that
you have and has an expectation when
you're going through and actually doing
a dev ops paradigm the old model would
be that you would capture requirements
from the customer then you do your
development then you do your testing and
there would be these barriers between
each of those as we move faster through
from waterfall to agile what we saw is
that with agile we were able to respond
much faster to customer demands so
instead of it being weeks or months
sometimes in some cases years between
releases of software what we saw it
would was a transition to weeks and
months for releases on software now we
see with devops is that the release
cycle has shrunk even further with the
goal of continuously delighting the
customer how further has that release
cycle shrunk to there are companies that
have gone from having releases of once a
week or once every two weeks or once a
month to now having multiple releases a
day indeed some companies have up to 50
releases a day this isn't something to
also bear in mind is that each of those
releases are tested and verified against
test records so that you can guarantee
that the code that's going to production
is going to be good continuous code so
what are the differences between the
different phases of devops so
effectively there are two main phases of
devops there's the planning and coding
phase and then there's the deploying
phase and you have a tool such as
jenkins that allows you to integrate
between both environments some of the
core benefits that you may have to
devops are going to be some technical
benefits and some business benefits so
when somebody asks you what are the
benefits of devops you can reply that
from a technical point of view you're
able to use continuous software delivery
to constantly push out code that has
been tested and verified against scripts
that have been written and approved you
can be able to push out smaller chunks
of code so that when you have an issue
you're not having to go through massive
blocks of code or massive projects
you're going through just very small
micro services or small sections of code
and you're able to detect and correct
problems faster on the business side the
benefits are absolutely fantastic from a
customer that's coming to your website
and or to your mobile app they're going
to see responses happening continuously
so that the customer is always aware
that you as a company are listening to
their demands and responding
appropriately you're able to provide a
more stable environment and you're able
to scale that environment to the demands
of the number of customers that are
using your services so how you approach
a project that needs to implement devops
so this is really an exciting area for
you to be in so there are effectively
three stages when it comes to actually
working in a dev ops the first stage is
an assessment stage and think of this as
the back of the napkin ideation stage
this is where you are sitting with a
business leader and they're giving you
ideas of what they would like to see
from feedback that they've had from
their customers this is blue sky
opportunity this is thinking of big
ideas that second stage and this often
comes as a fast follow to stage one is
actually proving out that concept so
developing a proof of concept and a
proof of concept can actually be a
multiple different things so it could be
something as simple as a wireframe or it
could be something that is as complex as
a mini version of the file application
depending on the scope of the work that
you're delivering will really depend on
how complicated you want the poc to be
but with that in mind whatever choice
you make you have to be able to deliver
enough in the poc so that when you
present this to a customer they're able
to respond to that creation that you've
developed and able to give you feedback
to be able to validate that you are
going with the right solution and able
to provide the right product to your
customers that third stage is where you
get into your dev ops stage and this is
just the exciting part this is where the
rubber hits the road and you start
releasing code based on a backlog of
features that are being requested for
the solution in contrast to doing agile
delivery where you just continuously
work through a backlog with devops what
you're also looking at is putting in
analytics and sensors to be able to
validate that you are being successful
with the solution that being delivered
so that once you actually start
delivering out code that customers can
interact with you want to be able to see
what are the pieces of the solution that
they are using what do they like what is
complicated where are the failure points
and you want to use that data and feed
that back into your continuous
integration and have that as a means to
be able to back fill the demand or work
that gets completed in the bank log so
what is the difference between
continuous delivery and continuous
deployment so continuous delivery is
based on putting out code that can be
deployed safely to production it ensures
that your businesses and functions are
running as you would expect them to be
so it's going through and completing the
code that you'd actually see continuous
deployment in contrast is all about
ensuring that you're automating the
deployment of a production environment
so you're able to go through and scale
up your environment to meet the demands
of both the solution and the customer
this makes software development and
release processes much more faster and
more robust so if we look here we can
actually see where continuous
integration and continuous deployment
come hand in hand so when you actually
start out with the initial pushes of
your code that's where you're doing your
continuous integration and your
continuous delivery and then at some
point you want to get very comfortable
with deploying the code that you're
creating so it's being pushed out to
your production environment one of the
things that's great about working with
the tools that you use in a devops
continuous integration and continuous
delivery model is that the development
tools that you use uh the
containerization tools the testing tools
should always reflect the production
environment what this means is that when
you actually come to deploying solutions
to production there are no surprises
because your development team have been
working against that exact same
environment all the way through so a
question that you'll also be asked is
you know what is the role of the
configuration management in devops and
so the role of configuration management
really has three distinct areas and the
first and this is really obvious one and
this is the one where you probably
already have significant experiences is
the ability to manage and handle large
changes to multiple systems in seconds
rather than days hours or weeks as that
may have happened before the second area
is that you want to also demonstrate the
business reasons for having
configuration management and the
business reason here is that it allows
it and infrastructure to standardize on
resource configurations and this has a
benefit in that you're able to do more
with fewer people so instead of having a
large configuration team you can
actually have a smaller more highly
skilled team that's able to actually
manage an even larger operational
environment and thirdly you want to be
able to highlight the ability to scale
so if you have configuration management
tools you're able to manage a
significant number of servers and
domains that may have multiple servers
in it allows you to effectively manage
servers that are deployed on cloud or
private cloud and allow you to do this
with high accuracy so how does
continuous monitoring help and maintain
the entire architecture of the system so
when this question comes up you want to
dig in and show your knowledge on how
configuration and continuous monitoring
is able to control an entire environment
so the number one topic that you want to
bring up when it comes to continuous
monitoring is that with being able to
effectively monitor your entire network
24 7 for any changes as they happen
you're able to identify and report those
thoughts or threats immediately and
respond immediately for your entire
network instead of having to wait as it
happens sometimes for a customer to
email or call you and say hey your
website's down nobody wants that that's
an embarrassing thing the other three
areas that you want to be able to
highlight are the ability to be able to
ensure that the right software and the
right services are running on the right
resources that's your number one
takeaway that you want to be able to
give of continuous monitoring the second
is to be able to monitor the status of
those servers continuously this is not
requiring manually monitoring but having
a agent that's monitoring those servers
continuously and then the third is that
by scripting out and continuously
monitoring your entire environment
you're creating a self-audit trail that
you can take back and demonstrate the
effectiveness of the operations
environment that you are providing so
one of the cloud companies that is a
strong advocate for devops is amazon's
web services aws and they have a really
five distinct areas them that you can
zero in on board services so when the
question comes up what is the role of
aws in devops you want to really hold
out your hand and list of five areas of
focus using your thumb and finger so you
want to have flexible services built for
scale automation secure and a large
partner ecosystem and having those five
areas will really be able to help
demonstrate why you believe that aws and
other cloud providers but aws is
certainly the leader in this space are
great for being able to provide support
for the role of devops so one of the
things that we want to be able to do
effectively when we're releasing any
kind of solution is to be able to
measure that solution and so kpis are
very important so you will be asked for
three important dev of kpis and so three
that really come to mind that are very
effective the first one is mean time to
failure recovery and what this talks
about is what is the average time does
it take to recover from a failure and if
you have experience doing this then look
at the experience you have and use a
specific example where you are able to
demonstrate that mean time to failure
recovery the second is deployment
frequency and with deployment frequency
you want to be able to discuss how often
do you actually deploy solutions and
what actually happens when you're
actually doing those deployments and
what does the impacts to your network
look like when you're doing those
deployments and then the third one is
really tied to that deployment frequency
which is around what is the percentage
of failed deployments and so and how
many times did you deploy to a server
and something happened where the server
itself failed what we're looking for
when you're going through and being
asked for these kpis is experience with
actually doing a devops deployment and
being able to understand what devops
looks like when you're pushing out your
infrastructure and then the second is
being able to validate that
self-auditing ability and one word of
caution is don't go in there and say
that you have a hundred percent success
uh the reality is that servers do
degrade over time and you maybe want to
talk about a time when a server did
degrade in your environment and use that
as a story for how you're able to
successfully get over and solve that
degradation so one of the terms that is
very popular at the moment is
infrastructure as code and so you're
going to be asked to explain
infrastructure as code and really it's
it's something that actually becomes a
byproduct of the work you have to do
when you're actually putting together
your devops environment and
infrastructure's code really refers to
the writing of code to actually manage
your environment and you can go through
many of the other tools that we've
covered in this series but you'll see
that xml or ruby or yaml are used as
languages to describe the configuration
for your environment this allows you to
then create the rules and instructions
that can be read by the machines that
are actually setting up the physical
hardware versus a traditional model
which is having software and installing
that software directly onto the machine
this is really important when it comes
to cloud computing there really is a
strong emphasis of being able to explain
infrastructure as a service and
infrastructure as code is fundamental to
the foundation to infrastructure as
service and then finally allows you to
be able to talk about how you can use
scripted languages such as yammer to be
able to create a consistent experience
for your entire network all right so
let's now get into the next section
which is source code management and
we're going to focus specifically on git
the reason being is that get is really
the most popular source code management
solution right now there are other
technologies out there but for the types
of distributed environments that we have
uh source code management with git is uh
probably the most effective so the first
question you'll be asked when it comes
to git is to talk about the difference
between centralized and distributed
version control and if we look at the
way that the two are set up older
technologies such as older versions of
team foundation server though the
current version does actually have git
in it but older versions required a
centralized server for you to check in
and check out of code the developer in
the centralized system does not have all
the files for the application and if the
centralized server crashes then you
actually lose all of the history of your
code now in contrast a distributed model
actually we do check in our code to a
server however for you to be effective
and building out your solution you
actually check out all of the code for
the solution directly onto your local
development machine so you can actually
have a copy of the entire solution
running on your local machine this
allows you to be able to work
effectively offline it really allows for
scalability when it comes to building
out your team so if you have a team that
may be in europe you can actually then
scale that team with people from asia
from north america or south america very
easily and not have to worry about
whether or not they have the right code
or the wrong code and in addition to
that if the actual main server where
you're checking in your code does crash
it's not a big deal because you actually
have each person has a copy of the code
so as soon as the server comes back up
you have to check back in and
everybody's running back as if there was
nothing that happened at all so one of
the questions you'll be asked is to give
the answer to some of the commands you
use for working with git so if you were
to be asked the question is what is the
git command that downloads any
repository from github to your computer
on the screen we have four options we
have git push git fork get clone and get
commit the answer in this instance would
be git clone now if you want to be able
to push code from your local system to a
github repository using get then first
of all you want to be able to do is
connect the local repository to a remote
repository and in the example you may
want to talk about using the command
gate remote add origin and then the
actual path to a github repository you
could if you wanted to actually at this
point also talk about other repositories
such as get lab that you can also work
with or a private git repository that
would be used just for the development
team once you've actually then added the
local repository into your local
computer then the second action you want
to use is a push which is to actually
push your local files out to the master
environment so you use the command git
push origin master so one question you
may be asked is what is the difference
between a bare repository and a standard
way of initializing a get repository so
let's look through what is the standard
way so the standard way using git init
allows you to create a working directory
using the command git in it and then the
folder that creates is the folder that
creates all the revision history related
to the work that you're doing in
contrast using the bear way you have a
different commands for setting that up
so it would be git init dash dash bear
and it does not contain any working or
checked out source files locally on your
machine in addition the revision history
is actually stored in the root folder
versus a subfolder that you'd have with
the normal git init initialization so
which of the following cli commands
would you be used to rename a file so we
have get rm git mv git rm-r or none the
above well in this instance it would be
get mv a question that you'll be asked
around commit is going to be what is the
process to revert a commit that has
already been pushed and made public and
there are two ways you can address this
the first is to actually address the bad
file in the new commit and you can use
the command git commit dash m and then
put in a comment for why that file is
being removed the second is to actually
create a new commit that actually undoes
all the changes that were made with the
bad commit and then to do that you would
use git revert and then the commit id
and the commit id it could be something
such as 560e 0938f
but you'd have to find that from the the
commit that you had made but that would
allow you to revert any bad files that
you had submitted so there are two ways
of being able to get files from a get
repository and you're going to be asked
to explain the difference between git
fetch and get paul so get fetch allows
you to fetch and download only new data
from a root new repository it does not
integrate any of the new data into your
working files and it can be undone at
any time if you wanted to
break out the remote tracking branches
in contrast git pull updates the current
head branch with the latest changes from
the remote server so you get all of the
files and downloaded it downloads new
data and integrates it with the current
working files you have on your system
and it tries to merge remote changes
with your local ones so one of the
questions you'll get asked about git is
what is a git stash so as a developer
you will be working on the current
branch within a solution but what
happens if you come up with an idea
where it's something that will take a
different amount of time for you to be
able to complete but you don't want to
interrupt the mainline branch so what
you can actually do is you can actually
create a branch that allows you to start
working on your own work outside of the
main line branch and this is called git
stash allows you to be able to modify
your files without interrupting the
mainline branch so you once you start
talking about branching and get be
prepared to answer and explain the
concept of branching so essentially what
it allows you to do is
have a mainline master branch that has
all the code that the team is checking
in and checking out against but allows
you to have an indefinite number of
branches that allows for new features to
be built in parallel to the mainline
branch and then at some point be
reintroduced to the main line branch to
allow the team to add in new features
and so if we look through the merge and
get rebase these are the two features
that you'd be using continuously to be
able to talk about how you take a branch
and merge it back into the mainline
branch so on the left hand side we have
git merge which allows you to take the
code that you're creating and merge it
back into the master on the right hand
side what you have is a slightly
different approach this is for projects
where you reach a point in a project
where you go okay we're going to
effectively restart the project at this
point in time and we want to ignore the
complete history that's happened before
that and that's called get rebase and
that would allow you to rewrite the
project history by creating a brand new
mainline branch that ignores all other
previous branches that have happened
before it you can if you want to very
quickly and easily find out all the
files that have been used to make a
particular commit so when somebody asks
you the question how do you find a list
of files has been changed in a
particular commit you can actually say
that all you have to go is find the
command git diff dash tree dash r and
then the hash that you use for the
commit and that would actually then give
you a breakdown of all the files that
have been made with that particular
commit a question you'll be asked when
you're talking about merging files is
what is a merge conflict in git and how
can it be resolved so essentially a
merge conflict is when you have two or
more branches that are competing with
commits in git and you have to be able
to determine which is the appropriate
files that need to be submitted and this
is where you would go in and to actually
help resolve this issue you'd actually
go in and manually edit the conflicted
files to select the changes you want to
keep in the final merge so let me go
through the steps that you would take to
be able to illustrate this when you're
talking about this particular question
in your interview now there are
essentially four stages the first would
be under the repository name you want to
select a pull request and you want to be
able to show how that pull request would
be demonstrated inside of github so
within the pull request there's going to
be a highlight of conflict markers and
you'll be able to select which conflicts
and you want to keep on which you want
to merge and which ones you want to
change so it's a step through how you'd
actually resolve emerge conflict and the
first step would be under github you
want to be able to pull the repository
name and then the pull request around
that repository in the pull request list
click the pull request with a merge
conflict and that you'd like to be able
to resolve now pull up a file that will
list out all of the conflicts for you
near the bottom of that file will be a
list of the requests that need to be
resolved and then if you need to make a
decision on which branches you want to
keep or which ones you want to change
that will have to be something you have
to put in instructions inside of the
file you'll actually see that there are
conflict markers within the instructions
which are going to ask you which files
you want to change which ones you want
to keep if you have more than one merge
conflict in your file scroll down to the
next set of conflict markers and repeat
steps four and five until you resolve
all of the conflicts you will want to
mark your file as resolved in github so
that the repository knows that you are
having everything resolved if you have
more than one file with a conflict then
you want to go then onto the next file
and start working on those files and
just keep repeating the steps we've done
up to this point until you have all the
conflicts resolved and then once you
have all of the resolutions created then
you want to select the button which is
commit merge and then merge all your
files back into github and this will
take care and manage the resolution of
the merge conflict within github so you
can also do this through command line
and with the command line you want to
use get bash and so when you want to as
a first step open up get bash and then
navigate to the local git repository in
command line by using the cd change
directory and then list out the actual
folder where you actually are putting
all of your code and then you want to be
able to generate a list of the files
that are affected with the merge
conflict and in this instance here you
can actually see the file style guide.md
has a merge conflict in it and as before
with working with github you actually go
through and use a text address and use
any text editor but as you go through
and edit out what you want to keep and
what you want to uh manage in your
conflict so you actually have a
resolution that's been created so that
you'll be able to then once you you're
using the conflict markers you can
actually merge your files together so
that the solution itself will allow you
to
incorporate your commits effectively
into the resolution once you've gone
through and applied your changes you're
able to then merge the conflicted
commits into a single commit and able to
push that up to your remote repository
all right let's talk about the next
section which is continuous integration
with jenkins so the first question
you'll be asked about with jenkins is
explain a master slave architecture of
jenkins so the way that jenkins is set
up is that the jenkins master will pull
code from your remote git repository
such as github and will check that
repository every time there is a code
commit it will distribute the workload
of that code and the tests that need to
be applied to that code to all of the
jenkins slaves and then on request the
jenkins master and the slaves will then
carry out all the builds and tests to be
able to produce test reports the next
question you'll be asked is what is a
jenkins file and simply put a jenkins
file is a text file that has a
definition of the jenkins pipeline and
is checked into a source code repository
and this really allows for three
distinct things to happen one allows for
a code review and iteration of the
pipeline it permits an audit trail for
that pipeline and also provides a single
source of truth for the pipeline which
can be viewed and edited so which of the
following commands runs jenkins from the
command line is it java jar jenkins dot
war java dash war jenkins.jar java.jar
jenkins.jar java
jenkins.org
and the answer is a
jar jenkins dot wall so when working
with jenkins you're going to be asked
what are the key concepts and aspects of
working with the jenkins pipeline and
you want to really hold out your fingers
here and go through four key areas and
that is pipeline node step and stay so
pipeline refers to the user-defined
model of a cd continuous delivery
pipeline node are the machines which is
which are part of that jenkins
environment within the pipeline step is
a single task that tells jenkins what to
do at that particular point in time and
then finally stage defines a
conceptually distinct subset of tasks
performed through the entire pipeline
and tasks could be build test and deploy
so which of the following file is used
to define dependency in maven and do we
have a build.xml
b
palm.xml c
dependency.xml or d
version.xml and the answer is
palm.xml working with jenkins you're
going to be asked to explain the two
types of pipeline used in jenkins along
with the syntax and so a scripted
pipeline is based on groovyscript as
their domain specific language for
jenkins and there are one or more note
blocks that are used throughout the
entire pipeline on the left hand side
you can actually see what the actual
script would look like and the right
hand side shows what the actual
declaration for each section of that
script would be the second type of
jenkins pipeline is a declarative
pipeline and a declarative pipeline
provides a simple and a friendly syntax
to define what the pipeline should look
like and then you can actually at this
point use an example to actually break
out how blocks are used to find the work
completed in a decorative pipeline so
how do you create a copy and backup of
jenkins well to create a backup
periodically back up jenkins to your
jenkins home directory and then create a
copy of that directory it's really as
simple as that a question you'll be
asked as well is how can you copy
jenkins from one server to another well
there essentially there are three ways
to do that one is you can move a job
from one installation of jenkins to
another by copying the corresponding job
directory the second would be to create
a copy of an existing job directory and
making a clone of that job directory but
with a different name and the third is
to rename an existing job by renaming a
directory so security is fundamental to
all the work that we do within devops
and jenkins provides the center core to
all the work that gets completed within
a devops environment there are three
ways in which you can apply security to
authenticate users effectively and when
you are asked about this question of
security within jenkins the three
responses you want to be able to provide
is a jenkins has its own internal
database that uses secured user data
from and user credentials b is you can
use a ldap or lightweight directory
access protocol server to be able to
authenticate jenkins users or c you can
actually configure jenkins to
authenticate by using such as oauth
which is a more modern method of being
able to authenticate users you're going
to be asked how to deploy a custom build
of a core plugin within jenkins and
essentially the four steps you want to
go through are first of all copying the
dot hpi plugin file into the jenkins
home plugins subdirectory
you want to remove the plugins
development directory if there is one
you want to create an empty file called
plugin.hpi.pinned
and once you've completed these three
steps restart jenkins and your custom
build plugin should be available how can
you temporarily turn off jenkins
security if the administrative user has
locked themselves out of the admin
console well this doesn't happen very
often but when it does it's good to know
how you can actually get into jenkins
and be able to resolve the problems of
authenticating effectively into the
system as an administrator so when you
want to be able to
get into a jenkins environment what you
want to be able to do is locate the
config file you should see that it's set
to true which allows for security to be
enabled if you then change the user
security setting to false security will
disable allow you to make your
administrative changes and will not be
re-enabled until the next time jenkins
is restarted
so what are the ways in which a build
can be scheduled and run in jenkins well
there are four ways in which you can
identify the way a build can be
scheduled on running jenkins the first
is when source code management commits
new code into the repository you can run
jenkins at that point the second can be
the after the completion of other builds
so maybe you have multiple builds in
your project that are dependent to each
other and when so many other builds have
been executed then you can have jenkins
run you can schedule bills to run at a
specified time so you may have nightly
builds of your code that illustrates the
changes in the solution you're building
and then finally you also can manually
build a environment on request
occasionally you will want to also
restart jenkins and so it's good that
when a question around how do you
restart jenkins manually comes up that
you have the answers and there are two
ways in which you can do it one is you
can force a restart without waiting for
bills to complete by using the jenkins
url that you have for your environment
slash restart or you can allow all
running bills to complete before restart
are required in which case you would use
the command of the url for your jenkins
environment slash safe restart so let's
go into the fourth and final section of
this first video which talks about
continuous testing with selenium so the
first question you will be asked most
likely around selenium are what are the
four different selenium components and
again you want to hold open your fingers
because there are four distinct
environments you have selenium
integrated development environment or
selenium ide you have selenium remote
control or selenium rc you have selenium
web driver and then selenium grid you'll
be asked to explain each of those areas
in more detail but let's start off with
by looking at selenium driver what are
the different exceptions in selenium
webdriver so it's useful to remember
that an exception is an event that
occurs during the execution of a program
that disrupts the normal flow of that
program's instructions and so we have
four we have a timeout exception an
element not visible exception no such
element exception and a session not
found exception and each of those if we
step through them are the four different
types of exceptions that can be thrown
up when using the selenium web driver so
as we evolve in our digital world with
the different types of products that are
available for us to be able to build
solutions onto multiple platforms you're
going to be asked can selenium and other
devops tools run in other environments
and so a good question around this is
cancelium test an application in an
android web browser and the short answer
is absolutely yes it can we have to use
the android driver for it to be able to
work so you want to be able to talk
about the three different types of
supported tests within selenium so when
the question comes up what are the
different test types supported by cyan
you can answer that and there are three
different types of tests first is a
functional test second is a regression
test and third is a load testing test
the functional test is a kind of black
box testing in which test cases are
based on a specific area or feature
within the software a regression test
helps you find any specific areas that
functional tests or non-functional areas
of the code wouldn't be able to detect
the load testing test allows you to
monitor the response of a solution as
you increase the volume of hits in how
you're using the code are put onto it an
additional question you'll be asked is
how can you get a text of a web element
using selenium well the get command is
due to achieve text of a specific web
element and it's important to remember
however that the command does not return
any parameters but just returns a string
value so you want to be able to capture
that stream value and discuss about it a
question you'll be asked around selenium
is can you handle keyboard and mouse
actions using selenium and the answer is
yes you can but you have to make sure
that you're using the advanced user
interaction api and the advanced user
interaction api is something that can be
scripted into your tests and it allows
you for
capturing methods such as a click and
hold and drag and drop mass events and
then keyboard down or keyboard up key
release events so that if you want to to
capture say the use of control shift or
a specific function button off the
keyboard you'd be able to capture those
of the following four elements which of
these elements is not a web element
method a get text b
size
c
get tag name d
send keys and it's b size you're going
to be asked to explain what is the
difference for when we use find element
or find elements and so if we look at
find element find element finds the
first element in the current web page
that matches the specified locator value
in contrast find elements finds all of
the elements on the web page that
matches the specified value when using
webdriver what are the driver close and
driver quit commands and these are two
different methods used to close a web
browser session in selenium so driver
close will close the current web browser
which your focus is set and driver quit
closes all the browser windows and ends
the web driver session completely the
final question that you are likely to be
asked in using selenium is how can you
submit a form using selenium well in
this instance that's relatively easy the
following lines of code will let you
submit a form in selenium which would be
web element el equals driver dot find
element and then you put in the id and
the element id and then l submit open
close parentheses semicolon so let's
just get into the first section which is
configuration management so one of the
questions that you'll get asked right
away is why do you have ssl certificates
used for chef really fundamentally your
immediate answer should be security ssl
provides for a very high level of
private security and private and public
key pairing this really is essential to
ensure that you have a secure
environment throughout your entire
network the second part should be that
if you're using ssl and you're using the
private public key model within ssl
you're able to guarantee the systems on
your network that the chef that you'll
be able to validate that the nodes
within your network that chef is
validating against actually are the real
nodes themselves not imposters so you
will also be asked some questions such
as the following which is the following
commands would you use to stop or
disable the http service when the system
boots and you'll typically get four
responses and there'll be
system ctl disable httpd.service
or is it system disable http.service
system disable httpd or the final option
which is system ctl disable
httpd.service
your answer should be the first one
which is hashtag
systemctldisablehttp.service
so chef comes with a series of tools
that allow it to function effectively
and one of the tools that you're going
to be asked about is what is test
kitchen and test kitchen is essentially
a command line tool that allows you to
be able to test out your cookbook before
you actually deploy it to a real note so
some of the commands that you would use
are for instance if you want to create
an instance of test kitchen you would do
kitchen create if you want to destroy an
instance after you created it you do
kitchen destroy and if you want to be
able to combine multiple instances you
would do kitchen converge so a question
you'll get is around chef is how does
chef apply differ from chef client so
fundamentally the difference between
them is that chef apply will validate
the recipe that you're working on
whereas chef client looks to apply and
validate the entire cookbook that's run
in your server environment so one is
focused on the recipe and the other is
focused on the entire cookbook
so there are some differences when
you're working with different command
lines so for instance when you're
working with puppets and you're working
with one version of puppet and you want
to do what is the command to sign a
requested certificate the top example
here is for public version 2.7 whereas
the lower option here is for puppet
version 3. now something to bear in mind
when you're going through your interview
process is that the tools that are used
within a continuous integration
continuous delivery devops model do vary
and so you want to be able to talk
knowledgeably about the different
versions of the tools so that when
you're talking to your interviewer
you're able to show the deep knowledge
that you have which open source or
community tools do you use to make
puppet more powerful and essentially
this question is going to be asking you
to look beyond the core foundation of
puppet itself and so the three options
you have is uh being able to track
configurations with jira which you
should be doing anyway but it's a great
way to be able to clearly communicate
the work that's being done with puppet
our version control can be extended with
get and then the changes should be
passed through jenkins so the three
tools you want to be looking at
integration with jira git and jenkins so
what are the resources in puppet well
fundamentally there are four the
resources are basic units of any
configuration management tool they are
the features of the nodes they are the
written catalog and the execution of the
catalog on a node so as we dig deeper
into puppet one of the things that you
are likely to be asked regarding puppet
is what is a class and puppet and so a
class in puppet is really the name
blocks in your manifest that contain the
various configurations and this thing
can include services files and packages
and we have on the screen here an
example of what a class would look like
when you write it out and you may want
to memorize just one class don't
memorize just a whole set of classes
just memorize one the person that's
interviewing you is just really looking
for someone who has a working knowledge
they're not looking for you to have
memorized complete massive classes but
having one small class to be able to
illustrate the experience you have is
extremely valuable to the interviewer
particularly if it's a technical
interview so as we move into ansible one
of the things that you're going to be
asked around ansible is what is ansible
role so a role is an independent block
of tasks and variable files and
templates embedded with inside of the
playbook so example we have here on the
screen actually shows you one role
within a playbook and in this role it is
to install tomcat on a node again as
with previous question within puppet of
a class it's probably good to have
memorized just one or two roles so you
can talk knowledgeably about ansible
when you're having your interview so
when you're working with ansible when
should you be using the curly brackets
and so just as a frame of reference uh
there's often two different ways that
these kind of brackets are referred to
now they're either referred to as french
brackets or curly brackets either way
and what you'll be wanting to ask is
when would you use these specific types
of brackets within ansible and really
the answer comes down to two things one
is that it makes it easier to
distinguish strings and undefined
variables and the second is for putting
together conditional statements when you
are actually using variables and the
example we have here is this prints the
value of and we have foo and we have to
then put in the variable conditional
statement of vu is defined as something
so what is the best way to make content
reusable and redistributable
with ansible and there's really
essentially three the first is to
include a sub module or another file in
your playbook the second is to import an
improvement of an include which ensures
that a file is added only once and then
the third is roles to manage the tasks
within the playbook
so a question you will be asked is
provide a differences between ansible
and puppets so if we look at ansible
it's a very easy agentless installation
it's based on python you can configure
it with yaml and there are no support
for windows in contrast puppet is an
agent-based installation it's written in
ruby the configuration files are written
in dsl and it has support on all popular
operating systems so we dig deeper into
the actual architecture ansible it has a
much more simple architecture and it's
definitely a push only architecture in
contrast to puppet it's a more
complicated but more sophisticated
architecture where you're able to have a
complete environment managed by the
puppet architecture so let's get on to
our next section which is
containerization
so let's go through and you're going to
be asked to explain what the
architecture of docker is and docker
really is the most popular
containerization environment so docker
uses a client server architecture and
the docker client is a service which
runs in a command line and then the
docker daemon which is run as a rest api
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an executable package of
applications and its dependencies
together and then finally the docker
registry is a service to host and
distribute docker images among other
users so you'll also be asked to provide
what are the advantages of docker over
virtual machine and and this is
something that comes up very
consistently in fact um you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a docker
or docker-like environment and really
the the arguments for docker are just
absolutely fantastic you know first of
all docker does contain and occupy
docket containers occupy significantly
less space than a virtual machine or a
dedicated machine the boot up time on
docker is significantly faster than a vm
containers have a much better
performance as they are hosted in a
single docker image docker is highly
efficient and very easy to scale
particularly when you start working with
kubernetes easily portable across
multiple platforms and then finally for
space allocation docker data volumes can
be shared and reused among multiple
containers the argument against virtual
machines is significant and particularly
if you're going into an older
environment where a company is still
using actual dedicated hardware and
haven't moved to a cloud or cloud-like
environment your arguments for docker
are going to be very very persuasive be
very clear on what the advantages are
for docker over a virtual machine
because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to working with the
tools like docker you need to be able to
effectively share with that team what
the benefits are so how do we share
docker containers with different nodes
and in this instance what you want to be
able to do is leverage the power of
docker swarm so docker swarm is a tool
which allows the it administrators and
developers to create and manage clusters
of swarm nodes within the darker
platform and there are two elements to
the node there's the manager node and
then there's the the worker node the
manager node as you'd assume manages the
entire infrastructure and the working
node is actually the work of the agent
as it gets executed
so what are the commands to create a
docker swarm and so here we have an
example of what a manager node would
look like and once you've created a
swarm on your magic node you can now add
worker nodes to that swarm and again
when you're stepping through this
process be very precise in the execution
part that needs to be taken to be able
to effectively create a swarm so start
with the manager node and then you
create a worker node and then finally
when a node is initialized as a manager
node it can immediately create a token
and that token is used for the worker
nodes and associating the ip address
with the worker nodes question 17 how to
run multiple containers using a single
service it is possible to run multiple
containers a single service by using
docker compose and docker compose will
actually run each of the services in
isolation so that they can interact with
each other the language used to write
out the compose files that allow you to
run the service is called yaml and yamos
stands for yet another markup language
so what is the use of a docker file so
docker file actually is used for
creating docker images using the build
command so let's go through and show on
the screen what that would look like and
this would be an opportunity where if
you're actually in a technical interview
you could potentially even ask hey can i
draw on a whiteboard and show you what
the architecture for using the build
command would look like and what the
process would look like um again when
you're going through an interview
process as someone who interviews a lot
of people one of the things i really
like is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to the
whiteboard and actually show what can
actually be done through actually
creating images on the whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as an architect this
should be something that you're
comfortable doing and by doing it in the
interview and certainly you want to ask
permission before you actually do it but
doing this in the interview really helps
demonstrate your comfortable feelings of
working with these kind of architecture
drawings so back to the question of
creating a docker file so we go through
and we have a docker file that actually
then goes ahead and creates the docker
image which then in turns creates the
docker container and then we are able to
push that out up to a docker hub and
then share that docker hub with
everybody else as part of the docker
registry with the whole network so what
are the differences between docker image
and docker containers so let's go
through the docker image so the docker
images are templates of a docker
container an image is built using a
docker file and it stores that docker
file in a docker repository or a docker
hub
and you can use docker hub as an example
and the image layer is a read only file
system the docker container is a
collection of the runtime instances of a
docker image and the containers are
created using docker images and they are
stored in the docker daemon and every
container is a layer is a read write
file system so you can't replace the
information you can only append to it
so while you can actually use yaml or
writing your so a question you can be
asked is instead of yammer what can be
an alternate file to build docker
compose so yaml is the one that is the
default but you can also use json so if
you are comfortable working with json
and my that is something that you should
be uh get comfortable with is you want
to be able to use that to name your
files and as a frame reference adjacent
is a logical way of being able to do
value paired matching using a javascript
like syntax
so you're going to be asked to how to
create a docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a mysql docker
container so to do that you want to be
able to build a docker image or pull
from an existing docker image from a
docker repository or hub and then you
want to be able to then use docker to
create a new container which has my
sequel from the existing docker image
simultaneously the layer of read write
file system is also created on top of
that image and below at the bottom of
the screen we have what the commands
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have a docker
registry is an open source server-side
service used for hosting and
distributing docker images whereas in
contrast for repositories a collection
of multiple versions of a docker image
in a registry a user can distinguish
between docker images with their tag
names and then finally on the registry
docker also has its own default registry
called docker harb for the repository it
is a collection of multiple versions of
docker images it is stored in a docker
registry and it has two types of public
and private registry so you can actually
create your own enterprise registry so
you're gonna be asked you know what are
the cloud platforms that support docker
really you know lifts them all and we
have listed here amazon web services
microsoft azure google cloud rackspace
but you could add in their ibm bluemix
could put in red hat really any of the
cloud service providers out there today
do support docker it's just become an
industry standard so what is the purpose
of expose and publish commands in docker
so if we go through expose is an
instruction used in docker file whereas
publish is used in docker run command
for expose it is used to expose ports
within a docker network whereas with
publish it can be used outside of a
docker environment for expose it is a
documenting instruction used at the time
of building an image and running a
container whereas with published is used
as to map a host port to a running
container port for expose is the command
used in docker whereas for publish we
use the command-p
for when we're doing our command line
used in docker and examples of these are
expose 8080 or with docker we would put
in or for publish we'd do the example
docker run dash d dash p and then
0.0.0.80
a colon 80 as our command line so let's
look at continuous monitoring so with
continuous monitoring how does nagios
help in continuous monitoring of systems
applications and servers and so this is
really just a high level question of
using nagios within your environment and
you should be able to just come back
very quickly and say nagios allows you
to help manage the servers and check if
they've been sufficiently utilized and
if there are any task failures that need
to be addressed and so there are three
areas of utilization and risk that you
want to be able to manage this is being
able to verify the status and services
of the entire network the health of your
infrastructure as a whole and if
applications are working properly
together with web services and apis that
are reachable
so the second question you'll be asked
is how does negatives help in continuous
monitoring of systems applications and
services so it's able to do this by
having the initial negative process and
scheduler and the additional plugins
that you would use for your network
connect with the remote resources and
the negatives web interface to be able
to run status checks on a predefined
schedule so what do you mean by nagios
remote plugin executor or the mpre of
nagios so mpre allows you to execute
plugins on links unix machines that
allow you to do additional monitoring
and machine metrics such as disk usage
cpu load etc what are the ports used by
nagios for monitoring purposes in this
example there are three and they're easy
to remember so i would memorize these
three but they're essentially ports five
six six six
five six six seven and five six six
eight so there are two types of checks
within that year so you will be asked
for what is an active and passive check
in nagios so an active check and is
initiated by the nagios process and is
run on a regular schedule a passive
check is initiated and formed by an
external application or process so this
may be where a system is failing and
checks are results are submitted to
nigeria's for processing and to continue
with this for what is an active and
passive check active checks are
initiated by the check logic within the
nagios daemon negas will execute a
plug-in and pass information about what
needs to be checked plug-in will then
check the operational state of the host
oil service and then process the results
of the host or service check and send
out notifications in contrast with the
passive check it is an external
application that initiates the check it
writes the results of the check to an
external command line file and i guess
reads the external command file and
places the results of all passive checks
in a queue for later processing so you
can go back and revalidate and then
negotiate may send out notifications log
alerts etc depending on the results that
they get from checking the information
so you're going to be asked to explain
the main configuration file and its
location in nagio so the main
configuration file consists of a number
of directives that affect how nagios
operate so consider this as the
configuration file that's read by both
nagios processor and the cgis so this
will allow you to be able to manage the
main configuration file that's placed
into your settings directory so what is
the nagios network analyzer and again
hold out your four fingers because there
are four options here so the nagios
network analyzer are one provides an
in-depth look at all your network
traffic source and security threats two
allows system admins to gather
high-level information on the health of
your network three provides a central
view of your network traffic and
bandwidth data and then four allows you
to proactive in resolving outages
abnormal behavior and threats before
they affect critical business processes
so what are the benefits of http and ssl
certificate monitoring with nagios so
with http certificate monitoring it
allows you to have increased server and
services and application availability
obviously very important fast detection
of network outages and protocol failures
and allows web transaction and web
service performance monitoring the ssl
certificate monitoring allows you for
increased website availability frequent
application availability and provides
increased security so explain
virtualization with nagios so in
responses the first thing you should be
able to talk about is how nagios itself
can run on many different virtualization
platforms including microsoft visual pc
vmware zen amazon ec2 et cetera et
cetera so just make sure you get that
right off the bat now yes it was able to
provide capabilities tomorrow assortment
of metrics on different platforms it
allows for ensure for quick detection of
service and application failures and has
the ability to be able to monitor
against many metrics including cpu usage
memory networking and vm status so name
the three variables that affect
recursion inheritance in nagios and it
is name use and register so name is a
template name that can be referenced in
other object definitions use specifies
the name of the template object that you
want to inherit its properties and
variables from and register indicates
whether or not the object definition
should be registered to nagios and on
the right hand side of the screen we
have an example of what that script
would look like again you may want to be
able to memorize this as it's something
that you can actually write down and
show someone if you're going through a
technical interview so why is nagios
said to be object oriented and
fundamentally comes down to the object
configuration format that you can use in
your object definitions it allows you to
inherit properties from other object
definitions and this is typical of
object-oriented development and is now
applied for the nagios environment so
some of the objects that you can inherit
are services hosts commands and time
periods so finally explain what is state
talking in nagios and so there are
really four options here when you're
talking about state stalking so state
stalking is used for logging purposes in
nagios it allows you to enable for a
particular host or service that
neighbors will watch over very carefully
it will log any changes it sees in the
output of the check results and then
finally helps the analysis of log files
and so with this we have reached the end
of our devops whole course we hope that
you guys found it informative and
helpful
do like and share it thanks for watching
and stay tuned for more from simplylaw
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here
