[Music]
hey everyone welcome back today we'll be
talking about dimensionality reduction
pretty quick video
so let's just start with what is
dimensionality reduction so let's say
you have some training data
which is n rows by p columns so you have
n
observations and p features p predictors
the goal of dimensionality reduction is
exactly what it sounds like
is to get the number of dimensions
number of features number of predictors
number of columns down
significantly so we're basically trying
to go from n by p
matrix of training data to some n by k
matrix of training data where k
is supposed to be much smaller than p
why would we want to do this so a couple
reasons that come up are storage
obviously if you're going in the realm
of big data
big data takes a lot of storage on
whatever machine you're running it on so
it would be nice
if we could compact this data into a
smaller space
and hopefully not lose too much
information along the way
another consideration is the model
training time so obviously if you have a
larger amount of data
your model might take a lot longer to
train because it might have to consider
many many features
if you have less features then your
model will probably take less time to
train
and finally maybe a lesser one but also
one that's often overlooked is
interpretability
this is not going to be true in all the
dimensionality reduction techniques as
we'll see one of the techniques does
have this and one does not but
under certain situations this actually
helps you isolate
some of the features or columns that you
care the most about
and you can throw away the ones that are
not very important so it could also be
helpful for that but it's very very
important to note in this video
that when you do dimensionality
reduction however you do it
you're always going to be losing some
data and this is just intuitive i don't
think it requires a formal proof you
could do a formal proof but basically if
you're going from
this much stuff and you're trying to
press it into this much space
you literally just have less numbers and
so you can't capture
all of the original dynamics in the
original data set
and so the real goal of dimensionality
reduction is not purely just
giving me less data it's to compact my
data but lose
as little of the valuable information as
possible so keep as much of the
important stuff
and throw away the stuff that's
irrelevant and so let's talk about two
methods to do dimensionality reduction
there are many out there but i want to
go over two intro methods that you'll
probably learn early on
during your studies in machine learning
so the first one here is called pca or
principal components analysis now this
is not a video where i'm going to go
into all the depth of pca i have videos
on that which i'll link in the
description below
but just to remind you here's a very
common picture you see during pca
you might have some two-dimensional data
x1 x2 coordinates and all these green
x's are your data points
you can see they more or less line up
along this red line
and so we project them into a lower
dimensional space in this case we're
projecting
our two-dimensional data into a
one-dimensional linear space
so you notice when you project these
green x's onto this red line
you're still going to be losing some
data because the projections are going
to be basically smooshing them into this
one-dimensional space but because they
already line up with the line pretty
well you're losing a very small amount
of data
and so that's what's going on here
you're projecting your data into a
k-dimensional space where k is less than
the original number of dimensions
now the pros of this is it's efficient
so if you go through the entire process
of pca
it's basically built to compress your
data in such a way that you're going to
be retaining as much of the variation
in the original data set as possible
that's the entire goal of pca
and so it's no surprise that it's doing
that in an efficient way
but by being very efficient you actually
lose a lot in terms of interpretability
for example to go a little bit deep into
pca for a second you take the covariance
matrix of your data you find the
eigenvectors of that you project your
data onto the eigenvectors of your
covariance matrix
obviously kind of a complex process and
so you lose a little bit of what's
actually going on what does my transform
data represent anymore because you're
kind of
now having to think about your transform
data as coordinates in this new
dimension
this new basis basically new coordinate
space and so it's a little bit tricky
it's not impossible but it's a little
tricky to
map that back to your original features
so it's important to note these pros and
cons
now the other major method i've seen in
intro machine learning courses to do
dimensionality reduction is lasso so
we've probably seen this formulation
many many times but this is just a
regularization
of ordinary least squares and we
typically have this in two flavors we
have lasso and ridge
and today is the lasso formulation so
we're taking the true labels minus the
predicted labels we're taking the
l2 normal bat and then this extra term
we tack on is for regularization and
when we have l1 norm
as we do there this is called lasso
regularization
and the key feature of lasso
regularization as we know
is that it's going to send many of your
beta i's to exactly zero
and so we can actually use this for
feature selection because we can run
this
so let's say we fix some kind of lambda
which is a parameter
and then we can just pick the variables
whose beta i's are not sent to exactly
zero
and in some sense the model considers
these as important or predictive
of this target variable y so
the pros here are that this is
interpretable notice we're not
projecting our data into any dimensional
space we're simply just eliminating
columns that are not helpful in
predicting this target variable y
and so at the end of the day we have a
smaller set of columns but the key
observation is that those columns are
coming from the original data set itself
we're not transforming the features so
you can just say that oh this column was
important this column got thrown away
easy to interpret and the other kind of
bonus is that it takes the response into
account
notice that y is explicitly taken into
account here which is not actually true
for pca and of course the con here is
that it's not going to be as efficient
as pca because
again pca was designed to compress your
data in the most efficient way possible
this is simply just marking columns for
deletion so it's not going to be as
efficient in compressing your data
so i hope this very quick kind of run
through of dimensionality reduction
techniques was helpful if it was please
like and subscribe for more videos just
like this
i'll catch you next time
