hello everyone in this video you'll
learn all about reinforcement learning
how to train chat GPT and build the
world's best go playing program
but let's start with something simpler
remember pong
it was one of the very first video games
the player on the left has a simple
policy it just follows the ball up and
down
let's train an AI pong playing Agent to
compete with it
to do this we'll need to define a policy
that takes the state of the world as
input and outputs an action up in this
case
as input we'll take the ball X and Y
positions and the two paddle positions
we'll also include the velocities so our
policy will get eight inputs
to keep the figure simple I won't show
the velocities
we'll represent our policy as a neural
network our network will multiply each
of these values with a weight
and sum them up
we'll add a sigmoid function to convert
the result to a value between 0 and 1.
if the result is larger than 0.5 we'll
move up otherwise down that's it
great let's play some pong
we'll start with random weights
a random Pond player isn't very good it
just hides in the corner
this is like if you've never played
tennis and you're matched against Serena
Williams
you'd probably hide too
or maybe you'd ask your opponent how to
play
suppose your partner told you exactly
what to do for every step
we can use this kind of supervision to
train our policy
let's say our coach says to follow the
ball
hear the balls above our paddle so the
right answer is up
but suppose our Network outputs 0.2
which means down
we'd like to change that 0.2 to a 1.0
here's a different case where the right
answer is down
and we'd like to drive the output to
zero a more confident down answer
we can accomplish this by defining an
error function
and optimizing the weights to reduce
this error
instead of numbers let's draw these
weights as colored lines
positive voids are blue and negative
weights are red
stronger weights of thicker lines
and the weights change a tiny bit after
each optimization step
if we train on thousands of actions the
weights on the right start to converge
now that we're trained up let's play
some games
our pompling agent is pretty decent now
although it doesn't win very often
how can we train our agent to win more
well you can think of the game as a
sequence of actions
which may lead to a loss or a win
suppose we lose
we'd like our coach to tell us which
actions were at fault maybe we messed up
at the end
then we could retrain our policy based
on this feedback as we've been doing
before
training a network like this is called
supervised learning and you need access
to a really good coach
but suppose you don't have a coach all
you know is that this sequence of
actions resulted in a loss and this one
produced a win
this kind of problem is called
reinforcement learning
we don't know which actions were
responsible
so we'll just penalize all actions we
made in the loss and reward or reinforce
all actions we made in the win
this converts it into a supervised
learning problem just like we solved in
the first part of this video
actually we can do a little bit better
by penalizing later actions more as the
game losing the stake typically happens
towards the end and similarly for
Rewards
after training on 3000 games our agent
has improved to play about as well as
the opponent
the score is pretty even
and after about 12 000 games our agent
figured out that hitting the ball with
the corner of the paddle makes it go
faster
cool
and now it wins most of the time
reinforcement learning is cool you learn
by trying stuff out and sometimes you
discover things that even the coaches
don't know
but there's some challenges with this
approach our neural network tries to
minimize an error function
you can think of this function as a
landscape and we minimize it by Rolling
downhill using an approach known as
gradient descent
gradient descent can get stuck in local
Minima so we need to explore a wider
range of policies to find the best one
so let's make the policy probabilistic a
0.7 now means 70 chance of moving up
so the agent will actually move down 30
of the time
this bit of Randomness allows our agent
to get out of some of those local Minima
to sum it up here's the approach we've
described so far
we'll start with randomly initialized
weights
we then train with supervised learning
and refine with reinforcement learning
amazingly we can skip the middle step
and run reinforcement learning from
random initial weights
it takes longer to train but can still
work for many problems
and we can do something even more
amazing
so far we've assumed the ball and paddle
positions are provided as input to the
policy
suppose instead all we had was a photo
of the screen
the computer will see this as a bunch of
numbers it has no idea that this number
is the ball and these are the paddles
in other words the policy has to learn
not just how to act but also how to see
it's helpful to encode velocity
information as well
one way to do this is to subtract the
previous from the current frame
now positive values are from the current
frame and negatives are from the
previous one
instead of a 2D image we'll stack all
the rows into one long Vector of pixels
and we'll connect all these pixels to a
neuron
now we'll add more neurons 10 in total
we'll use rectified linear activation
functions
these are pretty standard
they connect to a sigmoid neuron at the
end
and it will output the probability of
moving up
this may look complicated but it's
actually really small for a deep net
there are only 11 neurons in this
network
compare this to the human brain which
has billions of neurons in the visual
cortex
how can you see with only 11 neurons
let's train the network and find out
we'll start with completely random
weights
it took about 6 million games to learn a
policy that beats the computer on
average
that's almost a week running on my
MacBook Air
but what's this network actually
learning does it really learn how to see
let's examine the first neuron
it has a weight for every pixel in the
image
remember these pixels form a 2D image
and each one has a corresponding weight
so we can visualize the weights as an
image in the shape of the pong screen
it looks like noise because I've
randomly initialized the weights the
white ones here have the largest
absolute value
now we'll start training the policy by
playing games
and you can see that it converges to a
specific pattern after 1 million and 6
million games
these streaks correspond to ball
trajectories that this neuron is
attending to
and it's particularly interested in
these paddle positions
it's also looking at the opponent's
paddle which is a good indicator of ball
position
this is just one neuron
the others look for different patterns
so our simple network has kind of
learned how to see
this approach to reinforcement learning
is called policy gradient and you can
make it even better by limiting the size
of each update in a variant known as PPO
together they power many exciting
applications
we'll talk about two of them alphago and
chat GPT in the next two videos
stay tuned
