feature engineering it is one of those
black art ninja data science tricks that
the masters use to
raise the accuracy of their model
whether it's a neural network support
vector machine
xg boost whatever and get better
accuracy
now feature engineering i've definitely
used this particular black magic and
kaggles of my own you can see here in
the core of question pairs
i got to a top seven percent ranking
and i did this not by throwing crazy
amounts of compute power at it but just
looking at the data using my human
intuition
and understanding how i can take the
data that i already have recombine it
and create additional features that give
additional lift
to the model now feature engineering is
nothing more than taking
that input vector of your data set that
you're given
here you see cars and picking out a
couple of these that you want to
recombine in a different way
for example let's just pick horsepower
and displacement of the engine
we can then combine those into a ratio
where you're dividing one by another a
ratio is one of the most
often touted ways of doing feature
engineering
and that gives you a new number that you
then augment onto
the data set and use that to then train
your model
now we don't want to just randomly throw
ratios and all these things around we
need to think about this
if we're doing this using data science
intuition
let me show you how you can take these
formulas
and build them like legos and know what
to combine to
engineer your features
[Music]
look at this cartoon it's great it's by
phd comics and i feel like it really
captures feature engineering i don't
think that's what it's even attempting
to do
maybe produce a very small model but
this
shows you how you would assess whether
you wanted to go
to a particular seminar or a conference
if you're in the corporate world
maybe another team wants to show you
something cool they've been working on
and to get you there they lure you with
food so these are the three inputs
and to make it more simple for your
model the human mind in this case to
grasp
we use that formula to combine
these relevance of the information
food quality and how far away it is all
three very important things to decide if
you're going to go to a meeting or not
and reduce those kind of like feature
reduction it is feature reduction
to one particular value that's now going
to go into the model and you'll probably
still send those other three
and it'll be correlated so you have to
deal with that in
in models that need that dealt with but
that is how we've been using feature
engineering for a long long time to
make things multiple variables work
better for the model
in this case our mind so that we can
think of them better
square feet square meters if you're
looking at a property
a flat area of ground you're going to
multiply the width times
the length that tells you rather than
thinking
okay it's this wide it's this it's this
long
you just have square meters and you're
done
but maybe taxing authorities always
throw
[Music]
curve balls into this maybe taxes are
more
if you have more street front so the
width becomes much more important
than the length so you
square the the width or multiply it by
something
it doesn't really matter that you're
squaring a cubing it or
multiplying it by a ratio that's for the
model to figure out you're just giving
it a hint you're pushing it in the right
direction
another example of simplifying things
sort of for the human mind
is medical if you're looking at how much
does somebody weigh their mass
you're looking at how high they are
their
their height height those are two values
mass alone is not enough to tell you the
person's
real health that's what bmi body mass
index is for so you need a way to
combine those we have this formula here
and that is showing how you can put
those two in relation
now height that's not saying that height
is necessarily so much more important
than the mass those two
values are in different
ranges mass can be much higher
compared to the height of an individual
so that's just kind of bringing that
number you don't want this
if the numbers for human consumption you
don't want it necessarily
exploding into very very large numbers
you don't want to have to talk about you
have a
bmi of 32 100 and whatever
so this is just looking at some
basic examples of how feature
engineering is used not just for our
minds in these cases
but could be used for a model now let's
look at how to build these
so here are your lego blocks to work
with i divide these into really four
quadrants
first you have your normalizer which is
division
that is used often i mean think of
averages here
you don't know really how well somebody
did on a test based on how many points
they got
it is points divided by the amount
that is on the test so now you get this
percent
you know what 90 means you don't know
what i got 90
points are 90 points out of what 900
points
the next quadrant is combine combine
is when you have two values and
really they're they're working
very much together so you would add them
together you would multiply them
like the length and with looking at
total value of a property
like in the seminar appeal
the relevance and food pretty similar
kind of additive to
each other you can combine things with
addition
with multiplication if you want to go
completely crazy with powers
if you're raising something to the power
though make sure i mean you're going to
very large numbers that you're going to
probably have to
back back down some other way then
the third quadrant is scaling
maybe you want to take individual values
as you're building this lego block
equation together and you want to
increase or decrease its importance
now you can use a division here just
divide something by
a value and that decreases it but more
commonly you'll probably use a power to
increase it so square something cubit
you might use a constant to multiply it
which is sort of the same as dividing it
you would use say a logarithm
or really like the the big o
notation in computer science there's all
kinds of modifiers
to control how much you want to
signify the importance of something heck
if you want to make it really important
take the factorial of it it'll grow
quite rapidly
and you can use radicals which are
really powers
but like the square root and other
things as well because that also
controls how fast something is going to
grow
now the fourth quadrant you're
contrasting
so if two things are working sort of
in opposition to each other you could
potentially be dividing them but
if you're subtracting them then you're
you're you're dealing with one thing
sort of as the baseline
of another and you might want to
look at only the magnitude maybe
negative numbers don't
matter because which side do you put
the two values on on the negative
the subtraction so if you put an
absolute value around it now
it's only the magnitude that is going to
be important you can also just
square the quantity and that's another
way that
commonly these equations are set up so
that you're taking effectively an
absolute value and then often you'll put
a radical around the thing later to
sort of back that out some of the root
mean square
and other error calculations are good
examples of these
so now using those building blocks let's
look at this comic again
you've got relevance times food quality
so those two things they're both
important to you you're looking
at the relevance and you're looking at
the food quantity now maybe food quality
would be more important to you so you
might square that one
or you might have another value in here
which they allude to
in the final frame how hungry am i when
have i last eaten
you may actually then do food minus
the time that you've last eaten
something such as that but relevance
and food quality is trumped
sorta by distance because you're lazy
and you just don't want to walk very far
for this free food or
this information the relevance
so you divide the whole thing by
distance
okay you're really lazy let's square
distance or maybe you're worried about
those
distance units overwhelming the
units that relevance and food quality
are in you don't have to worry so much
about that overwhelming
factor always with the models sometimes
they can figure it out but
i have done adjustments when i'm
creating these and combining these
together
that helps it be able to figure out
how to build that feature so here's one
that i put together
these are the values that i had i had
the value of somebody's house
the average value of houses in that zip
code and their age
and i was feeding all three of these
into the model separately and i was
trying to predict
kind of a score on affluence or there's
interest in buying a particular
product a propensity model is what these
are usually called
so these values i was looking at a way
how can i
extract something out of these three
because i think they work
together and then put it in with all the
other and i had
quite a few other features that were
also going into this model
so what i thought about is okay the
house
value what zip code are they in
it might be a very expensive house but
they're in beverly hills
so that's really imp impressive or
it might be not as expensive of a house
but they're in the midwest where i live
and then a lower house value might be
impressive
so i did a contrast i took the
average house value and the house valley
combined them
by taking house value minus average
house value
so if you're below the average that's
going to be a negative number if you're
above it's going to be a positive number
and we're
really looking at just how much you
differ
by the average house value in your area
i don't want to put absolute values
around that because
it's important whether you're above or
below
that average that's part of what i'm
tracking
but then another thing you'll tend to
notice in data is as
people age they trade up they buy
different houses
not always but on average you'll have a
bigger house
as you are older maybe not bigger size
wise but it'll cost more
so that's why i then took the whole
thing
divided by age i'm now doing sort of a
normalizer
and the units are different here the
houses i mean if i'm dealing in u.s
dollars the houses will be i don't know
100 200 300
in that order of magnitude whereas the
age is going to be
100 and below or slightly above
so that i squared the age
to give it some size against
the the house values that i were
subtracting
and this became a feature that i was
then able to calculate
i don't really care what this value is
it's not like i'm looking at it like
bmi's
it's just another piece of information
that i am giving the model along with
all the other socioeconomic
information and it lets the model
then have less to otherwise the model
has to figure out how to synthesize
and create these formulas on its own
it's
information in a more pure form that the
model can deal with
models are inherently mathematical so do
they really need
us we humans to be doing this feature
engineering
for them well what i continue to read
about in kaggle competitions they they
tend to
but this was something that interested
me so i wanted
to take a look at really what types of
formulas would be better to be
engineering for these models so i did a
research paper a few years ago
that i published it's also gotten a
number of citations so it's
been useful at least to somebody and
what this
what this paper looks at i'll probably
do a video about it in the future
is i took a bunch of different formula
types
took those 16 or so i believe
and created data sets that
had just random data and i looked at how
well
can these various models interpolate
values so synthesize these equations i'm
just
asking them to pick values that are
right in the range
of the values that i gave these types of
models don't typically extrapolate well
so i didn't give them anything outside
of those ranges
and i looked at for neural networks
random forest support vector machine
and gradient boosting how well
how low of errors did they really get
synthesizing all these various equations
and you can see the results here
there are some real differences so the
different model types
are not always creating the best
of models when
this math is not performed for them
thank you for watching this video i hope
this was
a good crash course in feature
engineering at least the approach that i
do when i'm doing it completely
from hand i typically just look at my
data see where
my model is predicting very accurately
and not so accurately and i just
look at how can i engineer additional
values that will really
segregate those
confusing rows that it's just not
predicting that well on
and maybe give it something that will
give it a bit more lift it certainly has
worked in a couple of the kaggles that
i have worked on particularly for
tabular data this doesn't work so well
for
image data because it's not like you're
dividing one pixel by another pixel
all right if you find this kind of thing
interesting please subscribe to my
youtube channel
i'll be definitely talking about more
things feature engineering because i
find it to be a very fascinating topic
