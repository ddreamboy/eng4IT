big data analytics helps organizations
harness their data and use it to
identify new opportunities that in turn
leads to smarter business moves more
efficient operations higher profits and
happier customers understanding this we
have come for this tutorial on big data
analytics now before we go ahead with
the session I'd like to inform you guys
that we have launched a completely free
platform called as great Learning
Academy where you have access to free
courses such as AI cloud and digital
marketing you can check out the details
in the description
below now let's have a quick glance at
the agenda we start off by understanding
the concepts of big data and Hadoop then
we will learn about hyve and then
finally we will learn about spark so
today we will be covering um Big Data
what exactly is Big Data and then we
will look at something called Hadoop
ecosystem and Hadoop architecture
basically so these things are the
foundation for all your uh machine
learning statistics spark anything that
you are learning the foundation is this
so today we will be understanding a big
data and Hadoop ecosystem we'll ensure
that we go through all the basics and
all so my name is Ragu Raman you can
call me Ragu uh I have been in the IT
industry for about 12 years as of now
and I have been working on Big Data
platforms for for about 7 years 7 to 8
years uh from 2011 onwards I started on
Big Data I trained people on Hadoop
spark you know the Big Data uh typical
tools that we use uh I train a lot of
people from corporate backgrounds I'm a
regular trainer at Flipkart GE EMC and
and many other companies the definition
of Big Data if you Google you will get
first thing you can just go to Google
and say what is Big Data you will get
tons of Articles PDFs and presentations
that is not what I'm going to teach you
right what I'm going to teach you is
that let's take a practical use case
right let's take a practical use case
and try to understand how big data
actually makes sense right so first of
all big data is related to it so you
need some level of it knowledge to be
fair right this has something to do with
it uh I will share my personal
experience okay so when I started
working in 20072 2008 and all okay I was
working uh with a company in Bangalore
and we had created an application a
sales application so basically what we
do if you install this application there
was a retail company uh the customers
will input all their sales data in that
you know so they will say that today
this much quantity item sold know sold
and sold and sold and this application
will capture all the data okay and store
it now that was a very simple world we
did not had mobile phones we did not had
Facebook WhatsApp nothing was there 2005
right and we were using an rdbms system
to store this you know what is an
rdbms okay at least you good at that
right so this is your culprit we are not
going to use this kind Big Data okay so
rdbms is what we were using to
traditionally store the data right and
what is an rdbms system it has a row
column format you have a table you
insert the data now if somebody is
teaching you rdbms they will say that
well rdbms is great can store everything
you know that's all fine but unless your
start data starts increasing right so
originally rdbms can store everything I
don't have any I'm I'm not against rdbms
or something even for transactional
management we use rdbms there is no
doubt about it MySQL Oracle we were
using MySQL actually so whatever the
customer is inserting this MySQL will
capture it end of the year you do some
processing you will understand how many
items were sold everybody is happy happy
this is what was happening in 20 05 fast
forward to
2011 I working with IC Bank IC Bank want
to migrate to Big Data platform so 2011
we run a project in which we migrated uh
the entire ICC Bank into Big Data
platform okay so I was part of that
project I was not doing everything in
that project so what is the problem with
icci bank right so that is a question
why ICA Bank want to go to Big Data
right now this example will also help
you to understand the the IT world right
so for people who are not coming from
the IT background they might need to
understand how things are working right
so we we went to IC Bank we spoke with
them we said hey what what do you want
they said you know what we are facing a
lot of problems we want to go to Big
Data we want to go to Hadoop we'll see
what is Hadoop later so we asked what is
a big deal what is your problem so there
were a lot of problems which they were
facing one of the problem is that if you
look at this rdbms system right
your typical rdbms system what is rdbms
systems doing they will store the data
in the form of a table right now if your
table size is very very small it's not a
big deal even if you look at my SQL or
if you go to Enterprise scale Oracle and
all you know they can store gigabytes
and sometimes up to terabytes of data
but as the size becomes big so let's say
I ask you okay you want you have to
store the data in Oracle the data is
keep on increasing every week you are
not ready to migrate anywhere Oracle if
you talk only about Oracle they have
storage separate and dbms engine
separate I don't know how many of you
have worked extensively with Oracle in
Oracle what I can do I can purchase
Oracle and I can add storage boxes as
many as I
want you know so these boxes will be in
the network so my database engine will
be connecting to them and storing the
data whatever data I'm giving so you can
ask argue saying that I can increase the
storage capacity and then Oracle can
store as much data as you want there is
no doubt about it right but there is
also limitation for the storage one
problem second problem is that as this
box increases the storage in Oracle
increases you have to divide your data
you have to partition your data so let's
say for example you know uh you are
working with Oracle okay so let's say
you working with Oracle and let's say we
are looking at U iPhone
sales iPhone right so you are selling
iPhone let's say you're working with
apple right so one of the example is
that if you are creating tables to store
iPhone data typically if you look at the
schema and all what we will do is that
there will be one table okay there will
be one table wherein you will be having
properties like uh you know how many
items were purchased at what rate they
were purchased how how many people
bought so on and so forth so this will
be your items table so in the items
table you will have all the data related
to the iPhones you were selling right
you will also have another table where
there will be the user
data you will also have the user data
user data means which customer bought
from where he bought all this data this
is called
normalization you know what is
normalization in rdbm systems in order
to avoid duplication of data what we do
is that we normalize the data we always
keep the data in different different
tables and whenever you want to get the
full data what you
do joints right so if I want to get both
the data from this I have to do
something called a join query I I fire a
join query and all the data from here
will come and join and produce me the
output this is good if you're having
small amount of data what if you have
terabytes of data here and here my join
query will fail
right or the join query has to wait now
another point that people say is that if
your rdbms is becoming bigger and bigger
why don't you partition your data that
is possible I can have like four boxes
my one table okay if this is my table
right it can be logically partitioned
and kept on four machines if it is
becoming bigger and bigger okay this is
possible there is something called
logical partitioning look at the iPhone
data I can say all the data where people
from India are buying the iPhone go to
this machine that's called logical
partitioning based on the country column
so here I have all the India data where
people are buying iPhone here I have the
US data where people are buying the
iPhone but the problem is here I'll have
the Burma data also India data will be
let's say 2 terabyte China data will be
3 terabyte this will be KBS actually so
the problem is that if a table is
becoming so big okay I can cluster it I
can take four machines in my BMS or data
warehousing architecture and I can say
partition the data because if a table is
so big if I fire a query the query will
take a lot of time so what I do I say
that okay don't store the Full Table
divide the T table or partition the
table to partition I will say that you
have to use some Logic for partitioning
I cannot physically divide the data
that's not possible in rdbms I cannot
say that take a table and cut it into
four doesn't work so I have to logically
partition so I have to say that there is
a country column how many countries are
there in the world maximum 155 right I'm
not much of a geography person but I
think somewhere around 155 right or at
least people who buy iPhone 155
countries are there so I will say take
the country column so everybody who is
buying an iPhone will have a country
column So based on the country column it
will partition your data right now I
have four boxes this box has India data
us data Burma data and something data
again same story if you're running the
join query this box has to process all
the India data for join query this box
will complete very fast this guy has to
wait so again you're so the problem is
that traditionally people were using
rdbm systems and data warehouses to
store the data now what is a drawback
one of the drawback is that as the size
of the data increases your processing
will become very very slow right you
can't do it now uh the problem with
classes in Big Data is that there are
lot of Technologies so when you start
talking about them why don't you
denormalize the
data why don't you denormalize the data
for example all the transaction data
user data everything I'll keep in one
table is that
possible H one duplicates second what is
the maximum number of columns you can
have in a table rdbms table th000 Oracle
says th000 what if my data has more than
thousand columns if I keep all the data
I can't do so that is the reason they
don't provide the denormalized solutions
if you look at rdbms systems they will
always say normalize your data so that
it will be represented in different
different tables and whenever you need
the answer you do a join and you get the
result second drawback this is only
structured data meaning anything which
you can represent in row column I can
dump here like an Excel sheet data right
can I dump images and audio files in
this
table yes or no can we store
unstructured data in
rbms
yeah yes you can there is clob blob and
all objects right but can it process
unstructured data not possible so in the
modern day if you look at the modern day
what is our problem so if you look at a
typical uh uh Bank like so let's first
complete this part so one of the
drawbacks of rdbms is that as the size
of the data increases it cannot cope up
with processing one of the problem right
second problem is the
schema meaning anything which you can
represent in a row column format I can
dump in an rdbms anything apart from
that I have to figure out a solution so
what is the actual solution if I have to
store images and retrieve them in real
time how does Flipkart stores the data I
was working with Flipkart for some time
flip cart has around 10 million products
each product has 10 images that is 1
billion images so when you click on a
iPhone x picture on flip cart flip cart
will say that come after 10 minutes or
immediately
display have you ever thought how it is
happening good question right how it is
happening right so do you think they're
dumping in an rdbms and picking it from
there 1 billion
images no no SQL
databases there is a whole Arena of
databases called No SQL fortunately or
unfortunately it is not in my course so
I don't have to teach right but if you
are going to so your uh end goal is to
become data scientist and I think that
is why you're learning this course right
you're learning spark and other things
right so I would suggest you to Google
and learn a little bit about something
called No SQL databases if you have
heard about m ODB Cassandra hspace Neo
4J vmot these are all popular vendors of
nosql database so nosql databases will
help you to store and retrieve
unstructured data at any speed you want
so what Amazon amazon.com actually does
Amazon has something called Dynamo DB
Dynamo DB is an nosql database so in
Dynamo DB I don't have a row column I
have key value pairs so what they do the
key will be the product name
say you clicking on iPhone x iPhone x is
the key the value will be all the data
you have so the moment you click boom
your page is there so now SQL databases
are really faster also it helps you to
store unstructured
data how are you booking a cab using
Ola right I go out I didn't get a cab
Yesterday by the way in Chennai but if I
try to get a cab using Ola right I open
up Ola how many customers Ola has
millions in India and all of them are
booking at the same time right do you
think all your requests are taken by an
rdbms and search in the table and give
you a cab does it work can an rdbms
system handle 30 million 40 million
concurrent sessions on this amount of
data calculating route mongod DB no SQL
database Ola runs on mongodb I was
working with Ola for one of their design
problems actually so I didn't design
anything but they told me that they are
using mongodb so the qu the problem is
the real the the new world that we see
today does not fit our old
description when I went for my first job
interview in
2006 my technical guy who interviewed me
asked what is your strongest skill I
said Java he said you're selected
because Java is like if you know Java
you will get a job without an interview
now if I say Java he will not allow me
to sit in front actually because nobody
I mean Java is still good but the new
world demands new skills actually that
is my whole idea so the second the
drawback of your rdbms is scalability is
there okay third is
price price is a problem right I mean
obviously if you calculate the cost if
you look at Oracle and all they are very
very costly
Solutions IC Bank was paying around 30
crores only in support fees to Oracle 30
crores not even an year I think 3 months
they were paying 30 cres 40 cres only
for support for Oracle because it is an
Enterprise grade product right you have
to pay money there is no other way all
right now if you switch to nosql
platforms they are all open source you
don't have to pay this much amount of
money but that's not our discussion I'm
just saying so all this problems means
traditionally storing the data on an
rdbms and retrieving is not good for the
modern day in the modern day what is
happening is that every company want to
collect and analyze the data be banking
industry e-commerce industry or any
industry IND that you want they want to
collect and analyze the data so to give
you an example if you visit Amazon.com
or flipcart.com what is happening they
start tracking from which IP address you
are uh visiting the website how long you
are clicking on a page which items
you're adding to cart which images
you're clicking which offers you are
clicking all this data is analyzed to
understand how a customer will
behave that is how they do marketing
right so the more you start visiting the
e-commerce companies you start seeing
recommendations and all the more the
more you visit Amazon they will start
recommending products to you asking you
to buy so they have to collect all this
data so when I was working with Flipkart
flipcart said that they collect data
from the apps that you're using from the
website that you're having even from
social media Flipkart is very active in
social media like Facebook Twitter all
these places they are active so if
anybody tweets about flip cart or visit
their Facebook page all this data is
being
collected now this so This is actually
called a big data so the technical
definition of big data is this huge
amount of data which you cannot process
using your traditional methods that is
called Big Data right so why that is of
interest to us we are going to figure
out how to store this data huge amount
of data right and how to process this
data so using the traditional techniques
you will not be able to store it or
process it that is not possible at all
so we will be seeing how you can use
Hadoop as a solution to to store all
this data and process all this data
right so interrupt me anytime if if you
want to ask anything I'll keep on
talking yeah so when I say no SQL
databases are there it does not mean you
rbms is gone so many people ask this
question okay rdbms is the only solution
for transaction management so even if
you're buying something from flip cart
who will uh complete your transaction
Oracle probably or any transactional
system for transactional management and
why is that because rdbms systems are
the only ones which can ensure the asset
properties right if a transaction goes
or not goes you bought something from
Flipkart tomorrow Flipkart cannot come
back to you and say that I don't really
think I don't know whether the
transaction happened or not probably you
got the money it's not possible right
then you will start complaining against
flip cart right so flip cart comes to
you and say that you know what we are
using Cassandra as per Cassandra your
transaction is we don't know we'll come
back to in another one week is that
possible no so transaction management
very good question uh is still handled
by
rdbms technically it is called polyl
persistence so I'm not a person who use
a lot of technical words but polyl
persistence that means uh uh coexistence
of uh traditional databases and you know
the other ones even if you look at
Amazon or any company you know they will
have their transactional data and a
bunch of no sqls to handle other type of
uh things okay polyl persistence well
you don't have to really I mean I'm not
a person who really use a lot of uh uh
what do you
say polyglot
persistence persistence is e or a I
don't know even my English skills are at
stake when I'm training people
persistence is e I think right what do
you think take a vote for that
probably no no no no yeah so that's what
we have not come to Hadoop so far uh I
will explain Hadoop is not a database
so teaching big data in 16 hours itself
is a challenge actually because it's a v
topic actually in the world of Big Data
there is a separate category of systems
called nosql databases and they are
databases so what is a database does
realtime uh you know queries right and
in that area we have mongodb Cassandra
hspace and these guys now these guys are
used in real time if you want to fetch
the data say for example you are opening
an e-commerce website probably it is
given by a nosql database right or
millions of people are booking flights
through to clear trip.com and clear trip
has to identify you know how many people
are booking in a particular route so
that it can give a better offer this is
probably done by reading realtime data
from a nosql system our area is not
nosql nosql is a DB actually it's a
database our area is not uh not SQL we
will come to Hado right now so this is
that technical term for coexistence of
DBS and no SQL and all so now let's
understand um so we establish the idea
that rdbms is not good for us at least
in the world of big data but don't say
that rdbms is not good because
transaction management is still handled
by rdbms so you cannot say that get rid
of rdbms right now there is a very
important term even though it is not
directly related to your syllabus but I
think it makes a lot of sense how many
of you are aware of this term
raise your hand I will raise oh lot of
ETL guys good to know that so you know
what is an ETL right and some people do
not know what is an ETL that's fine so
ETL stands for uh extract transform and
load so what is the idea here is that
see if if I'm working uh with a company
right the company will have rdbm systems
right so these are all rdbms systems so
probably this guy is using MySQL
and he's also using
Oracle right a lot of rdbm systems are
being used by a company right now these
are all serving the customers so if you
take the example of ICI Bank all their
core banking
data is an
oracle all their core banking data is an
oracle so if you're making an online
transaction the data hits Oracle all
their CRM data is in my SQL what do you
mean by CRM customer relationship right
your name address blah blah blah so they
have these two type of data now is this
the only type of data that they have no
IC bank had Al also acquired a lot of
other companies when we were working
with them so their subsidiary companies
were giving them data in the form of XML
files XML right now IC Bank also had a
Facebook page okay from where they were
getting around 10,000 people visiting
every day the Facebook page and they
used to click on some item so they have
to do this cross selling upselling if a
user goes to Facebook page and say that
I really like your loan take more loan
right so that is your cross selling and
upselling right so what they do they
collect all this data from social media
that comes in the format of what
Json JavaScript object notation Json key
value pairs basically so this is where
their core banking data this is where
their CRM data this is from their
partner companies this is from the
social media data they are getting okay
and they had customer care this one the
social media data they want to do
analytics so every day 10,000 people are
visiting their web page they want to
know a person clicked on let's say a
lawn offer and how much time he stayed
on the page and what age group he
belongs to so that they can recommend
products recommendations actually a lot
of recommend
so social media data was being used they
did not reveal the all the things that
they are doing because that were
internal but basically using the social
media data they were doing
recommendations so they will download
all this data so if a customer is
clicking on the Facebook page they will
get the access of this guy if you are
clicking on IC bank's Facebook page they
will know your age your occupation
everything you shared in Facebook that
is by default given to me in most of the
cases you can restrict the information
but most of the public customers they
will share this so by downloading this
data today they will understand 10,000
people clicked and out of them 5,000
were youngsters and they clicked on this
laan that means the trend is towards a
particular bike laan if we give an offer
on bike laan youngsters may actually buy
it so this is analytics right so all the
social media data they can use in
multiple ways actually so this was
getting in the Json format actually and
this type of data was there they also
were collecting their customer care log
files so that is again flat file it was
coming in a format of flat
file flat file means text file so
customer care log actually means the
chat logs that they were having also the
call logs they don't record they don't
get the actual call but how long the
call happened and what you pressed so
when you call customer you press right
this this this huh that is actually
captured and then they want to figure
out how many people are called on what
category how much time they spend right
so if you have clicked on a complaint
button and you talk for 1 hour chances
are very high you are a bad customer
right I mean whenever you're scolling
probably right so they want to figure it
out right so that is the call record
that was coming in flat file this format
right and at that point in time IC Bank
also wanted to offer a solution so
previously what they used to do uh their
agents were there people who go to your
house and make you buy loan and all so
they used to go to every place so this
agent will get a list and he will go to
that place and he will go to your house
and say that sir please take a car loan
you will say no he will say bye he will
get money he is getting salary right so
this technique was not working because
if I get a list okay one customer will
be in uh here this is what place what
no thisr right one guy will be INR
another guy will be in some other remote
place in Chennai so by by the time he
travels from here to that place day will
be over so he is able to cover only two
customers so what ICA bank had in mind
they will track his location he has a
device okay and give him customers in
real time so if he's in kanja he'll get
all the customers nearby him so this guy
has to go right he cannot say I can't go
so he will the customer application it
was a tablet will track this guy and
wherever this guy goes the nearby
customers will be provided to him so he
can cover more customers right and this
has a lot of impact because why it was
practical because sometimes the
customers will call and say that please
come I'll give you a loan so what you do
from here you might travel to there is a
European city Paris in Chennai yeah so
you'll go to Paris right and because the
customer called right so you go to Paris
and meet the customer now you don't know
how many customers are around Paris that
this application will track and send to
you so these guys were collecting that
data also from all the tabs and that was
uh quite some big amount of data they
were collecting how many customers they
met and whether they closed it or not
closed okay that was coming in I think
some other format so some format I write
I forgot actually how it was coming so
this is the whole data they have right
and some more data was there and this
data is lying on different different
systems it's not in one place the core
banking data is is an oracle some other
data is in some other place right and if
you want to analyze this data one thing
is that I cannot go to Oracle and say
that I want to analyze your data why
because this Oracle DB is already busy
serving
customers correct this Oracle DB is the
core banking database so if you're
making a transaction Oracle is handling
that right so I cannot go to Oracle and
say that let me run some analytics on
your data doesn't work that way because
then Oracle will slow down I can't do
that so I have to take all this data
from all these places and dump it into a
central place so that is what is you
call as your data warehouse
right that is called a how many of you
know what is a data warehouse so a data
warehouse is nothing but a database only
in one way it understands the language
of SQL but the major difference is that
it is not facing the customer
right so you will do something called
ETL okay so there are ETL tools which
can get the data from all these
places right and this guy will dump it
into a data
warehouse right so data warehouse is
also called as your olap online
analytical platform so on and so forth
and what is the purpose of this guy so
this guy will have all the data from
here okay and you can transform the data
as well so that is why it is called ETL
extract transform and load you don't
just take the data and dump it here then
it is useless right I can read the data
from a flat file and give some structure
and then dump it here like a proper
table format so now all my data is here
from this right and I can run business
intelligence here bi tools Tableau right
if you have heard about micro strategy
Tableau what I can do I can connect a
visualization tool here and it will show
me in charts and diagrams what is
happening I mean this is traditional
Enterprise architecture this is how
people used to work forever I mean every
company you go this will be there by
default they will have these these
desperate uh discrete data sources and
ETL tools so if you have heard about
Informatica Informatica is one of the uh
biggest ETL tool Informatica Talent uh
pentaho ssis so these are all ETL tools
so these tools what they do they will
connect with here and take all the data
and apply some schema and push it to a
data warehouse now the data will be here
so now all the data is in one
place so if I'm Chandra Kar I wish I
would be I can go here Chandra Kar is
who IC Banks CEO right who is at place
so I can go here so Chandra Kar may not
know SQL right you can't EXP expect that
right if you're a CEO of the company I
can't ask you do you know Java I'll be
fired next moment right so but the CEO
want to know what's happening right by
getting all this data only you will
understand what's happening in the
company but you don't have time to sit
and learn all this language or anything
so if I'm Chandra Kar I'll go here I'll
connect my Tableau and I'll ask my
Tableau show me the last month's uh
transaction this quarter this product
this area show me it will show a nice
bar chart
I'm
happy now why this is very very
important if you make any mistake in any
of this this entire stuff will be
changed right and what she sees will be
different
right she has no idea how this is
happening right only you know how this
is happening right so your ETL tool
should run proper the data should be
captured and dumped here and then
analyzed so for CEOs and CTO they use
visualization tools to connect with a
data warehouse then analyze the data so
all the data is inside this data
warehouse right and this is not
accessible by Public public has no
access to this only internal company has
access like maybe CEOs of IC bank they
can go here and visualize the data from
here others public does not have any
access to here right and then they
visualize the data they make a decision
they may not make a decision that is up
to them right in the whole setup there
is no Hadoop so far and this is how IC a
bank was running so far now what was the
what were the challenges they faced one
of the challenge was that this ETL tool
okay I don't know how many of you have
run on ATL tools okay they typically run
on a single
machine okay so they get installed on a
server so there is a limit as to how
much this guy can pull to a data
warehouse right how much amount of data
this guy pull from here to a data
warehouse there is a limitation actually
you can't have terabytes of data being
pulled Point number one point number two
none of this is real
time none of this is real time I go to
Tanish store Tanish is a jeweler stuff I
go to a Tanish store I buy something
from there for 20,000 rupees I use IC
credit card I should immediately get a
message saying that again you shop 20,
20% discount that is real time I can't
do it
here because all this is separate data
and these ETL jobs run at the night so
today morning if I'm shopping tomorrow
I'll get the message 20% off by the time
I'm out of T shop right I may not use it
at all so IC bank's first problem was
that since the data was increasing you
know they were not able to run ETL
properly second problem none of this is
real time they wanted to push customized
offers their requirement was that
whenever any transaction is made they
want to capture it analyze it right and
based on that they have to recommend
something to the
user right say for example you're using
credit card or debit card or even you're
clicking on some link immediately
something should happen now that is not
so easy right because your data might be
somewhere in here Oracle and I cannot
track each and every transaction here
because this is my transactional DB I
can't I can't say that track each and
everything from here right that's not
possible third problem this data
warehouse
this data warehouse is a very costly
Affair I have worked with a company
called teradata they are the leader in
data warehousing
Market I had a US client uh Terra they
were using a product called Terra data
for data warehouse they were paying
around $5 million every year only for
this setup now it is good you can
analyze the data and all that's okay but
you're paying a lot of money actually so
data warehousing is actually a very
costly Affair it's not so so cheap if
you want to implement but companies like
IC bank they must use it right so they
can't say that we will not use it so so
they wanted a cheaper solution they
don't want all their data in this data
warehouse they want an alternate
solution should be real time cheaper
scalable right and anything they want to
integrate you know any any type of data
that comes they wanted to integrate and
that is how they came to us and asked us
like can we migrate to some big data
solution and we recommended Hadoop and
then the migration took a lot of time
that is a different case altogether but
if you talk about Hadoop Hadoop actually
comes
here in a way I can say it is yeah
sorry right
right
exactly right exactly
right
exactly not possible not
possible exactly so here you have your n
dimensional cubes and other things right
in the data warehousing you will have
your what you call um uh schema what
schema you call Star schema snowflake
schema right and so this is actually
olap is actually not designed for Speed
that is true it is only analytical it is
batch job actually right so with this
architecture I can never give a realtime
solution
exactly exactly so a lot of people
argued saying that you can actually beat
it if you do parallel processing there
is something called MPP
right there is something called MPP MPP
means massive parallel processing so a
lot of people came saying that so these
systems are actually Terra data and all
are really faster actually I mean not
like rdbms but they're faster okay so a
lot of people came up and saying said
that okay if your data warehouse is very
slow why don't you do parallel
processing okay you install it on four
five machines and you can do par that is
possible but still the same problem
exist you cannot physically divide your
tables only logical division is possible
like I said country based you can divide
or month based you can divide end of the
day if you want to do a join operation
or something same questions will happen
so I can never propose this as a
realtime solution so that is my Crux of
what I'm trying to tell you and this is
how uh even ICA Bank was running and
cost was another problem they had a lot
of cost involved so and obviously since
they wanted to collect a lot of
unstructured data they wanted to go to a
better solution okay and that is how
Hadoop came into the picture
not really mostly here you have
structure data mostly structure data
only they are used as the final place
where you get the data you know from
there somebody can visualize and in here
you very rarely you modify the data you
don't modify the data because this is
the place where the final data is
available know from where you can
connect and visualize the data that is
all you do or probably you can run a
query and get the output that is all you
do it is analysis only you're not doing
any transaction transaction management
all you're doing here
right now so now the point is that now
let's discuss a little bit about Hado so
what I thought is like today probably uh
we can cover the architecture and little
bit of
theory so that uh tomorrow so if you
look at your syllabus that I'm supposed
to teach U we have something called
Hadoop and Hadoop architecture that is
anyway required right and then you have
map reduce now there is a catch here map
reduce is the default programming
framework on top of Hado so if anybody
want to analyze the data on top of Hado
they use something called map reduce
Point number one now map reduce came
long back okay and today there are lot
of ways to analyze the data not only map
ruce now as per your syllabus the
curriculum says teach map ruce and then
some Concepts in map R and I will be
doing that but one other thing is that
if you want to learn map ruce ideally
you should be an expert in Java because
map ruce programs are written in Java or
any other programming language so you
can write in Python and other things but
natively people write map ruce programs
in Java now I know that not all of you
are familiar with Java and comfortable
with Java so if I talk a lot about Java
it may not make much sense out of you
but then why you are learning map reduce
is to help you in spark so next you will
be learning something called spark right
16 hours of spark content will be there
spark also uses similar Concepts as map
ruce so even if even though you're not
able to understand each and every line
of code that I'm writing that's okay but
you need to understand the logic how it
is working because Java programs not
everybody will be able to follow and
understand and Deb so that is not
expected also from you but if we discuss
map reduce I write a map program and we
run a map reduce program you should be
able to understand what is happening
inside that right logically not like
syntax wise line by line wise right so
once we complete Hadoop architecture
like tomorrow I will be discussing map
ruce Theory and map ruce programs will
be running and then in the next 8 Hour
syllabus we will be completing bit of
map reduce and going to something called
Hive P Hive so Hive is your data
warehouse in in hadu so that is more of
SQL side so you can understand that
point yeah somebody had a question sorry
someone asked a question in the middle I
just said I will complete who
asked nobody
asked okay I mean so um just rub
this right now what I'm going to do I'm
just going to delete this part for the
time being okay I'll just remove this
part for the time being
yeah H that's what so now what is
happening the traditional data
warehouses are there like the data and
all but they are not so popular and in
the market like people used to say if
you cannot convince what you
do you confuse
people and it is very easy to do that
right because why am I saying this
because I see a lot of people getting
confused because this so when Hadoop
came Hadoop became a very popular
solution for data warehousing right and
since Hadoop is open source and you can
read the mckeny report it will be only 1
by4 of cost of a traditional data
warehouse so what will happen everybody
will use Hadoop obviously why should I
pay extra money but then what is a
drawback the traditional data
warehousing companies they cannot sell
right so if if today you go to the
website of Informatica or even Terra
data they will say that we are selling
uh Terra data plus
Hadoop previously you had to give me $10
million now you give me 9.5 take it
that's useless actually because why
should you buy Terra dat but they are
saying because teradata has a name in
the market that is a different thing
right so teradata will say that buy our
data warehouse we will also give you
Hado that was cheaper right previously
you paid us 10 million now pay 9.5
you're like okay great solution I'm
getting Hado also you don't actually
need to use ter dat
you can build a custom solution but the
problem couple of problems are that one
uh the name of the company like Terra
data and support also from them second
it takes a lot of time to migrate to
these things one fine morning you cannot
go and say that okay from Tomorrow
onwards we are using only hadu doesn't
work like that right in a company you
need time to do migrations and processes
and all so people stick with their
traditional tools so what I have been
seeing uh in some of the companies where
I went for Consulting they have
completely migrated to Big Data
platforms like hadu and they keep Terra
data and all in a very less level like
the hot data we call right hot data cold
data warm data we have so the
immediately accessible hot data will be
there in Terra data and all and you have
the cold warm data which you don't need
to immediately analyze you will dump it
into Hadoop or something and keep it so
you can reduce the overall cost in this
architecture
right so now speaking of Hadoop uh our
architecture is here still I just
changed little bit okay I'll just write
it
here uh the question is when all this
happened right so lot of people so I
handled a lot of trainings in Hadoop and
Spark so a lot of people ask me that
okay so we are learning a completely new
technology called Hadoop and I will say
that you are wrong because Hadoop came
in
2005 so almost 13 years now so it's
almost like very old uh but for for
people it is still very new for those
who do not understand Hado so uh in 2005
this framework called Hadoop was created
okay long back right uh and there were
two guys called duck cutting and Mike
Cilla who created this framework they
took the original idea from Google so
Google was already doing a lot of
distributed computing and Google was uh
dealing with a lot of Big Data right so
they published couple of white papers
and this duck cutting and my Cilla they
took the idea from Google and then
created a framework called Hadoop and
later they gave it to Apache as an open
source project so Hadoop is a open
source project actually which means it
is with Apache right now you can
download it for free install it for free
you don't have to pay any money but if
something is really free you have
drawbacks also right correct for example
uh you have Android phones I have an
iPhone I used to use Android phones but
you know the difference right the
difference is not show off that is not
the difference the real difference is
that Android phone might get a lot of
bugs which may not come in an Apple
phone if you have used both you will
understand the problem is Android is
open source right and open source means
every company will modify according to
the requirement as Samsung phone will
have their own Android right so
accordingly a lot of bugs may come the
platform may not be so stable as it is
if it is an Apple phone only Apple
produces it right and if there is a bug
also if I call Apple or somebody call
Apple they will fix it because there's
only one version of it available and
they know what bucks may come may not
come so the same story here originally
it was Apache Hado and even still you
can go to the Apache website and
download Hado but soon people realize
that if they are using Apache Hado it is
open source uh but if something does not
work something crashes nobody's there to
fix it because it is open source and
accordingly a company called Cloudera
King there is a company
called clera you might have probably
heard clera is the first company which
created uh uh a commercial distribution
for Hadoop
and related tools so today you can go to
Cloud era and ask them to give give you
Hado what is Advantage you will be
getting the same Hadoop almost similar
what you find in Apache website but if
you install it and if something is not
working they will fix it you have a
support since you paying money you get
support just like a vendor right so one
of the companies which are which is very
popular in the world of big data is
cloud era so they Stell Hadoop spark
everything as a bundle okay even your
lab actually has cloud data distribution
the lab we are going to use actually
runs a cloud data distribution uh and I
can also use the this thing right if
internet is there because I just want to
show you this so if you just go here and
simply type not now probably you can do
it later simply type Cloud era you can
see Cloud era's website
right and uh Cloud era came in 200
eight so today if you're buying any big
data platform the leading vendor is
cloud guard so they will give you
everything packaged in one one product
like Hadoop will be there no SQL hbas
will be there other related tools will
be there data science tools will be
there so commercially they are actually
the number one provider cloud data right
and somewhere in 2012 we will discuss
Hadoop in- depth okay I'm just giving
you some idea we'll discuss the
architecture and all there is also one
more company called hoton
works hoton works came in somewhere in
2012 okay and honb is also the next
major competitor to Cloud era the major
difference is that honb sells original
hadu Apache hadu they don't modify it
clera sells a modified version which
they say it is better so
they no no so it is actually Apache open
source license so what Apache says is
that anybody can modify it and sell it
only thing they should also provide a
free version and they do it so if you go
to Cloud era you can download the
Cloud's product for free you don't have
to pay any money but you will not get
support so what people do they will buy
only with support right so if I want to
sell Hadoop I can start a company I can
download from Apache modify the source
code I can sell it to you guys but two
things one I should give a free show
offer also no support second thing that
is the story with every open source
right you have ubu Linux you have Red
Hat Linux what is the difference Red
Hat yes not open source whatever their
product because hoton works is open
source hadu and you can download from
their product cloudas is not 100% open
source it's a custom product but you can
download it for free from their website
and install it on as many machines as
you want but you will not get technical
support So ideally people will not do it
right because you need technical support
so you will pay money and get it
actually right but these are the
companies the hoton works and clera as
the leading vendors in the Big Data
World they also provide certification so
if you want to write a certification
exam probably after the spark class you
can try that uh claer provides
certification exams in the Big Data
world you can write it and get there is
also one more company called mppar the
three are there there are many companies
actually the most uh common ones are
three there is also somebody called
mapar apach doesn't do it right it's
open source Community it's not a company
it is like you and me so I can edit for
free you can also edit for free but you
will not so if something crashes what do
you do you post a question in the
community hey yesterday my old servers
were crashed I'm fired from the company
please help me to fix it and you don't
want to do it in Enterprise world right
so the real thing is
to be very fair with you why still
people are trusting all these companies
Microsoft or apple or Oracle they get
support right something doesn't work you
cannot be able to fix right you have to
call them and make them fix so mappa is
the third company which is sort of like
popular this company if my information
is correct was created by a couple of
people who quit clouder and hoton works
Indians only right and they found this
company called mapar mapar is very
popular these days actually no very very
popular vendor
the highest paid Edition is also
map cost wise they are bigger than cler
and hotworks cloud and hotw license is
cheaper than mapar map's license cost is
very high actually map's architecture is
also very different not like traditional
Hadoop architecture right so for that
reason uh like I went to GE I went to
Flipkart they are all using either hot
works or Cloud era they don't use map
much
they also downloaded the open source
Hado modified it and selling it same
same story but they made a lot of change
to the architecture of Hadoop actually
apart from this IBM has their own flavor
it's called Big insights Microsoft has
their own flavor that's called HD inside
H HD inside is there Amazon has every
company has an addition of their own uh
Hadoop and B data platform but most
popular ones are claa hoton works and
mappa these guys are the most popular
ones actually
[Music]
so huh so for a developer there is not
much difference for example if you're
writing a map reduce program it'll run
the same way on all three platforms
there is not much of a difference the
difference actually is that hoton workor
is totally open source they sell the
same what Apache gives Cloud modifies it
a bit and they say that with the
modification Hado is more cable that is
their claim we don't know it's all
marketing gimmick it is same like buying
an Android Samsung mobile phone and HTC
mobile phone what is the difference end
of the day internally it is Android but
Samsung will say that our Android is
better more uh bug free or something HTC
will say that this phone will never hang
so only HTC or Samsung know what they
have done inside the phone but end of
the day if I download at say WhatsApp
it'll run on both the phones right
because it is Android so same thing only
it's just a commercial uh distribution
same story with Linux you can buy from
Red Hat there is also one more provider
right
sus Red Hat Linux sus Linux internally
everything is same if I type an LS
command it'll work both ways kernel
might be slightly different but the end
of the day they are paying open source
original Linux only so same story here
uh but considering the platforms Cloud
seems to be more more popular because
they got a head start 2008 they started
right long back they started hoton works
is also gaining a lot of popularity
these days I've have been seeing
that so we are using Cloud era in our
lab we have a cloud era distribution
okay so but we have not we have not
talked about hadu right so we have been
talking a lot about the company but when
did we start the session I didn't check
2:30 so when do you have brakes and all
I think 3:30 right
now see I don't have any control on that
brakes are controlled by
uh her right I don't have any control
she has to approve I don't have any
control trust
me uh see she is talking about schedules
and all I don't know any of this okay I
can teach I can stop you say stop I'll
stop okay by the way if you don't like
my class you can work out also but you
will not do it because you paid money
right if it was a free class half of
them could have walked out but since you
paid money you will stay I know
that
okay yeah sure please ask ask as many
questions as you
want yes
correct no no map reduce nobody is using
even in the Enterprise map redu is gone
spark is that is what I'm saying in your
curriculum there is map ruce and why you
are learning map ruce two reasons one is
migration a lot of companies will
already have a lot of map ruce programs
and they want to go to spark so if you
want to go you should know what is map
ruce second reason even for learning
spark the concepts of map ruce are
important spark also was built on the
basic idea of map ruce so how map
produce Works how the program runs these
basic concepts if you can understand
that will help you to learn spark in a
better way that is the reason we are
learning spark uh map ruce actually in
the Enterprise totally gone there is
nothing called map
produce no spark uses python in your
course uh that is the reason I'm not
teaching spark so initially uh Great
Lakes asked me can you teach spark I
said yes and then there was a problem
because I teach spark using
Scala huh I a Scala developer you you
are you are learning spark using python
spark can be programmed in four
languages Scala python R and Java four
languages are there and you learn with
python I'm not a python guy so that's
the reason I'm not teaching spark I
actually like to teach spark because
most of my trainings these days are on
spark actually but I know python also I
can teach but not immediately I said
we'll pick it up later probably and and
teach that
sorry Scala similar to Java
no not like uh similar to Java 8 if you
know Java 8 glorified verion of java ah
it's a glorified version of java
actually Java 8 was developed based on
Scala so Scala is another programming
language it's a very nice language
actually uh but I don't know people do
not really like it okay so like it in
the sense it's not as popular as Java
right Java is very popular but Scala is
not so popular but it's a it's a very uh
interesting programming language
I used Scala a lot in my programs and
trainings and all anyway so any other
questions before we go to Hadoop and
architecture and
all not many
questions so if there are no questions
ideally either people understood
everything or they understood nothing
right
so
dat yeah you can consider but it is not
just a data warehouse that is a
difference so uh you will see soon uh
what is a difference because if I'm
buying a traditional data warehouse it
can do only one thing it can become a
data warehouse I can't do anything else
Hadoop is not like that it can become a
data warehouse and much more and we will
see
that no they still so your architecture
is
huh see now the situation is very
complicated actually because if you are
starting a new company and ideally you
don't have much money what you
do you go to Cloud right so every
solution is offered in the cloud and
typically uh companies will go for this
Hadoop and all are available in Cloud
also so they will go for cloud
distributions and so always you cannot
say that Hadoop is a 100% replacement of
data warehouse okay it can work as a
data warehouse but performance
considerations are there so sometimes
you might need some reports within
millisecond second 2 second which
probably Hadoop may not be able to do
Hado is a batch processing system it's
not real time so if I have terra data or
what is that EXA data Neta know these
guys are really fast right MPP so what
companies do they keep the hot data
there probably my company has 100 terab
data in that 2 terab Data I need every
day very fast I'll keep it in teradata
98 terab I'll dump it in Haru I need it
once in a while and even if it takes
some time it is fine okay same story
with the cloud so most of the
organizations are now deploying
everything over Amazon web services so
Amazon offers fully managed Hadoop
Solutions on the cloud like you can go
to Amazon ask it give me a Hadoop
cluster in 10 minutes you will have
fully functional Hadoop solution
everything installed up and running
servers everything automation right so
then they also give you TR data
warehouses they have something called
what is the data warehouse red shift
there is a solution called red shift red
shift is a online cloud-based data
warehouse so Amazon offers red shift you
if you want H data to be stalled so like
I said everything cannot be given in one
solution you need cheaper you need the
same speed you know everything you have
to package in one solution it may not be
possible right so like someone was
suggesting right you can compromise
speed either right or uh realtime access
right so same like that so had do is a
solution most of the people adopt but
not everything can be guaranteed at the
same
time okay so that takes us to the
question what is Hadoop and why this is
so popular or what is the architecture
of Hadoop it's fairly easy to understand
actually because the idea is very
simple I have a single machine
how much amount of data can I store in a
single
machine what do you think I I'll give
you a server how much will you
store based on
what okay so and that is based on
what yeah so storage capacity how do you
decide if I give you a
server how many hard disk or how much
space you can dump
yeah so I'll give you a server how do
you decide how many hard disk I can add
in
that motherboard right motherboard is
who decides so the motherboard will say
I'll support 10 hard disk you can have
10 hard disk so the idea is very simple
so people initially thought I can use
one machine and when that machine was
not sufficient what they
did no no
no no no no no no distributor Computing
did not came so early
right ah they added dis they reached the
dis so one machine is over what do you
do no no I don't want to
Archive super Computing uh no I want
cheaper I can't sell my
house now another machine is what I'm
going to talk but in between something
came
external storage Nas and all right
network storage probably you guys are
not much from Storage background there
was something called network attached
storage very very popular have you heard
about EMC and all company called EMC
square they survived only because of
these things so you have something
called Network attach storage Nas box
you can buy a
box install the Box anywhere and you can
dump the data there so the system can
dump and process from there that is
called Nas so that will support more
storage actually but it is not within
the
machine when Nas was not enough came
sand storage area network sand are even
popular these days okay so a full room
full of hard disk you get a fiber
channel connection so if I can dump all
the hard dis there I can store as much
as I want but the problem is even if I'm
using a
sand this is my sand
this this give me unlimited storage okay
problem is it gives me only storage no
processing if I want to process the data
can one server process all the data in a
sand no not possible right so processing
cannot happen storage can happen in a
sand and Sands are very costly clumsy to
manage you need separate people to
manage and all so people got fed up with
sand also so Nas was there San was there
so and that is where you started
thinking about distributed Computing
distributed computing is not a new idea
it was already there okay so in Hadoop
what we are doing is very very simple
rather than one machine you take a bunch
of machines okay so I will say the bunch
is just for sake of argument four okay
so I take four
machines 1
two
3
4 these four machines are
let's say Linux machines to be fair they
can be Windows machines but uh the
support for Windows is very limited in
the open source community so typically
we recommend the boxes to be Unix or
Linux so I take a bunch of boxes say
four boxes for servers right and I can
download and install this Cloudera or
hotworks any of their Hadoop on this
while installing it will ask me which
machine is Master which machine is slave
the idea is one machine has to be Master
the rest of them can be
slaves so in this architecture I come up
with an idea one guy is master and three
guys are slaves fine so we have not
discussed what is the master doing what
is the slave doing that's later okay but
you install Hado in a setup that when
there is one master and let's say three
slaves right now the idea here is that
if I want to store anything okay Hadoop
will help me to store among these three
boxes right so I can use the story space
of all three boxes so if each of them
are having a 2 tbte hard disk I will get
a total capacity of 6
terabyte in my Hadoop cluster this is
called a Hadoop cluster a cluster means
a group of computers where Hadoop is
in yeah so Hadoop is a framework
actually it is not a software so one
confusion people have is that is Hadoop
a software no it is a plan
platform okay and I will show you the
different different components we have
in the Hadoop platform so if you install
Hadoop let's say we don't know anything
about hadu but let's say we install it
in a four system setup while installing
it will ask you to select one as Master
remaining as slave so I have one master
and three slaves right and what Hado can
do it can take the storage space of all
these three machines and project it as a
single SLE 6 tbte storage box right so
the beauty of this architecture is that
let's say you start dumping the data and
the data gets stored here and let's say
your 6 terab is over I can add machines
on the fly from three I can expand to
30,000 without shutting down the
cluster meaning your storage problem is
solved you don't have to ever worry
about storage in a Hadoop cluster Hadoop
allows you resizing without any downtime
so I can add more machines or I can
remove machines up to me to decide
right I I'll tell you how the data is
stored I will I will come to so as of
now we have not discussed storage and
processing I'm just giving you a 10,000
foot overview like what is happening in
a Hado cluster so one of the advantage
is that you can start with like three
machines and expand to 30 300 or 3,000
machines
yeah like
is yes you can also add when you have
your data here and you analyzing the
data okay I can keep on adding machines
without shutting down
them you can remove but if you're
removing it'll have a problem for
example if the program is running here
if I remove it the program might crash
because this machine is needed that is
not the idea if you have one more
machine here where nothing is running I
can remove it gracefully nothing will
happen so you are getting something
called scaling out this architecture is
called scaling out meaning you keep on
adding more machines so that uh you know
uh you can get unlimited scalability now
I will tell you how storage happens so
far I did not discuss how storage
happens yeah why is it not
possible which
one this sizing yeah it is not possible
sqls are designed in that fashion if I
want to modify anything I have to first
stop reading and writing from my
database then only I can alter my table
or increase anything or decrease
anything probably I'll be able to add a
machine extra but it doesn't uh make any
sense in a SQL World actually and in a
SQL world I cannot come up with a 30,000
server setup that's impossible cost
First of all scalability like I said the
major difference here is that in the SQL
word logical division of data you cannot
physically divide the data so when
you're logically dividing the data like
I said country India 2 CR people and if
one machine is taking that it'll have
more data another will less data I don't
have any way to manage it here that that
will be handled I'll show you how that
is happening so the B so this is called
a Hadoop cluster so the technical term
for this is a cluster cluster is nothing
but a group of machines right so that is
available in a Hadoop cluster now
typically uh if you go to companies and
check with them like I was with this 20
2 bar7 company called 25 bar7 they are a
call center company and they run Hardo
clusters for analytics and all they had
a 50 machine cluster 50 node cluster
each node has around 256 GB
RAM and 10 tab
storage so 50 into 10 is
what sorry not 10 what am I saying 100
terab storage so so 50 into 100 is
what 5 petabyte of storage they had and
256 * 50 that is a ram capacity of the
cluster that is a very minimal Hardo
cluster cheap Hardo cluster so typically
you'll have 500 knots th000 knots Hardo
clusters because the amount of data
you're storing is in petabytes th000
terabyte is one petabyte actually so
most of the companies they will have
huge amount of data to dump in this so
the size of the cluster will be in that
that uh uh level actually and Hadoop is
distributed meaning if you install it it
is not getting installed in one machine
it is installed in all the machines and
the components will talk so it runs in a
cluster not in a single machine or two
machines actually right so so the basic
idea in storage is that you can add more
machines and get more storage
right no no no no I got your question
what if the master goes down right I'll
come back to that now the slave will be
acting only as a slave actually
yeah so good question another thing is
that let me ask you this question so I'm
asking you to build a Hado cluster with
50 machines take an example 50 slave
machines there is 50 servers do you
really think it is a cheaper
solution in terms of
Hardware like you buy 50 servers or 500
servers isn't that really a cheaper
solution what do you think
yeah so let them
say
huh correct now my point is if let's say
you're buying 500 servers each server
will cost you some money right so if I
say 500 times that money right it is
really a huge amount of investment for
me that is the
reason we call this machines commodity
Hardware what do you mean by commodity
Hardware
Ware commodity Hardware means any
machine without any label you will never
buy a Dell server to build a Hadoop
cluster you will bu build it with cheap
throwaway
servers right you're not going to buy a
Volvo bus you're going to buy a TM what
is that Tamil Nadu I'm not blaming Tamil
Nadu don't take it regionally okay even
Kerala whatever you're not going to buy
a Volvo bus to accommodate your
passengers you're buying a bus you know
which can break down at any point in
time see you want to carry all your
citizens will you be buying only Volvo
buses no right if you look at most of
the buses that are here what is your
idea Citizens need to travel from one
point to another point and the bus
should be there that's all you are not
investing crores in buying Volvo buses
everywhere right you buy just some bus
same idea here in a Hardo cluster
typical Hardo cluster the machines that
we use are called commodity Hardware
commodity Hardware means assembled
servers so you can get it actually so
you're not actually buying an IBM server
because I was also working as a design
architect for one of the Hadoop solution
uh there we got servers for around
35,000 rupees 40,000 rupees 40,000
rupees you cannot buy a server if it is
an IBM server you'll pay 2 lakh three
lakh so this was like somebody will
assemble and give you the only thing it
will work it will crash
also it will definitely crash 100% so
then you will be thinking if I'm having
a 500 not Hadoop cluster if my slaves
are crashing what is the reliability
there I will answer that question just
just par the question for the time being
no no no server is just a big storage
box right storage and
processing yeah so server is what a
server is so this is your desktop normal
desktops right so normally you have
something called desktop at home and all
server is something which can serve
multiple clients in one way it is a
bigger box with bigger storage and
processing so if I want to buy a server
I have multiple option one is that I go
and tell IBM or D give buy me a server
so they will give me a branded server so
they will charge a lot of money they
will also give me support and all but I
have to pay two lakh three lakh for a
server so normally when I say server
this is what people buy but in a Hadoop
cluster you never do that you buy
cheapest servers possible because you
can afford to have failures in a Hardo
cluster I'll show you how failures will
be handled in Hardo cluster but you are
not inves invting a lot of money on the
hardware that is idea because otherwise
the solution is not feasible if I'm
buying 500 IBM servers I can take the
same money and give it to teradata so as
good as same thing right so I don't need
support or anything I just need some
normal machines and Hadoop also does not
say what kind of a machine you need you
can even build a Hadoop cluster using
desktops if you want performance will be
less but it'll beautifully work or
laptops or anything that you want it
doesn't say that right so that is the uh
so I'll keep this diagram because I need
this diagram again that is I'm not
removing it actually
yeah H right
correct right so new businesses might
take Hado more because after five years
I may not buy Terra data right if this
is a situation I might buy a Hadoop
solution probably
I'll tell you what is the biggest
challenge in my experience in my
Consulting experience the biggest
challenge is
knowledge see all the companies want a
better solution only right no company
want to unnecessarily pay money but one
thing is that if it is a Terra dat
solution in the market if you go there
are a lot of people who understand data
warehousing data warehousing is a new
Not A New Concept if I go to the market
I can get n number of people who know
what is data warehousing and they can
easily install a Terra data for me I
just have to pay money but building a
team of Hadoop expertise is not easy
that is why you're sitting here right if
I go to a company also if all of them
have to understand how Hadoop is working
and Hadoop is not like click click
install there are a lot of things you
have to configure and tune so people
don't want to spend that much of a risk
so one reason is that uh the knowledge
curve we call right the learning curve
is a bit High you know you can't learn
it in a day and say that okay I'll
Implement had do it's not possible right
so most of the companies are stuck in
this uh uh
position I'll tell you a real story
since you asked this question so when I
went to one of the companies for
training okay the company said we want
to learn hadu Basics I said okay no big
deal I'll teach you hadu Basics I went
to the company I started the training
they started asking me like a name Note
Block size which are all related to hadu
I said hey I'm teaching you Hado Basics
you asking me all these questions they
said we already know Hadoop then why am
I here they said their company got a
project related to hadu they implemented
Hado they're working on Hado for two
years okay they learned everything on
their own now they want some training to
clarify everything is
correct no it it's actually something
really happening you check with any of
your friends who are in any company who
are working in Big Data they will tell
the same story because the manager will
say okay tomorrow your big data project
will start who those who are here can
join half of them will run away okay
half of them will stay okay and these
guys have to learn on their own I'm able
to share this experience because I go to
many companies and most of them are
self- Learners there are no resources or
training programs like this to learn
they will sit and learn on their own and
and and then learn over a period of time
so that is another problem because if I
want to learn Oracle I can get official
trainings a lot and I have lot of people
who know oracle they can teach me right
so the learning curve actually makes a
problem in these cases actually so I
think you guys are better because you
will know all this right if you're
joining in a company you can say that
hey I am aware of all these things
probably you can get an immediate
project anyway you are learning to get a
job I believe right end of the day your
idea is to get a job
right I I'll show you I'll show you the
architecture so now what you need to
understand so since we were looking at
the high level definitions only let's
dive
bit deeper Hadoop has three major
components there is something called
hdfs there is something called
Yan I can share the slides and PDF if
you want you can make a note that's all
it's not
mandatory third is called map
reduce so when you download and install
Hadoop only Hadoop these three things
will come by default hdfs Yann and map
produce these three components will be
uh installed by default and you don't
have have to do anything they will get
installed right in this this guy is the
storage this guy is handling storage
okay this guy is Resource
Management this is the resource
manager this is
processing the pen is not
liting so hdfs is the guy who is
responsible for storage to say that this
is a software piece that manage huh
inside so if you're if you're installing
hadu it'll have three components
actually you can see them actually hdfs
you can click and see I'll show you okay
so hdfs is the hdfs stands for Hadoop
distributed file system it's a file
system actually it is called Hadoop
distributed file system I'll write it
here
probably Hadoop
distributed file system that is
hdfs now from your course point of view
uh hdfs is important because this talks
like how the uh data is stored yarn is
not that much important but we will
cover yarn map reduce we will learn for
sure so map reduce I'm pushing for
tomorrow so the processing part we will
do tomorrow we are not going to learn
how to process the data today okay
because if I teach everything also end
of the day you have to go back and
recollect all these things right you
will not get it so this part I'll push
for
tomorrow and how many of you are from
java
background oh some of us are here at
least okay so because map is actually in
Java okay so we will look at the code
and many people are not from the Java
background also so and like I said in
the industry it will never happen that
you have to write a map ruce code
because map Ru is gone from the industry
migrations only happen so even if you're
able to understand the logic that's more
than sufficient
actually so first let's look at this guy
called
hdfs right what hdfs is doing so I will
probably push yarn and map ruce for
tomorrow because once I complete hdfs I
just want to show you something called
Hadoop ecosystem okay which will I have
to draw here so we'll complete hdf a
storage part Yan is a simple piece map R
inway tomorrow we will be covering right
so this is the guy who is handling the
storage and lot of you are asking how do
you store the data in hero right okay is
this
pen this is gone actually we have extra
pen not writing
actually this one
these are used pens I think also is it
red and green you'll get
black this is okay I think
right so let's take a hypothetical
situation where you have installed
Hadoop in one master okay and in my
example we have six slave
machines now this picture will uh pose a
lot of questions usually okay and that's
okay
so there is one master and six slave
machines and we are talking only about
storage no processing the master will
have a process called name
node the Master machine will be running
a process called a name node when you
install it will start running this
process called name node okay the slaves
will be running a process called Data
node data node so I'll call it as data
Note 1
2
3
4 five and
six so technically we will say that
there is one name node and six data
nodes I mean that is how you say in in a
Hado cluster right so name node is the
storage master and data node is the
storage slave okay so that thus far we
have understood now the the next thing
is that let's say you want to store a
file in a Hadoop cluster Hadoop does not
care what you're
storing meaning you can store any format
can be XML Json images video Hadoop
doesn't really care what format of data
you are storing while processing you
have to make sense of the data so that
is in map ruce if I stored a text file I
have to write it in map ruce that read
this text file and do whatever I want to
do the hdfs file system does not really
uh bother about what type of data you're
storing second point and probably the
most important point is that once you
store any file you cannot edit
it modifications are not possible you
store it delete it that's
it in hdfs modification is
impossible and now you have to
understand these facts really well
because you get confused later right so
how will I edit it is not possible in
hdfs as it is so why you may be asking
why it is not possible because hdfs is
designed as a file system for handling
huge amount of data terabytes and
petabytes amount of data right and if I
want to modify or edit you know uh row
by row like in a transaction that is
happening you know it is not possible
because if you look at a hard disk right
there is something called seek time if a
hard disk want to fetch a record there
is something called seek time right and
if I'm having one terabyte file I want
to edit the 100th line it's not possible
for my hard disk to seek and get the
data in that if you're because you're
using all commodity machines here they
are not even faster I'm not using solid
straight drives or anything very
commodity uh level Hardware I'm using
here so hadoop's idea is that you want
to read files sequentially there is no
Random Access there is only sequential
access so if I store a file I can read
the entire file and process it I cannot
say needle in a Haack
problem needle in a Haack problem
meaning if you have a 1 million row
table you cannot say take the 100 line
and edit not possible in Hado at least
in Hado it is impossible so either you
can read the whole data and process it
that's all you can do and the result can
be stored obviously if you process the
data you get some result that can be
stored but on the file system editing is
not possible you can't
edit not possible so
so H but directly update is not possible
I cannot open a file and update
say huh that is appending is possible
appending is always possible I have a
file I want to append the data that's
okay because you're just adding in the
end but I'm just reading the 100 throw
and edit that is not possible now you
may be wondering what if I want to do it
there is a solution you can do it you
have MPP engines on top of hu which can
do it I'll show you how to do it I mean
it is possible but considering only hdfs
ideally if you're storing a file and you
you got some modifications in the file
you delete it and restore the file
meaning you don't edit it actually
right can we take the Del yeah you can
delete you can delete the whole file
delete is possible I'm saying that
minute edits are not possible because
Hadoop is considered to be a system
where you are doing analysis of the data
it is not transactional system where
will your minute edits and all come O TP
systems if I'm having an rdbms I want to
do insert update insert update it's not
an rdbms right if you're rdbms world it
is possible I don't have any objection
but here it is not possible by
default huh
exactly not costly
exactly correct and also we are not
doing any transactional it's not a
real-time transactional DB right so I
don't want to minutely edit also but
worst case if I want to do it it is
possible so we have mppp engin on which
can do it actually
exactly so if I'm storing a one TB file
if I want to process the file probably I
want to process only the 100 line I have
to load the one TV into RAM hdfs alone
with hdfs alone so then the business use
cases will come so that is where you
need to make a decision I will teach I
will train you on that meaning if if my
if I'm writing a group by query or an
aggregate query right probably I need
the whole data I have a very big table
I'm writing an aggregate query right
which will say group buy and Order and
something like that it makes sense I
load the full file because I don't know
where is a data whole data need to be
scanned probably but if I'm doing a
select something something where name
equal to something name equal to
raguraman only one line need to be
processed for processing only one line
why should I load the whole data not
required for that you have an option
that's called MPP engine I'll show you
how to do that a massive parallel
processing engines on Hado so you have
tools like a hawk La uh Impala so there
are some guys who can do that actually
and I will show you how they work on
that no no no they are dumping in data
warehouse right from there they don't
want to update transaction data update
happens here right this is where the
transactions are happening correct so
this is where customers are placing
their order and and this data you don't
want to change this data was created by
customer I just want to get the data
right probably I want to modify and get
it so once it is here I don't want to
make any changes I just want to analyze
the data we will see that anyway now
let's take a hypothetical situation
huh no I'll tell you what is a role okay
so what is going to happen let's say
that you want to
store a file I got a new marker by the
way right so let's say you want to store
a file of
size I don't
know 192 MB
you have a file text file imagine and
you want to store it the size of the
file is we'll take a break at 4:30 okay
it's 410 actually so probably we'll take
it we'll take it 4:30 anyway so the file
size is 192 megabytes you want to store
it in a Hado cluster now there are a lot
of things which actually matters to your
realtime experience here we are just
learning the basics because in a real
company if you want to connect with a
Hadoop cluster the Hadoop cluster will
be somewhere else so if you sitting here
the Hadoop cluster will be separate you
will be separate right so you will get
something called a Hadoop client package
okay so one way is that you get
something called a Hadoop client using
that you can access from your laptop
second is that you can log into a node
there is something called Gateway
machine this is
called Gateway machine so if I'm sitting
here this is me very nice right so I can
just connect to this machine from this
machine I can connect to the Hado
cluster you don't directly get inside
the Hado cluster that's my point for
security reasons and many other reasons
so companies do two thing either they
will install something called a Hado
plan even that is very rare what we used
to do there will be a Linux machine
called Gateway node you will get a
username and password you type it you
connect to this machine this machine
will already know how to reach here you
don't have to worry you issue the
commands here it'll run
here got it right but whatever way so
let's say you want to push this 192 file
from your machine you connected to the
Gateway machine you said okay upload the
data what is going to happen is that
first thing which is going to happen is
that your name node machine this is a
master this guy will tell you something
called block size there is something
called block size this block size can be
configured for every Hadoop cluster when
you are installing Hadoop so I am
assuming that the block size for this
cluster is 64
megabytes so what is block size block
size uh tell you block size will tell
you what is the maximum size of data
that you can store meaning if you're
storing a 192 MV file what will happen
it will divide that into three blocks
same thing happens on your laptop if
you're storing a a a song on your laptop
what happens will the song get stored as
it
is what do you
think okay you store a song MP3 song
right on your laptop what do you think
the hard disk will actually take the
song as it is or it will divide
it it will divide it it will not store
as it is I think Linux uses 4 kilobyte
of 4 KB block size Linux Windows also
uses similar any file system will chop
your data and store same thing happens
in Hadoop also so I I'm assuming that
this Hadoop cluster has a block size of
64 MB so this name node will get back to
your client package and say that divide
your file into three or whatever you
want because the block size is 64 MB so
what is going to happen this file will
get divided into three
blocks you don't have to do this this
will happen behind the scenes like you
don't have to say that what is your
block size it will happen automatically
who will do Hado Cent Hado client
package gate this Gateway has Hado Cent
installed that is what so this G Hado
client on the Gateway will already
communicate with name node name node
will say that boss in my cluster the
block size is 64 MB you manage yourself
so the client will divide your data into
blocks of 64 6464 total is 128 so now
you have three blocks B1 B2 and B3 you
have three blocks now of the data right
and then again you go back to the name
node you ask the name node I have three
blocks okay now tell me where to store
and the name node what it does it has
communication with all the data nodes it
knows where how much space is available
it will tell you that do one thing store
the first block probably here just an
example store the second block probably
here B2 store the third block here
B3 most of the cases the name node will
give you different different machines it
will never allow you to store everything
in one machine so you divide and
distribute that is how you're storing
the data so now your data is divided
into three blocks and they are stored in
three different data nodes so this is
like six in reality you have 50 500
machines so three blocks is easy right
but sometimes a couple of blocks may end
up in one machine also depends on the
storage but ideally
it gets stored like this now you may be
wondering why the block
sizes it's not random it has name node
has an idea like how much storage spaces
available on each data node so it will
come up with the free machines okay
within that it just picks randomly three
of them and say that you dump it there
what yes that has to do with processing
and I will tell you so if you think
logically now if I want to process this
data okay three machines can process
this data right if I store all the three
blocks on one machine one machine has to
process three blocks right that makes a
difference right so ideally you divide
and distribute the data most of the
cases you don't have to do it name not
will do it for you
automatically this is all inside a lan
this is all inside a landan all these
machines are communicating with each
other the data nodes all send a hardbeat
to the name node saying that they are
alive also so that the name note can
detect how many of them are alive or
down or anything
but yeah so but now we have a problem
what is a
problem if this machine
crashes because this is our bmtc bus
right it's not a Volvo bus right and it
can crash at any point in time if this
crashes my block one is gone I can't get
my data so by default in Hadoop there is
a replication of three which means each
block gets replicated three times to
other machines I'll just draw this then
come to your questions give me a moment
B2 again B2 and I'm just drawing
randomly I mean just taking that these
are the machines B3 probably from
here B3 so if you look at each block
each block is replicated three times to
different different machines so now now
let's say this machine crashes this B2
is still available here and here I can
recover it from here and now what
happens is that the name node will store
the metadata meaning the name node will
write here there is a file called
abc.txt it is divided into three blocks
block one is available on where
three where is it 3
2 6 6 so B1 3 2 6 B2
where where is
B2 huh one
B3 yeah so this is your metadata meaning
if tomorrow you want to read the file
you have no clue where is the file you
just go to the name node not you the
Hadoop client will connect with the name
node and the name node will pass this
information you go to B1 B2 B3 sorry you
go to data Note 3 42 you will get it if
any of them are down it will give the
alternative one so the purpose of your
name node is to actually store the
metadata or the index and if the name
node
crashes you cannot access the cluster
that is for sure because if the name
node is gone then everything will be
gone to prevent that in most of the
Hadoop clusters you will have an active
name node and a p uh standby name node
meaning there'll be two name nodes these
two guys will be in constant
communication so if the active name not
crashes the standby will take over I
will talk a little bit more about this
later how this active standby is working
but you don't have to worry if the
active name not crashes standby will
immediately take over so now coming to
the
questions yeah
please
correct now three blocks are there the
replication huh
three sorry sorry
[Music]
repeting very rare all the three notes
going down at the same time is very very
rare even though we say commodity they
are not really commodity these days I
mean even though we say that had because
the storage and server cost is very
cheap these days so all the three
machines at the same time going down is
very very uh rare but what what is the
other possibility King JN fired a
missile right from North Korea what will
you
do exactly nobody will be there but
let's say King joh fired only to Chennai
okay and so in Chennai the Hado cluster
is gone right so what you do you are not
there to run the query but the company
should survive
right Disaster Recovery will be there so
typically we set up a Dr disaster
recovery
and the disaster recovery to set up that
uh there is a company called van disco
disco disco dance van disco very
interesting company so van disco is one
of the popular companies who set up
disaster recovery for hadu but you don't
do Disaster Recovery like in traditional
systems meaning if I have a 100 not hadu
cluster I won't set up a 100 not backup
cluster right that is waste of money
very rare king Jong will fire a missile
right if he fires God Saves right so
what I will do in even in the Hadoop
cluster you have to classify the data
you are storing 100 terab data probably
not all 100 terabyte is so important
right is it no a lot of data is like
archiv and all so in the 100 terab data
I will identify probably 10 terabyte
which is very
critical and I can periodically back up
that data to my Dr Center I don't set up
a 100 node cluster in NOA to back up
cluster from here so we use a tool
called distributed copy disc CP in hadu
that can synchronize these two clusters
primary backup dist CP it's called
distributed copy I'm not lying so some
people say I'm making up
stories I'll show
you um yeah so very excellent question
I'll just come to that just give me a
moment so go to Van
disco very interesting company right I
like the name who named the company I
don't know van disco so van disco is not
only a Hado company by the way they
provide replication
Solutions uh Solutions
backup Disaster
Recovery if I go to Disaster
Recovery why your internet is very very
slow
right see it is not even opening up the
page yeah Disaster
Recovery H 100 per Hardo availability
right see this is what I'm talking
about we used vanis in one of our
projects that is why I know usually
people do not be aware of these things
so I was working as a Hadoop
administrator also for some time that is
why I know this because this is these
are all like real times things
okay sure
sorry DP is the
tool want
to yeah it is same thing yeah but
managing this on your own is very
difficult actually DP can be run I know
that even cloud and hot work supports
but there are lot of things you have to
consider in running D CP first of all
resource utilization will be a problem
because you keep on reading and writing
the data second if it fails you don't
know how to manage DP can fail at many
points point in time so if you are
implementing vandas Co they use this CP
only again but they have their own ways
to handle it actually I mean they'll
ensure the replication happens properly
you don't have to bother about it now
I'm not suggesting that you must use it
or something if you want dis you can do
it on your own even claer provides their
own some tool for disaster recovery for
Hadoop actually but we are here right so
my question why the block size is 64 MB
not
4kb block size can be 4kb right
right maybe I don't know I'm just
asking seek time seek time if I'm
storing in 4kb 4kb I can't read how do
you read one
TB you want huh exactly now this I
showed you because my math is very weak
64 * 3 is 198 I know that that is why I
showed this example in real production
cluster the block size is 128 MB not 64
I don't know what is three times 128 MB
to be honest okay so real production
cluster block size is 128 MB it is
configurable they use 64 also there is a
parameter I'll show you where you can
change block size okay Hadoop the
default uh block size is 128 MB you
should also know also understand one
more thing there are three major
releases of Hadoop Hadoop 1 2 3
Hadoop 1 is the older release and
nobody's using it these days so you
don't have to worry about Hadoop 1 it is
not available in production right now
2013 was the last year when this was
used so no longer used Hadoop 2 is what
you are using I am using we are using
this is the current edition Hado 3
released in 2017 December 15 like 3
months back okay now let me ask you a
question if you are storing 100 terab
data in a Hardo cluster how much storage
you
need is that
true but only replication considering
300 right into three is it good or
bad bad what is your thoughts I mean see
trainings are always interaction right I
might know a lot of things I might not
know a lot of things you know right
right so you can so what is your thought
on
that can fine two can be fine I don't
I'm just asking I mean it's not a yes or
no question I'm just asking what do you
think so there are multiple approaches
one thing is that some companies this
replication Factor can be changed by the
way it's not hardcoded some companies
say two
right so you get two copy of data also
while copying the data you can mention
the replication Factor very interesting
thing I'm copying a file one TB file
this file I can afford to lose I can say
copy with replication
one but it had led into a debate in the
Hadoop world so a lot of people said
that this is very bad actually because
three times replication means you're
losing a lot of space that is why in
Hadoop 3 there is no
replication very interesting point I was
just beta testing Hado 3 even I don't
know completely in Hado 3 we use a
technique called eraser encoding it is
similar to raid RAID 10 Z parity how
many of you are aware of it parity based
raid okay there is something called raid
not the Akay Kumar movie something
better okay redundant array of
independent discs that's called raid
raid is a fall tolerance technique okay
so based on raid they have formulated a
new strategy in haduk 3 where you don't
replicate the data there is another
technique called eraser encoding if you
enable that you can still recover the
data if it fails they say it will use
only 30% of additional storage meaning
if you're storing 100 GB you need 130 GB
better right much much better you don't
need 300 GB but Hadoop 3 will not make
it to production very soon so don't
worry right still we are on Hadoop
2 who needs T
break yeah t break so you want the
question or you want the
break
huh oh so you guys are too interested
actually I thought you will all sleep in
the class we should get the Hado class
in the morning and other class in the
evening right no no no don't do
that
why
why that is more stat I actually wanted
to learn statistics it's good right I
can probably come and sit in the morning
probably now statistics if is after
you'll definitely sleep 100% is
guaranteed I'm not blaming the trainer
okay don't take it that way statistic
itself is a subject which is a bit
boring actually right
today no no tomorrow again second half I
am coming talking
STS is it so first half what you
learned r r programming okay okay got it
I have question huh
Tex file I can
understand
individually
yeah I know picture
fct I will give you a solution so the
processing has to be handled uh by the
framework so we have something called
map reduce in map produce you have a
Java class there is a abstract class in
Java I mean so for those who do not know
Java there is a there is a class there
is a a technique where you tell what
type of a file you dealing with and
there are only limited types you can
read text uh XML uh sequence file key
value so some limited number of types of
files are there and how do you so this
is a very good question how do you
handle unstructured
data so it is also very confusing
concept many people believe that in the
world of Big Data you are always playing
with with uh videos and uh images
actually no see many people believe
every day you'll go to office and run
some facial recognition like you seen a
mission impossible movie and yes yeah I
got big data no you don't do that
actually you can analyze images and
videos there is no it's very difficult
actually to do analyzing video and image
data is very difficult because you can't
directly analyze it you have to convert
it into binary once it becomes binary in
Hadoop you have a format called sequence
file you to RIT a sequence file and
write a program to analyze it it's
actually very difficult job to analyze
these kind of files and not everybody
need to analyze
it let me give you a realtime example I
was working with 25 by7 right the
customer care company what is their
primary business customer care in
Customer Care people
call right do you think they will listen
to what customer is calling
yes or no so you are running a customer
care company not your company you are
giving support to other
companies and you are running a call
center in a call center customers will
call right do you actually listen to the
what they are talking let's say take
idea for example how many calls idea
will be getting in customer care do you
really think they analyze your
calls two things one it is legally not
possible I think I'm not 100% sure maybe
it is possible they say that be calling
it your call be
recorded fine fine that is just for the
quality accent and all that is not doing
big data analytics for example if you
were angry customer you spoke for 1 hour
that call is recorded the person who
took the call will go to the manager and
the next meeting manager will Analyze
That call and say that this was your
mistakes I'm saying do you actually
analyze big data analytics on these kind
of things not really right you will
analyze the metadata
not the actual data how many people
called what was the duration what they
press this is the data not the actual
audio data because you can do it I'm not
saying it is impossible but then you
have to so how do you understand what
he's
speaking you need voice recognition
right and it may fail it may not fail
you can you are not 100% assure if it is
a Us customer or something we can
approximately match with an Indian
customer how do you identify all this
it's not possible possible right he may
speak any language any accent right so
practically these things are not
possible I'm saying you can analyze
unstructured data there is no doubt
about it and companies like Facebook and
all do Facebook actually analyze video
data they have to for some reasons that
they have to do right and there they
have written complicated algorithms to
read and convert to Binary and all but
if you are going to work in a regular
project you will all see structure data
CSV files Json data
normal text data but if you're storing
also so you need a class to read these
type of files that's what the question
was
right yeah
sorry go
huh oh uh that's an excellent question I
also missed another question from this
side that was same question no
yeah so uh I think we'll take a break
because I need more time to explain
anyway when I say Hadoop it is this we
know there is hdfs Yan or map ruce but
that is not only Hadoop you have lot of
other tools on top of that so you need
to have at least a rough idea as to what
are the tools we have in the Hado
ecosystem the whole stack right so I
will talk about it once we complete hdfs
discussion I will go to the ecosystem
stack because in the ecosystem stack you
will have a lot of questions how this
work how that at least a basic idea
because I'll take the same ICC scenario
and I'll I'll tell you how we mve then
you will get a better idea probably
right uh yeah so we were on this
question
side long I'm sorry I didn't notice you
uh that's an yeah that's an excellent
question so the question was that if one
machine is totally gone if I plug an
additional machine will Hadoop
automatically take care of copying the
data no the Pro the problem the point is
that if you that's called commissioning
so if you add a new node here or let's
say we are simply adding a new node
let's say the existing setup is that if
I add a new node this node will be empty
so you have to do something called
balancer there is a utility called
balancer I will run balancer
so that Hadoop will analyze and
understand that this is underutilized
this is over utilized it will move some
blocks here if you don't run balancer it
will keep it as a new machine but the
data will not be balanced so we run
something called balancer to manage that
there was another excellent question
this replication how does it work so
replication it works in a very
interesting way because if you're
writing the data the first block goes
here B1 right now this guy will tell
this guy to get a
copy and this guy will get a copy and
then this guy will tell this guy to get
a copy so third copy is got finally this
will send an acknowledgement here here
and you will get an acknowledgement then
only a block is written so it is called
pipeline right you are not pushing
everything at the same time as a
pipeline it writes and gets an
acknowledgement that was one
question uh
yeah sorry
yeah so this will be uh communicated
when you are doing a Hadoop copy
operation you will talk to the name node
and the name node will give you a list
of data nodes a chain of data nodes
which should get the replica primary and
replica this information will be pass to
them what is
Maxim maximum storage uh excellent
question because the name not actually
stores metadata this metadata is in the
ram when the name node is running the
metadata is being served from the ram
actually it also persists in the hard
disk so that in case it crashes it gets
uh for 1 trillion files 64 GB is the ram
required so you should actually
calculate the Ram size not the hard disk
size uh one trillion file a file size
can be anything I'm not talking about
file size because for a single file 256
KB of metadata is
created one file means how many blocks
are there it's replica that is the
metadata right so 64 GB can handle 1
trillion files in hard cluster so
accordingly you to increase the Ram size
hard dis size is fine you don't need
much hard
size kb per
file
exactly no no no you can increase the
ram of that machine right and the name
node is not a commodity Hardware very
important point the name node is a
costly machine you don't want a that's a
vol bus right because you don't want
that to crash
unnecessarily
has exactly so if let's say this machine
is down very good question if this
machine is down this B3 and B1 is gone
automatically it will create one more B
and B1 somewhere and if this machine
comes back online it will delete them
because it has to always maintain a copy
of three always the replica should be
three should not be four or two so that
is automatically handled you don't have
to worry it'll delete and recreate if it
wants
while consider
dat
name it will take it will take data
locality near near by considerations
will be there in that how the Met will
be okay so I will come to that just a
moment metad data I have to talk about
it yeah one more the first thing you
told Lo SI is
64 that was because 128 is actual okay
let it
what how as I understand this hdfs isct
system on top of
Lem correct
is going to
the so when you are installing hdfs even
though internally it is Linux file
system it'll talk to the Linux file
system and the 64 MV will be further
chop down the 64 MB is on Hado
internally on Linux it is 4K
but this entire 64 MB will be one point
you don't it will not fragment it you're
getting my point right if I'm simply
dumping 64 MB on Linux it'll divide into
4 KB and fragment it in many many places
will here it will be continuous so read
and WR are
fast uh any other questions you
have ah so balancer has to be ideally
run by the administrator Hadoop
administrator you have to manually do it
and there is a command called Hado
balancer you can run it from the GI also
you can run it uh I will show you so
tomorrow if you guys can remind me about
I'll show you all this I may not be able
to run a balancer but I can show you
where it is I will forget right so
tomorrow I'll show you the Hado cluster
I can show where are these things
actually so the H balancer is part of
your Hadoop installation hdfs hdfs so
you can say run balancer but normally we
will run balancer only during off PE
covers like during the daytime if you
run it it has to move the blocks and
your processing and all will will get
interrupted W interrupted but it'll be
slow so let's say night midnight 12:00 I
run the balancer I make sure that the
blocks are evenly
distributed huh so this block size that
is also a very good question uh this
block size is just a logic okay so my
question is that if I have a 200 MB file
how many blocks will be
there
two uh no let's take 64 MB uh and 200 MB
file what will be the size of fourth
block so how many of you say that the
fourth block is 64 MB I will say
yes how how many of you say fourth block
is 12 Mb not 8 MB
right very
less it is 8 MB it's not 128 MB the
block size is valid only when the file
size is more than the block
size any file you divide ultimately you
will end up in a 10 MB or 8 MB so had
will keep it as it is otherwise if it is
storing in a 128 MB piece remaining data
is unnecessarily so it is a logic
actually it is a logic even the physical
division happens here okay logically it
will keep a block of 12 Mb only because
you have only 12 Mb data left there got
it it's not like physically I'm marking
a space and filling only this much that
doesn't happen like
that
said to contact
system when you are storing only it
contacts right if I'm storing a file of
say 200 MB it knows four blocks need to
be created right fourth block size it
will say that 12 Mb only I need because
I have only 12 Mb data so this 12 M also
will be sequence exactly it will be in
sequence
only otherwise remaining space is wasted
right and every file will have some
wasted space so why are you wasting that
much space if I store one trillion file
how much will be wasted can you think
think lot of file size right that is not
required actually so it is a logic
internally built in to hdf is that the
block size is valid as long as the file
size is bigger than the block size once
you chop chop chop and last 10 MB that
is 10 MB only it's not 128 MB or 64 MB
right it will increase exactly so if you
append that block size will increase so
that's a good question so in this 200 MB
file if I'm appending the data right it
will increase it will provide a room for
that actually because the last block was
only 8 MB now you keep appending the
data that block will become a 128 MB it
will keep on
aable no in Linux what it does is that
when it is creating a block if a block
size is small it will say that gives
some room when you're creating the next
block don't keep it immediately next to
that the hard disk is so big that it
should not immediately create it there
right most of the cases happen will
happen very rarely probably it may not
work there very rarely okay it is almost
impossible in most of the cases you will
get that data
there yeah so that is for processing I
told you I will come back to that so if
I'm putting all the file one big file
here if I want to process it this
machine only can process my file right
if I'm splitting it now three machines
can parall process my data right my
processing is faster so if I'm and the
same time you will not run all the
processing another misconcept that
people have is that in a Hadoop cluster
if you have 100 TB data all the 100 De
developers at the same time will say run
no it doesn't work like that are you
getting my
point you have a lot of data in a hard
cluster you wrote a program I wrote a
program he wrote a program we won't come
to office exactly 10:00 and say run the
program same time doesn't work like that
you may have a lot of data people would
have written programs but you schedule
the program so that is where scheduling
comes I was about to talk about it so if
you're working in a Hadoop cluster
resources are very important right
because each machine has processor then
hard disk and network right and each
machine will be processing your files
whatever you have written so we have a
tool called Uzi Apache Uzi Uzi is a tool
actually that comes along with Hadoop
when you install as a tool so in Uzi you
will be scheduling your program you
wrote a spark program analyze 100 TB
data if you immediately submit the
program probably the program will not
work because already somebody's
analyzing the data where do you have uh
space to fit all these things right so
what you do you write it in Uzi and you
contact your Hado pmin say that I want
to analyze 100 TB data when can I run
this he will say tomorrow 11:00 okay
submit most of these analysis is batch
not real time real time is a different
case but bat jobs you schedule it bya
Uzi also you have something called Q in
Hadoop for example you are working as a
developer I'm a tester can we have same
access in the cluster of of course
testing cluster will be different but
let's say I'm a researcher you are a
developer he's a data scientist can we
have the same access on the Hado cluster
no probably you are a developer you need
more resources okay probably I need less
resources so what we do we create
something called Q okay there is
something called Q we create in those
que we allocate resources if you're
coming from the developer team you will
get 30% of the resources if you're
coming from other team you get 40
percentage right otherwise I can utilize
all the resource right if I write a
program I can make the cluster only for
mine that's not possible in a Hado
cluster so management is entirely
different how do you run these jobs are
all different but to just answer the
question why you are dividing is that
you can use parallelism otherwise a
single machine has to process everything
right quota size is different ESS Q
right Q means multiple teams will be
there each team can get a que so when
you submit a program you will submit to
a que you will not direct ly submit to
the cluster so let's say uh just for an
example take a in terms of
processing queuing in terms of resources
say for example in your haduk cluster
you
have huh you have 10 data nodes each
data node has let's say 10 GB Ram right
so total you have 100 GB Ram in the
cluster right H and let's say 10 data
nod each has four core processor so so
you get what 40 core
processor right now if if somebody want
they can get all these resources in
their program if he is getting nobody
else can run a program in the cluster
should not be like that so what we do we
create four qes let's say or two cues in
this que I will say 60% of resource
should be there here I'll say 30% should
be there here I'll say 10 per will be
there so this will be the developer CU
they will get 60% of the resources that
60 GB RAM and what is that 30 UH 60
percentage right 24 24 core processor so
they will never get more than 24 so
multiple ways of scheduling is there in
the normal scheduling they will get
maximum 60% only or if the other Q is
empty they can claim the resources from
there no no Q is actually a Java class
you can create something called
schedulers in Hado there is something
called capacity Eder Fair scheduler and
all so the scheduler is called a que so
if I'm creating something called Fair
scheduler within Fair scheduler it will
allow me to create cues so I can say
create three cues and whenever somebody
is submitting a program they have to
mention the Q name also otherwise the
program will not
run
okay
[Music]
okay correct but the data might be
different in no no this is not talking
about the data data they will get their
data I'm talking about RAM and
processing power okay data is available
okay so the location of R from H so if
I'm running my program we will come to
map ruce tomorrow this exp y h so if I'm
running my program I need a ram and
processor to run my program right how do
I run my program I need RAM and
processing power that the cluster should
give me
right so I can limit People based on Q
only this much resource you can get that
is possible that is called scheduling
in can be out depend upon theu ah so
when you submit to a queue within inside
the queue you can write a policy the
default policy is fifo first in first
out right apart from fifo you can also
have priority ities and other things you
can write your own policy whatever at a
time in the cluster only one job is run
no no at a time three jobs can run these
three qes can people can submit right
three qes three jobs at time three or
more than for example this Q has let's
say uh 60 GB Ram right I need only 20 GB
Ram you can also use 20 right three
people depending on how much you want to
process huh how much resource you want
that has to be I'll tell you how to
calculate that all this
none of this is important but these are
all admin activities none of this I
discussed is actually related to your
course but this may all make sense when
you start working see learning is one
thing okay if you if we just want to
learn we'll just learn map produce and
run some programs and go home but when
you actually start working uh you know
these these things are actually used in
real life want to know h no no no don't
worry scheduling and all Uzi nothing
comes in your uh lab part in the lab
part actually what is there hdfs is
there you will load the data read the
data map reduce is there hi is there
that's all none of this actually comes
but I can show you at least from the
cloud manager UI this uh this thing what
is that balancer and where is a name
node where is a data node these things
we can see since just we want to look at
it so are we clear about
hdfs I think at least we can say that uh
on a on a basic level we are clear
probably not everything but yeah
I didn't get records
means huh so here it is text file or
flat file any file you can select so by
default like I said if you're storing
something on hdfs hdfs does not care
what is your format you store an image
it store this image jpg you have to make
sense of it while reading the data it it
is not like rdbms so that is one thing
it is not like rdbms at all because it
hdfs is a file system so if you dumb say
for example on my Windows machine can I
store an MP3 file can I open a notepad
why don't know how to pass exactly I can
store an MP3 file in Hado if I want to
process I should write a logic to
process it actually
that is for only queries map reduces not
query map reduces programming language
queries you're talking about hi so hi is
your data warehouse on hadu hi will
accept normal SQL queries whatever
SQL on the data yeah so you have to
create a table and all from the
data exactly so Hive is like a you will
your data will be in htfs you have to
create a table give a schema then it
looks like an ADM table then you can run
your
queries no it's not an rbms at all
actually the data is running
L
exactly it can be a CSV it can be a text
file regular files it can save
yeah huh I will show you in Hive how to
store and process the data
right yes in Hive you have something
called called indexing but it is not as
efficient because uh indexing will
really work well if your data can be
managed by the rdbms system or the data
warehousing system if I'm indexing the
data in hi the data is actually managed
by
hdfs right so I can get some performance
improvements but not like in your rdbms
never compare hadu with rdbms you can at
the most you can compare with a data
warehouse because it is not
transactional right it's equal to a data
Warehouse I can
say I didn't get you you're asking if I
copy a file can
I no you don't create a record that's
what I'm saying you are again talking
transactional right you don't create a
record see this is a big dat platform so
you dump terabytes of file you are not
creating a record very rarely you edit
that's what I'm saying this needle in a
Haack problem right so very rarely you
say that I want to create a new record
you will be adding files continuous
streams that will be huge in size you
can afford to lose you can afford to
lose the data I will talk more about
that in the ecosystem side
okay huh
H so a lot of companies actually run
this using virtual machines it is
possible but performance we cannot
always uh depend on VMS I mean VMS are
ultimately what uh they don't use uh if
it is a bare metal virtual machine you
can guarantee uh so there are two types
of VMS right you something called bare
metal like a hyperv and all there you
can get good performance or VMware if it
is not a bare metal no uh performance
will be compromised because again you
have to go through one layer then talk
to the storage and
all it can be configured so I will just
talk about something called Hadoop
ecosystem and then we will discuss
[Music]
further located correct
is uh no it's all a Java call actually
these are all so the Hadoop is actually
written in Java so client is also in
Java everything is a Java method
actually so you are just invoking a Java
object there is a FS read object which
you call and this object will get all
this metadata and give it to the client
and the client will keep it in memory
and then go and read one by one from
wherever it want okay so I will just
explain this ecosystem and come back to
your questions further I know there are
lot of questions and it is good but we
should finish a couple of topics more as
well right so uh I'm just taking this IC
use case so you guys will understand
better so traditionally we do something
called ETL to bring the data right and
now what we have we have this hadu so
I'm just writing here
hdfs so this is your hdfs file
system this is hdfs and in the world of
Hado we very rarely do
ETL why because ETL means you are
reading the data transforming the data
and dumping the data if you're getting
big data transforming it on the Fly is
very very difficult actually so in the
world of Hadoop what we do is something
called elt instead of ETL you will do
something called elt that is extract
load and transform so first thing is
extraction how do you get the dat into a
Hadoop system so there are multiple
tools first of all if you are having
structured data say you have data in
Oracle you have data in MySQL so this is
all structured data which understands
SQL there is a tool called
scoop there is a tool called
scoop scoop can get this data to your
hdfs huh
scoop it is an ETL tool you can't call
it as an ETL tool scoop is used to bring
the data from any SQL system to Hadoop
transfer the
data
exter no it is part of Hadoop ecosystem
so what is going to happen when you say
that you want Hadoop from claer claer
will give you Hadoop on top of that it
will give you all these tools which I'll
be explaining now so scoop is part of
your Hadoop
ecosystem and scoop is used to bring
data from SQL stores it can take from
let's say Oracle it can take from MySQL
but it cannot take from flat file Json
nothing nothing only SQL only places
where SQL
understood so from ICA use case we got
all the data from CRM through scoop to
had okay so scoop is an Apache tool it
is a very light white
tool we faced a problem there I wanted
to discuss that let me ask ask you this
question this is IC Banks core banking
Oracle database do you really think they
will allow you to touch it then how do
you get the
data
meaning who will copy they will should
be
copy
replication any tools or or how do you
replicate I mean
excellent CDC change data capture so we
Face the same problem there is Golden
Gate Oracle has a solution called Golden
Gate so CDC change data capture meaning
they will not allow you to touch it
right so they will have some replica
solution so where they will write it
into a replica DB it's actually a log
file from there you to capture and get
it they won't allow you to directly
touch their Oracle DB by anyway it's
called C CDC change data
capture no that will have only the Delta
so what they will do they will give you
the original data backup data they have
the CDC will capture only the changes in
the
database got it and then you have to
from CDC you can write a Java code and
then get the updates dumbed into here
that is how you get
it okay CDC will discuss later because I
need spend half an hour here to explain
just understand the changes can can be
captured because it is anyway not
related to your uh know Big Data
platforms this is just a business
scenario yes yes yes yes of course they
going to VI all these exactly exactly so
we will not discuss CDC in detail
because it'll take another half an hour
I don't want to spend half an hour it is
not related to your syllabus anyway CDC
and even I didn't do CDC because there
will be teams who will do that even I
was not aware of it it's a technique ah
it's a change data capture is a
technique so whatever changes are
happening here it'll capture and dump
somewhere from there you can read
it database exactly from there you can
read it so I was just saying that we use
CDC to get the data so Golden Gate is a
product that Oracle uses for CDC anyway
we use scoop to get from here and here
right now comes the place where we are
having a lot of debate actually okay uh
now the real question is that okay I
know how to get the data from here
through scool right what about the other
data right now if you want to get the
other type of data there are multiple
ways you can get okay one of the most
popular used tool is called
Flume Apache
Flume Flume is a point too delivery tool
what Flume does it takes the data from
Source give to the destination The
Source can be anywhere and normally
Flume is used for unstructured data say
for example uh in a folder you're
getting all the Json data or XML data or
flat files I can create something called
a flume agent and this guy will keep on
polling that folder Whenever there is a
new data this guy will read it and it
will send it to hdfs this is what Flume
does now the real interesting thing is
that Flume can do two things one thing
is that it can simply read all this data
let's say this is in a folder
okay it can simply read this data and
say send it to
Hado this is fine but icsi Bank also had
another requirement so this is simply
you're collecting the data and dumping
into Hado they don't want to do that
okay so they wanted they had a
requirement in which what used to happen
was that this Flume was capturing some
data okay some data was getting
generated in the form of Json here so in
the social media if somebody is clicking
on one of their products they wanted to
send an
SMS real time so this Json data will
keep on coming you will keep on getting
it and Flume can directly dump into hdfs
but that is useless for me because I
want to analyze the data whenever the
data is getting generated that is where
this guy comes into picture
Kafka Kafka so there is a difference
between Flume and Kafka okay you have to
understand Flume is like a point to
point delivery you give me the data I
will dump it there that is all Flume
does it doesn't do anything and Flume
does not store the data anywhere meaning
it'll take the data temporarily it may
store and then immediately push the data
Kafka is a message Q anybody aware of
message cues so Kafka is your message
queue in the Big Data world meaning what
this guy will do this guy will be
running in its own cluster no Hadoop
cluster this guy will run on it own
cluster like 30 machine 40 machine this
guy Flume can send the data to this guy
Kafka and Kafka is a cluster so it can
store the data by default it can store
for seven days by default okay anybody
can go to Kafka and get the data publish
subscribe system we call it the
advantage is it will store the data so
that multiple people can ask it flu is
point to point if flu gets the data
dumps the data
here yeah so here what we used to do we
didn't do this okay we get we got the
data to Kafka from Kafka I can push it
to hdfs one way so I'm getting the
original copy here okay now in ICA bank
so the next question is that how do you
analyze data in real
time so somebody's clicking on Facebook
the data is coming as XML right and
immediately when the data comes flu will
read it push it to Kafka so now the data
has reached still Kafka but I want to
analyze the data in real time that is
where your spark comes into picture
there is something called spark
streaming in spark you will learn this I
think I'm not sure okay there is
something called spark
streaming so spark streaming is a
utility which analyzes real-time
data now fum can read any type of data I
didn't get
sorry point to point
so it just takes from here D to
Kafka is for contct the SQL Server
database and no no no flu will not work
with SQL Server only scoop will work any
SQL data scoop any other type of data
Flume now Kafka can also directly get
this data now this is a bit confusing I
know okay because this is very new to
you first of all but Kafka can also
directly get the XML file but but the
advantage is that Flume is pull based
pull based means if I install Flume
without disturbing the setup I can pull
the data Kafka cannot pull the data you
have to install a tool here and modify
the existing
setup Flume is pull based meaning if you
install plume here it can start reading
the data automatically I don't have to
modify anything if I install Kafka
directly Kafka can also get the XML data
it can read the XML data but I need
something called a Kafka producer that
has to be installed here on the ICS
Source system that is not possible right
so since it is an existing setup I can't
directly get through Kafka I get through
Flume Flume will give to Kafka Kafka
will send one copy to hdfs because I
need to keep the original data second
copy it will send to something called
spark streaming spark streaming is a
library available in spark where it can
do real time processing the moment you
give the data it'll process it real time
so somebody clicked on that immediately
the data is here spark streaming will
process it and say send an SMS or you
can take an action whatever you want now
what are the other tools which can do
real time apart from spark streaming you
have something called fling you have
something called
storm these are all realtime processing
tools you might have to make a note of
that
yeah sorry sorry yeah
yeah correct so Kafka can be Facebook FL
can be WhatsApp because what WhatsApp is
point to point right one to one and
Facebook is like distributed right
anybody WhatsApp also they will keep it
right I don't
know oh okay okay okay that that context
yeah yeah correct correct good I didn't
understand that point see so that's very
easy so in WhatsApp it is push right flu
is like WhatsApp okay once flu gets the
data it'll push it push it to somewhere
it will not store the data Kafka means
it'll get the data and store it for you
as long as you consume it so in in in ic
Bank this was the architecture we got it
through Flume send it to Kafka here one
copy will go to spark streaming so this
guy will do realtime processing and do
something okay one copy will go to hdfs
so the data will remain in Kafka as long
as you want you can configure by default
it is 7 10 days you can also mention the
size of the data I want to keep only 10
GB data 20 GB data there is something
called topics in Kafka where you can
mention how long the data should stay
here it's a cluster right we store the
data it can but it can do only one then
it cannot send here right you're getting
my point Flume can send here then this
will not happen it is point to point
delivery I need a copy here I need to
send there also at the same time also
prob tomorrow I want to build one more
application okay which will do something
on this data and that application I have
a new application I build tomorrow that
can also get the data from Kafka Kafka
is like if you give data to Kafka
anybody can take it from me I will store
it for you got it Flume is like point to
point got it Kafka will store it so that
anybody can come and get the data 10
applications can read from
Kafka yeah
sorry slow slow very slow hard dis to
read I can directly push in real time
right so before you get out of tanic
shop I want to send an SMS
right after processing no this will be
one golden copy will go here the
original Facebook clicks and all golden
copy will go here that tomorrow you can
analyze now you clicked on a page
Insurance page in Facebook I want to
immediately contact you so it has to be
process processed and understood and
then some decision has to be ven so I'm
sending one copy here one copy here here
the processing will happen here simply
the original data will be stored so
after data be deleted discard ah it will
be discarded because original data is
anywhere here you can store it
also yes golden copy will be here it's
called Golden
copy no no no map reduce cannot do real
time it is batch yeah
yeah because how do you read from Oracle
how do you read from Oracle no no no no
FL and Kafka cannot only scoop can read
from uh SQL based stores now Kafka is a
very interesting thing I teach a three
days course only for
Kafka three days it's not enough
actually to Kafka because it's such a
big topic actually because Kafka can be
a message
CU elk C stack right no no elk stack
will not come here in this example uh we
we didn't directly use elk stack we were
using that's what I was telling in the
break to somebody what we were doing is
that we used to store the data in Hadoop
to process we were using something
called Splunk there is a tool called
Splunk Splunk can actually replace your
elk stack it does elk both in one tool
so that is for visualization right you
have some log files machine learning you
can just apply and visualize we were
using some something called Splunk so
this is the big pipeline I'm talking
about you can also build it using elk if
you want I'm just
saying elk is a stack actually elastic
search log stash kibana it's a probably
he can explain
right ah so who was working on elk
somebody was yeah so you can explain
probably what is
Elk lightweight we can inst
but probably not for 100 TB 1,000
TB so let's say you're getting a lot of
log files you want to make sense out of
it right from different different places
let's say I'm collecting all the logs
from this network H I want to make sense
off of it so I need a tool which can
easily collect it clean it and then
display in a dashboard that is Elk what
it is doing there is a tool which is
very similar that's called Splunk that
that is not in the big day in the Hadoop
ecosystem it is not there that is a
third party tool
actually open source
yeah
exactly no no no no that's part SQL only
scoop exactly so that's a good question
so when scoop is getting the data right
scope will ideally push only to here
hdfs okay but I can say scoop to push to
Kafka also that is also possible once
the data is available in scoop okay I
can say give it to kafa so either it can
be read for live processing or scoop can
push to something called Hive we have a
hive here which is a data warehouse from
there I can analyze in live if I
want but normally this transactional
data is not much use for realtime
processing you are doing it based on
this Facebook clicks and all based on
this transaction I can get it now just
to answer his question um that's a good
question actually uh so uh can you uh
give me a suggestion so a customer is
making a
transaction uh a customer is making a
transaction using ICC credit card he
made transaction for
50,000 I have to immediately send him an
offer so if he's making a transaction
here I made a mistake here I want to
show that if he made a transaction here
50,000 okay where will be the data in
Oracle how do you capture the data from
here scoop you will say scoop we were
using Flume because your CDC will be
producing a log file the log will be
captured by Flume not scoop so here
scoop will not come into picture here
scoop will come because I'm directly
reading from CRM my CRM is in MySQL
MySQL is original table and I can read
directly but in the Oracle case is I was
not directly reading the table if I want
to read a table scope but we were
reading the CDC change data capture that
was a log file that was being collected
by Flume actually so you can actually
make it so this is the answer if you
want to make real time okay you need
some CDC system which can push the data
to flum
actually huh
exactly
exactly real time uh I have worked on a
banking use case where so I'm talking
about this side spark streaming Flink
storm so spark streaming is what you
will be learning Flink and storm are
other Frameworks which can do realtime
data processing so you will be wondering
why realtime data processing is so
critical all are three all three are
real time you may use any of them fling
is Apache storm is also Apache storm is
almost out of the market okay uh spark
streaming and Flink are popular right
now we have worked on a use case with
City
Bank you swipe your credit card right
how do you know it's a fraud or not
you're are swiping your credit card I
want to find out whether it is a fraud
transaction or a real
transaction correct
exactly no even you don't have to be a
fraud let's say I use a credit card
regularly if I One Fine Day if I
suddenly pay in US dollars or something
I'll get a call from customer care did
you really make it that means imagine
HDFC Bank how many credit cards will be
in circulation and how many people will
be transacting every day do you think
somebody is sitting and monitoring every
swipe spark streaming you are collecting
all the data all the credit card
transactions will get dumped okay they
won't come in a relational form they
will come in some flat file or something
your Flume will catch it the moment it
comes it'll push it through Kafka or
directly to spark streaming or Flink or
storm and there I could have written a
logic Okay match this transaction if
this guy's limit is something or his
last uh transaction made places outside
India or something trigger a mail to
customer care that is how you're getting
this Emi calls also when you make a big
transaction immediately they will call
and say do you want to convert Emi right
these guys are responsible for that the
stock market stop ah same stock market
very big example yeah same thing because
all real time right because tomorrow if
I get a message for today's stop loss
doesn't make any sense right now since
you're interested in this I think in
your Spar course streaming is not there
very bad
okay but I can give you some example
because what is the difference the storm
can process one event also meaning One
credit card transaction storm can
process ideally One credit card
transaction it will process minute spark
streaming can process only Bunch it
cannot process one meaning it will
collect probably 2 seconds worth all the
transaction and predict I mean so that's
called micro batching it is creating a
batch and then from that it is
processing real time a near real time
exactly 100% real time is only possible
with storm and fling also does the same
I think I know there are lot of
information I'm passing and lot of you
are actually almost like dead by hearing
these terms and all but I can't really
help it because I'm not making it up you
can Google for that okay and and
probably now it may not make sense if
you're listening to me but probably
later when you start working if somebody
talks about spark streaming and a use
case you will think ah okay okay oh this
is what he at that so IC bank's use case
was that if somebody is clicking you
they wrote a spark streaming code okay
now I have a question for
you no no this Securities we were not
handling I'm not 100% sure how it is
working our use case was only limited to
this right but now I have a question
okay uh the question is very much valid
you are making a credit card transaction
and Spark streaming catches it spark
streaming catches it but that is the
realtime
transaction how does it know that this
is a fraud based on the previous
transaction fine yeah the bat data is
there so but how it is working machine
learning
they could have built a machine learning
Model H profiling all your past within
hard
only SP Spar spark
ml
needs yeah they already have historical
data all these customers transactions
here these are all on top of hdfs only
this is running on top of hdfs only this
is part of HD
huh when you install
spark exactly Bas on your last so spark
is getting installed on top of Hadoop
only hdfs is the storage for spark so if
I'm making a credit card transaction
today all my previous transactions are
already here they could have already
built a machine learning model and if
you simply feed this new transaction it
can predict otherwise how do you compare
with batch data I cannot run a batch job
it'll take a lot of time right all the
customer data so they will run a MLB
it's called MLB in Spar machine learning
model will be running that will easily
compare and say it's a fraud if you want
to compare the batch
otherwise see now I'm just making a
transaction right it has to compare with
my previous transactions to understand
whether I'm a
fraud so like he said I was traveling
only within Chennai for the last 10
years uh today I'm swiping in New
York h so New York swipe if you want to
understand is an anomaly you should
compare it with previous swipe right now
each time you cannot go and compare for
each customer so they build a machine
learning model based on all this data
that can if you feed a data it can
predict you know whether he will be a
fraud or
not no no uh Kafka okay so Kafka will
not send spark will get because it is a
published subscribe system it is called
spark streaming is an application which
will PLL a Kafka topic and get it from
Kafka it getk pulling the DAT from
fine but you said on HTS
correct no this is your permanent
storage Spar can read from here also
from here Spar can read from a variety
of
sources never for theod of time huh like
one week by default so this is your real
time data yeah that's right I I'm just
reading it to get my current information
all my original information is going
here only right the golden copy is here
so if tomorrow I want to run a spark job
yesterday's data I can read from
hdfs using the
htfs huh so this original data will get
from here current data and there will be
a machine learning model which is
already built based on this data it will
not read the data the model will could
have already read and learned that is so
you guys have to teach me this you are
learning machine learning right not
me you have to tell me how it is working
right how would I I am a data Eng
what is you machine learning right your
topic is machine learning
right no no no no I'm saying in this
hdfs you have all the transactions made
by all the customers for last one year
credit card rep Ah that's a repository
you feed it to Sparks machine learning
algorithm it will build a model which if
you give any new data it can predict
whether it is fraud or not it not have
to go here I didn't
last that you have to adjust in the
machine learning that's what I'm
saying's rate is available in the huh
andk will me what my l
andit no it will not directly compare
and get this I'm talking only about
fraud detection so your use case is your
this thing right uh stock market price
variation ah recommendation
recommendation
hisorical price hisorical purchase is
there in today's price is there in so
I'm comparing One Price what I bought
price that's okay so my point is this
historical data will be huge it will be
in terabytes right so you could have
already created machine learning model
by feeding this data which if you give
any new data can predict whether it is
fraud or not that is happening in real
time I cannot say that once if somebody
swipes a credit card I will tell you
whether it is fraud tomorrow doesn't
make any sense right I have to do it
immediately now the flip side is that
all this requires resources nothing is
going to work if you don't have
resources so you need to give enough
resources for all these things to work
machine learning and all are very
resource hungry CP power ram power right
so this cluster should be capable of
doing this anyway for the time being
let's move on from Kafka right so we are
just taking a use case right now I'll go
here once your data is in hdfs if you
want to do batch processing the default
framework is map reduce
Mr what is map reduce we will see
tomorrow but I have my data I want to do
batch processing I don't care how much
time it takes one day can take I just
want to process the data I can use
something called map reduce right you
also have one more guy called
Pig this is uh almost gone from the
market pig pig the animal
pig pig is a scripting tool on hadu but
not very popular now because of spark so
nobody is actually so even if you're not
aware of pig is perfectly fine I think
okay you don't have to worry and then
you have somebody called Hive this is
there in your
syllabus so Hive I will teach in the
next month but basically what Hive
allows you to do is to write SQL on top
of hadu but the drawback is you say
create table and select count star from
the table hit the query it will convert
your query into a map reduce program so
it's a translator it is not a native SQL
engine yeah if I'm writing an oracle
query Oracle will execute it for me if I
write a hi query map reduce will execute
it for me
so you will understand when we speak
more about Hive don't worry
so huh c c C++ exactly so map reduce
itself is very slow because this is
batch processing in hi you write a SQL
query and hi will not execute your query
it will write a map reduce program for
you even more
slower to spar hi to spark can be
configured yeah so but the problem is hi
was the original tool it was was built
in 2005 by Facebook okay and at that
point it was very popular because there
was no other tool right so people were
writing High queries even though it is
slow it is equal right see which is
better you can suffer some slowness or
you have to learn Java you can suffer
SQL
right ah right so
ETL 15 minutes it is fine yeah so
Facebook developers thought that instead
of learning Java right we will learn we
will stick with SQL ETL jobs we will run
they created this tool called Hive so if
I'm a hive developer I don't have to
learn anything it's SQL I say create
table create table join but once I say
join it'll write a map ruce for me then
the map produce will run I may have to
wait for in fact in Flipkart there was a
hive query which took around 12 hours to
run today they will fire tomorrow
they'll get the result 12 hours was the
typical runtime a huge amount of data
obviously but then they find it and all
but that's a different story but so lot
of time batch batch processing so these
are all batch oops these are all batch
processing systems built on top of Hado
right and then you have spark you will
learn spark later I don't want to
confuse you a lot so spark is
there and this guy is inside spark okay
even though I wrote it here this guy is
inside spark spark is an inmemory
execution Engine spark is again
classified as a batch processing system
but very very fast than map reduce only
difference is that spark programs are
faster than map reduce
programs near real time not real time so
spark streaming is actually real time I
would say near real time not like 100%
real
time there'll be a buffer always like 1
second 2 second you can Define that and
it'll process like that I cannot have
microc real time that's not possible
basally ah so yeah so Mr and pig are
obsolete Hive is not obsolete because
Hive is your data warehouse where you
create tables and store the data so this
guy will remain but now what is going to
happen spark will connect with
Hive and Spark can
write yeah so here you can write a SQL
okay spark has a spark SQL Library it
can read tables from hive and run on top
of that right so spark will talk start
talking to Hive so Hive is not really
obsolete hi will be there for some time
we can say no no hdfs storage is hdfs
only Mr there is pig is gone no there is
nothing pig is
almost pig is a scripting to so Pig also
will convert your query to map
ruce
obsolute Spark spark only not new
version spark has practically replaced
Pig
actually so this is your batch
processing Frameworks right now some
other guys came and they said that so
that was a typical question that asked
during the training so if I want to run
okay again there is a confusion if I
want to run a very big group by query
let's say I want to read the Full Table
scan and run a query Hive is great even
spark SQL with Hive is great because
they will read the whole data and
process it but if I have minute queries
let's say select uh something from the
table where name equal to
raguraman only one row it needs to
process for that why should I load the
full data and process it is useless but
Hadoop says load everything in
sequentially and then process that is
only possible that is where you have
this MPP engines Impala there is one guy
called Impala there is another guy guy
called Hawk
Hawk and
LP there is something called
presto presto there is also something
called uh what is
that the
drill
Phoenix these are all MPP engines just
just be aware of the name you don't have
to be an expert on like there is no
chance where your manager will ask okay
name the top five what you say MPP
engines or you fired nothing is going to
happen like that any one of them will be
used actually Impala is claa specific
claa created Impala so it will be mostly
available only on cloud era what Impala
does is very simple it has its own
execution engine you fire a SQL query it
will run it for you no map
reduce right so what is going to happen
for these queries like where name equal
to raguraman you can fire an Impala
query it will scan only that
data right so it can connect with hdfs
and pick only the data because it
remembers the metadata where is what
data available it can ah columnar it can
pick that data and query but then you
will be wondering why can't we use this
instead of
Hive if and these guys are very fast so
if these guys are too fast then why
don't I use them instead of Hive they
don't have
reliability they are all inmemory
queries if an Impala query is running in
my hard cluster if any one of the
machine crashes my query will be gone if
a high query is running even 10 machine
crash query will complete it is map
reduced buil-in redundancy is there map
reduce will never fail or spark will
never
fail
yeah
correct no no no no it is loading the
whole data into memory then only
analysis happens
right going
to name something
correct no no no before so when you say
Select Staff from this table first that
table data will be loaded into the ram
then only it reads from there right so
initially the whole data is
loaded exactly that's the problem but if
I'm using Impala okay it has its own
metadata it knows where is the data it
loads only that and queries will be
faster so if I run an Impala query
considering a hi query Impala queries
are much much faster but no fall
tolerance so for this very fast queries
like your queries can take like 2
minutes 3 minutes time you can afford to
lose a query useing Pala but if I'm
having a ETL job where I don't want to
want the job to fail run it using Hive
or spark SQL because these guys will
never fail
only it is distributed it can run on N
number of
machines okay Sy gets down it query will
be gone because there is no I will show
you tomorrow in app reduce we will
persist the data intermediate results
that is how it is fall tolerant Impala
is all in memory so if a machine crashes
that data is gone what was in the ram
query will
abot so all MPP engines are like this I
think in the traditional World also in
this the difference is Impala is on
cloud era Hawk and LP are Hots so there
is a competition Impala was originally
created by Cloud era so they promote it
if you're in a cloud era platform you
will see Impala if you're coton Works
platform you will never see Impala they
will promote haulk they will say Hawk is
the best engine okay and LP is the new
version of Hawk Presto drill and all are
similar ones but not by default
available this guy is very very
interesting so there is something called
hbase I will add it here somebody was
asking me what is hbase hbase is your
database of hadu no SQL database
realtime database real real time because
hbas will get installed on
hdfs that is your storage right but it
can do random reads and writes not like
entire block so normally when you are
reading the data from hdfs you have to
read the whole block right but hbas has
an API so if you install hbas on top of
Hadoop it can do random reads and wrs
individual records it can read and write
and process because it is a nosql
database of hadu the default no SQL
database but the drawback is hbas has
its own language so if somebody has to
work with hbas you have to learn the
language it will not allow SQL queries
phix is SQL on a
space Phenix and hbas is
like Roti and paner butter masala deadly
combination I mean very deadly
combination because you writing SQL on
top of
non-sql right very deadly so so they say
that in another two years it will not be
slow is really fast because hbas itself
is fast SQL there is a conversion okay
but that's already built in any SQL
query there are some limitations but
most of the
SQL no no no it will not fail so finix
will get all the metadata and already
all the queries are converted whatever
query SQL you are writing it knows what
is equivalent hbas query it has so it
has just push the query so you just
write a SQL it'll run on hbas and give
you the result and it can do things like
secondary indexes and all it can index
your data apart from HB it's very fast
actually now the real news is that in
another two years what is predicted is
that this combination the drawback is
Phoenix is still in Alpha Beta Phenix is
not production ready once Phenix becomes
production ready this is oil
TP it's oil TP that means Hado will
provide you olap and O
TP huh
it it will support in future not now so
they are trying to make it acid support
so that this entire combination become
an oil TP system like that right so then
what will happen you will get
transaction Management in Hadoop so far
it is not there another two years down
the line they say that so Phenix is
getting integrated with hbas originally
hbas was only there the drawback is it
doesn't understand SQL so Phoenix
started Phoenix is a layer on top of
that you can write SQL so in another two
years they are developing this as as a
system for transaction management so
you'll get true transaction Management
in
Hadoop uh so yeah so if you talk about
the clera and all if if I'm going to
clera and say that I want your product
they will give you all this in one
package but if you go to Apache and
download you get only hdfs map ruce and
Yan only Hado all this you have to
separately install so commercial
distributions will package it and you
will not get everything okay for example
Hawk will not be there this Presto drill
Phenix you will get hbas Hive Pig map
reduce spark Impala so these things you
will get you have a doubt I'll show you
that uh in the cloud
page yes there is something called spark
SQL where it runs SQL query it is really
fast
actually that's what the tables you will
be creating in high Spar doesn't have
storage right so whatever tables you
create there uh it can connect and run
so I'll say cloud CDH
the HB has its own processing engine its
own processing engine so this is cloudas
product why I'm showing you this to tell
you that I'm not blabbering okay you may
be thinking that this guy is talking
about a lot of tools do they really
exist in the world look at
here look at here spark High Pig map
ruce spark Impala uh yarn right what you
have hdfs hbas Kafka Flume
scoop this is a real product from clera
you get go to claa this is what you get
some of them I have not even explained
because I don't know kite I don't know
what it is to be very honest Kudo is a
new file system relational even I'm not
aware of kudu I don't know what it is
but these are like new entries you even
if you don't know them you will work
without any problem and for security you
have something called Sentry so a lot of
people are wor worried about security on
Hadoop right so there is a service
called Sentry which will handle like
access management authorization
authentication all this will be handled
by Sentry we covered these things right
these things we covered this we covered
this we covered uh something extra I
have wrote I mean that is okay I mean
here this part right this prestor drill
Phenix and all is not there but I just
wrote
them one thing which is not present
there but it comes is Uzi
this I told you right this is the
workflow scheduler like if you want to
schedule a workflow and all uh you can
do
that I think we can do one thing uh we
have half an hour left and you are like
almost tortured I'll show you a cloud
cluster how about that that'll be better
than talking a lot so I keep on talking
all this right so I'll show you a cloud
cluster uh we will not do the handson
today uh in the cluster we are using
using for the lab we have only readon
access from the admin side which means
you cannot change the replication factor
or block size or anything these are all
done by admins we can upload the data
and then say that run the program that
will work but that we will do tomorrow
but let's look at the cluster right so
so far we are talking cluster cluster
and we are not seeing any cluster so far
so I
think cloud hoton works and uh are these
three are getting used actually both
both all the three companies are sort of
famous I can
say right okay so do you have this in
your LMS like your lab and
all just check
once
yeah so um because some of the things
are a bit confusing so I have to tell
you we have to use use web console web
console will connect with the uh Linux
file system where Hadoop is installed we
will do that later Cloud era manager is
the admin interface of the cluster so I
will connect with it but we will not be
able to modify but we can see stuff so
if I go to Cloud
era no no Cloud era manager is a utility
that comes with Cloud era to administer
the cluster like you want to add a
service remove a service add a machine
remove a machine so the administrator if
you are a Hadoop administrator you have
to do all these things add users remove
users permissions everything can be done
by Cloud manager and why I did not speak
about this because we will not be doing
anything using this it's an admin
utility but to look at the cluster this
guy is good so let me show you that
cloud
manager now you have to install it so if
I'm running in a company I have to
purchase let's say 20 machines
physically yeah and then I have to there
is a process of installing Hado this
cluster already has installed it is a
running cluster if you're creating from
scratch you have to purchase let's say
20 machines and download something
called Cloud manager and run it and it
will help you to set up the cluster it
will tell that which is your master
which is your slave what is your block
size what is like if I don't have I
useal cloud also this is physical this
is not even physical this cluster is
actually running on AWS Amazon web
services huh
practice you will have uh access to this
right I mean if you want to practice
this setup is already available to you
right giving some bad request try to Lo
uh let me try to log in okay just
probably you don't have access I
don't yeah so just pay attention uh uh
so this is your Cloudera
cluster okay this is just an admin GUI
okay to the cluster and once you login
this is your dashboard so you are
looking at a Hado cluster first point
you have to understand second Point what
is there in the Hado cluster these are
all the services installed see
hdfs why am I talking like this I can
see it here right okay hdfs Hive Hue I
will talk about it Impala Kafka Uzi
solar Spark yarn zookeeper all these
services are running in the cluster you
see a green right this is just to get a
quick overview of the cluster like what
is running in the cluster fine how many
machines are there in the cluster you
click on this host
menu can you
see can you see these machines these are
all machines one1 right so how many
machines are there I think there are
nine machinein see good health n nine
machines are there in the cluster and
you can also see their configuration if
I scroll
here can you say there see their RAM and
hard
disk what is the physical memory that is
the ram the first two machines are first
four machines are 8 GB Ram next one is
16 GB Ram these are their hard disk can
you see and you can see how much storage
is full and all everything right see
here so this guy is having 32 GB RAM and
18.8 is currently used some process will
be running this guy also had a 249 GB
hard disk 600 GB hard disk 50gb hard
disk you can all see from a single
window it's very easy so it is not as
complicated as you thought by the way
right so this is just the host menu
right uh you can
just uh from cloud manager this is the
homepage click on host host you will see
this
right uh now if you look at hdfs this is
hdfs click on hdfs
okay you will see this hdfs menu this is
all related to hdfs in hdfs what we
learned replication block size so where
do you configure this there is something
called
configuration and do you want to change
the see it is saying read only I cannot
change it but do you want to see the
replication factor of this cluster DFS
sorry DFS
do what is the application factor three
click there you can change it to two or
10 or whatever you want if you are an
admin you are not an admin do you want
to see the block size DFS do block
size can you see so I can click here and
change if I'm a hard admin and it will
be applicable to everybody immediately
and all uh somebody was asking about
balancer right where is balancer I can
say actions okay uh the problem is that
some of these things I will not be able
to run
where
instances H this is balancer where it is
running uh but I can run a command huh
sorry it's not it's not green it is not
running if it is running this process
will come as green
actually uh then chart commands
configuration um so there is a lot of
configuration okay so that is one thing
we saw and and
uh host menu we saw already right so so
far we discussed only hdfs and here is
Kafka if I go to Kafka me just go to
Kafka okay Kafka says Kafka broker three
that means three machines are running
Kafka I told you Kafka will be running
in a cluster since this is a lab setup
we are running Kafka inside Hadoop only
ideally we run it separately because it
needs storage spaces to hold your data
but here Kafka Brokers are three we are
running inside here
only because this is only cluster we
have I mean these machines if I go to
these machines I will show you if you go
to this host menu you can expand a host
and see what it is doing for example if
I expand this
guy see what is it it is your name node
right so and also I can go to hdfs and
see that for example I go to
hdfs and
instances what do you see
here active standby active standby two
machines two name notes I told you right
active and standby they are running
right so the same machines are running
Kafka here it's not a separate cluster
by the way that's what I meant by that
uh I don't have really admin access
otherwise something could have been done
I don't want to mess up the cluster also
by the way as so some of the menus are
actually disabled we can't run them
see exactly ideally that should not be
the case because then it is you not
utilizing the proper space right so
ideally you will set up a separate
broker and that will download the data
actually based in
us they are all running on AWS
actually I don't know I mean I don't
have access to that can be anywhere
doesn't matter aw you can create cluster
anywhere I I don't have any idea as to
who created this cluster and all it's a
third party they just give access to us
even I have only read only access like
you you guys have now uh this is to just
brush up for tomorrow tomorrow we'll be
actually doing it but I want to clarify
one more thing uh if you go to this big
data Cloud lab this cloud data manager
you don't really need because that's the
admin interface you're not going to do
any admin activity the important thing
that you need is web console so if you
click on web
console I just clicked on that for some
reason it is not opening up let me just
see do you guys see
anything why am I not getting login you
can log in but if I have to show I have
to log in
right so where is
moini she's not
here yeah you can log in that is that
will connect you to one of the machines
in the cluster and you will see Linux
you can just log on to one of the
machine that is what you see there and
I'm am not able to log
on you're getting the Linux
shell take some time no I'm taking
long long time and there is a there is a
product called Hue this is very
important there is a product called Hue
this Hue is a third party
application okay but it is very very
popular why do you need Hue is that if
you're a developer and you want to
upload your files to Hadoop you can use
Hue one of the use cases for example I
can click this I'll click Hue and last
the username and password if I sign in
okay I'll just sign in if I click on
this hdfs browser see
there this is my Hadoop these are all
the files in my
Hadoop so hu is like a web UI to your
Hadoop hdfs file system so these are all
the files and folders if you connect you
will not see anything here because you
are having a fresh account one more very
important point I want to tell you is
that in Hadoop you will have a home
directory user SL something I have GL
faculty you will have your own directory
can you try this Hue thing on your PC
just click on this Hue type username
password see whether you can see this
thing are you able to see
[Music]
another point is that sometimes the
cluster will become very slow because it
is not designed to accept request from
36 people right if all of us try to log
in it'll say timed out and other
things
no what username and password you're
typing empty that's fine because in your
Hado Home D
there are no files I have already run
something so that is why these files are
there so we will create it tomorrow
don't worry we will create it using
using the same data notes individually
for our own file man huh the same
cluster so different different users are
created in the hard cluster so if you
log in you will get a home directory
like in Linux in Linux what happens if
you log on you will get a home directory
so this is my home directory in hdfs and
since I already created some files I can
see them you may not be able to see
because it is a fresh uh home directory
for you tomorrow I will show you how to
upload the file run a sample map ruce
program explain the program and all we
will do that tomorrow don't
worry so there was some unanswered
questions I said I will pick it up later
I forgot also anybody had any questions
I
pushed did I tell you I will show
something and I forgot also some of the
things
huh
correct no it will be uh in a non-c
commodity Hardware like a real server
huh the problem is I'm not able to
connect with the web console see it is
not giving me option to
connect uh I need to connect to that to
show you something
hu
Kafka Impala is running
here yeah uh okay I missed the concept I
wanted to talk about it it's a very
important concept right uh do you know
what is a rack
you know what is a
rack server
rack rack is like a rack right how do
you explain what is a rack even I don't
know how to
explain Dev H servers and
all because from the point of view of
Hadoop this might be important sorry I
will explain this one and then pick up
your questions images.google.com
so this is for people who do not know
server rack
do you know what is a rack
now this is a rack it's a housing where
you push all the servers in all the high
five movies you will see somebody
sitting something typing something in a
very big data center and all right so
that's a rack why racks are important
they are important for one uh reason
because can I erase this this
diagram this one
even I don't know what I
wrote that's
fine so why racks are important is
because of one particular reason
see let's say you have three racks
you are having three racks okay and each
rack has a data node not one there are
many so let's say two data notes are
there so uh in reality when you are
installing Hadoop in a cluster this is
how you do right because in a data
center you will have racks and you will
have servers the servers are called your
data nodes right so I have six data
nodes in three racks now there is
something called rack awareness in
Hadoop meaning if let's say you are
copying a block what you can do this
first copy will go
here the second copy will go here
imagine and the third copy will go here
meaning
if you enable this feature called rack
awareness Hadoop will ensure that the
blocks are spread across multiple racks
why yeah a rack can fail rack is a point
of failure because normally you will
supply this power and network everything
through through rack so if a rack fails
uh you know if I'm keeping all the three
blocks in the same rack if that rack
fails then my blocks are are gone so
Hado can if you enable rack awareness
you have to tell Hado which data not is
in which rack if you do that it can
place them like this but why don't it
keep like all three in three different
racks why the uh rest of the two copies
are in the same
rack yeah but same location means these
are all in the same data center right
huh so this inter rack bandwidth is very
important rack to rack the bandwidth is
very important so uh this is just
replication right you will be running a
lot of programs which will use this
bandwidth so to save that bandwidth what
it does is that the next replication
will happen within the rack server to
server but this is enough now even if
one rack and all the three racks going
down is very remote possibility right
and my point was I can show you that
here so when I saw that if you go to all
host can you see
this so in the rack one four are there
rack two two are there rack three two
are there uh and default rack two are
there default means I don't know they
have kept something here but 422 so this
is actually the
distribution it is actually available in
your cloud data
manager Sy no no no no no each rack is
not a system what you going
to not one many will be in one rack
right see
here all the Masters will be in a
separate rack see if you look at this
picture right in this machine some 30
data notes will be
there each slot you'll have one server
they are all rack server so server is
not like this it is like this like a
pizza you get a pizza how does the p
looks like that is how you a server will
look like not like
this there is also that kind of server
Tower servers are there but these
servers typically we use are these kind
of servers I had a picture actually uh
Yahoo Hado cluster at
Yahoo
Hado cluster at
Yahoo
H so the biggest Hado cluster on Earth
is with Yahoo
actually Yahoo holds a record for
that they have 42,000
machines and still they went bankrupt
sold it to some useless
company so the picture is not visible
actually but you got the idea this
is no that's a good question so can I
have multiple name nodes yes the concept
is called Federation meaning ideally if
you cross more than 5,000 data nodes you
need one more name node like 42,000
machines I cannot have a single name
node to handle because the amount of
metadata will become slow that is a
problem so ideally Yahoo has done some
R&D they said that if you cross more
than 5,000 data nodes you need one more
data node that's called name node that's
called Federation Federation means multi
multiple active name notes like you can
have 10 name notes all will be working
they'll share the load actually right so
like that Federation is called
it yeah
sorry all in one rack actually you can
put inside the same data center only you
need all of them right because all the
data notes are in one landan so the name
note also should be inside that only but
they will handle the load actually you
know so some data nodes will be not to
this name node and all they can handle
load sorry just a
moment file server can store but it
cannot process right that's the use case
Hadoop is not only for storing whatever
data you store the data can be processed
then and there by the machine
correct
correct huh so there we are not using
Hadoop right I we discussed it in the
beginning of the class now SQL databases
if you don't want processing if your use
case is such that I just want to dump a
bunch of images let's say 1 million
image just I just want to process and
display in an e-commerce website I don't
use Hadoop Hadoop is not used for that
there you have no SQL databases like
Dynamo DB or mongod DB
right SharePoint if it is a smaller
setup SharePoint you can
use which
one
hastac uh Haack is built on top of this
on top of this yeah Facebooks right
correct correct so it is all business
use case the question is that what do I
want to do with the data there are other
companies who generate terabytes of data
they just archive the
data I just generate data I don't want
to analyze I just archive the data who
cares so it is up to me to decide what
should I do with the data right
for huh I was working with GE very
interesting use case GE right the
company called GE I have a training next
week there so they have an aviation
Department G is building this aircraft
engine you know that flight engine 80%
of the commercial aircraft engines are
from GE and what they do so most of the
planes are flying with G engine there is
also Rolls-Royce and one more company so
all these aircraft engines are having
sensors so when the flight is flying
they capture the sensor data like
pressure and blah blah blah and they
analyze it to predict when the flight
will crash or turbulence will hit or
something like that whether you're going
to die or something so the idea is that
but so I said okay good deal you're
doing this they said no it's not a good
deal the problem is if a flight is
flying from let's say Dubai to
Bangalore there will be two
engines it takes 6 hours each engine
will generate one TB
data how many flight are there in total
and how many flights fly every day can
you imagine the amount of data they
don't analyze how can you analyze I mean
if I'm giving you let's say th000
terabyte every hour or every minute you
can't analyze it so they take a subset
of that they have an algorithm which
takes a subset of the data and analyze
rest they delete and dump because it is
not humanly possible also you don't need
that much amount every sensor data need
not be analyzed these very interesting
because they also have locomotive
engines train train all the Europe and
us they run trains and these trains are
having sensors so they get also that
it's very difficult to work with G
because huge amount of data they also
don't know what to do with the data
right and they have no clue how to say
make sense of the data locomotive data
is huge actually so the trains will
generate a lot of data actually so they
get it uh and then they take it and
clean it and just get only a part of the
data and they analyze it in Hadoop you
also have certain file format
for example AO you may not have heard
about it AO is a file format we use only
in Hadoop Park orc these are all used to
compress and store your data in hadu you
can also have compression techniques
because sometimes storing the data as it
is is not good why don't you compress it
but what is a drawback of
compression you have to decompress you
need a lot of processing power if you
want to decompress it so AO is a
serialization format it doesn't compress
the data par and all will compress the
data column and push the data so I will
show you in Hive when we discuss Hive
when you create a hive table you can say
that the data has to be compressed it
will compress and store the data
actually that is
possible so my point is not everybody
want to analyze all the data in the
world that is what so humanly not think
about Facebook how much data they'll be
getting probably they're not analyzing
the whole data as subset of interesting
data that is all they will be
doing so any questions we'll wind up at
6:30 I think but before that we can pick
up
questions
huh yes spark is memory intensive
ideally spark requires Ram so if you
give Ram it will perform better if you
don't give Ram it'll use hard disk I
mean rest of the space use hard disk
performance will be slightly
degraded so for streaming and we need
lot of ram the problem is uh football
World Cup is going to happen right yes
right
when this year only right so let's say
you want to write a program which will
download all the tweets and find out the
most favorite player for football World
Cup possible right so I can easily
stream the data from Twitter the problem
is I want to find out
uh you know the most popular player
every five minute imagine probably not
five minute you want to do so I want to
get five minute for data hold it in my
memory and process what if five minute
data is so huge I cannot hold it in
memory then it'll be
slow yes I'm just giving an IM imaginary
situation where if five minutes for data
is in terabytes you can't hold it
actually not only tter any data that you
have then you might want to persist into
disk some of the data then processing
will become slow it is not real time
then so when you are running spark
streaming and all you know your RAM
should be available otherwise it will
become very
slow today's match you will predict
tomorrow right so things like that can
happen so that's not really good
actually so a lot of considerations are
there where you have to use all these
things so I think we will wind up what
do you
think yeah so it's almost 6:30 and also
that you to is SL yes so I will be
sharing slides also books so uh there
there are a couple of good books written
on Hado which will give you a lot of
idea about Basics like how hdfs is
working or all these things actually
much we need to understand to
basically um so I would say whatever I'm
training here is enough but go through
the book also say for example hdfs you
don't have to to be an expert on high
availability see there is an active name
node and standby name node if the active
phase standby will take over how that
happens there is a process so probably
that topic even if you don't know it is
not a big deal because that's an admin
topic if you are a Hadoop admin you
should know how that should work but
since you are a developer you don't have
to go in depth into those topics
actually so map reduce And Hive should
be more than enough I think and you are
building this awareness to learn
spark the big basic idea is that you're
building this so you can look at spark
so I will say concentrate on spark so
let's talk about Hive right which you
people might be mostly interested and
I'm not so usually I'm not a slides
person but for Hive I need some slides
this has around 60 slides I will not run
all of them don't worry I'll share it
with you you can go through that I'll
run couple of them to get an idea right
and so that otherwise everything I have
to draw and explain it will be difficult
for Hive right so Hive is actually very
simple uh in 2005 when Hadoop came most
of the major vendors started using haduk
like Facebook Yahoo and all uh but they
all faced one common problem the problem
was that uh Yahoo started using Hadoop
uh but Yahoo had a lot of developers who
did not know Java so the problem was
that they had to write map ruce programs
and uh skills set was the problem you
can't say that everybody should learn
Java and start writing code so Yahoo
found it very troubling same time
Facebook also started using Hadoop and
they also had the same problem Facebook
had mostly SQL developers and they were
not comfortable with Java and way back
in 2005 and 6 we had only map reduced
there was nothing else on Hadoop okay
plain map reduce nothing else uh so the
Hadoop ecosystem we saw in the last
class none of that was available so
everybody was confused so Yahoo and
Facebook started two projects same time
uh so Yahoo created something called a
pig the creature pig animal
pig seriously it's called
Pig pig. apache.org
see that's a pig real pig creature so
Yahoo created this tool called Pig it's
a scripting language I teach a course
called a pig on Hadoop only on Pig it's
a very interesting language scripting
language uh Advantage is very simple you
learn this language uh and then you
whatever analysis you want you write in
that language it's a script so very
small like four five lines Max or 10
lines and once you write your pig script
you say run it will convert your pig
script into map ruce program so you
don't have to learn map ruce you learned
Pig and somewhere in 2010 and 11 there
was a heavy job inflow for pig
developers 80% of Hadoop job was handled
by Pig in 2010
80% it was such a successful language
that everybody was dying for pig I have
taken only Pig trainings people don't
because that was the effect of pig
actually right because it's a very easy
language scripting language you just
learn it and everything will happen
behind the scenes you want to do a join
operation you say join a comma B that's
it even SQL queries are more complicated
in join you have to say a dot b dot here
nothing take a b join a comma B done
joining done simple inner join I'm
saying so people started using Pig
extensively but at the same time your
Facebook developed Hive hive's idea is
different you write sequel okay you
create your databases your tables and
all the blah blah blah you write SQL and
hi Will convert your SQL query into map
reduce same concept so two companies
developed one Pig and one Hive they
didn't know that they were doing the
same uh thing
ultimately but then what happened
somewhere around
201101 Pig started losing the attraction
because spark came so when spark came
and became predominantly useful spark is
also very short and like a script you
can write in Python and all very easy to
write so spark and pig are almost
similar so people thought like if I'm
learning something very short and
interesting and very fast why should I
learn Pig I can learn p uh spark itself
right so pig is no longer used by
anybody very rarely pig is gone almost
Hive is not gone because it is SQL SQL
will be there till the world exist right
no questions asked right no questions
SQL is SQL so you can't get rid of SQL
right how do you get get rid of SQL what
what if all the ATL developers and you
know there's a huge Community to back
SQL not like Pig I actually like Pig
more but there is nobody to back Pig
right there is a lot of people who back
SQL right and and that is not the only
reason hi is a beautiful data warehouse
it is treated as a data warehouse of
Hado meaning you can create tables and
uh you can query the data so most of the
visualization tools and all uh you know
can use Hive in the back end to fetch
the Rell and all so Hive became very
popular and now what is happening Hive
is there uh plus spark is using Hive in
spark you have a module called spark SQL
that uses SQL language uh now that guy
can process itself it can also talk to
Hive so whatever data you have in Hive
spark can also read so Hive is
predominantly used these days and hi
will always be very uh popular uh it may
be I think one of the most popular
ecosystem tool is Hive so far in the
industry because it's been 10 years 12
years and still uh uh Community is very
much uh so the the um command wise it is
very simple because it is SQL and I
don't think I have to teach you SQL
that's not the idea behind the session
you will already know SQL but we will
just see what is happening behind the
scenes if you write a SQL query uh and
some of the advanced concepts Hy like
partitioning bucketing indexing uh these
kind of things and from the industry uh
experience there is one major thing uh
there are two major vendors hoton works
and claa right now hoton works uses
original Apache Hado that is their
propaganda so what Hive is also Apache
product it's open source right and
whatever major changes are there in hive
are predominant in hoton works not in
claa claa is actually not so good at hi
we are using a claer lab that's bad
actually I mean I'm not blaming the lab
but claa is actually a bit bad in hiide
I'll tell you the reason
because I I teach at hoton works that is
the reason I'm able to tell uh in hoton
Works they have a project called
T some weird people from Delhi created
this project T is a Hindi word there is
a weird movie by that name also right
have you heard t t t speed in
Hindi
huh huh so T is a uh how do you say it's
not a replacement the is a project
created by Indians then they gave it to
Apache it became open source th is
faster map ruce so what these guys did
normally in map ruce what happens you
read the data right and uh then you do
mapping and then you store the output in
hard disk again Shuffle you read the
data do the shuffle store the output
back in hard disk again reducer will
read from hard disk so some weird guys
thought why don't we do all this in in
memory we don't want to use hard disk
read the data once do all the map reduce
Shuffle and then push the result so your
map reduce becomes 10 times faster that
is T so it's a dag a directed a cyclic
graph concept and th was given to Apache
what henbs does if you write a hi query
they will convert it into a taze job not
map reduce job so hi queries are very
faster on hoton Works in a claa cluster
claa doesn't support the because the is
contributed by hoton hoton BS they are
competitors right so if you are working
on a claa cluster you will never see th
in your life they won't allow so the
problem is that if I'm writing a hi
query on cloud era it's very slow
because it'll convert to map reduce run
as a map reduce program takes a lot of
time if I'm writing the same hi query on
hoton works it's at least five times
faster because it converts to T job by
default it is T and T is superb you
can't compare right so that is one
drawback I mean so when you start
working in the industry so these days
mostly people are moving to hoton works
for majority of reasons but the stake is
at Cloud around Li number of of users
are more for cloud era but over a period
of time probably people will shift to
hoton work so these are good to know
information so then you will be thinking
that from claa there is no answer to
this no they have
Impala there is a product called Impala
Apache Impala Impala's contribution is
mostly from claa Impala does the same
thing what does in memory fast real time
queries so I'll show you Impala we have
Impala in the cloud cluster I'll show
you hot also if you want to so T itself
as a project is not really uh uh uh yeah
not really uh how do I say important
right now since spark most of these
things are gone including T because
spark is also in memory T is also in
memory so when spark came T relevance is
not there but the only relevance is that
if you're running a hi query it converts
it into a t job so it's faster actually
on hoton Works platforms so I will show
you a hoton cluster probably tomorrow
okay I have an access I'll show you the
T job running you'll see the difference
actually so SP but there are a lot of
companies who run traditional hi only hi
like they don't want to use Spar they
just want to use hi for them going to a
cloud era may be a bit challenging
because hoton works by default give you
free T right so T is fast also T is fall
tolerant Impala is not I'll discuss
probably tomorrow probably too early to
discuss but uh there are SQL engines are
too many actually in this category and
there is a tight competition between all
of them and it's all marketing gimmick
you go to Cloud they will say that
Impala is the best in the world never
listen to hoton Works hoton work will
say that and hoton works recently Hive
recently announced something called
LP Apache Hive LP is realtime queries
using Hive Hive is normally batch
processing because it is converting into
map reduce right but recently Apache
announced that how you can support
realtime queries that's called
LP um the the uh uh real abbreviation I
don't know Long Live and process that is
not the real one but we used to say long
live and process it is still in beta not
so but the problem LP is available only
on hoton Works no Cloud era open source
right because it is a contribution of
Open Source Hive which is available only
on hoton cloud cannot get it so they are
still with Impala Impala is the only
solution they have so when you talk
about H there are a lot of things but
you should be aware of the industry
what's happening right so these are the
reasons why people are shifting towards
hot and work some of the major things I
see compared to that doesn't mean cloud
is bad okay it's not like they are also
having great support their technical
support is excellent and even with the
Impala they do a lot of tricks actually
too so if you have a problem they'll
give a solution but uh a lot of people
are preferring this open source
initiatives hoton works and all are open
source but the problem is I told you
right I was with G they use LP none of
the queries work most of the time it is
open source right if it doesn't work you
can't blame Cloud will fix it because it
is propietary like iPhone and Android
right so open source you get lot of
features but it it will work or not work
you cannot guarantee that's one thing
they but they will blame internally on
Apache they say that we'll give you
technical support within the limits so
originally the product is by Apache so
hoton Works cannot fix directly because
it's an open source Community most of
the things work I mean I'm not saying
that everything doesn't work but open
source has its own drawback sometimes
right uh but I'm seeing a lot of people
migrating to hoton Works actually these
days
right so in Hive what you need to
understand is these things it provides
SQL like interface on
now very important Point Hive does not
have
storage it uses the data in Hadoop for
example you copy a CSV file into hdfs
and what I can do I can go to hve say
create a table and give the schema then
I say load this CSV to the table what is
going to happen the CSV will be in
Hadoop it is on hdfs it is as blocks and
replicas and all but in hi you'll just
project a structure
so you get a table once you get a table
you can do any SQL most of the SQL right
so the idea is to project a structure to
the data that is already available in
Hado and hi can create tables from a
variety of formats structure data even
log files unstructured data uh Json
files different types of files can be
read by Hive and put it in a table
format right so that's the importance of
Hive uh so it's treated as the Hadoop
data warehouse system so the data is in
hdfs and this is just giving a table
like structure on top of the data hi
will just maintain the schema only
because the data is already in Hado
right so you don't have to load the data
even though you say load the data the
data will just become uh in Hadoop I'll
show you how to load the data I mean how
to create a tables and all yeah so if
you want to run a query you can directly
say write your SQL say select star from
the table but the problem is it will hit
hi hi will read the query and it will
convert it into a map reduce program
Java base create a jar file so that will
take some time then when it runs it's a
map ruce job and the funniest part is
Hadoop never understands what is hyp for
Hadoop it's a map reduce program you're
getting the point right from the
hadoop's point of view there is nothing
called hi it understands only map reduce
so whether you write a map reduce or hi
write a map reduce it's the same thing
right if you want you can write a map
reduce program to query structure data
it's very difficult that's why you're
using hve it'll translate and create a
jar file push it to the cluster and the
cluster will run the map reduce regular
program show you the output you can save
it in a table or or anything you can do
so you can use all regular SQL
statements you can say insert into a
table from this table so the result will
be if it is matching with the SCH will
insert the same way the only thing is
projecting a structure had Hadoop only
hdfs only everything is on hdfs it is
slow Hive is expected to be slow so that
is one thing hi is considered to be a
bat processing engine so it is not a
database another common problem is that
people may not be able to differentiate
between a database and a data warehouse
they're different right a database is O
TP you call transactional real time
right where you want a millisecond speed
right subse speed that's a database and
you always have insert update all these
kind of operation a data warehouse is
different a data warehouse is a place
where all the cleaned data comes and
lands right from there your reporting
tools can connect and visualize the data
right so Hive is a data warehouse where
you store your final data so let's say
you have lot of tables you ran queries
and you got the final output you st
store it in height
so that your visualization tools
can huh store it in hdfs but we say
store it in hi but it's in hdfs only
meaning if I open Hive and say select
star I will see a table if I go to the
location open the file I see a text file
same thing so the text file will be
there if I open and say select star same
text file it will show me in a nice row
column format and another thing is that
hi's language is actually called hql hi
query language it's not
SQL but it is based on SQL 92 syntax
most of your queries will work okay but
it's technically called hql hi query
language right it's it's up to us to
decide I mean um so the idea is to
familiar with familiarize with h then
depending on our business use case we
can write any SQL queries you can write
complex queries simple queries you need
to have some idea about SQL queries like
what is a join operation what is a group
by query to understand what is happening
even a person without knowing SQL can
learn High I mean by looking at a query
you can understand what is the intention
of the query
right Hive is not a DB Hive queries take
minutes even for small data sets and
can't be compared to databases like
Oracle Hive does not provide realtime
queries so that is what the original
Hive which came was converting into map
reduce so it was very very slow then
came th so what hoton works did they
started using High plus th which is a
bit more faster right claer cannot use
th so claer still runs Hive into map
reduce and their answer to that is
Impala Impala is a inmemory execution
engine but Impala will work only on
cloud era so I will show you Impala and
same query will be very very fast there
is no difference apart from that I mean
syntax and everything is same so uh uh
data warehousing in the sense like
usually it is used as a storage engine
rather than so what is a data warehouse
a data warehouse is where you store all
your massive data right structure data
and the intention of Hive is also same
and all the bi tools can connect with hi
so if you're having something like
Tableau for example Tableau is a
visualization tool so I want to
visualize terabytes of data from Hadoop
I can connect Tableau with Hadoop and
Tableau will be firing SQL queries using
hi and the data can be visualized so
that's a typical application of a olap
system so it's an olab system online
analytical processing it's not an olp
system so you can't compare it like
Oracle because that's like real time uh
but again hi cannot
replace a proper data warehouse okay so
if you're talking about something like
Terra dat for example so they are like
how do I say the expensive and fast
reliable data
warehouses understand hi is built on top
of Hadoop only so it has all the
drawbacks of Hadoop right so most of the
organizations what they do these days is
that they will categorize their data H
data cold data uh the hold data will go
to something like Tera data you know
where they need to immediately fish the
data run faster queries and the cold
data which it's okay you can take some
10 minutes to query we'll go to Hive
Hive supports insert statements now it
was not there before insert statements
are already there but the problem is
that each insert statement will fire a
map Produce job so you do a bulk load
that's better so in in Hive normally
when you load the data you do a bulk
load you don't insert it and the latest
editions of Hy support all the cred
operations create read update delete
everything is supported uh but again if
you want to get the speed you should use
LP that's what I'm saying real time
queries LP and LP as of now cloud data
doesn't support so I can say if you are
on a cloud era platform inserts and
updates will be very slow not like terra
terra and inserts are very fast but the
question is that in a data warehouse
usually you don't have to do an insert
very rare right because you will do bulk
loading of the data or your ETL tools
will be dumping the data there they will
collect data from the rdbms and then
dump it into there so very rarely you
modify the data in a data warehouse so
the use case is like that for he yes all
the cred operations are supported even
insert update delete but it'll be slow
on hotworks platform you have acid
support enabl transaction management but
it is not as reliable as a database okay
uh because you can never replace an
rdbms system with something like he but
you have transaction management
supported that is with LP LP is the
feature which gives High realtime
performance but LP also requires
resources it cannot so it requires more
resources to execute but you get all the
asset properties transaction management
everything and this is very important to
understand
so there is a command line interface for
hype it's called Hive shell you can open
the hive shell and start typing your
query say create database is create
table and then type the query so that is
one way to work with hyp uh but people
may or may not use it because if you are
you're in the production and all you
might have some client right so some
what type of clients you use like to
connect with uh DBS and data warehouses
so SEC server or Todd or something know
something like that you might either you
can use that so one thing is that you
can use use a CLI and hi is very vast
actually so I'm getting confused when I
talk this CLI there is two clis the
original CLI is called Hive and you
simply type Hive it'll open you do all
the blah blah
blah the new CLI is called bline it's
called beline okay and beline is a
proper jdbc client so through Bine also
you can open the CLI or you can simply
say hi it will open the command line for
you you can do all the create a
statements and all there is a web GUI
but it is very rarely used Hive is just
a client it's not a jdbc client so when
I open the CLI and say Hive jdbc you
have drivers right to connect so when
you want to connect from an application
right to a database you need a driver
that's called a jdbc or odbc drivers are
there that's one way to connect uh the
so the CLI original Hive CLI is called
Hive client that's not a jdbc client you
just open it it'll directly hit Hive and
there is it'll directly hit the hive and
you can type all the queries and all the
B line is a proper uh jdbc client which
means you can install it in some other
machine from there you can connect you
can give the server address port number
and it will hit right and it is not
written here there is something called H
server this whole part is called H
server this part for some reason in the
diagram it is not written but this whole
part is called hi server because if you
are connecting with a data warehouse
there should be a server component which
will accept your queries you know
whatever you're typing right so that is
this whole part so whether you come
through CLI and there is a web GUI or
jdbc you all hit the hi server
right and H server is the component
which will accept your query so once you
write a query there is a planner parser
Optimizer it'll optimize your query and
convert a map reduce program and push it
into the Hadoop
cluster in hi server itself you have H
server one and hi server two okay so the
old one is called hi server one which
nobody is using now you don't have to
bother uh the latest one is called hi
server 2 okay that is every everybody is
using so H server 2 is responsible for
managing all your connection so if you
open a connection with h it'll directly
hit this H server to and from there
whatever query you write this guy has a
you know compiler Optimizer executor
it'll get the query compile it optimize
it push it as a map reduce program right
I think I can show you that from here if
you go to Cloud manager should be here
it is very difficult to talk about Hive
rather than showing something because
talking is fine but you know if if I
show something probably people will
understand more so if I go to hi
configuration uh where is H
server H see H server 2 so this is
called H server
2 the the default Hive server is hi
server 2 because there was a old H
server one which nobody's using now
because of performance issues and all so
right now whether it is HBS or cloud or
any platform the server component of hi
is called hi server 2 okay so either you
come through a jdbc client or the
command line you are all hitting H
server 2 okay from there it will uh
compile your query and optimize your
query and push it and there are tons of
optimization techniques in Hive to to uh
you know read your query
and better it and all probably we will
discuss that tomorrow some of the
optimization
techniques another very important point
in this slide is this metast
store this thing meaning if I open a a
hive session and I say create a table if
I exit it should not go right because
the table should persist whatever you're
creating so that is where the meta store
comes into picture that that is where
all the metadata will be
stored and in production setup what we
do is that we will create a myql server
and in the MySQL server meta store will
be configured so all the metadata of
your table like the schema access rights
everything will be in the meta store
right uh and U so you have to ensure
that the metast store service is running
otherwise hi will not start because it
needs to talk to metast store this meta
store is very important because even in
spark spark will talk to this meta store
so if I create a hive table I can read
it in spark because it can talk to the
same meta store and get the data from
there all the metadata will be there in
the meta store okay meta store is the
metadata metast store is a service and
that will push the metadata so you have
to give a destination in the so here it
will be there I'll show you where the
metast store is I don't know where they
have configured here if I go here to hi
configuration
right hi meta store server
thread ah database type is my SQL right
uh
yes this is where it is
see on this machine they have configured
MySQL and that is where the metadata
will be pushed and we don't have any use
of the metadata hi will read and write
it that's fine okay and it uses it to
persist all the table information and
DBU create and everything right we we
don't have much of a use for that but it
is there and in hi you have something
called a cost based Optimizer CBO
CBO H I don't know why it is
disabled enable cost based optimiz
disabled it is disabled for some reason
I don't know so this is called CBO cost
based Optimizer you are aware of rdbms
right if you write a very complicated
query it will generate multiple query
plans how to execute this query this
Optimizer so what Hive does if you
enable it if you write a very
complicated query it will generate
multiple plans and based on the cost of
each plan it will select the best plan
so that is the cost based Optimizer it
is disabled I don't know why it is
disabled so usually uh you can enable
this and this is how optimization can be
done in Hive right so this is one thing
CBO we will look into other things also
um
okay all the configurations are here to
start hi you simply say hi that's
it you simply type hi it will start the
hive
shell may get some warning that's
fine and you will easily see a warning
saying that hi CLI is deprecated and
migration to beine is recommended that's
okay so what it actually means is that
this is the original hi
CLI and I told you that beline is the
new client so they are saying that use
beline whenever possible so that's fine
I mean to learn Hive I think the best
way is this CLI they have added some
more commands in the bline section
actually because the old high CLI
support is uh getting lower every day uh
so but for learning purpose and it's all
the same whether you use the CLI or
bline I'll show you how to use the bline
also um so this is the H shell what you
open right and it's very easy to get
started with hi all you can do is that
you can start by creating a database you
say
create database so first thing is that
we create something called a database
that is the highest level of abstraction
so I say create
database I don't know
something May
19th so um I just created a database so
the command is create database and the
database name so I'm just calling it as
May
19th and if I do a show databases
you can see lot of DBS are there people
have created right see
here so these are all the databases that
got created inside Hive and minus main
19 right you could have created
something else right whatever I mean you
remember and you have to say
use to switch the DB you have to say use
May
19 these are regular commands if you
aware of SQL right H there is a way to
show that it will display the DB name
Set uh
hi there is a property where it we can
enable it to see the DB I'll search for
it you can even print the column headers
so normally when you say uh select star
or something it won't show the column
headers
the column
name you have columns and the names
right it won't show but you can make it
show there is a command to print
that so first we'll make sure
everybody's on the same
page
hi
print current
DB set High CLI print current
DB so this is how you can uh print your
DB you simply say set High CLI print
current DB equal to true and then it
will display the DB there can you try
this everybody is able to log on to hi
and start and all anybody having problem
in Python you don't have right no you
have
indentation indentation is very
complicated actually you miss one
indentation nothing will work in Python
I know that a
lot very difficult people who are coming
from the Java world and you are familiar
with curly bracer here everything is you
start a control structure there is an
indentation right but the editor will
take care of it I think you're using
some editor the that will be in the
metast store only the definition right
metast store it will have it all that's
what when you normally start H I said
the use may9 right that the DB it won't
show me in which DB I am in so now it is
displaying it here right see this it
will show in bracket what is your
current DB uh so it will display
whichever data datase you are in if you
change it will show
right and another important point is
that probably towards a little bit
towards admin site there is a file
called hi site
XML hiy site. XML and that file has
around thousand
properties and the whole Hive is
controlled by that file so these
properties are actually from that file
meaning either you can do like this you
can open a session and say set High blah
blah blah true but if you exit and relog
in it will not print the DP again you
have to type this or you can ask your
admin change in Hite XML uh this
property so that for everybody who log
in the DB will be visible so that file
has lot of properties actually I'll try
to find that for
you problem is that I was with Hon works
for the past 10 15 days I'm finding it
bit difficult to read Cloud era but
it'll be
here
configuration ah or do one thing no it's
easy uh what you can do all of you can
do open one more command prompt uh web
console and just
copy uh um if you go to
CD
Etc
Hive
con uh yes if you navigate to this
location this is common for cloud hoton
Works everybody ITC Hive con folder uh
you will have a file called Hive hund
site. XML this file controls end to end
of hi all the properties right and if I
open it you can see all the
properties H see hi metast store URI
this is the metast store URI remember so
where the connection is there then some
other things are there autoc convert
join uh how do you search in VI
editor this one right then I can type
right uh what did we set now High
CLI oh no hi
dot it says pattern not found it should
be there right I mean I'm just wondering
or what if I search DB saying pattern
not from DB should be there let me just
search for
Hive probably I don't know uh see all
this um what you say properties are
there in this file probably some of them
are uh so another thing is that you are
accessing this file locally I mean from
this machine if I go to Cloud manager
configuration I'll say
DB should be there somewhere I think
some of them are uh actually not visible
to us but all these properties are read
from Hite XML all these things will be
in Hite XML actually so that is how you
can print the uh current DB you are in
well that's not our intention what is
Our intention so let me show you the
data that we have so if you go to this
folder right you will have two files now
once you start loading data to
Hive it is very complicated even though
the queries are very simple people try
to get it is like watching a Christopher
noan movie you know you won't understand
what happened once you finish the movie
also you won't understand what happened
so the point is I will write first then
probably we will do it okay so you will
create a table that everybody knows this
you will do in hi you will say create
table blah blah blah it will create a
table fine then next step is that you
want to load the data into the table the
data can be loaded from lock file system
or
hdfs it's a bit
tricky so if you create a table and you
want to put some data right the data can
be loaded from Hadoop or from your Linux
file
system right okay and I will show you
what will happen how to do both this and
what is going to happen if you do both
this what will happen to your data and
all these things right so we'll do step
by step otherwise people get confused a
lot in in this stage it's actually very
easy but uh end of the day people get
confused a lot where is my data what
happened to my data sometimes you will
get local also so what happens these ETL
tools and out sometimes they dump it
into some local machine you can load you
can say hi take it from my Linux dump it
into so it is actually copying from
Linux to Hado but there is a way to do
it that's what I wanted to show so let's
look at the data right we'll also do
some small analysis uh can you open this
file
but I need a better editor
actually wordpad will do
right uh that file is already available
uh in your uh data
set let me download this okay real quick
can you see this file transaction. txt
TX
ns1 it will be here you will have a code
and data folder inside that hi inside
that I think uh it's it is a txt file
right because for me the extension is
not visible
uh just give me one moment I'll just
install this notepad++
so last class you are watching narcos or
what the ads are coming
right what happened in the morning
um so what we will do is that first we
will copy the file to the local file
system that is Linux and then we'll
create a hive table we say load the data
to The Hive table and we will see what
is happening in that process then so
there are two files actually another
file I'll copy to hdfs from there I'll
go to another table and show you what is
happening so okay we'll try without this
I mean you have notepad anyway right so
that's fine I just wanted to show you
this yeah so this is the transaction
data what we have here and if you look
at this data right uh I'll just uh take
one line and explain what it is so you
can understand right so if you take this
line so the first uh column is the
transaction ID that's nothing but 18
then then you have the transaction date
uh then you have the customer ID so
4244 is customer ID okay and then you
have the amount spent
$88 then you have the category of items
he bought team sports and what actually
he bought Baseball City of Salt Lake uh
state of Utah and credit so it's a
typical sports store Transaction what
category what item what amount credit
debit you know and you have n number of
lines like that it's okay that's okay no
problem so this is one data that we have
it's a transactional data and we want to
create a hyp table load this data so
that's one thing second data that you
have there is a file called a cust can
you see
this and I want to open this
with uh word
pad yeah so this is the c customer data
so First Column will be customer ID
first name last name age and profession
Christina Chung age 55 is a pilot like
that right so you have a store customer
data you have a transaction data right
and what we want to figure out is that
we want to figure out the total amount
spent age wise by the customers for
example I want to know in my store what
is the total total amount spent by
people age between 20 to 30 and 30 to 40
and 40 to 50 so that is the analysis we
want to do so in one data set we have
all the
transaction like how much amount they're
spending and all second um uh data set
we have the person's name and age and
all so we'll do a join and then we will
uh do a case statement and figure out
that's what we are going to do so the
SQL analysis is not the intention the
intention is to show you what is
happening happening behind the scene and
and how hi is dealing this right okay so
now what I want you to do is uh first we
will do uh only the uh local so we will
copy the data to the local file system
from there we will analyze it so I want
you to copy these two to our uh local
system so how do you do it you have to
open the FTP stuff right so I'll just
say FTP
login and hopefully it might work for
me upload
files so upload these two files one is
called cust CD another one is called
txns One
open yeah so it works
here okay I need to to open one
more
okay huh luckily for me it is available
you have to figure out so so use FTP and
copy the files to Linux not to Hadoop
don't upload to
Hadoop I'll just wait if uh you need
some
time yeah but uh we don't have to change
the extension yeah so uh if it is a text
file format it can access any text file
only thing you have to mention the D
limiter comma or what ever it is so hi
can handle text CSV Json XML variety par
so there are multiple file formats it
can handle so by default it'll read all
the text but if you having Json and all
there is so are you guys familiar with
something called serd do you have you
heard about serd no serd stands for
serializer
deserializer okay uh it's a very common
practice so what will happen is that
let's say you have a Json file you know
what is Json right key and value huh
like key value pair so if I have a Json
file I can ask H to create a table from
that but the problem is that Json is
semi structure data it's not proper
comma separated or something and how
will say I don't know what you're
talking about I can't find a schema so
to do that we can use something called a
Json serd you have to download it it's a
jar file I don't know whether the latest
H has it if not you have to download a
jar file which is called a Json serd
Okay add it into Hive and then you say
create a table using the SD so using
this serd it will parse the Json and key
will become the column value will become
the data like that so serdes are used to
uh what do I say read a semi-structured
unstructured data I will show you an
example of unstructured data where you
can use something called a reject serd
regular Expressions you can use to to
the data so hi Supports number of series
actually to read different types of data
Json I will see if I have a data show so
that is again an open source uh projects
more most of them so internally nothing
happens they will read the key and the
value if it is a Json serd and it will
give that structure like a table to Hive
so if Hive looks at it hi will not see
as key value pair I will say a column
header and a value it'll just apply the
same thing so it is reading the semi
structure data and giving a structure to
that it is a parser so we have parsers
right it's a parser
actually so this thing are you able to
do are you able to upload
everybody and and find this file you
have a file called commands can you see
whether it is there
uh just commands in the high folder you
will have a file called
commands uh no no no this is not uh uh
this is the commands we have to type or
copy paste so we don't have to upload
it so this is in the rug for me this
data is in the folder called Ragu okay
that is where the data
resides um let me close
this now we can actually create a table
so let me just copy paste this um you
can so don't do a create database
because the first two steps we have
already done we created a database and
then we said use it okay and the first
thing you need to do is this command
I'll explain it so just copy this create
a table just go here
and paste it and add a semicolon at the
end and hit
enter you have to add a semicolon
otherwise it will not work and hit
enter yeah so this is the typical uh
create table command in SQL it says
create a table and then the table name
is transaction records or whatever name
you want you can give and within the
brackets you are mentioning the schema
of the data so hi support
simple and complex data types so those
who are aware of data types so you have
simple data types like integer string
and all it also supports complex data
types like map strs and all right so
they are all supported but here we are
just starting with a very simple so I'm
just saying even there is a date data
type uh and and the important Point here
is not the schema schema I'm mentioning
you even have a data type for date I'm
not using it I'm saying string but hi
supports date handling and all uh so
after that the most important Point here
is row format D limited Fields
terminated by comma what that means hi
is expecting the data in row by row and
the D limiter is comma which means you
can load only
comma uh separated files ideally right
I'll tell you what will happen if I do
something else let's say I'm not using
comma what will happen I'll show you but
right now we are simply Crea creting a
table there is a record reader for Hive
so that will read no we don't have to
Define okay so now this table is created
right so all of you are created Hive can
be accessed through Hue you have a hue
right that I'll show you that there you
have auto complete mostly but the hard
way if you're working with h there is no
auto complete you have to type
everything
manually now now is the important thing
how do you load the data right and there
are lot of things I want to talk about
about loading the data first of all
identify where is your data right so in
my
case the data is in this
location so just make sure you
understand the location of the data in
my case this is the location I'll just
copy this and the data is txns 1.txt and
I will
say load data local input
path into
table so this is the
command uh the path I found here I went
to uh this location I did a
PWD see this is where I have the data
right
it is on
Linux it is on
Linux if and it is very easy if you make
any mistake it will say path cannot be
found it will not load if there is a
correct path loading the data right it's
very easy to identify whether the
command is correct or not in my case see
it says loading the data so that means
it is working fine for me are you also
getting the same thing so make sure it's
a local path okay so people get confused
it's is a local
path yeah because here we have not done
the analysis we just loaded the data and
here you can write SQL query you don't
have we'll be writing SQL you don't have
to write the Java program
and not at all you don't have to know
Java jar file it will create a jar file
actually if you mention a folder it will
load all the files in the folder so just
mention the exact file he mentioned the
folder name so there were two files he
loaded everything
so here since it is a data warehouse it
just happens two files but from where to
tret that will be another problem right
even I don't know this should be the
file size in bytes I
think uh roughly it is 4 MB right should
be 4 MB the size of the file
actually now this this part is fine
actually I mean you are just loading the
data and if you want to check you can
simply do a select star
from txn records you can do a
limit five can you try this do a select
star from the table do a limit five
don't simply run a select St there is
around 50,000 records I think so do a
limit I want to see the top five rows
right so that's the command this is to
make sure the data is loaded now you can
uh upload the data from local as well as
Hado so I'm just showing how to do this
I'll show you from Hadoop also both
possible h no select star mean select
everything from the table I want to show
see the Full Table Right limit five
means top five rows I want to
see now if you actually want to see the
query right you can write any query
doesn't matter but I can simply say
select count
star from
so this select count star is a query and
as you can see the moment I run the
query it is firing a map Produce job see
so the query will fire a map reduce jaw
because it is converting it into and see
map zero reduce
zero and this is this cluster is really
good the the what I'm telling you is
that we have around 10 nodes here in the
cluster and only I think around 20
people are using it very less data
that's why it is very fast actually it
is very very slow this is this is
unexpected if if Hive is fast there is
some problem you are to troubleshoot so
this means select star select star means
show me the full table limit five I want
to see the top five rows you just
verifying the data is loaded no the top
five rows because it has 50,000 rows if
I simply do select star it will display
50,000 I don't want I just want to see
no no select will not fire a map
producer because it's a simple read it
doesn't need to do any calculation so
when you do a select star uh it won't
fire a map reduce job it will simply
show you the
output so just let me know are you able
to do till this you have to do a select
count star it should fire a map reduce
job and you should see the
output uh one more thing I want you to
do in that file I have given to you
right this commands file uh it might be
slightly wrong not wrong ah don't copy
paste that's what I'm saying that I'll
write it here because I use it for a
different session also so there uh if I
open this here it will say load data in
path see it is saying load data in path
it is actually load data local in paath
right so don't follow the same thing and
you can correct it also whatever you're
running here just correct it there so
that it is uh reflected right otherwise
tomorrow you try and it may not work the
way you expect because it's the map
reduce uh output I mean it is firing a
map ruce job right so map ruce logs are
just being displayed that's what you can
suppress it there is a way to suppress
it but normally if you submit a jar file
map reduce it displays the same thing
MPP reducer that's one finally you will
see the result also it is a default
nature actually in hi because even
though it is a hi query it is not a Hy
query it's a map reduce program that's
why it shows map and this is good for
troubleshooting sometimes it will show
you uh errors and warnings and all when
it is running I'll show you how to
enable column but right now oh you doing
a select Star right if you want column
names
set hi. CLI do print
do header equal to
True okay now if I do
this this is countstar right it'll call
definitely because it it is some
analysis it has to call for it only the
select star will not call because it is
just reading the data
so are you able to load the data at
least run the basic query then that's
okay he's asking can you see the Java
code high is
writing no I I have not found it so far
how to do that no that that is internal
to Hy I mean how it is converting that
into a Java Pro actually we have all
this in map reduce for example how to
write a join in map ruce is there select
statement doesn't normal select star is
simply showing right it doesn't convert
to map where condition internally there
should be some logic again to everything
is key value pair end of the day ah
everything is so another drawback is
that a hive is good but you will not
know what is happening behind the
scene like end of the day how the code
is written you will not know you will
see the result but that's enough if you
know SQL you can just fire the query
right Hado into Hive huh in Hadoop to
Hive and Hadoop because it is in Hadoop
and it is visible in Hive anyway so that
now the important point is I don't
actually want to show you running the
query that's so once you now you have
this table and you can write all the SQL
query that is not my intention the real
question is where is the data first
question right so you loaded the data
and uh from local you said that means
from Linux so first thing you have to
understand is that data must be in
Hadoop so when you did a load data it
could have gone to Hadoop that's for
sure but where in Hado right so to
understand
this what you need to do um you have a
describe
command so let's say
describe and txn
Records so this is a very common command
if you do a describe on the table name
it will show you the columns and data
types and all and if you want you can
also
say
describe formatted and the table
name so describe formatted will give you
more details about your table that you
created and if I hit enter there is a
very interesting thing that you can see
the thing is it says table type is
managed
table so any table that you create in h
either it can be a managed table or
something called external table by
default everything is manage table I'll
tell you what is what you mean by manage
table okay but you can clearly say uh
see it says it's a manage table Point
number one point number two it displays
something called
location right it says location and it
says user Hive Warehouse so I want you
to do one thing just go to your Hue file
browser can you go to this
thing if you go to this Hue just go to
this user
folder there's a folder called user can
you go
here oh I think there are a lot of users
actually so n number of users are there
okay so do one thing user
slash
Hive so if you simply type it here user
SL hiive you will land on this page so
what will happen is that when you
install Hive there is something called
Warehouse folder hi will ask you where
to create it and normally in all Hado
clusters it will be user hi inside that
there will be a folder called Warehouse
okay and if you open this folder you can
see all the DBS people have created and
you can open your DB my database was uh
May 19 so this is my database May 19
open that and there is a folder called
txn records this is nothing but your
table you open this you have the
data so try to find out your data by
yourself so my point is uh what is going
to happen whenever you're creating a DB
in Hadoop this folder is there user High
Warehouse inside that will create a
folder with your DB name when you create
a table a folder with a table name and
when you say load the data it is simply
copying the data
here are you able to reach this point I
inan this place you can also do
something interesting I can simply go
here and delete the
data I deleted the data from from here
right and if I do a select star
now uh sorry it it fired the query
because the table is there it say zero
because the data is not there right let
it
run yeah so or if I do a select
star there is nothing so I did a select
star from the table and there is no data
because I deleted it from here
right and you can manually copy the data
here if you don't want to type low data
local in path I can just copy this
transaction here to this
location huh that is a schema but you
don't have a data so now I manually
uploaded the data here and if I go back
here I should be able to see the
table so this is where your data goes if
you're copying if you're saying low data
local in paath the data goes into this
user High Warehouse DB folder table
folder that is where it DS okay so are
you able to see this folder I think
yes and now you are now I am going to
confuse you okay so if you understood
this much I'm going to confuse you uh so
now what you did you loaded the data
from local file system and I have one
more data right so there is one more
file called um uh customer right I want
to upload this from Hadoop so what I'm
going to do you can also do along with
me I'll go to H okay and uh in my home
folder
Ragu okay I'll just upload the data here
step number
one so where is the cust file customer
file copy that into Hado in your home f
folder wherever you
want
uh this is a bit confusing I
mean so remember the location where you
have copied that in had
fine uploaded and now I want to copy
that into customer table but first we
have to create a customer table right we
don't have a customer table so I can go
here and there is a command here create
table customer see find sales based on
age group below that there is a command
just copy this
command come back
here and just say paste
it and it should create a customer
table fine
so now you can load the data from Linux
you already know how to do that okay but
I don't want to do that I want to load
from Hadoop how do I do it I'll say load
data inpath I don't say local I say
inpath I'll
say uh
Ragu
slash cousins
into table
customer try this only difference is
that you are not using local you say
load data in path and give the Hadoop
location now I want you to do one thing
where did you upload your data in hdfs
this cust file uh can you check the data
there it will not be there if you can
see there is a problem it will move from
there ah just see once uh see in my case
it was in it was in Ragu if I refresh
here it'll be
gone because uh the tables we are
creating is called managed table so when
we did a describe it showed it is a
managed table what is a managed table a
manage table is a table where high will
manage your data you don't have any
control so in Hadoop it has already
selected a location where it will dump
all the file so even though you say load
the data from this Hadoop location will
cut it from there and dump it into that
so you can go back to that user High
Warehouse folder there your data will be
there for
sure you don't have any control as to
where the data will be how will control
it try whether you can see the data
there in user Hive Warehouse house this
location no no no never it's just a
projection ah it's just a pointer that's
all it's just a projection the metadata
will get updated that's what is
happening because when I say move from
this folder to that folder rather than
moving that name node will update the
metadata it will point from here to here
that's all it is happening there it is
there I mean other other place it will
be gone where you have already uploaded
the data will be gone this will not
happen if you're Lo from local it will
not delete from local okay so that's a
bit confusing but that is how it works I
mean people will think hey where is the
data what happened where was my file
right because it is managing the data so
it will not allow you to keep the data
it has to be in the user highhouse
there so that's one thing and that's
called a manage table what we are
creating is called a manage table and
now ideally if you have done everything
correct like I am doing if you do a show
tables you should see two
tables
and uh you should also have data in both
the tables can you verify do show tables
and if you do a select star you should
have data in both the
tables because then only you can do
further any analysis what you want right
once you H have the data but here we can
use SQL you are not familiar with SQL I
guess SQL is the most powerful language
in the world for analysis there is
nothing which can beat SQL even though I
don't know much of SQL I'm not a SQL guy
so you might have to write complex
queries but end of the day that is not
the idea you should know what's
happening queries anybody can write
maybe you don't know how even I don't
know how to write some queries so I'll
take help or somebody will write the
query but if you fire a query what is
happening where it is running that is
I'll tell you a very interesting story I
have a friend of mine uh he is working
in Cape Germany so I went to train in
Cape Germany so he was in my training
team he is my friend he was also sitting
in the class I knew that okay so I was
asking the participants how many of you
are using Hadoop or hi or and most of
them said no they are all ETL
guys the point is they were running
hoton works and hoton works has a tool
called Hawk H awq Hawk uh Hawk is like
Hive only but with some modification
like Hawk has become LP now to be fair
previously they used to call it as Hawk
okay so Hawk is actually a connection to
Hive only end of the day so these people
were using hawkk for past two years but
they didn't know that they were using
Hadoop because for a SQL Developer it
doesn't really matter you write a SQL
query and do you really care where it is
hitting or who is processing your query
you don't really care your your idea is
to run the query right Hawk as such is
LP so uh Hawk is an improvement on hi
and now the project is discontinued it
has become that LP real time acid
queries on Hy previously it was known as
Hawk so I went like 3 years back that
time they were running Hulk and they
didn't know that it was Hado so most of
their queries were on Hado
but that's what I'm saying if you are a
SQL Developer mostly you don't really
care where the query is running you care
where the query is actually running or
not there's a problem or there's a
performance issue that is what you look
into so that's what I'm saying so I was
asking them do you actually use hadu
they said no we have never used hadu
they were actually running queries for 2
years
almost okay so once you have the data
what we want to do is uh see I want to
do it in a different different way you
can do it in many ways first thing I'm
going to do is that I'm going to create
a table so you can get the commands from
here it's called out
one um so I'm creating a table called
out one if you look at the schema of
this table you will understand what I'm
doing it's a join table meaning I want
to do a join and I want to store the
result in this right so usually you
don't want to ideally push it into table
but you can do it also so I'm creating
this empty table to store my join
results so I'll be doing a join
operation and this table will hold the
result and how do you do a join query
it's very very simple uh so if you know
SQL I think this will be very easy for
you you will do a insert overr right
statement table and it's a typical inner
joint right you will say a do customer
number first name age and profession
that is from uh one table B do amount
and product that's from another table uh
from customer a join transaction records
B on the joining colum is customer
number so for those who do not
understand this we are merging two
tables so this will produce a result but
we want to store it somewhere right so
we created an empty table called out one
so there it'll push it so this is a
typical syntax of a join so that a means
one I mean from a here is customer table
B here is transaction records table so
from a table I want columns customer
number first name age and profession B I
want amount and product and whenever you
want to do a join there should be a
common column then only join makes sense
so the common column here is customer
number this one is the common column
customer number and this is called a
simple inner join you have outer joints
left outer right outer full outer all
this everything will work no now it is
insert over right now the table has no
data it'll simply dump it if you already
had some data it will delete everything
and load it you also have an append
where you can simply
happend okay for SQL uh guys I have a
question just I thought I'll bug you
with questions what do you mean by uh
schema on
right sorry SCH scha on ah right do you
know what this is if yes what it is your
rdbms and typical systems are called
schema on right meaning if you create a
table let's say the table has six
columns if you try to insert seven
column data what will
happen it will say violated it you
cannot insert seven columns to six
column table that is because it is
called schema on right it will validate
the schema while you are writing the
data Hive is schema on read not write
meaning I can create a hive table and
upload an MP3 file it'll properly upload
no complaints I can upload a movie
properly uploaded no complaints because
it will not validate what you are
uploading right you will say load the
data it will simply copy that and dump
it in the folder but when you query the
data it'll validate there it will show
an error saying that okay uh and why it
is is because why Highway schema on read
is for faster uploads because otherwise
if it is validating everything it will
take time to load the data it's a data
warehouse right I think most of the data
warehouses are schema on read I don't
know much about other things but hi is
definitely schema and read right so
schema and write is typically on your
traditional
systems so run your join query see
whether the join happens
ah it's faster so impire will be more
faster I think if this is like
this now if hi is faster there is some
problem actually actually we have to
troubleshoot ideally we do it select so
now verify the data aels you do a select
star
from out
one limit 5 so you should see the join
uh output here and as you can see you
are having these details from the
customer data like camerone 59 actor and
these details from the other other table
right like the amount and the product um
these things are from the transaction
table right you should see the join
result so now you have joined the table
uh join two tables
actually and now what I will do uh
logically speaking what I will do is
that I will use something called a case
statement in SQL so those who are aware
it's easy for others what is a case
statement very simple I will say that
look at the age column okay if age is
between 20 and 30 categorize the
customer into something 30 40 do
something so you because you want to
group them right then only you can sum
the transactions so in SQL you can
easily do that using something called a
case statement okay that's what I'm
going to do so but to store the output
probably I'll create a table I'm just
creating intermediate
tables so create this table called out
two so this table is called out two and
if you look at the schema of this table
there is a last column called
level this column does not exist before
because that is the place where the case
will push the result right so that level
column will have low medium based on the
age of the customer I'll push the data
there
okay and how do you write a case
statement this is one way to do
it so I will say that select everything
and I will open a case here when age is
less than 30 I'll mark the customer low
age is 30 to 50 middle more than 50 old
else others so this is how the
classification will happen and that will
add to the last
column of our
table now by default simple case T it
will go to the last column only if
you're writing a simple case it will
have the last column added where the uh
conditions will be added whatever
condition you're mentioning so here uh
I'm just saying that only low low medium
high right three three categories I'm
mentioning so see this is the case
statement right and Hive is full of SQL
I can't really help it I mean there is
no other way to write in Hive right
without using SQL I cannot write
anything in Hive right so don't worry if
you don't understand SQL but get an idea
like this is what will happen if I do it
uh because high is extensive L used I
mean most popular tool I can
say uh so that means we are in the last
part now uh we have created this table
called uh uh out to and if I do
a select star from out
to limit 5 see now if you look at the
last column every customer is classified
old middle right and now we will do
something called a group by query we'll
just group them based on Lower middle
sum all the this thing and you will get
the final output this is just to show
you how Hive is running okay no other
um and to do that I will create a table
called out three and I want you to tell
me the query let's see how many of you
can tell so this is the third table I
created okay uh Group by by um uh you
know this what is that column level
yeah so I mean I'm just asking so what
we are doing here is we are simply
selecting the data grouping it and
counting it and now the final table if
you do a select star you will see the
final
result select star
from out three so now you can see Low
Middle old and total amount spend so who
is spending
more not low I think this is bigger
right older
I think older people are spending more
actually I don't
know maybe
right
huh based on what you are selling retail
store right Sports
items probably older people are actually
selling uh buying more these things uh
now quite interestingly if you don't
want to open a shell and type all this
stuff you can go to Hue there is a query
editor and there is hive
same thing but you can go to this
hi and in here you can select your
DB yeah so it is a default database go
back
cancel um my database is May 17th sorry
May
19th so I'm in May time and here you can
do the same
thing uh
uh
select so now it will do auto complete
and other kind of things a little bit
right just say
Play See it'll if you just hit play
it'll do that
so uh this tool called Hue is very um uh
you know useful actually because
um huh huh that is one thing apart from
that uh let's say I take a query
um so we have a table right out to
right
select star
from out to let's say
50 something like this right and if I
run
this the query runs and it gives you
options to visualize
see you can visualize the
data I mean right this is hug
property no in my knowledge no there is
a website called a GE hue.com um but
they are available on all the cloud by
default has hue if you buy Cloud era you
get Hue hoton Works doesn't have Hue you
have to separately install if you want
hoton Works has this ambari UI is very
good ambari is their admin tool that has
a file manager same like this you can
click and upload the file that's very
good actually uh who is third party in
in my
knowledge ambar is from Apache is just
using it even on Apache Hado you can
install ambari for so if you're having a
plain Apache do you want to administer
it you can install ambari but popularly
on hoton
Works uh because nobody uses much of
Apache hadu you are either hoton works
or Cloud era right mapar has the worst
UI it's called mppar control
center worst I mean I'll show you the UI
of mapa I had a mppar cluster I lost
it
maper control
system I don't know maybe they have
improved no they have not
improved mapar UI is called mapar
control system M MCS okay um
yeah this is how it looks like see how
weird I mean this is their like Cloud
manager you see this is their UI very
Dreadful phones and you can't find
anything in this I mean the worst UI
ever created probably I mean comparing
it with Cloud manager and all cloud is
way better the UI fonts everything this
map control system their
UI uh anyway I wanted to discuss one
more thing so so far we have done manage
tables I want to talk about external
table and that is again a bit confusing
okay uh we'll finish external tables and
wind up probably okay you had been
through a tough session actually so just
do one thing uh I want you to do one
small thing um I want you to create a
table
okay so I'm copying the command from
here and I want to change it so just
open one more
notepad open
sorry new
right so this command is for creating
the uh transactional table regular
command I just want to change it I will
say
create external table and I will say
transaction records uncore
EXT okay and then I will
say
location I'll explain what I'm doing
okay first let me try
this uh
contrl
C my
data so I'll just copy this and see
whether the command is working then you
can also
try yes it works so uh you want me to
increase the font let me just
increase font
uh amount double is there just only one
thing so what is the difference here you
are creating something called external
table first thing you can note down is
that you say create external table so
there is a keyword called external you
are using Point number one point number
two after all the schema and all I say
location in location I say user GL
faculty that's my home folder my data
what is going to happen if you run this
command it'll create a table that's for
sure it'll also create that folder for
you my data in Hadoop I don't have that
folder but it'll create that folder for
me let me show you I I'll tell you why
it is required if I go to my home
directory my home
directory
uh where is it yeah here it is my data
can you see
so this table is this folder is created
by my table creation command I didn't
create it okay now what is the idea of
an external table when you create an
external table you have the control as
to where is the data so right now there
is a folder called my data right and all
I need to do is open this folder okay
and copy my transaction files here
sorry just copy this
file here I'm just just uploading the
file and if I
say select star from
dxn records uncore
EXT limit file I have the data so the
difference here is that what is the
difference between manage table and
external table in manage table you don't
have any control as to where is the data
you just say load the data and hi will
always keep it in user High Warehouse
blah blah blah in external table you say
I want a location where I will keep my
data so here I mentioned a folder and I
can upload all my data here and that
will appear in my
table more than this so this is one
example where we actually use external
table is that customer will say that
they will have some data and uh we will
ask where is your data they will say my
data is in S3 Amazon S3 you can simply
say create an external table point to
Amazon S3 location all the data will
appear in the table the data will be
there I'm saying you can see that in
your table so any external location you
can keep your data and you can access in
the
table that's called an external
table try this if you
want create an external table and upload
the data see whether you can find it
because all the times you may not have
control on the data you cannot say that
all the time I will copy the data to my
place sometimes you might want to create
a table and point the data point your uh
table to a folder actually that is
possible from my local system I uploaded
to that
folder so once you upload the data to
this folder it will appear in the table
my data
folder the data has to be in this folder
then you can see in the table because
you are saying the location is
there see the business use case is
different I'll show
you I'll show you the business use case
okay so what is going to happen is that
let's say you are having some DBS so
here I have an oracle
okay here I have some my
SQL Etc right and what you do is that so
you have some databases right and you
let's say run scoop you know what is
scoop Right Scoop is your ETL
tool you say hey scoop do one thing
connect with these DBS every day at
12:00 p.m. 12:00 a.m. midnight so I can
schedule a job everyday midnight what
scoop will do scoop will connect with
these two DBS and it'll run the ETL job
it'll pull the data from here and once
scoop gets the data scoop has to send it
somewhere so I have my Hadoop cluster
here
right there is my Hadoop cluster right
what will do scoop will dump this data
into a folder so there is a folder
called I don't know
Ragu so SC scoop will keep on pushing
the data to this folder called ra
now I can create an external table okay
and point that location to
Ragu right so what will happen when I do
a select s automatically the data that
scoop is dumping here will appear in my
table are you able to understand what
I'm
saying so typically in an ETL leg what
is happening is that every day you'll be
running ETL jobs to bring data from
multiple places and all this data will
get dumped into some folder
now somebody cannot go there and say low
data local in paath rather than doing
that you create an external table point
to that folder so the moment the file
lands it is appearing in your table very
simple that is actual use case of
external
table Yeah Yeah scoop cannot do
scheduling Uzi has to do now if you are
asking scoop can directly dump it into
hi tables also it is possible scoop can
take the data from Oracle dump it into a
hiy table that's possible but many jobs
are there where you get data from
multiple sources say you're a flume job
if you're running Flume agent it may be
collecting log files all the log files
will dump it into some folder now I can
create a simple external table where I
can use some serd to read the log and I
simply do a select star all the data
will appear in my table otherwise I have
to say low data local in paath every
time to dump it into this table right so
the actual use of external table is that
you know you can simply point to any
folder and all the files will appear in
your table so Hive doesn't care what is
Insider you have to make sure that the
schema matches everything and uh some
people ask this what if I have a 20
column
data in the folder and you have a 10
column H table the first 10 columns will
come it'll truncate from there ideally
okay because it just fits wherever it is
possible and then it will remove it so
all the files will be appended in the
table and shown like five files wherever
your records it cannot match or cannot
understand it say null values I don't
know what it is or data types
mismatch only five columns will come
it'll try to fit but there is no
guarantee right so ideally that is not a
use case because in a data
warehouse you are not supposed to do it
actually you are not supposed to
experiment in a data warehouse the data
should be clean that is why you have
this cleaning pre-processing data
Factory operations and all right then
only you take the data to a data
warehouse
finally um and another important point
about external table is this so if I do
a show tables
now I have this table called uh
transaction records and this is a manage
table this is my original table I say
drop drop
table
dxn records I dro the manage table can
you tell me what will happen to the data
will it retain the data or data will be
gone data will be gone
sure no data will retain data will
remain okay I'm just asking so I just
dropped a manage table you know what is
a manage table I'm asking table is gone
for sure the file I manually uploaded
right right will it be
there yes no 50/50 we'll
check
user where is
it hi
Warehouse May
19 do you see a uh folder called
transaction
records the data is gone
so manage table has this drawback I
won't call it as a drawback if so this
is very common it happened once in a
project where I was working and that is
how I came to know I mean normally when
you work you don't learn these things
you learn by mistake something somebody
does so we had a table very huge table
which was shared among our project two
three projects were sharing this table
and they had full rights that point in
time one of the engineer he said drop
the table
and then it's gone so the pro so he said
drop the table accidentally he didn't
intentionally do it but the problem is
the table is gone but they had some
recovery mechanism at that point in time
in Hado in h there is no way to recover
But ultimately it's a delete from Hado
so they had some recovery trash
mechanism recycle mechanism and based on
that they recovered somehow okay uh that
is why when you create a table which you
to share always
create an external table drop
table I can just use the up Arrow right
what is the table name transaction
records uncore EXT so I'm dropping the
external table table is gone and I'll go
to
user ah I can just go to home folder
right I'll go here and we had a folder
called what was the folder
my data the data
remains because Hive does not have any
control on your external data you have
control you uploaded it will not delete
so this is easy you can recreate the
table data will not be lost right so if
you want to share the data share expose
the table always create external tables
manage tables are very rare actually for
learning purpose we it's easy to do
manage table and in the operation side
they are all the same the queries and
everything is same manage table there is
no difference but always remember create
external tables if you want to share
this okay uh this PPT has more data I
request you to go through that
also the official website of Hive very
very
important hive. apache.org
um and it has tons of information okay
like I I don't know how much it has lot
of things actually uh you can start with
getting started guide on hive to
understand what is
Hive
yeah I forgot how to connect using Bine
[Music]
um so I'll I'll show you tomorrow how to
run b line because I just have to see by
default high server 2 runs in port
number 10,000 so when you're starting b
line you have to say jdbc connect to
10,000 port number and then it will
connect with the high server 2 I will
show you how to do that tomorrow
anyway I forgot so please go through
this hi official website it has tons and
tons of information um lot of things are
there there is a high Wiki page which we
just saw
right and and uh just uh look at these
points we already
discussed uh query execution via Apache
T Apache spark or map
reduce oh probably last bit I can show
you
the but I forgot my username and
password it is not here right I'll show
you tomorrow the uh anyway query
execution via T map reduce or spark how
you can use spark as an execution engine
but people don't prefer it because from
spark they fire queries the other way
that's more easy you you can start spark
SQL and say fire it but how you can
actually use spark as an execution
engine it is possible right and this is
subsecond query retrieval via Hi
l only hoton works LP this is what I was
talking about LP LP enables uh asset
transactions faster queries everything
that you can think about a normal rdbms
I mean within few but that again
requires resources I mean even though it
says very fast you it will take lot of
resources actually uh so when you are
running th th itself is in memory so
anything which is in memory requires
resources in memory means Ram it'll use
only Ram so Tas itself is fast because
of in memory so you your cluster needs
to have a lot of RAM for LP and all
otherwise it will be very slow ah
procedure so that is hpl SQL is nothing
that is a combin that's called hi query
language hql only you can write your own
custom uh functions and all inside H hi
supports something called UDF user
defined function you can write so that
is the say it as
hsql as a fancy way of saying that only
right I would feel vice
versa uh in the sense yeah see uh if you
good in Java map reduces your choice but
map reduces extinct extinct right then
you go to spark spark uses Java but that
is Java 8
functional Java not Java 7 so a lot of
people have trouble there um but even in
Spar Community the most trusted
preferred is python python and Scala
then only Java comes so Java is
supported in spark 100 percentage but
developers are very less people want
either python or Scala because it's easy
to write actually right um I would say
some knowledge on SQL is or highways
very important whatever you're doing
right um I will I will tell you this in
spark um in spark if you write your code
using Python and SQL the SQL is much
more
optimized because if I'm writing
anything using SQL I have something
called
schema right which means I can
understand my
data what is schema if I have schema I
can understand my data so if I write a u
join and filter I can push the fil
filter first then do a join are you able
to
understand H in SQL I write a query by
default it does right by default in ssql
if you write a group by join then filter
what will happen filter will come
first because why should you load all
the data then finally you do a filter
first you filter then do it that's
called optimization that is not possible
if you write a
language in spark we will see when we
write spark code it is not optimized
because spark is not having a very tight
schema but spark SQL has so that's what
I'm saying SQL always has a preference
because of its optimization that is not
possible in other languages like even in
map ruce you write you have no idea how
is the data underlying right you're not
mentioning any schema it has to load the
full data to do analysis any map reduce
program you write the whole file need to
be loaded you cannot say that select
only this column and analyze not
possible Right first all everything has
to be loaded
so always the structur languages are
having a preference on this Java is good
I'm not saying Java is bad but I won't
consider it in the further Scala
probably you can relate with Scala Scala
is very similar to Java if you know Java
Scala you can very easily learn Scala
spark is a good combination very good
combination uh what else you have
questions we'll wind up in 5 minutes uh
after your questions so tomorrow I'll be
covering uh something called U uh
partitioning bucketing indexing and hi
which are a bit Advanced topics actually
okay but very much needed actually to
understand what is hi because they are
the highlights of Hive if somebody want
to learn Hive these are the points or if
somebody ask you about hi these are what
they ask nobody nobody will ask how do
you do a select star query right that
nobody's going to ask they will ask how
do you partition your data so at least
if you can understand the logic of it
that will be more than sufficient right
and we will look at this NASA data you
have a NASA data see NASA so tomorrow we
will analyze NASA data using hi so I can
show show you the serd there Rejects and
sdis and all I can explain in this NASA
data so make sure uh this data sets are
there with
you uh anything else let me know if you
look at this case study that we have
prepared so this is about the NASA data
analyzing the NASA web server Logs with
Apache Hive and this will be helpful
because we will see the same data in
spark in the spark class also this data
will be the same data so if you
understand the data now we can analyze
it using spark as well so I guess you
are aware of this server logs contain
lots of information from web servers
right and this case study will show you
how to derive insights from web server
logs so you are going to look at some
log files which are generated by web
server and if you have ever worked with
web servers um so let's say you type uh
google.com and then download something
all these request will be hitting the
web server so you will get get request
uh put request then HTTP status code so
that is how your log files will look
like okay I will show you the log file
and you can actually download the data
for free from this URL we have already
downloaded and uploaded the data you
don't have to worry but you can it's
public data from NASA and if I scroll
down look at here before moving to the
activity please go through the HTTP
response codes at this link right so are
you aware of something called HTTP
response
codes yeah if not you can just go
through this URL so for every request
there will be a code Associated right so
if you're browsing and hitting a web
server the server will respond with a
code so if it is 20 it is okay anything
starting with two is Success 2 not1 is
created 22 is accepted 204 is no content
and all uh 30 is redirection that
website will redirect you to some other
place right and client errors are
4 and 500 is server error so sometimes
you try to open a website and you will
say 501 server not found or like that so
anything starting with five is uh
internal server error right um so
Gateway timeout service is unavailable
uh something something like that you can
see so if you want you can just go
through this list right
and understanding the data so the data
set contain 2 months worth of all HTTP
request to the NASA Kennedy Space Center
server in Florida so this data is
collected from the NASA web server and 2
months worth all the requests are there
and I think around 1 million lines are
there in the data it's pretty huge data
actually somewhere around 1 million
lines the log files are stored in the
Apache common log format so that's what
a lot of people ask me
uh when you say text file it is not with
a txt extension normally you say text
file that is something with txt but
there are lot of other data which is
called text file it is
always not mandatory that you you will
have a txt extension right so this
Apache common log format it is a text
file but you cannot open it in notepad
right you can open it in wordpad or any
other thing and it will show you
properly like text Data I'll show you
the
data and now let's look at the data so
if I go here there is uh NASA access
logs and this is the data I'll say open
with
and I'll say word
pad say
okay uh so this is how the data looks
like and it's not really pretty I mean
in the sense like it's not really nice
uh it's not
structured um and what are the fields we
are interested in in this right so this
is log file and if you look at here the
PDF right uh we have the host identity
user identity time request status size
if I scroll down the first field is the
host making the request okay so where is
the host making the request this is the
host this guy in24 do inet something so
this is the uh fully qualified domain
name or the host name host making the
request then what we have next two are
the user identity from remote and local
machine these are unavailable so next
two fields are not used they are dashes
that's fine and then we have the time
stamp in day month uh hour so year this
format so here we can see
this so this is your time stamp right
the time when it is making the
request time Z on so this is this minus
400 is your time zone what you see here
is the time zone okay and then there is
the a request send to the server so here
you can see get it's a get request okay
and somebody is trying to get shutle
mission status news some news something
okay so that's a get request we have and
then there is an HTTP Replay code uh
HTTP Replay code is 200 it's a success
okay and then you have the size of the
file received so probably in bytes 1839
so this is the data h so you can see
that a lot of people are uh sending
requests to the NASA web server and
that's how the data looks like now what
we want to do we want to create a hive
table to load this data and then we want
to write some queries and the size of
this file is 167 megabytes so it's a big
file actually not very small and we will
test now how the cluster will behave
because if you start writing the query
all of us write the query we'll see how
efficient the cluster is anyway right um
and so the next question is that how do
I actually load this data into hi
because this data does not have any
structure to do that we are using the
serd or the serializer deserializer now
a little bit uh uh information to you I
also want you to give you an
assignment so if you simply go to Hive
and say Sir
D yeah you can see here this is called a
serd so what is a serd serd is a sh for
serializer deserializer like I said if
you are having data which is not really
structured like this log file or or Json
files or anything you want to read and
write you can use this parser called
serd right and hi supports a lot of
buil-in
SES so these are the buil-in SES we have
and we are going to use this guy
Rex you are aware of uh
Rex regular expression right so that
means you can write a regular expression
high will par this parse it and using
this s it'll read it there is an orc s
AO these things we'll look at
later and then custom SES you can write
your own series if you want that's also
possible right um we will look at this
later let's open this uh
rexer yeah so it says uh if you want to
create a table and use the serd you have
to say row format serd remember
previously we used to say raw format
delimited Fields now I say raw format
serd and then I will say this is the
sdy okay and then you say with Seri
properties and you will say input
regular expression whatever regular
expression you have you have to put
there and that should actually be able
to get the data whatever rejects you are
writing and then it should give the
structure to the data this is the idea
right um and this is very important
stored as text
file I mean I think yesterday I didn't
discuss it when you create a hive table
by default this option is there called
stored as text file which means whatever
data you are loading into the hi table
it will store it as a text file regular
text file be it a CSV file or any file
so yesterday we loaded some customer
data transaction data and they were
getting loaded as it is okay now you
have other options you can say stored as
orc par and all that is where the
Improvement comes and I will talk about
it what is the difference in when you
say text file or when you say orc file
where is the difference but as of now
we'll say is stored as text file we are
not
bothered It Is by default text file ah
so that is why yesterday we were not
mentioning it it is by default text file
now the data set is available in this
location uh can you see here forward SLG
data and
uh
huh one moment uh what was the name
access
right yeah yeah so this is the file it's
called access there is no extension and
it is available in a common folder
called GL data so first let's do one
thing let's start Hive anyway we need to
start
hi so start
Hive and let's create a table so we will
use the DB yesterday we created I'll say
use may9 that was the
DB right so you can switch to whichever
DB you
created and this is the command to
create the table you can copy paste this
from the PDF okay say copy and I'll
explain what it
is so you will say create a table if not
exist NASA log and the fields we are
interested are
host um identity user identity time
request status size and then you'll say
row format serd we want to use the
reject serd with SD properties you can
input the regular expression so this is
a regular expression we are using and in
the output we will just say that U so
that's part of serd you will say that
store it with equal uh spaces right and
then you say stored as text file so this
is where you're saying that store it as
text file so this is your regular
expression huh
uh you can go to the
PDF and if you cannot copy like this
right click say select
tool there's a select tool once you
select it you can left click and copy
like
this so you have to have some idea about
regular Expressions if you want to write
it from scratch but that's
okay yeah sure uh um I'll show you from
console
um yeah this is the command I just said
hi then use the database and then just
copy pasted the table creation Command
right now we have not loaded the data
you don't have to load the data that
output format string it is simply saying
that there are eight columns right and
with space uh each column with space uh
it's a bit difficult to explain
uh it actually reads and separates all
the data with how many spaces you have
and it is looking at digits and
strings uh and then extracting the
interesting Fields it's a bit difficult
to explain the Rex
actually so if you are writing a Rex
from scratch uh you have to validate it
so this was validated even I don't
remember how I validate I wrote it long
back actually yeah so it's a create
table
yeah normal table only difference here
is that in the table properties we are
saying that uh whatever data that comes
to this table apply the serd the regular
expression otherwise it cannot recognize
the data inside right so right now this
is the Apache common log format right
and Apache common log format has a fixed
uh schema not schema the way the log
files are generated so you can write it
in multiple ways this is one way of
writing it so in hi if you look they
will simply say that this is the Rex we
want here I'll show
you huh so here it is a input reject see
the output reject is not stored so here
you can see the input reject right are
you looking
here I mean this was written like if you
avoid the output string also it will
work but we are just ensuring that
everything looks string I mean just we
are manually saying that it is string
that's all I think it is not updated H's
documentation is not updated no it is
not
updated let me
see
um so where is
it more about Rex 30 can be found here
so they have given two links actually
probably it is here let me just check so
what happen happens is let me just see
if I can find it
first H
see in in open- Source communities what
happens is that let's say you are
looking at Hive or spark or anything
somebody will create the product so
let's say I created Hive and I will give
it to everybody everybody will start
working and then will what will happen
somebody will say that something is
broken so I may not add it in my
documentation I'll add it in a J jira is
your issue tracking right it's a jira
link in jira link it clearly says that
there is input Rejects and output format
string you should use so somebody added
a uh yes I will work on adding a serd
how to and some example is a new
directory so somebody said that uh this
is not properly formatted so in the J it
is added actually it is not an official
documentation originally when they
created probably that property is not
there I'm saying then somebody could
have complained and but they have
mentioned the link here right you can
see that it is here if you look at here
it says more about can be found here I
don't know why they're not updating the
documentation but it is there anyway so
one problem is this if you want to
search documentation you may not find
everything in the official documentation
sometimes so I always look at jira
because any issues they fix will be in
jira and also not everything not every
time things will not work the way you
are expecting so you have to Google gole
saying that high plus jira not able to
read this file some some issue you are
coming across and there'll be a j ticket
for that always right so once we now you
have to load the data so let's continue
with that and where is the location the
location is here GL data right so how do
you load you say
load
data wait wait
wait wait wait wait if I do load data in
path what will
happen I say load data in path what is
going to happen we can create an
external table right that's better I
didn't think about it because normally
the participants will have the data in
other classes and they will upload
themselves so what do you want you want
to create an external table everybody
and point to the data that will do okay
so then you have to modify this so how
do you modify
this if you want an external
table you will say
create external
table let's drop the original table okay
do a drop first do a drop drop table
what is the
name NASA log so drop
it and now we say create external table
NASA law these are all fine and the only
difference is location
SLG
data um wait wait wait
wait external table if you're creating
it should point to a
folder this will load everything from GL
data we don't want that
right in G data you have lot of folders
right so I will move it I'll see if I
can create a folder in G data let's see
new directory I call it as
NASA yeah and I'll move it inside here
right that should make
sense see you have to take care of
everything not this right this one right
access action move
to GL data
NASA
right there is no NASA folder yeah there
is move fine so ideally now if I check
NASA it should be
here H now it should work otherwise it
will load every file from the GL data
folder we don't want that
right so what should be your
location this should be the location
right and let me see if it
works uh no you can use small letter or
capital
letter so the table creation will work I
know that my question is what about the
query select star
from works so you have to try this and
uh let me know
it works but all of us are accessing the
same data
okay all of us are in the same
data so make sure you can see see
properly
structured now I already created a table
first right and I dropped it because uh
how do you load data to that table we
have a shared location so it is better
to create an external table to point
there so I dropped it and recreated the
table so here it is
location and also here is a location
this also should be there this external
you can type small letter or capital
letter folder name should be Cas
sensitive I
think and and do a select star and let
me know whether you able to see the data
so there are two approaches not every
data can be made structured also there
is one problem problem so Hive is a
structured tool that means somehow you
should give structure then only it will
work there are some type of data like
for
example there is a format called
AO AV so previously when you get AO data
because that's like unstructured you are
not able to do anything but now there is
an AO reader they have created so there
are still some type of data where you
cannot give a structure one point second
point
how this works in organizations is that
like I said there'll be a data inje team
right so I'm just giving you the example
of GE in GE what happens see ge's
business is so enormous that nobody has
any clue what is happening there G is
one of the biggest firms in the world
actually right and their size of data is
enormous what they are getting so uh
they have Financial applications okay
these Financial applications will
generate data okay and how do you get
the data they will have apis they will
pull the data in Json format so it comes
in Json format and sensor data they get
sensor data uh so they have this
locomotive engines train engines in
Europe G has created so these train
engines whenever they are running will
produce sensor data they collect it and
that data will come through Kafka it
will come in a AO format through Kafka
to Hadoop ultimate land on Hadoop but
the data is sensor data so it's like
events sensor data will come in events
format like whenever there is an event
it will generate a data it's text file
only but the format that they get is
called AO so the point is that data when
it lands on Hadoop nobody can give any
structure to that
data nobody on Earth can give so G has a
data inje team their job is to take the
data give some structure and give to the
Big Data team you're getting my point
right because if you directly give it to
Big Data team they have no clue what to
do with the data so this there will be a
staging area where all this data lands
okay and they had written some Java code
custom Java code what that Java code
will do it will read the AO remove some
unwanted fails give some structure send
it to next side so so that is how they
are injecting the data I'm saying so row
data most of the times you may not be
able to directly handle it
and Second Use case is if you are a big
data developer or like us we don't want
data in the original format but if you
are a machine learning guy you want the
data in original format are you getting
the difference like if I'm a data
scientist I don't want anybody to touch
my data I want the original data with
all the nonsense then only I can build
my machine learning models because they
want all the features they don't want to
modify the data but if I'm a big data
guy like like this I am happy with
structure I can lose some data I just
need the structured format I will work
so these two teams will be there so the
machine learning teams will be original
data they'll be collecting and then they
will have their own algorithms to Crunch
the data Kafka is used because their
data is coming from
Europe uh streaming data they don't
stream live they stream once in a day or
something I mean they collect all the
sensor data huh it's an Amazon the
entire setup is an Amazon so there is
some sensor which uh uh in Amazon you
have a solution right who is working on
Amazon sensor data you have a solution
Amazon what you
call Kinesis Kinesis right Kinesis is
your sensor platform in Amazon so
Kinesis will collect all this data and
from Kinesis will push to
Kafka because somewhere it has to store
right the data there are very less
real-time decisions they want to do on
that data because there is train data
and and only time when they're analyzing
that data is if the train is not working
like it crashed the train engine got
crashed otherwise they don't want to
analyze it in real time 99% of the time
the trains are running so this data is
useless for them so when the train
engine got crashed or something happened
that time they want to look at the data
last 24 hour what caused the crash so
they are not doing because the size of
the data is in
petabytes so analyzing it is also not
possible for them so they collect it
real time and batch it and send it once
in a day or something and it will go
through Amazon it will land in Kafka why
Kafka because from Kafka two three teams
will collect it one team is using the
data for something else distribute it
and then one copy will be stored in Hado
like original data copy anybody want
they can get it that is how their
architecture is right now G's isn't
evolv in an architecture they are just
moving to Hadoop now it's not a stable
architecture so probably after a couple
of years they'll find another way to
better this things and all right now
they're not doing real time on the uh uh
this train data they also have flight
data
flight this aircraft engines are all G
right commercial aircraft all are G
80% and all these engines generate same
sensor data they collect it but again
that's all not real time it's very rare
you get a problem and that time only
they want to
analyze so uh I hope you are able to
reach here right till this
point ah so this is the place where your
data is there right if you go to this
location I have copied the data there
now if you look at the PDF what it says
load the data select star now these are
some of the use cases I wrote I mean you
can also write your own use cases so
find the top end points that received
server side error
meaning which are the uh how many uh
endpoints have received uh server side
error right I'm just asking like there
is 501 uh 5 these are all servers side
error and how many times it has occurred
so how do you figure it out there are
multiple ways to do it but one of the
way is this you can say
select status count of request from the
uh what you say NASA log table Group by
status and you can say having status reg
regular expression extract so here I'm
saying that uh anything starting with 5
in the status column because 5 is server
side order by status descending limit
file so that's the query so I just want
to know all the errors starting with 5
and count them how many are there that's
what I'm writing now when you are using
having it has to be double uh operator
assignment in having close in hi when
you're using having close it has to be
assignment something if you say equal to
something equal to something it will
become a equal to one it will become a
become ones so here I'm just saying just
comparing that comparison operator is
double equal
huh no in having only I'm saying we
close and all it will be
normal in having you will say so
somewhere there was a documentation on
this I'll share it with you okay can you
try this query and see now it is taking
time 24 seconds only for the ver anyway
this will take some time can you guys
try the query it'll be slow
so when you mention the commands it has
to be either full small letters or full
capital letters he said capital L
location and it's a bug I know this
because because otherwise it should
throw an error it won't throw an error
uh and this is a very common thing if
you create external tables you mention
location um even if that location does
not exist it may not throw error and you
will try querying nothing will come up
you will think what is a problem space
is fine but ideally this location
keyword sometimes it has a problem so
that is what I did it started working
but you're getting or maybe maybe that
itself is not a problem I dropped it and
recreated probably it started working
not sure so how much time it is taking
for you to run the query 75
seconds uh there is also one more Point
um which I forgot
yeah so
anyway so somebody was asking about this
reference on regular expression here you
have a hi language manual it is in the
PDF
and which uh resource is requested most
frequently by The Host this is a good
question right because which page is uh
requested the most and I don't think
it's a very difficult query but still
we'll run it just to
see so uh now if you look at here it
says select uh request comma count as
request count from NASA log group By
Request order by descending limit 30 you
are looking at the top 30 requests
actually
right you can skip header and all but I
don't think you can skip the data part I
don't think you can skip columns no
I I don't think you can filter because
ultimately it's on Hadoop so the data
remains on hadu right so that's one
problem because if it is an
hdfs uh then there is no way you can
filter anyway locally I have to see
whether he is asking while loading the
data can I filter the data like I don't
want all the 10 columns I want only you
you have skip header that option is
there table Properties by the way there
is something called table
properties um if you just go to Google
say
hi create table TBL
properties so in creation of the table
there is something called table
properties where is it
huh so table properties right so here
you can mention um uh the uh you know
Skip header and all let me just check no
Auto compaction mapper
memory Auto Purge external
true you have skip uh header is there
that's for sure but I don't think you
can skip other columns that is not
possible I think anyway I will just see
if I can get back to you on that
okay and there are some more queries I
have written you can try that like
display the top 10 host who made maximum
request and all uh you can try that find
the total count of different response
Cotes returned by the server so just try
these queries let me know if they are
not working
I have not actually tried everything it
should work
okay so now uh talking uh about Impala
little bit right probably not in depth
but you should know a little bit of
Impala because it is heavily used in
industry imala if it is in the cloud
side we are on cloud side if it is hot
and work side everything is hi but in
the clera side we have something called
Impala right so a small intro not much
but and I will show you an Impala query
also so what is going to
happen if you're having data nodes right
so this is a data
node this is another
one and this is another one
uh you can install something called
Impala right and if you using a cloud
distribution Impala will be by default
installed you don't have to worry and
Impala is its own SQL engine it does not
depend on hi or anything right so when
you install Impala what will happen
there is a demon called Impala
D Impala D It's called Impala demon it
will start running on every data
note like something like Oracle I mean
I'm just saying right so this Impala D
will be running on all the notes first
of all right and Impala has a shell from
where you can fire the query it does not
depend on H and if you fire a query what
is going to happen is that uh let's say
it hits this machine so the query will
be accepted by one of the machine so
let's say this guy right and this guy
will look at the query now Impala can
talk to hive's metast
store which means whatever tables you
are creating in Hive Impala can see so
all the tables you create you can query
using Impala so and whatever tables you
create in Impala hi can also see because
they share the same meta store so that
is one advantage so and on top of that
another thing is that when you write a
hi query it will convert that into map
reduce so hi is not bothered about how
the query is running
because it is a responsibility of map
reduce to run the query Impala will not
convert to map reduce Impala has this
Impala demon which will run the query
for you it is like a rdbms I can say not
an rdbms I'm saying like if you're
firing a Oracle query who will run the
query Oracle will run the query same
like that if I fire the query this
Impala will get the query and it is the
responsibility of this Impala Dem s to
execute the query so they will have the
uh hi
metadata
okay they will also have the block
metadata
meaning if you create an Impala table
like a hive table and you say load the
data right now the data is actually in
Hadoop and the data is in blocks and
replicas now who has the metadata about
this name node these guys will copy that
to themselves because they should know
where is the data Impala is not going to
depend on anything to fire the query it
will run itself right so that is why the
queries are very fast and when the query
hits Impala it runs in memory so there
is a drawback so let's say the data was
here you have a block here you have a
block here what will
happen you will have
Ram this block data will come here and
this block data will come here
RAM and this guy will simply coordinate
the query because the data is here and
here right so this guy will coordinate
the query and it will run the query in
memory the drawback is if let's say this
machine crashes the entire query will be
abouted it is not fall tolerant but High
queries are fall tolerant because map
redu is fall tolerant there is no way
map reduce can fail right but imp
queries are not fall tolerant so if the
one of the machine because it is a
distributed query not one machine is
running the query probably 10 machines
are running the query and if any one of
the machine
crashes the query will be abot it but it
will not convert to map reduce or do
anything so your queries are super fast
actually directly hit hdfs query the
data and give you the output whatever
you want so it's faster but this
reliability uh issue is there with
Impala so what we do if you're are
having a cloud cluster if you having ETL
jobs we never use Impala because in ETL
jobs behind the scenes it'll be running
hi query and probably the query will
take 3 hours to run 4 hours to run
probably 5 hours that's fine even if
during that ATL job a machine crashes I
I'm safe because it's a hi query my map
reduce will take care of it but if I run
that ETL query using Impala let's say
the query will be faster 2 hours
probably it will take but after 1 hour
if one of the machine crashes my Impala
has to restart the whole
query it'll throw error but again your
ETL job has to start from beginning you
lost one hour right so actually it needs
High meta store for the metadata Impala
must use hive's meta store so Hive
should be installed so that is another
Advantage U hi will remain because lot
of tools requires Hive even spark SQL
spark SQL normally when you configure
spark you will tell spark SQL use H
metastore so that H metast store is used
by everybody so all the tables you
create all will be in one place right
and if the table is there you can either
query using Hive query using Impala
that's up to you to decide but Impala
also has some drawbacks this NASA table
we created you can't query using Impala
it is this Rex is not
supported so this table is created using
Rex SD that is only for hi I can see
this table in Impala but I won't be able
to query I think if my memory serves me
well but most of the regular tables you
can query without any problem both of
them can access at the same time because
meta store is a shared
place what is meta store it is your
MySQL it's a shared Place anybody can
access not only high and Impala 100 of
other people
access it need not run you have to
install hi set up a meta
store I'm saying whatever table you
create in Impala it stores in highest
meta
store so each data node will have an
Impaler demon and each demon will have a
copy of house meta store block data so
and what happens if you fire a query any
demon can pick it up that's called a
coordinator meaning if I write a Impala
query there are hundreds of data nodes
so any data node running Impala can get
the query and that node is called a
coordinator for that query so in this
example this is the coordinator because
this guy accepted the query but this
does not have the data that's fine okay
and once it gets the query it does a
local lookup so it's very fast it has
the metadata it understand immediately
here is the data here is the data it
just splits the query and streams to
here and here these run the query
collect the result to display to
you so ideally a lot of machines will be
there where you are having data but the
metadata lookup is very very
fast there is no frequency you to
manually refresh it it will not
automatically copy meaning if I create a
hi table it will not appear in Impala I
have to refresh the metadata there is an
option in Impala where you can say
refresh so if you feel that there is
some new table you say refresh then only
the metadata will come a block metadata
so that will come once your table
metadata is created say I'm creating a
table H once I create a table I load the
data then only the metadata of that data
will come it will not store the whole
Hadoop
metadata your Hadoop cluster has let's
say th000 terab
file okay Impala will not store th000
terab metadata that is useless
right you saying this Hado cluster has
let's say th000
files okay in which you created an
Impala
table okay you say Impala table you
loaded file
one it will have the metadata of only
this file why should it need all the
metadata the query will hit only this
file
right ah only related to that table I'm
saying when you create a table you will
say load the data what data so in our
example we loaded the NASA data so the
block information of that NASA file it
will
remember ah from hi or if you're
directly loading into it it will
remember itself it won't remember the
block information of other
files in Hado you have a lot of files
right not all the files are in
table are you getting my point what I'm
saying it'll remember only the metadata
of the files associated with the table
otherwise why it should remember all the
files manual refreshment is required I
mean block it may remember I'm saying
the table properties it will not
automatically refresh so you have to
manually refresh every time so usually
we won't do like that usually even
though Impala and H can communicate we
don't do it because hi has its
functionality Impala has its
functionality meaning whatever tables
you have in Hive normally they will be
used for reliability like ETL jobs and
all and that hi will handle Impala will
you you will create Separate Tables you
will say do the same thing load data
local in paath blah blah blah and that
will be handled by Impala only like
Separate Tables they can talk to each
other but what I'm saying if you want
the up to-date information you should
say refresh then only you can get it and
there are two three types of refresh I
don't remember completely there's a
complete refresh which will redo and
load the total metadata from high meta
store blocks everything there's a
partial refresh you can do it in Hue so
these things are available in h I'm not
just saying that I mean you will be
wondering so if you go to query editors
you can see Impala right so here is
Impala so probably the table you created
right now should come in Impala that's
the logic right so if you go back where
is the May 17 May 19 this is my table
my DB right see customer NASA log these
are in
Impala and if I do a select star from
NASA log I don't think you can see it I
doubt five I doubt you will be able to
see
it maybe I don't remember
exactly huh no
uh fail to load metadata uh fail to load
metadata invalid storage description
Impala does not support this table type
reason SD Library not
supported it won't support that SD
that's why you are not able to see
that no no it won't that's what I'm
saying Impala tables if Impala is
quaring your data it's not fall tolerant
so what the ETL will do it will query
and get the data who will query that's
the question if high is squaring it is
fall tolerant so here you can see the
NASA log thing is not supported but uh
if I look at any other table so let's
say
select count
star from let's say out one right if I
do this ideally it should
work for some reason imp is a bit slow
in this this uh hi is faster in this
case it is not really like this actually
it's very
fast there is no map reduce it directly
hits and gives a result the same query
if you run you will see map reduce job
starting map reduce nothing ah so we
will uh and there is one more thing this
refresh manually has to do normally we
do it manually uh you can write a batch
job to refresh so I don't know whether
the refresh is configured here or not
for the Impala because this NASA lock
table is available we just created it
right now but it is available in Impala
so probably and this is the option to
refresh uh can you see this refresh
three options will come clear cache
perform incremental metadata update this
will sync missing tables in hi which
means if you have a new table in hi that
is not here it'll do it and then there
is invalidate all metadata and rebuild
it this can both be both resource and
time iny when you say invalidate it will
delete the whole metadata and from
scratch it will build that'll take a lot
of time so these are the refresh options
in Impala that you have so try to create
a table see whether you can find it in
Impala I don't know so we'll create some
simple table
right which table you want to
create maybe I'll just change this and
run it as it is right
so I just created and if I go to
Impala May
19th where is
it no I don't have it so if I say
refresh uh let's say perform incremental
refresh H now it came transaction record
so ideally we do this manually because
imp cannot automatically identify you
created a table in high if you do it it
will come
IDE huh so Impala also stores a lot of
metadata uh and actual data recent
queries Sol in the cache so if you want
to clear it you can say clear cach the
reent queries you ran it'll store it in
the huh in the cash it will store so
sometimes the cash will become big like
you're running a lot of queries so that
will affect the performance so you can
say clear the cash yeah ideally it
should keep it in the cash hi does not
use Impala hi uses only map ruce no I
don't think that is possible probably
admin or somebody can do it from a
developer side I have never uh I mean
what is a use case I don't know I mean
masking table masking is there um I
don't know if it is possible I have
never tried huh so table masking you can
do so that has to be done by the admins
so you can say that mask the table it
will not be visible to any other process
like Impala or spark or anything if you
want H but usually we won't do like that
usually we restrict the access based on
user so the user so if I am a guy okay
so all the hi tables will be there in
Impala let's say but if I a guy who is a
developer and my access will be that I
can see only these tables in Hive or
these tables in Impala so behind the
scenes everywhere you will have tables
but user level access you can control
who can see what tables otherwise
everybody will be able to see all the
tables right I don't think directly
there is an option to stop them
communicating ah hoton Works Hado
doesn't not have Impala uh they have uh
this thing Hive is there and by default
hi plus th the is the execution engine
uh then LP LP is their uh highlight
actually LP is much reliable much faster
than Impala to be honest right so I'll
show you a little maybe not now I'll see
a
session so this part is clear right Hi
and hi plus Impala at least Basics a h
has an automatic way of identifying the
data but that will work only if you have
a proper delimiter like comma or space
in Hue you can create a table there it
will identify if you have space or comma
or something uh I don't think geospatial
data means what is the structure so impa
I mean this much only you need to know
as of now I mean I'm just giving it as
an option normally in Hive classes we
don't teach Impala at all the s why hi
is hi Impala is
Impala H not subject like it has become
like so people don't learn it separately
in the project because the queries and
all are
same the SQL you write everything is
same there is no difference at all only
thing is the use case is slightly
different uh no only it has its and it
is contributed by claer Impala is an
open source project but mostly uh the
contribution is from claa so it is like
a propery product so if any issues comes
claer will support a lot in Impala Point
number one and so another problem is
that if you go to other platforms you
won't see imala probably that is the
reason people don't care much about
Impala right so if I go to hoton works I
will not see Impala at all
right if you want security there is
something called Sentry service Sentry
you have to set up Sentry I don't think
we have Sentry here so in clouder I'm
saying clera you have something called
Sentry Sentry
clera so this is called uh Sentry here
you can set up all the uh you know user
level aess access service level access
everything ah centry again Sentry is
open source but so here you have users
groups uh and uh access see
authorization privileges model for
highand Impala so the problem is uh
centry is again Apache but only Cloud
uses you go to Hoten works you have
knock and range two tools one is called
Ranger and
nox again they are open source but only
on hotworks so one problem is a platform
wise there are some slight differences
Ian it happens so there are slight
differences Cloud uses this Sentry and
in Sentry you can configure who should
access which table and and all the
authorizations the admin has to set I
don't think Sentry is installed in our
cluster which means anybody can see
anything even hi for hi you have
authentication username password you you
will create ah here that is not there I
mean as of now we are just saying hi it
will enter
right H Karo is by default I'm saying
right now what you're doing you are
going you logging into one of the
machine in the cluster it doesn't work
like
that are you getting my point so I think
we discussed this in the first class so
this is your Cloud lab right
this is your Cloud lab and you have an
edge node so you log on here from here
you connect to the lab and any security
any service you need you have to enable
here and as of now nothing is enabled so
when you just start high you just hit
the cluster and anybody can see
anybody's table or anything that you
create Sentry is not enabled I think and
that even when you're working you won't
have access to Sentry and all it's
totally the admin side they only can
decide who should you can request them
you may be not able to see who has
access yes yes so they will be in the
cluster but once you hit the edge not
that will be applied to you like all the
policies whatever you're having
otherwise you can't restrict the users
right from coming
in uh hon Works has Ranger
Ranger hoton
Works Ranger is
similar I also like the hoton works
documentation a lot it is much more
precise and clear you read you'll
understand what they're talking cloud
documentation very weird nobody will
understand what they mean look at H
documentation very easy to read and
understand see comprehensive security
for Enterprise hard what Ranger does how
Ranger works right everything you have
here h
so Apache Ranger offers centralized
security for all these things hi so if
you go to Hive uh they will show you uh
what Hive does see there is even a
YouTube presentation very nice
documentation they have the best
documentation actually hoton
works see LP live long and process is
what I said I think there is another
explanation it's not actually live long
and
process uh something else is there aut
just say it is live long and process I
don't think it is live long and
process so hi details are here and some
optimization techniques are here okay H
hoton document is the best if you want
to learn only thing if you're in a cloud
production cluster some of the things
may not be applicable since it is
totally open source like they have their
own ways right so if you are using a
cloud era cluster in production you
should always ref for cloud data
documentation because not everything
will be same some difference will be
there right uh I
hope that clarifies uh Impala so we're
talking about
Impala um now I have to talk about
something called partitioning in Hive
okay so how many of you are actually
aware of partitioning some of you might
be aware of this Con concept yeah for
those who are already know this probably
it's a refresher right see the idea is
very simple so can you tell me so your
manager asked you to create a sales
table okay and U you are creating a DB
called retail and a table called sales
so where will be the location let's say
manage table uh it will be
user
hyve then what
warehouse and the DB is
what
retail and the table is what
sales this is the location right for the
manage table and let's imagine every
month you are getting the data month end
so in January you got the data so it
became what jan. CSV you said load data
local blah blah blah this is where the
data will go you know this already and
the data got uploaded here everybody's
happy no problem the problem is as the
next month and next month C happens
right what is going to happen in your
sales folder in February what will
happen February data will get uploaded
it's in the same folder by the way and
again what will
happen March
right and
April and May and so on so the problem
is if this is the model you are
following after after let's say couple
of years if you look at the folder you
will have 10 20 files inside the same
folder now the real problem is let's say
you are writing a query something like
this
select star
from sales
right
where let's say month equal
to
Feb let's say you're writing this query
the problem is Hive has no clue where is
your data it has to scan all the files
in this folder to give you the result
the result is here that you know H
doesn't know when you write a query it
will scan all the folders in the sales
folder so if you are having let's say
100 files Hive has to scan all the 100
files before producing the
result the drawback of this is that your
queries will be very very
slow so in order to avoid this problem
if this is the problem you're facing you
can do something called
partitioning partitioning of table okay
and in partitioning you have two things
there is something called Static
partitioning and dynamic
partitioning static and dynamic you
have two type of partitioning you can do
okay and I will show you that with the
help of an example that will be better
but what is the idea in partitioning
first you have to identify the column or
columns based on that you want to do
partitioning so I'm assuming that this
is my table structure and most of my
queries are based on month equal to
something right so what I can do in hi I
can say that okay create a partition
table where the partition column is
month and what hi will do if you provide
the data it will automatically identify
and
create folders like this and we'll place
the data like
this so when you partition based on
month okay there are multiple ways to do
this I will show you technically what is
happening behind the scenes is that it
will identify the month column and as
many months you have those many sub
folders will be created and it will move
the data now we know that there are only
12 months in an year so if this is the
data model you will first partition
based on year then month so that it will
create a folder called 2016 within that
12 subfolders then 2017 within that 12
so but if this is the model if you write
this query it will only hit
here will skip all these other
subfolders so your queries will be
naturally faster this depends on what is
the columns you are having so let's say
I'm saying here the column name is Jan
so what is the next column you have a
week one week two like that you
have I mean this depends on the
data now in this data I don't have date
I have a month column I have a day
column I have a year column now
partitioning is not a must it's not like
everybody should do partitioning there
are many conditions where you can not do
partitioning so like you said if you are
having date you can't partition directly
because if I have a date column uh and
let's say you are looking at last 10e
data how many dates will be there
3,650 if I say Partition by date it will
create 3,650 folders that's useless
there is another way to tackle it I'll
tell you but you don't do that so
partitioning is applicable if the
cardinality is very less something
called cardinality right how many unique
values you have in the column like in
this case or let's say you are getting
data from different countries in the
world fine maximum you have 150 I don't
know country country wise you can say
partition but yeah like you said or
another Cas is that you are having
transactions and I want to divide the
data based on transaction ID that's not
possible because every transaction has a
unique transaction ID it starts creating
those many partitions that's not
possible so partitioning should be used
only when your cardinality is within
control like this right now you're
partitioning only on the Jan colum I'm
giving him a different use case let's
say you getting the data from different
different countries it also depends on
what query you are running for example
let's say this is the partition data I
write a query select star from sales
where country equal to India it is a
worst query I can write because my data
is partitioned on month and I write a
query based on country it will have no
impact it's actually worse so if you
know that most of your queries are based
on country and month you Partition by
country and month I mean whichever way
it is possible probably first Partition
by month then country or country then
month you have to figure out the
cardinality so that and also if you are
ending up creating lot of partitions
that's not a good idea for Hado because
had does not like a lot of subfolders
and sub files right so and and drawback
I think I discussed this in the first
class this this partitioning if you do
in
traditional uh databases it is sometimes
not
effective why
because we had a use case where we had a
Oracle cluster like this know four
machines are
there okay and then you are saying let's
say you are looking at the iPhone sales
data you're selling iPhone iPhone sales
data and you want to partition the data
based on Country number of sales in
which country right so I will say that
Partition by country but that's a
logical partitioning you are not
physically dividing the data the problem
is it will create countries like India
China us and all right so probably us
partition will be here this much will be
there H and probably what this will be
what uh I don't know give me some weird
country
Congo you're getting the problem you're
logically partitioning you saying that
divide the data based on Country since
in US lot of people buy iPhone what is
going to happen us partition will be
very big this will be your us partition
right I'm talking about Oracle okay and
this will be your Congo partition and in
Oracle if you fire a query what will
happen the machine is responsible for
they work together so the US partition
queries will be very slow because this
guy has to CH the Congo partition will
be very
fast right in Hadoop also what happens
you will do the same partitioning okay
you will say that you have us data India
data and all it create sub folders but
the advantage is that even though this
file will be 3tb it'll be on 100
machines
blocks in Oracle this will be one
machine are you able to understand what
I'm saying
so if you are storing the data in Oracle
Oracle has no idea of blocks and
replicas so us partition data will be 3
TB 3 TB will be one machine it won't
divide this if this is in Hadoop 3 TB
will never be in one
machine blocks will get divided right so
my query will be faster actually so the
partitioning is much more effective in
Hado I'm saying right so which means
partitioning in traditional databases
and are not so effective it is effective
up to an extent but you have to ensure
all the partitions are having equal
amount of data so part partitioning is
logical in every case even in Hadoop it
is logical in Oracle it is logical in
Oracle I'm saying that all the
partitions of us should be in one
machine there is no way I can further
divide this
data you are getting my point right
because it is not Hado or something it's
just a DB so all the US data will be in
one machine so that machine will have
lot of data related to us but in Hadoop
if these are four data nodes you will
never have 3 terab on one data node will
always divide and spread are distributed
so my performance is better in
partitioning right so these are some of
the things we understand when we work
right uh so so that is why some of the
companies will say don't do partitioning
much ah sharding is there sharding is
dividing a table existing table um
sharding mostly we do in nosql Oracle
after LG support sharding
but it is not extensively useful
actually in in nosql databases you can
do something called table sharding table
sharding is like physical division you
take a table and say take 10,000 10,000
rows just divide like three or four and
then dump it
okay but that is not highly possible in
rdbms because in rdbms you have
normalization it is physically any
partition will physically divide the
data but can you say that I want to
divide this table into 2 GB 2GB 2 GB
that's not possible right ah that is
logical U physical division means like
he's saying sharding I take a table the
table has say th000 rows I can say
divide it into five 200 200 200 rows
keep in five machines that's not
possible Right in normal rdbms that's
called physical division you are not
giving any condition you are saying take
the Full Table split it into five 100
100 rows keep it in five machines that's
not possible impossible in rdbms in
nosql it is possible because it is
denormalized in nosql everything is
denormalized so I can divide my data and
push it in whichever way I want nobody
cares it's already distributed in hi or
traditional rdbms physical division is
not possible you logically divide you
say look at this column and if you have
100 values divide into 100 200 to 200
you don't have any control you can't say
this much Division I want or this size I
want there is there is no physical
control logically you're dividing the
data right there is no way for you to
understand after March again there is a
effect hi has no clue right it keeps on
searching all uh you know data so
creating this either you can manually
create this folders or you can so
normally we don't do like that we run a
command and create it
actually so it is better to do this
rather than talking about this is just a
basic idea I want you to find out the
data set and then start work working on
it so there is partitioning can you see
this
folder
partitioning and there is a word file if
you want I mean this has the
commands but I have been facing one
issue with this word file which
is when you copy paste sometimes the
commands will not work we will see
whether it will work here okay uh that
that code right what you call single
code it will not identify so we will see
whether it works or
not so we have not discussed what is
static partitioning or dynamic
partitioning and I will talk about this
first
thing um let's create a table and then
we can start talking about
it so do you have the DB yes right okay
the DB is here and I will say create
table just copy this command okay
now static partitioning is a bit
confusing but we can
understand so I'm creating a table
called user one as you can see if you
look at the schema it has first name
last name and ID only three columns and
then I will say partitioned by country
comma region this means these are my
partition columns I want to divide first
by country then by region fine but there
is a catch here what is the catch what
is the data you're going to load if you
look at the data this is the data user
info one and if you open this data can
you tell me how many fields are there in
this
data three which are uh first name uh
last name and uh ID right and if you
look at the table I
created it is matching here
right then I'm saying Partition by
country comma region right but there is
no country or region
here right this is static partitioning
so static partitioning means you are
getting the data you know from the where
the data is coming but the data will not
have that columns meaning I know that
probably this data is coming from
country equal to US state equal to
California right and while uploading
this data I should mention it the data
does not have any columns for country or
region or anything right if I upload the
data you will understand so do one thing
uh copy this to Hado this user info one
can you do
it uh go to
hdfs there is
uh I'll just copy to Ragu
user info
one in fact copy all so that we can save
time right I mean these are also
required uh yeah so then you have to use
FTP if you're copying to local you have
to use FTP better copy to H it's faster
right
anything and I have a question to you so
right now in Hadoop will it be already
created the country and region folders
what do you
think so partitioning means it is
creating
subfolders right I'm saying create a
partition table country and uh region
will it actually create a sub folder
yeah so no nothing will be created as of
now and then what you need to do is um
where is a word pad
so I will copy
this I don't want to type a lot but you
have to change it okay don't type it as
it is
because
um what is a location Ragu for me file
name is different I
think user info 1.txt
right is there a txt extension
yeah so now what I'm saying I'm saying
that load the data and this is my data
into the table and while loading the
data I'm mentioning the details I'm
saying there is a country New Zealand
and region
cubc so what will happen it'll create a
folder called country New Zealand
another folder called region cubec
within that this file will go can you
verify that in the warehouse folder if
you have loaded it it should be
available in the warehouse folder so
I'll go to
Hue uh I'll go to
hdfs and where is your U data user hi
right user
Hive
Warehouse minus
May so many DBS are there people
started May 19
DB uh here is the user one can you see
country equal to New Zealand there is a
folder in Hadoop called country equal to
New Zealand within that there is region
equal to cuc within that you have your
data try try yourself and see whether
you can see it this this will be there
in your project and assignments so
partitioning is very
important it's in the word file
right oh okay so it's wrong right sorry
sorry I'll just show here
so remember this point static
partitioning should be used like we had
a use case Okay where we were getting uh
user data from multiple countries okay
one of our clients's application the
user data was coming from multiple
countries but the data will not have a
country column so we used to manually
create that's called Static you will say
that put this data in us now we have to
do something called Dynamic partitioning
that I will show you
that so make sure you are uh able to log
to hi and always do use your DB some of
you are using default DB and when you
type the command you say already the
table exist because lot of you are
actually using default DB
so if you uh meanwhile if you look at
this data right
hi uh
partitioning can you tell me what is the
difference between this data and the
previous one you loaded exactly it has
the city and uh state or
whatever by the way what is Togo and
West Bengal doing together whatever okay
so have some City or region yeah so this
data has the country and city uh column
so the best way to do partitioning here
is you create a partition table you say
there is a partition by country and
state or whatever column and then you
tell hi I'll give you the data you
decide yourself I'm not going to divide
the data you look at the column I I
understand how many countries are there
or how many states are there accordingly
create them and put them that is called
dynamic how many H but it's only like 20
lines of data 20 or 20 those many will
be created ideally huh and I think uh
the first part it starts with Christmas
Island there is no repetition right but
there is one thing you have to
understand in Dynamic partitioning
Dynamic partitioning is by default
disabled in Hive because uh somebody can
come and say that okay create a dynamic
partition table use transaction ID
column then what will happen millions of
partitions will get created and it will
screw everything so by default even in
production clusters Dynamic partitioning
will be disabled so first you to enable
it first thing second thing is that uh
you cannot upload the data directly to
Dynamic partition table what you need to
do first you upload the data to a
temporary table or some table okay from
that table you insert to partition
table meaning previously what we used to
do we created a static partition table
then we said load data in part it it
went we are able to do that because we
know which partition where it is going
right now we don't know hi has to decide
so what I'll be doing I will create a
table first normal table I will upload
this data no partitioning nothing then
I'll create a partition table and from
the original table I will say insert to
the other table so while doing that it
will divide and send that is the only
way to do uh Dynamic partition in in hi
okay we'll do that practically so you
will understand you can't directly load
it has to be in a table from there you
have to do insert but in this example
it's not really useful because you're
having all unique
values like normally you should have
like five records for one country or
something like that but here everything
is unique values we are having but
that's the
idea um let me see if I have another
file okay because partitions in high
just this land property
case
study one moment
okay
load
list uh I will do one
thing once we finish this particular
example on Dynamic partitioning we will
do something called bucketing also and
once we finish that I will give you this
assignment this landed property analysis
you can do it by yourself I mean now
it's easy commands are already there and
that will help you to reinforce the idea
on partitioning this has partitioning
also just try that once I finish okay uh
so for the time being for dynamic
partitioning what we will do we will
create a table called user 2 you can see
what I'm doing from here same thing I
copy paste
um and this is the partitioning table
right look at here I'm saying that
partitioned by country and region so
this table is the one which will have my
data partitioned okay then what I do
I'll create a table called user three
that's a regular
table uh first name last name
country region first name last name ah
country region right
so do we have only four
columns where is the
data user
info three
right now we have first name last name
ID so your schema should have five
columns right then only it will work I
mean the original table so this table
called user three will hold your
original data so what you will do first
you will load the data here how do you
load load data in path because already
it is in Hadoop where is in Hadoop ru/
what's the file
name
user
info 3.txt right is that the
name into table
user
three so do till this load the data to
uh user 3
table Yeah so this user 3 table will
hold your original data no partitioning
nothing you'll just have your data from
there I will insert to the partition
table I cannot directly upload the data
to partition table in Dynamic
partitioning so I should copy the data
to one original table from there I have
to insert
H in static you are mentioning which
country which uh State and all manually
so you can upload the data say create a
folder like like and then dump the file
as it is here the file has to be divided
based on column values
dynamically hi has to decide so you have
to load the data to one table from there
you say insert to the partition table
and hi Will dynamically partition
it based on these two columns country
and
region here I have created right country
and
region yes first all the countries
within country regions so right now we
have around 20 25 lines of data only
that much will be there partition column
should be outside the schema when you
define a partition table so these are
the partition columns whether they exist
in the data or not exist in the data
they have to be outside the schema you a
partition by there you mention the
schema so it will it will not be here
it'll be here no no it's not required
only here because the total schema will
be including the partition columns so
it'll
understand okay now I want you to do one
thing if you look at the word file right
you see these things can you see
this right so these are the properties
you need to enable for dynamic
partitioning so the first one is very
easy if you read you will understand you
are saying that hey hi enable Dynamic
partitioning
set Hive exit Dynamic partition true
which means by default it is false by
default Dynamic partitions are not
supported so you're saying that yes I
want Dynamic partitions but what about
this let me copy
that so even if you enable Dynamic
partitioning by default in high what
happens if you create a partition table
at least one static partition should be
there so it's a bit complicat to
understand so there are lot of layers of
security but everything you can override
also so in hiive first they say I will
not allow you to create Dynamic
partition at all only static is allowed
so you say set High exic Dynamic
partition true which means now I can
create Dynamic partition but then they
are thinking if you are allowing you to
create Dynamic partition what if again
you try creating thousand partition
10,000 partition so there is a strict
mode now we are in non-strict what will
the strict mode tell you is that even if
you are in Dynamic partitioning okay you
have to first create one static
partition in the table rest can be d h
one one column you static you do rest
everything has to be dynamic you can
again turn it off so many people have a
confusion huh
each partition table so right now our
partition is based on what uh country
what is that country and region right so
what they saying country you manually
mention while uploading country
partition you manually mention Region
High will
decide in strict mode will not add it
will manage internally but it is saying
that manually you mention at least one
static partition need to be created it
is just a safety measure even in
production setups we disable it because
who want to do it you are you have three
columns and one column you have to
statically mention means nobody is able
to allow you to do that right but
sometimes it is very useful I'll tell
you because you are getting the data
where there is no country
column okay so you want to add a country
partition that will be static you have
some other data let's say I don't know
state or something that already is there
so that can be dynamic so in that case
you don't have to turn it off because
you will mention country static Al there
is no other way rest of them it will be
dynamically creating it so we are
turning it off because it's a developer
environment it's fine we'll create as
many partitions as we want huh normally
these are all true I mean uh like
Dynamic partition is false which means
you cannot create any Dynamic
partitioning this will be strict by
default so what we do is that uh
yesterday somebody asked me even I
forgot to discuss it you can create
scripts hi script like SQL script you
create SQL script right do SQL file same
way I can copy all these commands in a
text file save it as hql I say run it'll
run as a script that's you won't type
everything right who do who does that so
in the script while doing you will first
add these things you know set this this
this and just run it that's how usually
we do
it it can be SQL also SQL or SQL only
two it can be technically it can be
technically I have I not tried anything
apart from that normally we try either
SQL or
hql I don't think if you add any other
will it work if you have a SQL script it
work it'll work hi I have not tried
because all the scripts we have is hql
by default people say as hql only I have
not tried it might
work but I'm saying so and these are
specific to your session so H's language
is called hql hi query language okay and
this is not a script we are not writing
scripts this is this is the
shell ah this is the query script is
different what I'm saying all these
commands you can write in a text file
notepad and you can say run the notepad
it will execute one by one you don't
have to copy paste each time hql it is
high query language
so huh so that's what I'm said yesterday
the language of Hive is called hql which
is derived from SQL so 95% is similar
there are some uh very rare uh know
commands which only Hive has SQL doesn't
have I think it is SQL 2002 dialect what
it is using SQL 2002 there are ANC
formats right for SQL SQL 92 SQL 2002
SQL 2004 Hive is built on SQL 2002 if I
remember SQL format I'm saying this is
syntax right do you find any syntax
different from your regular SQL to tax H
partitioning close is different right
that's what I'm saying so if you're
doing partitioning in SQL it's not this
command slight difference is there so
these two properties I think you
understood what is the enabling Dynamic
partitioning and the mode is non-strict
now again for controlling this is hi xic
maximum Dynamic partition 100 which
means you are restricting maximum number
of dynamic partition is
100 then this property is not actually
uh correct the property is correct but
you should find a different value for
example uh let's write it as 100 what
this means is that so when let's say you
are creating Dynamic partitioning let's
say you have a very large file and let's
say in the column you have 150 values
countries imagine so 150 partitions will
be created each partition will be
created by one
reducer it it's map reduce job end of
the day so this creation of partitioning
table will fire a map reduce job end of
the day so one reducer will work to
create one partition so if you have 150
partitions 150 reducers will work here
you can control it this Dynamic
partition per node 100 means maximum
number of reducers will be 100
used so if you want you can control the
number of reducers okay the advantage
and disadvantage is different uh meaning
sometimes what is going to happen you
may not really need 150 reducers to do
the job correct so if you have 150
partitions if your data is very less you
can achieve it using 50 reducers maybe
so you say limited to 50 only 50
reducers will be launched
so this you are saying so these values
are wrong I'm saying wrong in the sense
you're saying maximum number of
partition is 100 that's fine okay
maximum reducers are 100 so ideally here
100 reducers will run if you have 100
partition
but mappers will depend on your input
split right again so like what data you
are getting so that you don't have any
control anyway reducers only you have
control my point is if you're having a
very small file
okay and probably 100 partitions are
there you don't need 100 reducers
probably 10 reducers can do the job so
you can control this property and I
didn't make up these properties it's
available in hi okay so people will be
thinking that okay from where did you
get all these properties it's properly
documented I'll give you the document
for
this you can copy any of these
properties go to Google Just paste it
and high language manual will come
uh not the first one okay so this is
hoton ver I mean the original Hive
language manual will come if you open
this oh just search
for here you have all the
properties uh um where is
it hi EXC Dynamic partition default
value false okay uh whether or not to
allow Dynamic partition hi exit Dynamic
partition mode default value strict in
strict mode the user must specify at
least one static partition right High
vexx maximum Dynamic partition default
value th000 High vexx maximum Dynamic
partition per node maximum number of to
create an each mapper reducer node that
means how many reducers will be used
total right and you also have uh other
properties for hi we will look into that
later
anyway so if you come here so we have
enabled all these features right and now
what you should do is
that where is a word file here you have
to Simply say insert so this command you
will say insert the data from the table
to the partition table this is how you
do
it and this will fire a map reduce job
you can see and you can see on the
screen it will create partitions look at
my screen or you can look at your screen
also whichever you can see the
partitions loading loading loading I'll
show you because Dynamic right so see
country null region null right now see
can you see country Nigeria region poosi
whatever om man see all all these
partitions are loaded dynamically and
verify that in Hadoop verify whether
this is created in Hadoop right if you
go
here um what is the table table
name so if I go to user two see how
Korea came I I think it didn't come it
says just Korea North Korea we had right
in courtes and
all yeah I disabled we are in non-strict
mode and any country you take there'll
be a region any region you take there'll
be a part file because it is output of a
reducer that is why that z0 comes
reducers will run
right huh uh I think I don't know how
many reducers ran for this this that's
another thing so this is zero right and
if I go to something
else yeah so that means those many
reducers ran
right because everything is zero so
otherwise
uh 0 1 2 if it is same I'll just check
uh we can actually see the statistics
here if you go
to not
workflow job
browser there is a job
browser and this is the job we ran did I
show you this no I think I showed you
first class if I go
here there is a task
it's not displaying
everything sewing stage
one I'll see anyway so can you are you
able to run this I said yes I think yes
right can you see the partitions I don't
know how many reducers were called we'll
see that later but are you able to
create this just look at your hdfs and
see whether you can find all the
partitions uh because of the input data
right so our data need to be either
changed I think because all other data
is like regular so here if you give like
this or you have to clean this I mean um
so usually what we do is that I mean if
there are only limited records you can
use either pig or Spar to clean the data
you can easily clean it you can say that
find codes and then replace so First
Column name that you give the partition
will be the top level partition within
that it'll create those many sub
partitions
actually but rest of the partitions are
loading right there is no problem I
think how do you search so there is no
way to search for uh uh rejections from
here you have to manually verify because
here it says country equal to Korea
region equal to North that's all it
didn't reject it actually it created
Korea then the region is North actually
that is incorrect okay as per so before
loading you have to validate the data
that is only way so we do that there are
validators which can do field validation
and whether all the fields are like this
and exception handling this you have to
do before you do it this data set has an
exception that's fine but usually you
have to validate now uh so this is
usually how you do Dynamic partitioning
and one more thing is that when you're
doing Dynamic partitioning um there are
performance problems sometimes like
creating too many Dynamic partitions is
not advised even Hadoop will not allow
because name node has to handle all the
metadata for the subfolders and the
files like you are having 150 countries
and each country has States so if you
say divide by country and state
thousands of folders will be created so
that will increase the metadata on name
node so in one way it is good your
queries will be faster but other way so
there is no limit as to how many you can
create partitions but we say that do it
with
caution no uh only thing is that your
name note performance will be affected
right like thousands of partitions are
not a big problem but so many huge
number of partitions are there that will
become a problem actually uh it's not
table size so usually when you use
partitioning is that if your queries are
running slow like and the reason is that
inside the same folder you're having
multiple files and it has to scan all
the files and there are some cases where
you cannot do partitioning you may not
be able to find out a logic to do
partitioning probably there is no common
column or something then I can't do
partitioning there is no other way there
it's not like everybody must do
partitioning there is no rule like that
right now somebody asked a question
right so this will lead us to getting
but uh what if I'm
having let's say you are getting data
from the country uh side so you decided
to partition so it becomes country your
partition column and there are
150
countries look at Amazon look at Dell
for example Dell has presence in 150
countries okay and so customers are
buying products of Dell and they have
some data which is country wise okay so
what they decided they decided to
partition based on Country fine no
problem now after this what happened is
that so every time when customer is
purchasing a product there is a
transaction ID right and they want to
write queries like this select
something
okay
where country equal
to India okay comma transaction ID equal
to 1 2 3
4 now the problem is it's already
partition based on the country so that
part is saved because it will look at
only India but within India you have
millions of transactions so this
transaction file will be huge it has to
scan all probably there are multiple
transaction files okay and trans and you
cannot partition based on transaction ID
because every transaction is unique
right so what you do that's the question
and I don't know whether this is
available in rdbms and all we do
something called bucketing
bucket uh you have something called hash
partitioning right can you tell me what
is Hash
partitioning in rdbms I'm
asking rdbms you have who knows about
hash partitioning what is Hash
partitioning because this bucketing is
very similar to Hash partitioning in Rd
BMS side I have not done hash
partitioning so what we do is that see
there is a transaction ID column right
this is your
transaction ID column and here what are
the values you can have let's say 1 2 3
4 right like this transaction ID and
let's say 1
million this is the data you have now
you want to divide this data based on
transaction ID and you cannot do
partitioning you say bucket it when you
do bucketing it can be done only on one
column you cannot say two column
bucketing not possible and I say I want
to create let's say 10 buckets so you
always mention the number of buckets you
want that is all you do what is going to
happen within each country partition
it'll create 10 files not folders it'll
divide this data based on an internal
hashing logic okay and create 10 buckets
10 files within each country folder so
when this query hits it will calculate
the hash of this okay and find out which
file is having it'll hit only there so
to easily understand this is not the
actual idea but 5 6 7 8 9 10 11 12 these
are the transaction idas now I want to
divide them into 10 files so there is a
internal logic which hi uses I'm not
going to tell the same logic but when I
say I want 10 buckets 10 files will be
created fine so I say 1 2 3 four five
these are the files
okay and let's say we take a very simple
logic you want to find the modulo 10 you
know modulo 10 right what is the Modo 10
of one one so this will go to this
bucket this will also go
where are you able to understand
this will go here this will go here well
it is not doing like this I'm saying ah
similar so there is an internal hashing
logic and then when you write this query
it'll look at four it'll go to Fourth
bucket that's a logic not this logic it
doesn't do model of 10 okay it does some
other complicated logic but it will
internally divide we don't have to
bother it will internally divide now
many people have a misconception that
you should always use party in and
bucketing together no I mean it's a
misconception people who are already
working I have seen a lot of people
discussing no but in most of the
examples when you search you want to
learn partitioning people will say that
I will first create partition then
inside that bucketing that's a practice
but you can independently create
bucketing without partitioning I can
simply take a table so I want to bucket
it that's all I don't want any
partitioning or anything so what will
happen in the same folder it will create
those many files 10 files
ah physical F we can see that bucketing
we can see practically we can see if you
can do partitioning don't do bucketing
because that's the most obvious thing
where you cannot do partitioning you
enable bucketing uh for example India is
there within that you don't have any
choice and there is no ideal way to
decide how many buckets are required I
mean so next question is how many
buckets 10 100 uh so ideally you should
take care that the file size should be
somewhere near to the block size don't
create like 1 MB size
bucket so if you are having let's say
the data is 10 GB within a partition
okay you say create 1,000 buckets you
know it will become very very small
don't do that because that will create
smaller files so ideally take a block
size calculation how many buckets you
need you have to decide some logic you
have to use for that actually if data
becomes big that's not a problem data
should not become small that's the
problem like bucket size should not be 1
MB then if it will create too many small
files in Hado right Hado doesn't like it
so if the bucket size is 500 MB fine it
can become 1 GB it will still divide
into blocks that's fine we are okay with
it not not the block size I'm saying
bigger than the block size if it is
smaller than the block size Hadoop
always doesn't prefer that because
smaller files are not very well handled
in Hadoop okay ah join operations these
buckets are excellent for join
operations there is something called
bucket join so H has lot of advanced
concepts for the time being you you may
not be aware of that okay so what will
happen is that I have a table
here okay and this is my column and this
is
bucketed H so this means this is divided
into uh let's say five files 1 2 3 4
five I have another table that also has
the same column that is also bucketed
possible right and this is also having
either same number of buckets or
multiple then you can do a real fast
bucket joint there is something called
bucket joint because since the data is
divided into equal equal partitions the
joints will be really fast actually when
you bucket it so when you write a joint
query you will say use bucketed joint
there is a command you can use I mean
there can be mismatch the when you do
join operations there should be a common
column join column and usually they will
have similar uh entries right that is
the idea you are doing join otherwise
what is the use of a join
so ah so this is like you are having two
tables and you want to do a join
operation normally you can do a join no
doubt but if the join column common
column is bucketed okay then there is
something called bucket joint you can
use which will be faster high will
internally make it faster since it is
bucketed the join operation will become
faster uh hashing so and there is
multiple types of bucket joints actually
in hi so it's very interesting uh it's
bit Advanced concept but these things
are possible
in no no that's anyway just an so it's
very rare that you may get a chance on
this but people are aware of these
things like bucket joints you can do
right so we can do bucketing practically
that'll be better right rather than
talking so why don't you look at another
data set
so can you open this bucketing
data how do you open with what Excel
right see this is a very interesting
case study because here you can
appreciate more uh partitioning because
we have similar things see you are
having street right and the street is
having almost unique values some some
unique at least city is Sacramento zip
code you have state you have California
uh bedroom bathroom Square fet so this
is this thing real estate data and type
is residential condo blah blah blah
price some price right so and how many
lines of data you have 986 lines of data
you have now Our intention is to do
bucketing but we will do partitioning
also just to see anyway we can do that
so what I want you to do is first upload
this data to
how do you
upload the file name is realore state.
CSV that's the file name okay
and I want you to open this word pad
which will be here bucketing
Hive yeah so what we are going to do
first we will create a
table
okay and this is a normal table
okay I'll say create table R
State that's called real estate and this
has the schema regular
schema no partitioning no
bucketing uh terminated by comma regular
stuff no
changes and we will load the data how do
you load the data
load
data in path
so I guess you will be able to load the
data so this if you create
what you can do is load the data and you
have to enable bucketing so again
bucketing is disabled so just say set
Hive enforce bucketing
true so the command hive. enforce do
bucketing equal to True will enable
bucketing
right and look at this command this is
is probably the most important
command create
table uh you are calling the table
bucket underscore table and I have the
schema Partition by City clustered by
Street into four buckets what do you
mean by this Partition by city so City
you have common entries right I think
Sacrament or something so I'll say that
partition column is City then I'll say
clustered by street so Street column
into four buckets so it will create uh
those many City partitions each
partition will have four uh this thing
what do you call buckets but it is not
mandatory another thing is that if a
city has only one line of data only one
bucket will be there if it has more data
than only the four buckets will make
sense some of the Cities will have only
one street let's say it'll create only
one bucket in that case um and now just
uh load it and see whether you can see
the buckets I want you to
try disabled yeah even bucketing is
disabled and now I want all of you to
try yourself uh I'm just doing an insert
once the insert is complete go back to
your user High Warehouse folder you
should be able to find partitions and
buckets
so I will I will even show you my
buckets I mean like we just want to see
the buckets so if I
user hive
sorry Hive
Warehouse uh minus may9
DB May 19
DB what is the table we haveck bucket
table if I open bucket table these are
the city if I open a city there are four
buckets but again uh bucketing also
depend on distribution of data see here
the last two buckets are not having any
data but if you're having only one line
uh or just two lines like then only one
or two buckets will be created high is
very intelligent like very less data it
won't create four buckets here I think
you're having more more like some four
10 lines I don't know how many you have
based on that it'll create but the last
two buckets are empty in this example so
ideally you should have more data this
is happening because we are having very
less data right some thousand lines we
have actually but number of mappers will
be there right H either mapper or
reducer I said reducer so I'll show you
here in that property it will be visible
it can be either a mapper or a
reducer now uh when we created the
partition uh he is saying I'm not able
to see any reducer jobs only only mapper
jobs Ran So see
here maximum number of dynamic
partitions allowed to be created in each
mapper SL reducer not so it can be a
mapper job or a reducer job if you're
having less amount of data it will be a
mapper only will run but sometimes what
happens you will have huge amount of
data and the mappers will first move the
data then the reducers will aggregate in
different buckets or
partitions now he's saying he is uh not
able to see reducers in the job I'm
saying it can be either mapper or
reducer in the when firing a partition
query it depends on resource allocation
if the cluster is having resources it
will allocate I got only one mapper mine
is actually only one mapper no reducer
at all yeah it can be either a mapper or
reducer it doesn't matter actually to be
honest uh what is going to happen
is when you created that partition table
right when you do an insert what is
happening map ruce job so and what is
that map ruce job doing that is my
question it has to segregate the data
that is what it is doing right it is not
doing any other logic it is just
segregating the data based on your
partition or bucket you know so right
now you did so did you check this
partitioning with bucketing is it also
not firing any
reducer you saying that the recent one
right with bucketing so you are getting
reducer zero mapper is there how many
one mapper yeah even I got one mapper so
sometimes what happens if the cluster
has enough resources it will fire
mappers and reducers so that it can
complete really fast so mappers will
first probably segregate based on
partition and reducer will just
calculate the hash and dump it but if
the cluster is not having enough
resources this job will be very slow
only mappers will be launched they have
to complete the whole thing but there is
no intelligence in it I'm saying this
just a way of the cluster managing your
partitioning process huh the logic is
written but you cannot really say it has
to be done by a reducer or a
mapper right sometimes it'll be achieved
only using a mapper sometimes it'll
Shuffle and fire a reducer also yes in
my case it was done there was only one
mapper in my map reducer in his case I
think four reducers were launched so
probably there was some Shuffle
operation depends on where is your data
also another thing is that when there is
a reducer there is shuffling of
data data shuffles and then goes to a
reducer so I don't know probably in his
case some Shuffle operation happened
that is why the reducer kicked in anyway
it is not really important I'm saying I
mean while inserting into a partition
table it really doesn't matter whether a
mapper is doing it or reducer is doing
it it does not have any impact on the
developer side because it is just a uh
finding by column and then move moving
the data that is all you're doing but in
a query and all it is very important
because if you write a proper query how
many mappers reducers get involved
that's very important actually Hive can
also store the result as text file so
let's say you write a query and the
output is normally uh shown on the
screen like you write a query it will
show there you can say save it on hdfs
the output whatever query output the
command
is uh insert all overwrite
directory there is a command called
insert overwrite directory then you
write your query what will happen is
that whatever is the output of the query
it will save as a text file on Hadoop if
you want to save it right so normally
you want to see but sometimes you want
to say if you want you can do that also
directly it is insert override
directory if I remember because we
rarely save it as text file okay
Hive in
insert overwrite
directory huh so this is the one insert
over right
directory uh but I have to see
it is there you can try this command
insert override directory and then you
give uh a path and then you say select
star so let's try that I will
say directory
um what we will
say uh where is the
spelling oh
direct insert override directory I will
say
ru/
ABC count
star uh give me a table name that we
created just now NASA log right
there is a NASA log
right so this means it will create a
folder in Hadoop in Ragu ABC there the
result will be stored query result if
you want to store it somewhere you can
do that you can try this out see so when
you're creating a hyp table it store it
as a text file by default whatever data
you're loading into a Hy table it is
available in Hadoop and that will be in
a text format like CSV or or any text
format
and over the period of years there has
been lot of compression techniques to
compress the data so while creating a
table you can mention how to store the
data now I have some very good hoton
works links I will share it with
you um is there a
way okay I can anyway you're not going
to see right I'll just open my mail
because I just shared some links I
wanted to show you
I think in your assignment this is there
orc
right correct and that is very easy I
mean you so storing the data as or is
very very easy you just need to
understand what is
happening yeah very rare uh customers
are there in hot mail
right I never quit hot
mail so here is it so there are some
hoton Works links which I found very
interesting
okay so orc is a format so this is hot
works link but uh so orc files in hdp
better compression better performance so
when creating a hive table last line you
can add store as orc only one thing you
need to do just say store as orc and
what is going to happen whatever data
you're loading into that table high will
compress it orc stands for for optimized
row column format it's a compression
technique what is Advantage First
Advantage your data is compressed well
that's not a great Advantage maybe
second advantage and the most important
Advantage is that orc has
indexing there is a technique called
indexing right now in Hive You by
default has indexing but that is very
poor if you enable indexing you will not
get any performance what orc does it
will compress your data and whatever
data is in the compressed uh file it
will create an index inside that so when
you hit queries columnar queries and all
your queries will be really fast if the
table is having orc property column
indexing regular column indexing you
have right in rdbms and all what is
indexing in order to faster the queries
you will create sort of like a how do I
say pointer to the original data orc
will create an index on row and column
as well there is row indexing and column
indexing it'll create on everything
actually and it is very very efficient
apart from orc if you want you can
create normal indexing in hyp there is
something called uh bit map index and
compact index but they are very
inefficient actually because even if you
create an index in Hive uh you are not
actually dealing with name node or hdfs
you don't know where is the data but if
if in orc what is happening is that you
are asking Hive to manage the data
compress data so it will create its own
index for the the whole data and store
it now I'll share this link but if you
scroll down one of the things that they
say here is this so these are so if you
have a 585 GB file in Hive if you store
it as text file this is the size there
is a format called RC file it will
become 55 there is another format called
par become 221 if it is O it is 131 GB
that is the level of compression you are
getting so par and RC are other
compression formats uh RC is not very U
common now because orc is optimization
on RC RC is raw columnar orc is
optimized ra columnar so RC nobody's
using par some people use so park is
also another uh compression technique
but Park does not have indexing or
anything it'll simply compress your data
that's all orc has this advantage of uh
you know indexing within that right so
that's one advantage
uh AO is a serialization
format uh how do I say so let's say you
are sending a lot of data you can send
the data along with the schema normally
where will you mention the schema in a
table now AO is a format which can apply
schema even to text files without a
table or anything while sending the data
you can have a headr around schema so it
is very common in Hadoop systems that
you use AO format okay and if you scroll
down um so this is how orc will store
the data this is one thing you need to
understand
so it will create index on let's say
first 10,000 rows you can mention how
many rows you need to create index there
is a row and column index so here I'm
saying that 10,000 rows I want index so
what it'll do first 10,000 rows it'll
create an index and store here next
10,000 rows it create an index and store
here so if somebody's quing it can
understand which uh you know uh group of
rows it has to push and it can even skip
these uh 10,000 rows in one shot let's
say you're writing a filter condition
since it knows what is inside here it
can simply skip this entire 10,000 row
and scan here so it's very fast if you
do it in orc other formats do not have
do not have this
ability and there is something called
vectorization in hi it's an advanced
property
vectorization will allow you to read
thousands of rows in one shot normally
when you reading the data it reads one
row second row third row if you enable
vectorization with orc orc can read
thousands of rows in one shot so
vectorization and all will work only on
orc orc is the only format which
supports vectorization also this batch
indexing this properties are only
available and when you go to hoton Works
uh I told you that you can have acid
properties is in height you can enable
realtime queries the condition is that
the table should be orc then only acid
will work so for all these reasons
people prefer orc wherever possible but
what can be one drawback these are all
advantages but what can be one
drawback exactly it has to
decompress meaning uh your CPU Cycles
might be required anything which is
compressed you have to decompress
so compress is okay you're storing it
and that's okay but when you're querying
it it cannot it has to uncompress the
data right so CPU Cycles are required
but still people are preferring it uh
because they are okay to spend some
money on CPUs but still it is very
faster and gives you all these uh uh
properties so just go through this link
uh one link is this okay
and stride is this this is one stride
this 10,000 rows is called one
stride that that one block of rows is
called one stride actually okay uh and
this is the uh speed of queries of 1
terab data so this is hi 1.0 this is hi
1.1 that's fine and then hi plus
vectorized query High plus PPD PPD is
predicate push down you know what is is
predicate push
down like sending your filter first
right so if you write a query like
this so let's say you are saying that
join the data and then filter the
data this is bad right you are saying
join the data then filter the data first
you should filter the data then join so
by default in rdbms and all it will
first bring the filter together it will
optimize in Hive you have to manually do
this that is called PPD predicate push
down so if you enable PPD along with orc
this filter will come first then join
will come so it will push the predicate
basically that's the idea orc yeah so
and I can show you this I mean I'm not
just uh blabbering if you look at uh
your uh Cloud era um this
thing uh Cloud manager right so where is
the cloud lab
you look at the cloud era manager you
can see whether these properties are
enabled probably not all of them are
enabled that's one thing let's check
I'll go here I'll go to hi and I will go
to
configuration and I don't know if I say
PPD uh no it is not showing
ah enable vectorization
optimization okay it is enabled so this
vectorization is the property by which
uh if you are storing the data as orc it
can read thousands of rows in one shot
if you enable vectorization it can read
thousands of rows in one go and then
give it to you so it makes so
vectorization is enabled PPD I have not
seen I don't know whether it is enabled
or uh something if I search for PPD it
is not predicate push down I don't see
anything predicate no it is not there
probably that property itself might not
be here it is not available no
ultimately when you are reading it will
not get bulk it will get row by row that
is a default nature it is very slow
actually in hi when you're reading the
data it's in the row format right so
when you're creating the table you are
saying row format Del limited and let's
say your query result requires let's say
output is 10,000 rows it will not push
10,000 rows it will say Row one row two
Row three like that it will come it's
actually very slow in original hi if you
enable vectorization it can read minimum
thousand is there in vectorization
minimum th rows you can mention whatever
you want those many will come in one
shot so it's very fast actually while
getting the result and vectorization
requires orc no other format will work
with vectorization there is a reader for
vectorization which is available in orc
only as of now I don't think any other
format supports vectorization right so
now the question is that how do you
enable this orc
right optimized row column
format
so huh so these things are very very
important okay let me see if I can show
you from another
one so these are some of the things
which you can go
through so when you create a table all
you need to do is that you say stored as
orc last line you add stored as orc and
it will become orc and it will take the
default values you can even disable
compression some people do it so what if
you don't have enough CPU power you want
to use indexing and all but I don't want
to compress the data you can say
compression none it will not compress
the data possible uh and so these are
the properties you can mention in orc
you can mention the compression the
default algorithm is called zip it's
very efficient you can either say none Z
or Snappy these are the three options so
zib and snappy are compression
algorithms if you say none
orc table will be created no compression
and then you have orc compression
size number of bytes in each compression
chunk so the more you put the more
compression will happen in this case it
is 256 KB by default orc stripe size
number of bytes in each stripe 256 MB so
each I told you right 10,000 rows like
that so the size of that is 256 MB you
can say that in my table probably your
table has 1 TB data divide that into 256
256 like that and row index stri 10,000
for every 10,000 row an index will be
created so this you can increase or
decrease compression size uh is the
number of bytes in each compression
chunk so that means uh when it is
compressing uh it creates something
called chunks inside so what should be
the size of each chunk minimum size
actually so here by default it is 256 KB
depends on the data as well so we leave
these things to default we don't change
the compression size and all because uh
the uh algorithm Z or Snappy or none you
can change and the stripe size is how it
is dividing the data like here the
stripe size is 256 MB 256 MB will be the
size of your one Stripe Right and in
each each stripe it will get 10,000 rows
and create an index so probably in one
stripe it will create like five or six
indexes depending on how many rows you
are having one you're mentioning the
size and within that how many rows one
index need to be
created it's okay so sometimes I mean
this these are the properties just read
you will understand anyway okay and you
don't have to mention this also so
normally when you create orc table you
just say stored as orc because that is
better orc will take care of the
compression itself these are the values
you can adjust if you want to tune them
if you simply say store as or that's
also fine yes so if you say compression
none it will not compress the data it
will index it it will definitely index
it but no compression will be there now
the default is text file because for so
long people are using text file I don't
know orc is an evolving standard okay uh
and orc has different different versions
and all with every High release they
will release a new standard for orc and
all text file is the standard format and
you don't want to force anybody to use a
particular standard right this is better
so if you want you can use it that's the
only thing you can't read an orc file
you can see that you cannot read it
because it is like compressed and only
hi can read it Hive can only create and
read it it will look like uh uh like a
zip file only but you won't be able to
open and see what is in side only hi or
orc readers can read it actually like
you want to set up a small setup or like
in a production you want to do
it production so obviously you need
machines and you may not be setting up
only Hadoop And Hive nobody people will
be having the complete package actually
when you say Hadoop And Hive that's only
two tools right so rather than setting
only Hadoop and Hy so it's very easy you
have to have machines then you have to
go to one of the vendor either cloud or
hoton Works get their distribution
install it if you are using Apache
Hadoop you have to first download the
original
Source extract it there are four files
core site XML hdf site XML Yan site XML
mapsite XML in each of these files XML
files you have to manually type the
properties probably take a year to
complete almost and you will not succeed
even I will not succeed because it's
very difficult nobody does that we used
to do it I used to set up Apache
clusters but very very complicated it is
because everything is manual there is no
automation everything and there is no
use for that also only use is that
probably you want open source Apache
that is hoton Works The hoton Works
Edition and Apache Edition are exactly
same no difference at all yeah even
claer and hoton works there is a free
edition okay so since all these
companies are selling Open Source
Products you can go to Pon works or
Cloud era and download their product
install them on as many machines as you
want the problem is you will not get
technical
support in Cloud era you have an Edition
called Express Edition okay what is our
cluster our cluster should be Express
Edition
right you can check it so this is cloud
era
right uh somewhere it will be
there I don't think maybe I'll be able
to see the
addition because you don't have access
to everything
right huh see claer Express this is free
so we are running on eight notes or 10
notes right this cluster it's absolutely
free you don't have to pay a single
penny to Cloud but if something happens
they won't help you like technical
support is not there the commercial
Edition is called Cloudera Enterprise so
when you are installing the cluster it
will ask you do you want Express do you
want Enterprise if you say Enterprise
they will ask please browse and upload
your license key then only the
installation will proceed actually so if
you want to test or try something any of
this there are people who run on
hundreds of notes and uh this Edition uh
Express Edition no the hon workor
sandbox will not run in multiple notes
it's only single machine setup that
manually you have to install uh I'm not
quite sure whether it will work because
the sandbox is designed for one machine
only it is is not designed for
distributed like it is running only it
cannot be expanded even if you connect
both of them they won't work because
they are working as independently
sandbox is a virtual machine which you
can download and install your PC if you
start it uh it will have everything hadu
P spark everything just to learn like a
single machine setup production setup is
totally different installing and all so
that comes in the admin side that is why
I'm not talking much about it you may
not be able to understand if I say
because there are a lot of other things
apart from just Hadoop or something you
have to set up a lot of repositories
then machines and mirroring and racking
and then the setup is same you will have
let's say 100 machines one will become
master and in every machine you will use
either Cloud manager or any tool to
download Hadoop and
distribute so then you have to first
what you need to do you have to download
this tool called Cloud manager download
it on one machine then start Cloud era
manager it will ask you how many
machines you have you say four machines
it will ask you which one you want
Master you say this machine it will
download install do everything for you
so Cloud manager you have to install on
one machine and then you start it it'll
immediately ask you how many machines
you have you give the IP it'll detect
then ask which one is your name node
which one is the data no which one is so
you select this this this it will
download Hado distribute it install it
set it up and give you the cluster that
is how this cluster is created actually
no no there is no limitation you can
have thousands of machines but people
don't do it actually I mean if it is a
production setup they go for Enterprise
licensing uh but even the express
Edition is pretty stable and the express
Edition uh does not have some features
of the Enterprise Edition like uh you
have something called rolling restarts
and rolling upgrades for example let's
say you want to restart a machine in a
cluster now usually when you restart it
it'll have tons of problems because your
name will detect this is down so
sometimes in the Hadoop cluster you
might want to restart all the machines
you applied some patches or something
and sometimes you want to upgrade the
Hadoop version you are running 2.6 you
want to go to 2.7 upgrade will happen
but after that every machine should
restart so uh if I do this in a
production cluster is very difficult so
in the express Edition there is no way
every machine will go down in Ender
press Edition you have something called
rolling restart which means one by one
it will restart machines without
affecting the cluster end of the day
every machine could have restarted but
your services will not be affected and
you also have mirroring backups Advan
backup mirroring solutions that are all
available only in Enterprise this
addition will not have all the features
will be there like spark or whatever
tools you want everything will be there
commissioning you can Commission on the
Fly you can add two machines nothing
will happen two machines will get added
immediately they start become part of
your cluster it's called commissioning
uh I don't know if I can show you here I
will not be able to add them but you go
to this host menu and there is something
called Commission State it is commission
11 now that means there are 11 machines
running now I can't add because I don't
have any uh permission but there will be
an option called add a machine here
commission and so right now it says
commissioned 11 H so as of now
everything is commissioned I don't have
any admin rights otherwise we can just
say click add it will add on the flight
no problem and Spark requires some level
of understanding before we actually go
to the hands on side we can do the hands
on in spark for sure we'll be doing a
lot of hands on but if I directly start
with okay write a program you'll not
understand what we are doing right so
that is why some slides are there I'm
not really a slides person but still
some topics we have to understand only
you can go to spark
right um so first thing you need to
understand is what is spark and from
where we got spark right so why people
are so excited about spark so uh you
your previous session trainer right uh
what is his
name uh he was asking me that can I
learn spark I really want to learn spark
That's The Power of spark I'm not saying
mean even I want to learn machine
learning I'm not saying that way uh but
he want to leverage spark so he was
asking me some doubts like uh can I get
started with spark I want to do machine
learning on spark uh because everybody's
excited about spark so and all my
trainings these days are on spark most
of majority of trainings are on spark uh
so there is a lot of excitement about
this tool and why is it why spark is so
popular or is it some magic or something
what spark is doing right so uh it's
very easy to understand why it is so
popular uh somewhere in
2009 okay we had uh a project called amp
lab in the University of
Berkeley there is a research project
called amplab so this amplab project is
even live today it's a research project
in uh University of Berkeley so there
people were trying to create a new
cluster manager do you remember
Yan yeah so Yan is what your resource
manager in Hadoop and Hado version 2
came in
2012 what you're seeing today so Yan
came in
2012 right Yan came with Hadoop we are
talking about 2009 so in 2009 there is
no Yan we are we were running Hadoop one
old Hadoop version and there was no good
cluster managers so some folks at the
University of Berkeley they were trying
to create a cluster manager very similar
to yarn okay at that time there is no
yarn okay and they call this project as
mesos
mesos mesos is the project name they
have given for this project okay and
these guys were doing some R&D and at
last they created the cluster manager
like very similar to Yan mesos is there
even now it's an Apache project very
similar to yarn so why yarn became so
popular because yarn is coming by fall
with Hadoop if you download Hadoop you
will get yarn if I want to use use mesos
I have to install it separately but
Hadoop will run on top of mesos also
there the only difference will be that
your yarn layer will be handled by mesos
okay so mesos is a very famous project
even today some of the folks use it
actually not everybody use it and mesos
was created in 2009 in amplab so they
created mesos and they said that it's a
great cluster manager we're going to use
it blah blah blah right now in order to
test the power of
mesos okay first they ran some map ruce
programs of course map ruce was what we
had originally and it was running fine
so then they thought why don't we just
use uh a different programming framework
and they created something called spark
so spark was created actually to test
mesos meaning mesos is your cluster
manager and they created a new uh
programming framework called spark so
you can write a program using spark
it'll run on mesos and the only
difference was that spark was completely
inmemory execution like it'll use most
of your RAM so if you have a cluster
manager and if there is a lot of ram
getting used you can actually test right
whether the cluster manager is good or
bad so they created this spark uh
programming framework wherein if you
write a spark program it will use
majority of your RAM for faster
processing and they tested mesos and
mesos became a success everybody was
happy happy people never thought much
about spark they just created as a sub
project okay but after an year like 2010
or 11 people thought that hey this spark
is good because if you write a program
in spark first thing people noticed is
that it is running faster compared with
map reduce I'll tell you why it is
running faster so people thought that
why don't we develop spark a bit more if
people can write a spark program and if
it is running faster should be great
right and the development started and
somewhere in
2012 uh I believe spark became an Apache
project so they contributed to Apache
and said that you know we have created a
new product it's an make it open
source and this is a framework like map
reduce if you write a spark program
instead of a map reduce program it will
run faster than map reduce Apache
thought okay let's try that it became an
Apache top top level project okay from
there everything changed from 2012 so
actually spark is quite new because uh
it became widely popular somewhere
around 2015 only and I started using
Spark from 2016 only this is me the
world
right so I'm like good right because it
became popular somewhere 15 16 I started
using spark okay and even in 2016 there
were very less trainings in spark
because people were like excited but
nobody knew what is Park so we started
using in 2016 all of a sudden end of
like December 2016 it became the most
active Apache project in the history and
in 2018 17 18 now this is the uh biggest
contributed Apache project so in a span
of two three years it gained a lot of
popularity right and now spark is like I
said the most popular Apache project so
far so Apache has around 300 100 plus
projects in that the number one is spark
as of now right also originally there
was a spark version zero then we got a
spark version one now we are in spark
version
2.3 this is the latest version and many
companies run something called spark
1.6 it very stable a very stable version
that is a reason they run it probably I
don't know right after 1.6 you have 2.0
there is no
1.7 after 1.6 the next spark version is
Major release 2.0 okay and we are on 2.3
right now if you go to the spark website
it will say we are on 2.3 currently 2.3
is not pretty stable because it's a
latest version so not so stable uh but
2.1 2.2 they are very very stable
actually and we are learning spark two
we are not learning spark one you are
not missing anything because in spark
you have some improvements and all the
features of one are available in two so
that part is covered and uh in most of
the trainings they will request you
please start with Spar two uh I mean
don't start with spark one meaning it's
it's it's a it's not so old but people
want to start with spark 2 so the major
version right now is two or 2.3 I can
say that we are using and something else
happened so what actually made spark
more popular is that in 2012 these guys
the founders of spark there is two guys
they gave to Apache and said that take
spark and go but they were not so happy
so what these guys did
parall they found a company called Data
bricks data bricks so the founders of
spark they gave it to Apache because it
became open source obviously but they
also want to make money right so it is
open source you can't make money so if
if you give something to Apache it you
are actually contributing to the world
probably like for the good scenario if I
don't give it to Apache like Microsoft
Windows Microsoft Windows is not
available with Apache right so it's only
moneymaking and so you are not getting
any chance to modify anything or add
your contributions or anything if it is
not in Apache the only contributions for
uh commercial versions are done by the
company so the growth will be limited
very simple the growth will be limited
like you will have your own developers
who can modify and all but others will
not be allowed so when it went to Apache
now what happened is that Apache has a
spark version Apache spark that anybody
can download that is 100% free open
source again the spk is available with
clera hoton Works mapar sure that we
know so we are using a cloud data
cluster that has spark installed so that
is again same spark only we can use it
but these guys in 2012 found this data
bricks okay company and their sole
purpose is to sell spark nothing else
but this company is so special because
it is created by the founders of spark
the people who actually wrote the sour
code found this company so if I want
only spark I can probably go to these
guys because they are the best but
people don't go here people go to cloud
or Hoten BS because chances are very
rare you want only spark you want other
tool sour like scoop you want Flume you
want now data bricks has nothing to do
with scoop or Flume they only Spark
right now uh one of the participant
asked this question I'll get back to
that in a moment but still spark is an
independent project which means it does
not require
Hadoop but mostly you will see it on top
of
Hadoop okay spark is actually an
independent project which means I can
download spark I have spark running on
my Windows 10 laptop beautifully Works
doesn't require anything right NTFS F
system it will work okay so but people
don't do it because the point is um like
right now we have a migration happening
in GE so I was with GE last week before
coming here so there we have a migration
happen G is migrating to spark spark and
Hado right so G already has a Hadoop
cluster so why should they install spark
separately right so they are already
having a Hadoop cluster so they will
install spark on top of Hadoop as an
ecosystem tool so it will start
leveraging hdfs for store storage and
then processing and all so most of the
times you will see spark on top of
Hadoop but that does not mean it will
run only on top ofu you can run it
anywhere practically and I will show you
in the slides wherever you can run
however you can run so on and so forth
uh so data bricks is popular in that
case and it is available Al everywhere
right so I think we'll uh run some
slides to get you an idea and uh
disclaimer these slides are from data
bre so some of them will be their
marketing slides uh the best resource to
learn spark okay so a couple of things
which I know I can teach you right uh a
lot of people ask like uh give me the
best book to learn spark and they
suggest one book then they will never
come back to me so the best book to
learn spark is something called spark
the definitive guide don't buy it don't
read
it just make a
note spark the definitive guide this is
the best book don't even touch it
why I'm saying this okay you can
probably buy I don't mind this is
written by uh Z mahara and the the
founder of spark the guy who wrote The
Source Code wrote the book and the book
is pretty much like the source code you
won't understand anything even I don't
understand half of the book but very
good so buy it and keep it so people ask
and you can show this is a spark right
uh very difficult to understand but it
has like accurate information because
the guy who wrot spark is writing this
book right so if you read about rdd he
will go to any level to explain what it
is so it's a bit difficult to understand
probably don't read it immediately once
you complete the spark class and do some
asignment then you start with this book
you it'll get give you a very good idea
to read There are some other books
related to spark available I forgot the
name
but huh learning spark right that is
good learning spark is easy this is very
tough actually okay this is and this is
version
two these guys also wrote the same spark
definitely got for spark version one
which was very easy actually this is a
bit tough I don't know so this is one
place where you can second is the
documentation from Apache and datab
Bricks this is where I learn
actually so you open Apache
documentation you open datab bricks
documentation they will give you a lot
of ideas I mean Basics at least if you
want to learn right
uh okay so that's learning spark so book
you can
keep I think it is not available in
India did anybody try buying
it is there it was not there before it
is
there flip card spark the definitive
Amazon
also ah this is the mat zaharia and Bill
Chambers these are the people who wrote
spark okay okay um so it's good I mean
I'm not against this but try if you want
you can get a how much it is in flip
cart is costly
right PDF you can download ah so this is
paper Gap back
2018 actually right so latest uh Edition
it is
right um and basic operation and u a lot
of things are there Hadoop there are two
books actually maybe there are new books
better one is Hadoop the definitive
guide then um there is one more book
right for
Hadoop one is Hadoop the definitive
guide this is Park the definitive guide
you also have for Hadoop Hadoop the
definitive guide uh then there is one
more good book for Hadoop um Hadoop in
action I think that's also a good book
okay hadu books are bit more easy to
understand uh in the sense like this spk
book is more more oriented on the sour
code side like how exactly stuff work
hadu books are a bit more
easy H this is the slide which you need
to understand if you want to learn spark
right uh and this slide is very
important also if you really want to
learn spark why it is important so
somewhere in 2004 before Hadoop came
that's what the year there uh and with
Hadoop you got something called map
reduce you know what is map reduce I
don't have to teach right till 2015 I
can say you can see 2007 till 2015 what
happened was that one of the problems
with Hadoop was that you have map ruce
fine no problem but what map ruce does
is batch processing it is slow and it
does batch processing so people wanted
to try different different different
type of workloads in Hadoop so some of
them wanted to write SQL queries on top
of Hadoop that is where you got Hive uh
then we got Impala then we got Presto
now here you have drill uh and and many
tools so around 10 plus different SQL
tools are there which can explore the
data in Hado and some other people
wanted uh you know machine learning to
be done on top of hadu so if you in the
earlier versions before spark when
people want to do machine learning uh
they use something called mahoot a
mahoot h here you can see mahoot
somewhere here this is your uh machine
learning library but what is the problem
machine learning is what okay you guys
can teach me right machine learning is
all about
what you all
sitting fine it is about
iteration what do you mean by
iteration like you are taking the same
data and iterating over and over again
right one of the things which map reduce
cannot do is iteration because if you
write a map reduce program the mapper
will run producer will run it will push
the data to hard disk again the second
iteration again you have to read it so
when Maho was doing machine learning the
problem was that it was very very slow
because internally it uses map ruce on
top of Hadoop so mahud came but this
became your machine learning uh tool
okay and so Interactive we covered for
graph processing we got graph lag pral
so these are the tools which you use for
graph processing on Hado traditional
Hado no spark or nothing then people
wanted to do a streaming data analysis
realtime data that is where you got
storm right so you see the problem right
over the period of 8 to 10 years people
started developing different different
tools to handle different different
workload so if if today I go to Hadoop
system and if I'm very new to Hadoop and
if I ask somebody what tool I should
learn then he will be confused because
all these things you have to learn Pig
Hive Maho everything to start working
with Hadoop another problem is if let's
say one fine day you start learning
storm storm is realtime processing you
have to first learn Storm's API Storm's
language then you have to install it on
top of hadu you should learn the
integration then only it starts working
so learning all these different tools
actually became a problem okay and what
happened in 2014 when spark came the
biggest USB of spark is that it is a
unified processing engine what these uh
50 plus tools on Hadoop are doing spark
alone can do that is the real uh
advantage of spark now many people
confus here because if you go to anybody
and ask like why somebody should learn
spark they will say because it is faster
now speed is a byproduct of spark it is
faster I know but the real reason why
people are migrating to spark is that
all these tools whatever they are doing
spark alone can do so you don't have to
learn like hundreds of tools you learn
one tool one language and you can do
everything but you need to have some
basic idea about what you're going to do
for example if you want to do machine
learning spark has something called MLB
machine learning library that will allow
you to do machine learning but you
should know what is machine learning
then only you can do otherwise you
cannot explore it but I don't have to
install 10 tools or 20 tools spark alone
can do it so the primary reason why
people are going for spark is this
General unified engine in my experience
the second reason is speed obviously it
is faster I will tell you why it is
faster how it is faster and all the
third reason is ease of programming like
you know python now so you can get
started with python and Spark the
language is Python and if you want to
write SQL queries or streaming or
machine learning for everything you can
use python in spark that is not the case
in the traditional World Pig uses a
different language storm uses a
different language so every tool here
actually use a different
language even for graph processing in
spark you can use Python normal python
so ease of programming is another thing
because I have struggled a lot because I
have worked couple of these tools I have
worked as part of the project and then
the major problem is that you will take
some three four months to learn the tool
first then when you come back that tool
will be obsolete they say go and learn
some other tool so with spark that Gap
is gone so that is why every company
want to migrate to spark now he is
asking um in in spark so there is
something called streaming we will come
to that streaming data means you want to
process real time data say I gave you an
example right so let's say uh credit
card fraud detection I swipe my credit
card and that has to be immediately
captured by some system and it has to
say whether it is fraud or not this is
real time right now to implement this
one solution I have is spark streaming
in spark I can't do it okay but spark
does something called micro batching
meaning it is not able to capture a
single swipe as it is it will collect
one second worth data
so I cannot go be below one second one
second worth data but if I'm using
something like storm it can even pick
one swipe in that microc second I say
there is a swipe but I'm saying that is
not so important like one second worth
delay you can suffer at least still you
can manage with spark most of the cases
but yes spark supports streaming so in
this and this pral giraffe these guys
are gone the graph processing is totally
gone we have sparkk for that most of the
Maho is gone I told you right T
T is there uh soon will be gone almost
soon will be gone because stas is middle
of map produce and Spark what T does it
makes your map reduce faster that is
what T does basically so spark makes it
more faster so it is in the middle so
almost it'll be gone in couple of years
hoton works is the company who's
promoting T so probably they will keep
it because they want market share right
that is a reason okay let's see what is
spark right so in spark you have
scheduling monitoring and distributing I
will show you this
practically and I like this uh diagram a
lot because this will show you the spark
architecture in a very nice way actually
so how does it look to you like if you
look at this diagram what do you think
it
is everybody saying it is solar system
so the point is at the core you have
spark so when you download spark you get
something called Spark spark core okay
and that is the lowest layer of
abstraction in spark where you can start
programming directly and I will show you
what to do there and you see there is a
ram and hard disk picture which came up
which means spark can use both RAM and
hard disk and I'll tell you why this is
very important later usually other
systems can also use and these are the
four languages currently supported in
spark code for programming so this is
Scala this this icon is Scala this is
python R and
Java as of now in 230 only four
languages are supported so you should
either learn any of this and you already
know python so it makes sense you can
continue with
python I teach spark with Scala that is
also a very uh the source code of spark
is actually written in
Scala that is the reason some of the
folks want to learn Scala and do it but
now uh so earlier in spark 1.6 version
and all if you write a python code it
was
slower there is one thing you have to
understand if you are in spark 1.6 and
if you write your spark code in Python
it will be slow and Scala code will be
twice
faster it was like that because they
didn't do any
optimization Java also slow slower than
uh Scala fastest one was Scala at that
point in time
correct correct still it will be a bit
slow because they didn't implement the
certain libraries that was related to
Java to support the uh you know
optimizations in Java so this is only in
1.6 and 1.6 is very rare in Spar two
what they have written they have written
an abstraction layer so that you write
the code in any of these languages your
program will run at the same speed so it
doesn't matter you're writing python or
Scala or Java everything will be same so
as of now we don't have to worry about
it SQL is also there but SQL is not a
language as such I mean you can also
write queries and all using SQL okay go
for one more
orbit yeah so these are the so you will
be thinking that if spark is able to do
everything right how it is able to do it
so these are the libraries available in
spark meaning when you download spark
you get something called spark fine and
that is like map reduce you can write
but maybe I'm not a core programmer I
know only SQL I want to write SQL you
use something called spark SQL in spark
SQL very much like Hive and all you can
create tables and there's something
called Data frame you can create them
and query the data and the good news is
they integrated spark SQL with hive by
default so what will happen in a Hadoop
cluster if you install spark okay and if
I open spark SQL I can read the data
from my hive table so previously when
you are running queries in Hive it is
very very slow now the same tables you
can read in Spar SQL and then query so
these days Hive is just used as a
storage most of the processing will be
done by your spark SQL because it is
faster ideally right uh another library
is ml this is what I was saying your
machine learning libraries so so
whatever you are learning you can
Implement here I don't think deep
learning and all are supported because
for deep learning and all you will be
using tensor flow do you have deep
learning in
syllabus yeah huh now by default it not
come I'm saying you have to download and
import it separately H third party but
your regression and all like normal
stuff right they are all supported by
default so what algorithms you learned
so
far which one only regression
so no na buys and decision trees and
blah blah blah okay H so they are all
coming in ml anyway and graphic is the
framework where you can represent your
data using a graph have you ever done
graph processing no right I have done
okay because why this is
important if you have studied uh
engineering computer science you could
have learned graph
Theory right I mean I have learned I
don't know was there in your syllabus
there is something called graph Theory
so there you have some stuff like you
have edges and relations you know you to
Define like this
right so this will be
me this will be you this will be U one
and and then you write relations between
them like I like you probably you don't
like me so you like someone else like
this so I mean you have property graphs
where you have an edges okay which
represents some property and then you
have relations between them okay and
then you can query the structure so for
some type of uh we partially work with a
social media company in one of our
projects not like Facebook and all they
were a startup actually there their
entire data was like this even Facebook
right if you download the data from
Facebook it's very difficult to do it
okay you can't directly do it but there
is a way to get the data data means all
the publicly available data they will
give you in the graph API format the
format in which the data comes is graph
it doesn't come in Json or XML it's
graph API format because Facebook is
actually using this graph structure to
store not The Spar graph I think they
have their own some system but the
representation is in the form of a graph
actually so many times this is required
this graph representation of data and
Spar can do this Graphics Library which
can represent your data and then stream
streaming data streaming data is where I
was saying you have real time data
coming in and you want to process it
real time and then make a decision based
on that yeah so some of the queries you
know uh if you write like this it'll be
much more efficient the multi-
relational queries say for example this
guy like this guy this gu so if you want
to find out patterns and relations
representation using graph is easy in
some cases let me give you another
example this airport data we had if
you're having Airport ports and flight
data right so let's say I represent
airports like this
Bangalore Chennai I don't know USA
somewhere right and if I represent all
the flights which are going in you
know I can do this using SQL also but
the graph queries will be much much more
faster for me because this directory
structure right the graph structure is
able it is easy to Traverse in the graph
API so probably between two airports you
have like thousands of flights currently
flying so you want to query them and
track something so always for airport
data use graph type of a system rather
than SQL will work but it will be slower
if the data is really high even spark
SQL will be slower so this airport
management right how many flights are
flying in real time and so on and so
forth they use this graph API a lot so
only for certain use cases where you are
having let's say two edges and lot of
relations between them social media is
another example right so I want to find
out what you like in Facebook let's say
so I want I have to do a lot of
comparison how many friends you have and
what your friends actually like do you
actually like that so traversing these
queries in SQL is very difficult in
graph I can easily Traverse I can just
say that these these these notes find me
these type of relation easily tell me so
social media companies do use a lot of
Graphics actually even Twitter use
Twitter how many people are you
following and how many were followed by
you so this relations right these are
all learned from graph apis in
Twitter we will not go to graph so and
and there is a no SQL this is different
okay there is a nosql database called
neo4j
neo4j it's a nosql database they store
data in graph format so I mean that is a
different use case that is for Real Time
queries and all but this company
actually uses only graph to store the
data okay Graphics is different that
comes part of spark NE 4J is a nosql
database I had worked in a small project
where they were using
neo4j I didn't learn it actually but
I've seen that people are actually using
it okay what do you most some of you
might understand but some of you may not
understand so we went one uh Circle
further right so we are in one more
orbit okay now let me ask you what do
you make out of
this who is
this no is there anything special about
this picture fine I mean
uh uh so what is this
this Flamingo okay what is the flamingo
doing huh waiting for who be be creative
right so what is a flamingo doing
standing on one huh yeah you very
creative exactly this is called
Standalone mode of
spark I didn't create this slide it is
Data slide so if you have to blame blame
them exactly Flamingo is standing in one
leg right so you have to be more
creative okay for learning spot so these
are different modes in which you can run
spark basically okay so don't get
confused with the name so the top one
this single PC that is called
local local mode means you are running
it locally on one machine okay you
cannot expand to more than one machine
and you use only for development and
testing purpose and that's
understood this mod is mesos M this is
icon of mesos so originally spark was
created on top of mesos I told you right
so still if you want you can install a
mesos cluster right and then install
spark on top of that it will run no
problem right and this is stand
Standalone mode so what is Standalone
mode I want to install spark in a
cluster but I don't have yarn I don't
have mesos I don't have anything then
spark give you its own cluster manager
that's called
Standalone but this is very rare because
ideally it'll be on yarn and this is
yarn spark on yarn this is yarn so
ideally in most cases what you see is
yarn this blue ball I mean that is yarn
uh because most of the organizations
will already have a Hadoop cluster and
yarn and it just makes sense to
integrate spark on top of that another
very important Point lot of people get
confused that is why I'm writing
it spark
has no
storage there is nothing called storage
in spark because it is an execution
engine like map reduce is map reduce
having any storage no it reads the data
from Hado process it then probably store
it back in Hado right similar to that
spark also has no storage there is no
storage component in spark it is just an
execution engine which means you should
provide the data somewhere so if you're
running spark on Hadoop the data is on
hdfs if you're running spark on let's
say your PC your PC file system has the
data Standalone or mesos whichever up to
you to decide okay if you're interested
some extra knowledge spark can also run
on
kubernetes
kubernetes spark on kubernetes is I
think production ready now you know what
is kubernetes so okay for those who
don't know probably these things might
be useful in future because kubernetes
are actually running somewhere so you
have something called Docker right there
is something called Docker this is a
very old tool very old like four or five
years I think I don't know what Docker
allows you to do you know what is a
virtual machine right you know what is a
VM so in your laptop if you create a VM
what will happen let's say I install a
Windows VM the problem is that this VM
will use a lot of resources right
operating system you to install
everything so if my laptop has 8 GB Ram
I need to give 4GB Ram only for the VM
what Docker does it allows you to create
something called a container
okay meaning on one laptop I can run
like 10 Docker containers one Linux One
windows it will use the libraries that
are already available on my base
operating system so it will not install
a full operating system that's what I'm
saying so instead of giving 4 GB RAM to
a VM I can give 1 GB to a Docker or 512
MB to a Docker and if it is on Linux it
is much much good performance so Docker
became a big hit because if you are a
programmer you wrote a Java program you
want to test it you want to test it on
Apple you want to test it on Linux
Windows how do you do all of this right
so you can just launch Docker containers
on your PC itself like 10 of them run
your code see whether it is working very
easy right kubernetes is the next level
of Docker where you have something
called container orchestration in a data
center you are able to run multiple
containers like Docker containers you
can manage all of them using
kuber if I'm correct it is is a Google
project okay so it is a data center
orchestration so Docker is like you are
running it on one machine probably 10
machines match so here once you install
ceretti it become a data center manager
actually so you can give like hundreds
and thousands of servers to kuber and
say that I want these many containers
Docker like containers it will launch
them and manage them for you so kuber
can become one of this instead of Y or
mesos or this thing kuber can become
your resource manager I can install
spark and then I can say that I want to
run spark program if I say hit Ender
it'll go to kuber it will launch
containers for me to run the spark
code Docker
containers yeah yeah cloud cloud
everything is uh available on cloud
cloud everything supports so in Cloud
you have multiple options either you can
directly run say I go to Amazon I create
some machines myself or I can use these
services like
EMR
EMR elastic map reduce I can go to
Amazon and say that hey Amazon I I need
you to create a h Spar cluster for me
say 10 machines 100 machines in 5
minutes It'll create and give it to me
and then I can run all my workload but
these are like disposable clusters once
you complete your job you have to delete
them otherwise you're paying money
continuously right
so uh so U kubernetes is on the cloud
also I mean it is local data center as
well as Cloud so it is available
everywhere I don't know I have not
extensively gone to kubernetes but uh I
think you can search because uh it is
something which is coming up in a big
way I think Google is
the Ku kubernetes kubernetes whatever
you call
so this is production grade container
orchestration so the problem with your
Docker and all is that it is very
difficult to manage the containers with
kuet you can manage them
like ah it is a cloud only I don't know
the exact architecture of kuber like
whether they launch their own container
or Docker container but basically so it
is all coming to an abstraction now
previously everything was very clear for
example you buy a
server and you you can see the hard disk
right then you install the operating
system now what kubernetes says that
give me a data center you don't worry
data center group of servers give me a
group of servers like a data Senter and
I will launch as many resources as you
want you just tell me how many
containers you want I will ensure from
where I need to launch how I need to
launch so like ah same like that so like
an abstraction it is getting right so
but uh probably so it say ah kubernetes
built up 15 years of EXP experience of
running production workloads at Google
yeah so originally this came from Google
Google was already running this not in
this name some other name so they now
even Hadoop came from Google right so
same like that they are creating this
kuber
netti I think spark on kuber is
available that is why I was speaking
about
this spark on
kubernetes yeah very much available
Spark 2.0 you must have a running kuber
cluster blah blah blah ah do ah it is
Docker only okay Docker images see so
internally it is using Docker only
nothing
else blah blah
blah yeah so it is supported so it is
not in the slide that's why I'm saying
in the slide you don't see kuber slide
is a bit
older and if you pay attention to the
slide I can show you one small change
you have to tell me what is a change
okay if you pay attention
did you see there is some change in the
slide what is a
change no no somebody's on top of this
right that is
Zookeeper zookeeper oh you didn't say
I'll show you once more see it's gone
now it'll come up you saw that so what
it technically means is that all these
three modes whether you are running on
Standalone or mesos or yarn can be
highly available using zookeeper
zookeeper supports your high
availability if you like spark if you're
running if one machine crashes or or uh
you know if all these states are
maintained by zookeeper so zookeeper is
integrated with
spark zookeeper is another Hadoop
ecosystem tool why it is used is for
communication between the machines
actually to put it very simply like in a
very large cluster if one machine goes
down then how do you know
no it's not like a lot balancer um how
can I
explain so I'll give you a simple
example otherwise zookeeper might become
another big problem
right and zookeeper is an admin thing
mostly so you don't have to worry about
it too much but
still do you remember I told you you can
have a active name not a standby name
node right so in a Hadoop
cluster you have two name notes
right this is
active this
standby correct now what is the idea
only only one should be working only
active if the active
crashes this question who will tell this
that's a question let's say I'm
connecting to the cluster how do I know
who is active I can't connect to two
machine right so this guy will have an
IP address this guy will have an IP
address one way that I can keep on
pinging who is active
currently how do I know who is
active who will tell me right there is
nobody to tell so one way is that I can
keep on pinging both the machines to see
who is alive who is not alive so like
that there are many situations in which
you have to understand which machine is
alive or not alive so with zookeeper
what happens is very
simple it is a service
so this active name node will register
with Zookeeper okay standby will also
register with zookeeper now you just ask
zookeper who is active it will tell this
is active if this guy crashes okay this
guy will inform Zer that I'm the new
active you again Ask zookeeper zookeeper
will tell you so it's a coordination
mechanism zookeeper is used for
coordination and high availability what
this slide means is that spark support
zookeeper so for talking between the
machines and management and all it can
use zookeeper if possible if available
right so this this is the icon of
Zookeeper it it didn't just come this is
the actual icon of
Zookeeper you are asking uh I know
because normally people are not much
aware of
Zookeeper no no it is it is for um all
the services actually let me show
you so zookeeper is for any service to
coordinate so one example I gave you is
that how do you know name no is working
ideally this will not automatically come
up the Zookeeper has to inform this guy
to come up if you look at the
architecture okay if you implement High
availability in Hado normally what
happens if I don't have zeper if this
guy goes down I have to write a command
to make this up that is actually a
wastage of time right it should
automatically come up so these guys will
be connected with zookeeper and there'll
be a keep alive message sent so if ah
heartbeat so if this guy goes down
zookeeper will know that this guy is
gone after let's say a second or two
there is a timer you can configure it
will ask this guy come up okay and
update the metadata here so basically
this guy holds metadata who is doing
what in the cluster otherwise you see
there are lot of services in a hard
cluster where there is active and
standby not only name node resource
manager your yarn resource manager is an
active and standby if you take hbase
there is a service called now SQL hbas
has a master and standby so if I go to a
Hado cluster if I start asking everybody
who is the master who is the slave it's
very difficult so I go to zuker and this
guy will have all this knowledge even
for for spark you have a active and
standby Master okay if you're installing
spark independently and that can be
coordinated with zookeeper that is what
the slide is
saying so zookeeper by default will be
available in most of the
Clusters we again went
back now now what so this orbit is
over spark can read from almost any file
system and that's by
hdfs uh I don't know what is this Cube
by the
way this I don't know this is S3 Amazon
this is local file system that is some
other file system maybe any file system
basically uh so it's supports a lot of
file systems even which we are not aware
of right local file systems
it can read from all these things any
nosql database any rdbms that is I think
a very cool feature because I will show
you if you have a MySQL DB Spar can
directly read from the table or if you
have mongod DB or any nosql database it
can read from the table and process the
data output probably it can store back
it into uh that table
right it also supports Hadoop input
formats
spark streaming can work with fluman
Kafka meaning you are doing realtime
streaming right so the question is that
how do you get the
data so this is always a challenge
because what happens is that normally
you will have a spark cluster
running so let's say this is a spark
cluster Hado cluster only where you have
spark
installed so let's say you have a spark
cluster running I want to get Twitter
data so actually spark can directly get
the Twitter data there is no problem it
can directly come to the cluster and you
can process it but the problem is if one
of the machine who is receiving the this
data crashes you will lose the data for
some
time right in
spark so what you can do in this
architecture is that for high
availability you can either use Flume or
Kafka so you can ask these guys to get
the data for
you Flume or
so Flume is pointto point delivery of
data like I can configure a flume agent
we will see in the webinar which can get
the data from Twitter to here Kafka is a
message CU so you can get the data from
here so even if your machines are not
working data will come here right you
are not losing the data so for that
reliability you are using a flume and
Kafka if you have a spark cluster right
so this is a spark cluster and I'm
talking only about spark streaming okay
not like normal processing when you
configure spark streaming one of the
machine one of the machine will start
working as something called a
receiver okay so what is this machine's
job to get the data that's all it cares
and it's a normal data node now the
drawback of spark streaming not the
drawback I can say if this machine
crashes if this machine crashes okay
your stream will be lost
of course it will switch to another
machine but it may take some time so
that let's say 5 seconds or 10 seconds
you losing the data right you're not
storing it anywhere correct so if I want
to avoid this if this machine goes down
then my stream will be gone if I want to
avoid this what I can do I can say Kafka
hey get the data in Kafka okay from
there I'll get to spark even if this
machine crashes another machine will
come up the data will be available here
so spark streaming if you want
reliability you have to use Kafka or
Flume direct stream you may not have
reliability okay
so if we are covering spark streaming
probably in that architecture side I
will speak the distributions we already
know and these are some statistics and
uh I think you should be aware of it
developers from 500 plus companies you
have 3,000 developers and etc etc
right uh this is a slide some of you are
asking will some of the tools go extinct
yes your hive now nobody's running a lot
of Hive queries everything is spark SQL
queries these days and Maho for machine
learning everybody has migrated to spark
M storm most of them are in spark
streaming now uh and this slide just
compares the difference ways in you can
run spark for example instead of map
reduce now most of us are using spark
the the uh resource manager can be Yan
or
mesos doesn't matter any of this and the
storage level in Hadoop is hdfs this
tachon is not there now tachon was a
storage manager like in 200 N long back
when when originally mesos came the
storage was handled by a system called
ton now this is renamed as alusio aluk
or alusio this project is still there
there is a storage layer like hdfs only
distributed storage you can get so one
common problem in spark is that you will
try to bring the data from different
places like spark will be running on
Hadoop but maybe some of your data is in
Hadoop some of them is in rdbms some of
them is in Cassandra so if you use
normal hdfs it's fine if you're using
this tachon or now it is known as alusio
it has a caching layer so it can speed
up your processing by caching the data
that is only Advantage not extensively
used because setting it up and all are a
mess actually okay I have seen it once
that time it was called ton only not
alusio I think they renamed it if my if
my memory serves me well ton is alusio
now spelling I have to look
into alusio alio or alio whatever you
call I don't know I don't
know uh alio formerly T
you open source memory speed virtual
distributed
storage I don't know how many
Technologies are there so many are there
actually this is like so let's say you
have tons of data and you need a storage
layer in between caching layer to make
it faster then you use alio that is only
use case otherwise everybody is using
hdfs only nor normal hdfs
in SSD SSD caching it will do so it is
costly or it is costly but faster but
only use case is that if you're
processing a huge amount of data like uh
we had like Bank of America Bank of
America they had like tons and tons of
data to process like terabytes of data
and it keeps on coming so storing them
on hdfs first time read will be very
slow anyway will be very slow so they
push directly to your this thing alusio
because that's an SSD caching layer from
there they read it so read is faster
actually that is only use case you are
seeing for aluo and this is another use
case of it right you have on premise
storage and cloud storage and you're
getting the data it comes to an alosio
layer from there you can start your
Computing so it is a storage layer
abstraction you can store data in
multiple places okay and alio will get
it in one place from there we can start
processing in so it is like bringing the
data into a caching layer and then
processing it
okay don't worry if you don't know alio
also it's perfectly fine it's not like
an mandatory component or
something okay now can you tell me what
is a drawback of map reduce if there is
any drawback what do you think so spark
is replacing map reduce right so what is
a drawback of map ruce that you
s yeah so mapper output you purchase
then output you persist then of course
reducer output you have to finally
persist right so that is why map redu is
very very slow so that is what is in the
slide actually written so and if I have
to do an iterative processing right I
read the data and 10 times I have to
process it then I have to schedule the
jobs 10 map ruce jobs I have to run
because it is very difficult to change
them together intermediate read and
write always will be there right uh that
is where Spark
becomes very different
because and this slide just says that
you can use Uzi to schedule all these
jobs nothing else spark does something
called inmemory
processing and this is very confusing
for many people the first thing you need
to understand is that inmemory
processing means it will use Ram if
available doesn't mean that you always
need to give Ram if Ram is provided so
so let's say you want to process 10gb
file and if your cluster has 10gb free
Ram it will read it into the RAM and it
will do all the calculation only the
final output it will push it into your
hard disk intermediate results are not
stored onto hard disk second point is
that it will if what if Ram is not
available then it will start using hard
disk also step by step it'll read it'll
read whatever data that fits into the
RAM process it then again it'll read it
and process it like that it has to go
there is no other way right if you don't
have Ram still it is faster than M
produce okay because from ground up they
have designed the code of uh spark they
have not modified map ruce code or
something to create spark uh and that is
why here it says 10 to 100 times faster
than map reduce typical map reduce
right so in memory processing we will
see uh how it is faster and all
ah that end is Cassandra that I that is
Cassandra it just demonstrate that you
can store the result in Cassandra from
anywhere you can read and store
also uh these are the Distributors and
applications which uses Spark so of
course data brick is the major
distributor and hoton works Cloud era
all these guys are having uh you know
spark as of now and these all
applications can use spark for
processing so mostly bi tools and all
right visualization tools so previously
if I'm using something like uh where is
it uh pentah okay to visualize my data
what it will do it'll fire a query to my
had cluster hi will run the query and
visualize it and now spark SQL will run
the query so it's much much faster so
all the bi tools and ETL tools now use
spark for moving the data and processing
the data and so on and so forth uh this
slide is a bit old this is the 100 tbte
uh sort competition in
2014 so every year there is a sorting
competition that will happen even you
can participate if you want so the uh
the thing is that they will give you one
terabyte of data okay uh you have to
sort it okay and the data they will
already give you the format and all
based on how you have to sort it is it
integer sorting or quick sort or what
sort they will tell you they already
give you the data also sample data and
you to write an algorithm to sort can be
a Java program any program you can write
and whoever sorts the data fastest will
win that is the uh conclusion and in
2014 when this ran uh spark did it in uh
23 minutes Hadoop in s map reduce in 72
minutes but look at the cluster size
spark was running on 206 machines map
rce was in 2,100 machines that is a
difference 1 by 10th of machine still it
is faster because lot of ram was
available everything was in memory so
fasting faster sorting will happen and
even for one petabyte they did a sorting
and again spark became the winner there
okay so you are having these notebooks
you know what's a notebook
right ah in Python you use right so you
know what it is luckily I don't have to
teach that so I mean the code is
actually written in a notebook okay uh
but we will use the shell also bit
because I don't like notebooks
personally
okay do you know what is a driver these
are some things you require otherwise
you cannot understand spark that's
why what is a driver you know not device
driver
okay which drives the program that's
actually correct right driver is the guy
which so when you're writing a spark
program the program will have something
called a driver okay and this is the
master of the
program driver is the master of the
program and then you have something
called
executor this is the
slave so let's say I wrote a spark
program in the spark program definitely
there will be a driver and an Executor
without that the spark program cannot
run I mean these are logical Concepts
I'm saying okay now when you want to run
that program okay one way you can run is
that you can say run it
locally I can say that hey I wrote a
spark program okay run the program local
when I say local what will happen is
that a jvm will be
created like a jvm okay both my driver
executor everything will be inside
this so this is the local mode of spark
Java virtual
machine
H still a container will be created
similar to this I'm jvm in a general I'm
saying okay so a container will be
allocated so this is like a yarn
container H so a container will be
allocated in that both your driver and
executor will run so the local mode is
not very efficient because you will get
only one container and everything is
running inside that right so if you're
submitting the Spar code actually when
you say you you have a python code right
it will read your logic and it convert
and run it inside a j end of the day
everything is inside a jvm only without
that it cannot run even map reduce
programs we wrote in Java right we can
write python code in map ruce so if I'm
writing a python map ruce program if I
run I will say uh submit the program
then I have to mention some jar files
and those jar files will be in Hadoop
they will read your python code and
convert that into a format which will
run inside a jvm and execute inside a
jvm only because end of the day it is
written in Java or if it is spark it is
WR Scala right so everything has to be
inside a jvu so the local mode the uh
problem is you get only one container
inside that the driver and executor
everything will run okay so this is only
good for testing purpose right right so
you just want to test a spark program or
you want to learn spark you will
normally say that hey Spark Run in the
local mode I write some code it just
runs if you are in a cluster this is
where things becomes
more interesting and this is a bit
confusing also okay so this is my Hadoop
cluster
right and I have four data nodes
imagine I
have four data nodes imagine I created a
spark program now the first question is
that when you running the spark program
what are you analyzing okay I'm
analyzing a file what is the size of the
file right so let's imagine the file is
in Hadoop okay just a simple use case uh
the file is here here and here just an
example and somewhere the total size of
the
file is let's say
10gb so I have a 10gb file in Hado that
that may be in blocks and all so that's
understood and I want to process it
using spark now and that is in a cluster
right so when I submit my program to a
cluster I have to ask Yann for
executors how many executors you want
and what is their
capacity meaning I can either say he hey
y give me one executor huh and in that
executor 20 GB Ram I want no problem
what will happen an Executor will come
here this is your
executor it's a container nothing but a
container and this has let's say 20 GB
Ram okay and some four processor core or
something and then your entire Spar code
will get executed inside this but
normally people will not do this this is
like one machine is executing everything
so possibly what I will do is that I
will ask Yan hey Yan give me let's say
four executors just an example I'm
saying give me four executors so Yan
will give me four
executors right and each executor I tell
y give me 5gb each ex this is 5gb this
is 5gb this is 5gb this
5gb ram ram in memory so total 20 but
5gb right and so imagine there is and
then there is a driver right so this is
Executor imagine you have five nodes in
this Hado cluster so here my driver
driver is
running okay so what will happen when
you submit the program in one of the
machine the driver will start
running and in the program you have
asked Yan give me four executors and
each executor I want 5gb Ram two
processor core blah blah blah so Yan
will launch 1 2 3 4 here and whatever
code you have written this driver will
push into all four machines at the same
time and each machine will process your
code this is how in a cluster spark is
processing but this is not so easy as it
looks like because you should know how
much memory you need for processing
right and how many executors you want
will ra will Yan actually give you those
I cannot say that give me 100 executors
each with 100 GB Ram Yan will say that I
don't have capacity I can't give you so
normally wherever I have went for
Consulting and all uh if you write a
spark program you will discuss with your
admin okay I want to write a spark
program because in spar Mark if you want
to get the full performance it should be
in memory you need maximum RAM and that
is a challenge in every Hado cluster so
you will go to your Hadoop admin or
spark admin and say that I have written
a program I need 100 GB Ram in the
cluster so he will tell you in your
program you ask for let's say 10
executor 10 GB each something like that
right that you will configure as the
number of executor how much mam you want
and then you submit your program so Yann
will launch these things and the driver
will be running on a separate machine
right and whatever logic you have
written so driver will read and start
pushing to all executors and the output
normally comes back to the driver
whatever output you have in the driver
you can write a logic either store the
output on Hadoop or store it in
Cassandra or wherever you want that is
possible and this driver normally will
be your appm this is your app master
Master remember App Master from
Yan that is your driver so a lot of
people ask what if this machine goes
down my driver machine goes down
obviously if your driver machine goes
down then the processing will be
disturbed now spark program will not
crash okay uh in Yan you can configure
application Master restart timer so
restart so if this machine goes down it
will restart your am on another machine
application Master on another machine
so that it can continue processing this
application Master is in constant touch
with your resource manager ah so there
is an ID application ID where it will
not tell what you have processed uh or
what execution is going on currently it
knows those things so your resource
manager will launch one more application
master that will become your spark
driver get the code it starts running
from
there so the so the driver is the master
part of your program and these execute
uors are where you push your code so
depending on your cluster you can ask
for the number of executors and how much
memory you want for each executor that
is how it actually runs and there is
something called application Master
restart timer you have to configure in
Yann there you can say three five so
it'll try to restart it the application
Master on the same machine sometimes
what happens this machine will not crash
your application Master will crash the
driver process will crash so it'll
restart it on the same machine can be
due to many reason maybe the resource
not available so you can configure the
timer and if it doesn't work then it go
to another machine and say start from
there uh you need to understand
something called R
DD okay so that is why rdd fundamentals
it is written so rdds are the basic
building blocks of spark so first thing
you need to understand is that in spark
your data is represented as something
called
rdd right any data that you have if you
want to process in spark the first step
you do is that you create something
called an rdd rdd stands for resent
distributed data set it is like a
variable or a pointer you can say right
so these slides will give you some idea
about rdd I will also practically show
what is an rdd so just uh look at this
picture this picture is very uh very
good in understanding uh how spark is
working right so I have four blocks of
data in Hadoop so that is what is
represented there hdfs right so you know
four blocks of data is there right and I
want to process them now that is one
file even though it is divided into four
blocks it is one single file I want to
process and what is this data it's some
text Data imagine each block has like
this error then a Tim stamp and a
message again warning some time stamp
and a message some log file imagine
right four blocks of data so we are
assuming they are on four data nodes and
you want to process them now from here
onwards it is going to confuse you okay
so I'm saying in advance you will get
confused okay but don't worry so where
is your original data in hard disk right
as blocks in hdfs so that you have to
keep it in your mind now I have already
written a program imagine to process the
data right in the program the first step
I need to do is to create an
rdd okay so I have to say that create an
rdd an rdd is like a variable you can
say or a pointer so here the name of my
rdd is called log lines
rdd you can call call it Ragu if you
want I just say create he spark create
something called log Lines rdd by
reading the data from this file you can
give the location where is the file and
if I run this what is going to happen
and imagine in this case I have asked
for four
executors in my spark program I said I
want four executors for some reason so
what will happen in each data node one
one executor will be launched in ideal
situation
right and I have one one block on each
data node and I say hey Spark create an
rdd called log lines rdd for me and if I
hit enter all this data will be copied
into the Ram or the main memory because
rdds represent your data in
memory assuming Ram is available ideal
condition so if I redraw this picture
this is actually uh data bricks picture
right but if I redraw this picture now
don't think that always the file is in
Hadoop right in in my typical example I
have the file in Hadoop but there is one
small thing here let's say you
have uh six Nots in Hado cluster 1 2
3
4 5 6 that is the same architecture and
block one 2 3
4 now the big question
you want four executors which machines
will launch the executor who is
launching the
executor yan yan has no data locality
awareness right Yan doesn't know where
is your data right in map reduce data
locality is there because your blocks
are residing here let's say your blocks
are here right so these are the four
blocks I write a map ruce program what
is going to happen in my map ruce
framework it is written that okay I need
to have four
mappers one more very important point
you need to remember in map reduce if
I'm processing the same data four jvms
will get
launched correct one here one here one
here one here and this will copy here
this will copy here this will copy here
this will copy here that is how your
mapper runs because can you process four
blocks using one
mapper not possible you will have number
of mappers equal to number of blocks or
number of input splits we say right
ideally you can say number of mapers
equal to one but then what is going to
happen this block will process this will
go then next block will come and it will
process all no no that's not possible
because the size of this mapper this
container is like 1 GB or or the default
yarn container size is there this is
your yarn container in map reduce so
when yarn is launching containers for
your maper
ideally how it launches is that it will
launch one one container for each block
right and your block size is fixed 128
MB right so in yarn settings you will
say that my container size is 1 GB I
want 1 GB container right and what will
happen 1 GB 1GB 1GB 1GB four containers
will be used it is very rare that
manually you mention number of mappers
you can't ideally do that because that
will affect your performance and also if
you're launching one container only you
can't resize it so this is like a 1GB
container so there is a property called
set Spar do Dynamic execution enable or
disabled that is to Yan so you are
telling Yan I want eight executors but
don't kill everybody I may use it or may
not use it so it'll keep it for you if
you uh uh enable it that is where it
will kill you are saying that if it is
not getting used just kill them I don't
want them so if your data size is small
it'll kill them but coming back to rdds
and our discussion I'm saying that in
that example you have four blocks of
data so that is these four blocks my
Hardo cluster is let's say six KN and
you want to read this data and process
it so what you say you say I want to
create an rdd the name of the rdd is
called log lines rdd or whatever rdd is
just like a pointer okay and when you do
that when that code runs what is going
to happen is that you ask for four
executors also and let's assume the
executors are launched here so there is
an executor here okay there is one more
guy here one more guy here and one more
guy here so these blocks will be copied
to here so now where is your data your
data is residing inside the ram of four
machines or four executors this is
called an rdd because otherwise how do
you call it is your data you need some
way to mention like like a variable so
that is called your RTD so now once the
data is available in the ram let's say
now the data is there there you you have
a representation to your data so this
entire log file is now called log lines
rdd also in spark there is something
called
partitions meaning right now your data
is lying as four blocks so we say that
this is a four partition rdd so normally
when you're reading from Hado you will
have blocks each block will become a
partition what if you're not reading
from Hado let's say I'm reading from my
local PC I can mention how many
partitions I need for the data say on my
Windows laptop okay I have a uh 1GB file
if I simply read it it will come as a
single 1GB partition okay if I want I
can say that hey from my this is in my
Windows laptop okay I can say that hey
Spark read this 1 GB but create four
partitions for me on this what is
Advantage each partition can go to a an
Executor an Executor will process a
partition not a
file so always your data has to be
partitioned the more partitions you have
the more parallelism you will get no
that's just an example anyway in the
personal laptop I cannot launch multiple
executors I'm saying you're reading from
a personal laptop and dumping into a
Hadoop cluster imagine it's not from a
Hadoop file system or let's say you're
reading from
Cassandra a typical example Cassandra is
an SQL database Cassandra doesn't have
blocks or anything blocks are on Hadoop
so if I get the data from Cassandra I
read a table I will get 1 million rows
right 1 million rows I cannot give it to
an
Executor right then the processing will
be very slow so what I will do I have to
have an idea about my data let's say 1
million data is 1 GB or something I will
say that when I'm creating an rdd I can
say that take this data from Cassandra
and divide it into four parties
each partition will go to an executive
ah an Executor can manage more than one
partition also minimum one partition it
should get depending so let's say I have
an Executor with 20 GB memory H so let's
say a partition size say 1 GB it can
manage 20 partitions data locality is
not 100% guaranteed when you launch an
Executor 100% is not guaranteed so is
this clear because you have to figure
out the number of
executors okay first thing that depends
on what is the size of your data so in
Hadoop normally this is not required
because in Hadoop what happens by
default a block is a
partition right but again that will make
sense because what is the block size in
Hadoop 128 MB 128 MB is the block size
right so I have a file with uh that is
divided into 10
blocks so total size of the file is
this MB that is
1.2 GB I want to process 1.2 GB file now
the question is that how many executors
I need that depends on your cluster
configuration I can even process this in
a single executor I can ask hey Yan give
me one executor 2 GB Ram or 3 GB Ram
just a safe side I'm saying so Yan will
launch one executor let's say 3gb Ram
okay and this has eight uh 10
partitions this file has 10 partitions
10 blogs all 10 partitions will come in
that single what is say container get
processed ah copy original data is in
hdfs anyway it's happen in the same ahuh
so in this case I'm asking for one
container so this executor right so this
executor you're launching okay so you
can ask how many executors you
want he's not understanding what I'm
saying so this is my my hard
cluster that's what it is not very
effective I'm
saying you can do it it is not very
effective so the more partitions you
have more more processing speed you will
get the same example if you take so I
have four partitions right four
executors I launched and each is getting
loaded into one partition this will be
faster I can also launch a single
executor here copy all this four here
it may not be so fast and another very
important point so these are all related
to Resource Management but you should
also understand this stuff
right an Executor is a
container right a jvm One processor core
is required to manage one
container meaning if this is a dual core
machine how many maximum executors you
can launch
two if you ask for eight executors what
will happen doesn't
work so an Executor is a jvm to manage a
jvm jvm has RAM and CPU Ram you can say
I want 8 GB Ram 10 GB Ram fine but
within that to manage ideally for a
single jvm one processor core will be
allocated minimum One processor core
right but again the question if I have
16 GB Ram I mean 16 GB RAM for a
container that will have lot of
partitions inside that maybe one CPU
core is enough to run all of them
depends on what is the processing power
of your uh machine but you have to keep
all these things when you're launching a
cluster performance side I'm saying so
to put it short ideally you don't have
to do these many things when you're
launching a spark program you should
know the size of your data and how it
should be chunked correct so the idea is
if the number of partitions are more so
let's say I have a file whatever the
size May X size is X right if the number
of partitions are more that means the
number of splits are more ideally so I'm
saying that I want 10 partitions so I
get 10 partitions now if each partition
can be inside an Executor 10 executors
can process that data so parallelism you
get but if it is less let's say I want
only two partitions then they are in two
executors then you know two executors so
ideally your this is less what you say
processing so the more partitions you
have the more processing power you can
get so uh if you are loading let's say
text file right so if I'm loading a text
file I can say that create an rdd number
of partition five so what it'll equally
split that text file into five and then
give it to one one executor but the
point is you need five executors to
process it so if you go to Yan Yan will
say I don't have enough res I'll give
you only two executor so two executor
will get five partition one guy will get
two another will three so and each
executor is having one CPU core right so
it'll may be a bit slow so if you have
enough resources parallelism Works
actually better so usually what we do in
theuh huh I will I will redraw this
picture so things will become clear to
you I think I just draw it in one manner
only so my point is I just delete this
okay give me one moment no number of
partitions so that's what I'm saying if
you have a file this is your file right
this is your
file and this file is divided into 10
blocks in Hado okay so I have
what I I won't exactly write it but
let's say 1 2 3 4 5 6 up to 10 blocks
you have okay now to answer your
question right when I read the data into
spark it'll automatically do something
called partitioning and each block will
become one
Partition by default and you cannot
change it okay by default it is one and
most of the cases we keep it but what
you can do so so I get this is one block
this is one block so all this is red so
this is my partition one partition two
partition three right this is what
partition four and I have partition
10
right huh similar to input splits so you
have 10 partition now you want to
process it that is where the speed and
all matters right to process it what you
need you need RAM you need CPU power
right so where this partition will go
this is hard disk blocks are
where this is hard disk blocks are in
your hard disk when I say I create a
partition partition is in Ram it won't
just sit in the ram it has to be inside
an
Executor or a jvm right that is where
the data should come then the processing
should start right so then then you can
decide I have 10 partitions okay how
many executors I need can I ask for 10
executors yes you can do you can say I
want 10 executors okay so you will get
what executor 1 and here you will get
executor 10 and each uh partition will
go to an Executor partitions always go
to execut so this P1 will go to this
executor now this executor I have asked
for 1 GB RAM for this executor I also
have one processor core that means this
partition can be uh managed by this
executor same your P10 will go here okay
this executor also have one
processor and 1 GB Ram but the point is
your yarn should allow you to give you
10 uh this these things the same thing I
can do in another way I ask
for only two
executors example okay I ask for two
executors this executor has
5gb this executor also have 5gb this has
one processor core this has one
processor core and P1 P2 P3 up to P5
will come here P6 p7 P10 will come here
now if you write a program so let's say
I wrote a program I say filter the data
H so my driver is here right this is
what my driver in my driver I said
filter the data H where is the data in
Partition so this filter is your logic
this logic will be pushed to this
executor and this filter will apply here
here here here here who will apply this
processor and RAM together right so One
processor core I have using that one
processor core can I uh uh parallely run
this probably I have multi-threading and
all I can parall process this much
amount of data good otherwise one by one
it will process it take take some time
depends on yes your processor core you
can increase you can even say I want
four cores for the container when you're
launching this uh executor I said One
Core by default right I can also say hey
Yan give me my executor each executor
requires four
core but then the problem is if this is
a quad core data node only one executor
can be launched because you already took
four course if you ask one more you will
not get
right if this is a quad core data node
if you launch one executor you cannot
launch another executor already you
taken four cores the same system the
same system there is multi-threading so
I think uh by default it can handle only
two partitions Max multi- threading
means two threads right so in production
how we do it is that once we design our
application we look at the data from
where the data is coming will it already
come partition or you have to partition
because if you're reading from Hado it
already is partition like blocks and all
but if you're reading from Cassandra
Cassandra doesn't by default partition
the data so you get like one big chunk
so while reading we decide should we
partition yes we partition how many 10
so 10 means what will be the size this
much so how many executors you need so
then you go to your admin and say that
can I launch a spark job with this many
will I be getting he will say yes you
will be getting then you launch
them
now no no no no it is a nosql
database out of this but you can read
from there and you have connectors in
spark where you can read from Cassandra
so are you able to follow what I'm
saying I mean some of you are like he is
talking Greek and Latin right there is
also a way where you launch the job
default so if you don't want to mention
all this stuffff you just say run my job
so Yan will figure out the best way to
launch probably it may not be the
fastest way but it'll just execute your
program that is also possible so now
you're thinking that this guy is trying
to fool us because he's saying you can
ask for three I will practically show
you three executors running by launching
you can actually do this it's not like
some Theory stuff when you're launching
a spark shell you can go to Y say num
executor executor memory driver memory
all you can ask and it'll give you those
and you can see them okay I got three
executor or four executor or whatever
you want
right now if you have understood this
much I'm going to again confuse you
because if you ask for an Executor right
you say I want an
Executor and how much memory you want
executor you say I want 10
GB by default you get only 90% of
it okay because 10 percentage is
allocated for system
calls it is a container it has to access
system calls and respond so you say 10
GBS 10 GB is full not you it will not
give you full 10gb 10 percentage system
will take so what you
get 9 GB in this you will not get in
this you get only 60 percentage so
ultimately some 54 percentage of the ram
only you will get because it is a jvm
right there is garbage collection jvm
management all that requires memory so
in reality if you look at a spark
cluster if you ask for 10 GB container
you get around 5.8 GB for rdd remaining
all system will
take please keep it in your mind because
this is an interview question so you
will go and say 100 GB Ram I get they'll
say no it will not boss ah so by default
if you ask for 10gb 10% system calls
will
take H container has to communicate with
your operating system yes right and yarn
so for these things it will reserve some
memory right so from 10 GB you became
9gb in this 9gb your jvm has to manage
it's garbage collection and
communication uh all this it will take
another I think uh I don't know the
exact number 30 percentage or something
it will take it will drill down to Total
I think 54 or 56 percentage of this 10gb
only your rdd memory you will get to
actually fit the party
which means in 10 GB you get roughly
around 6 GB to fit the partition so
don't think that you get a jvm 10 size
you can fit 10 GB of wor partition no it
doesn't work like that so these are
actually internal to spark but when we
run we will understand I mean in
production when we run because then your
calculation will be wrong right if you
don't understand this your calculation
will be wrong okay so this part
partitions and uh parallelism so now if
you look at the the picture does it make
more sense this
picture so you have four blocks you ask
for four containers four partitions
ideal case each is loaded and that's
called your rdd right now the real
question how do you write a spark
program right basically that is what you
want to do apart from rdd or RAM and all
once you get the data you should analyze
the data so how do I analyze the data in
in Python did you uh learn something
called a higher order function function
you know right so normally when you
write a python function you will say
function then function name blah blah
blah let's how Def and all then you
write a function and you will reuse the
function why are you creating a function
you can call it anytime there is
something called Anonymous function or
disposable function let's say I want to
create a function I will use it only
once I don't want it anymore so you
don't have to really give a name for the
function or defin you can create it on
the fight that is called Anonymous
function
it's called Anonymous function
Anonymous okay in in uh spark
programming in spark programming what we
do is that we have something called
higher order function there is something
called higher
order function what is higher order
function let's say I have a function
called
ABC I can pass another function to this
function that's called called a higher
order
function meaning this ABC is a function
and normally you will pass some
parameter or something some value right
you will say a is this B is this that is
what you s but I can have a function and
I can pass another function to this
function that's called a higher order
function so we will be passing Anonymous
functions here this Lambda is an
anonymous anonymous function I will show
you the code it will become better but
in sparkk basically what we do is that
once you create an rdd now you have the
data ready your data now I want to
process the data how do you process the
data you have something called
Transformations so it is actually there
in a spark website it's also uh good
that you can look at spark official
website how do you go spark. apache.org
hey why it is not showing
yeah so spark. apache.org is the
official website of spark and if you go
to documentation it will say latest
release is
2.3.0 right and these are the older
versions and all if you click on here
you can see all the spark versions you
can see that
163 was the last spark one version now
we are on 230 this is our version I mean
latest version and if you go to
documentation let's say latest release
imagine latest release okay if you
scroll down okay here you can see rdd
programming guide see and if you click
on this okay and scroll down a
bit this we will come back you can see
resel distributor data set or rdd this
is what we created right so we just
created an rdd at least theoretically so
once you create an rdd I will show you
how to create it okay you have rdd
operations so now my data is available
as an rdd what can I do with the rdd
right so that is where you can start
writing your functions okay Anonymous
function uh and yeah these are the
Transformations this this is what you
need to understand so these are all
transformations you can use in spark map
filter flap map there are many actually
so if I want to filter my data I will
just call this filter okay if I call
filter it will ask me what do you want
me to filter within this bracket I will
write my expression to filter that is
how you filter your data map is another
it is like four each okay uh map I will
call map will ask me like what do you
want me to do so you'll write an
expression within map what map has to
perform so these are all higher order
functions map filter flat map these are
all higher order functions actually so
you do something called
transformations in one rdd if you apply
any of these functions it will create a
new rdd that's called transformation
that is how you analyze your data let's
say I want to filter my data I will call
the filter transformation and rdds are
immutable very important Point once you
create an rdd you cannot change it you
can only create another one by applying
some logic you can never edit an rdd
they are immutable
right so if I go to my
PPT yeah so we have created our log
lines rdd fine till this we have
understood and then what I did probably
I'm interested only in error messages
from this rdd so you you know you see
there a lot of data info warning error I
want only error messages to filter so
what I can do I can call the filter
transformation
okay so I can call a filter
transformation and then I can say that
hey Spark Okay match only error lines
and give it to me and I'll show you how
to light the logic this will produce
another rdd and I can call it as errors
rdd this is the steps in which you write
a spark program first you create your
rdd and now I want to do a filter I will
call a filter and it will you know
change whatever I mean it will filter
only error messages and that I will will
store it as another
rdd now somebody was asking me what will
happen to the memory right so it'll
delete this
rdd I mean if there is not enough memory
so let's say this rdd fit into the
memory and then you call this filter
action it will filter whatever is
required and this will be gone this is
not required now because you have this
right because next processing will start
from this step
right I will I will show you what you
can do okay originally let's assume in
the normal use case you call function it
creates another rdd and this rdd is gone
now now this is your current data um so
if you have enabled in Yar Dynamic
allocation then this will be gone I told
you right this is actually a partition
and there is a executor running here now
this guy will be idle because it doesn't
know what to do it cannot predict that
there will be no data right so that exe
so that is one more problem huh now your
problem is you have four executors the
second executor has no data to process
because it filtered all the error there
is no error now this guy will be sitting
idle so one data node will be there one
executor will be there that has no data
it'll simply sit idle so that becomes a
problem right how can I solve that
problem it's actually very easy in spark
there is a transformation called
coilies coilies is a very common
transformation and why do you call coil
is you pass a number it will reduce the
number of partitions you can resite SI
it because and you have to calculate
this but in this example assume you know
that see now if you look at the data
second partition is empty third
partition has only one line that's also
not so so I'm assuming that I want only
two so I can tell spark take this rdd
apply coilies keep all the data but
delete two partition and two jvm I want
only two so it will just bring it into
two partitions now for example just an
example if you are having one terab of
data right and you take a good sample of
that data uh let's say 10 GB or 100 GB
and you run it once so then you can
understand you know uh after the filter
okay you can call a collect action so
let's say after the filter I want to see
the data I can do it so I know that
originally I loaded let's say I don't
know 1 GB data or 10 GB data after the
filter when I get the data it's only 5gb
that means half is reduced so I can
calculate okay if I'm loading one
terabyte of data I don't have to manage
this many jvm so after this step I
should call a coilies I can do that now
the most important Point surprising to
is that let's say you wrote a spar code
in which you wrote these three lines
create an rdd filter the rdd you say
coilies and you say run nothing will
happen this is the surprise actually you
wrote a spar code where you wrote all
these three lines okay read from hdfs
Filter error message then you set coil
is and you submit the job to the cluster
nothing will happen in the cluster
because all of this are lazy there is
something called lazy execution meaning
spark will not start execution unless
you call something called an action
these are transformations meaning you're
changing the data but you are never
saying that show me the output right you
are saying repartition the data filter
the data okay so what where are you
saying show me the output you're not
doing that so unless you call something
called an action where you say that give
me the output nothing will work to do
that you can call an action called
collect collect is a most common action
in spark when you call collect okay you
are telling spark that I want to see the
final output that's it so collect will
look at this cleaned rdd so you are
asking give me the output of this
cleaned rdd and Spark will understand if
I want cleaned rdd I should have errors
rdd if I want error hard I should read
first it'll go to your hard disk read
the data do all this stuff and then show
this is output right show this on your
screen and then the entire pipeline is
emptied spark never keeps your data in
memory once the processing is over okay
you got the result on your screen right
this you so everything is gone no rdd
nothing is there no container no
partition nothing will be there
everything is gone so only for that
Split Second all these things happens
now I if I want to know whether I should
rerun or repartition what I should do is
that I should go back here okay I will
do a save of this error you can also do
a save method on an rdd when you say
save it will save as a file a text file
so I will say save as an action on this
filter rdd and then I have to check what
is the size right my original data is 10
GB after the filter it is 5gb so to
process 5gb why I'm launching these many
things so I can rewrite my code in that
way so collect will simply display the
output on the screen there is also an
action called save as text file okay I'm
not uh just talking I'm showing stuff
can you see actions now his question was
what if I don't want to do it so he had
a very good question so he is asking so
the problem with this is if I run The
Spar code in a split second everything
will happen and then I see the output on
my screen no rdd nothing is there in the
memory because uh the this is called dag
director a cyclic graph spark will
create a dag a graph of execution and it
perform step one step two step three
step four how a result and then
everything is gone again you run again
it to load that's a different thing uh
now so he had a question I will come to
that so I will give so collect is an
action where you return all the elements
in the driver program meaning you see
the output you can also say save as text
file there is an action uh that will
save see save as text file if if I say
save as text file for an rdd whatever
data is there in the rdd it will save as
a text file so I can see that in Hadoop
or wherever you are storing it right now
is this the only action available no I
have save as Cassandra table or or many
other ways to save the data ideally text
file save we do to just to see the data
okay uh in the initial uh phase of
reading the data even map reduce or
spark everything will be same because it
is blocks you are reading no difference
it makes once the data is available in
memory then the difference comes because
here you see we did a filter then what's
that coilies and then let's say
something else also we did for doing all
this the data is still in the memory you
not writing anything there is no
intermediate result you're are pushing
to hard disk once you call collect you
can write 100 functions and then call
collect all those manipulation will
happen finally the collect only will
display the output that is a speed h no
very good question only one action can
be called it once very good question
yeah so there are many actions or let's
say collect and save as text file
collect is one action I cannot say
collect and save as text file no so
first it collect will run again if I say
save as text file again the whole
operation has to start so you will be
wondering is there a way I can improve
it I will show you okay so probably that
is his question so um here I will show
you this so once you
execute execute the dash dag is nothing
but directed a cyclic graph it is a
fancy way of saying spark creates all
the steps to be executed in a graph
format and I will practically show you
the dag when we run spark job you can
see that you can see partitions you can
see the dag you can see how many
executors it is launching everything is
visible it's not just Theory Theory I'm
seeing but unless I speak about it and
just show then you will say how this
came you know how the partitions
actually came you'll be confused right
and the driver collects the data so this
is the full step if you want to look at
it once more uh no if you wanted a
filter you to write it in mapper reduce
is only aggregation so either you can
say sum or multiply or or it's an
aggregation function but because you get
one key and all the values in the
reducer so what do you want to do with
all these values you can't filter them
so you can aggregate them or do whatever
so if I have to write a filter I write
it in the mapper side here I can so here
the final St stage is the collect so
before the collect I can do whatever I
want I can filter and here the advantage
is it's not just key value pair you can
also work with key value pairs in spark
so I can create a key value pair and
similar to map produce I can operate I
can reduce and I can do all these things
uh yeah he was asking a question I think
I will answer that because if you look
at the picture I have a very good
picture to explain this just see this
picture same stuff I'm doing there is
dog lines rdd errors rdd cleaned rdd and
finally I'm calling an action called
count count is another action so if you
say count everything will work from the
beginning because it has to count right
so count action called and what happened
it show me the result as five because
there are five lines in the rdd no
surprises now I also want to call one
more action on the rdd called save to
Cassandra the problem is if I call this
action pipeline is empty spark has to
start from Reading From the Block
creating the original rdd then filtering
then errors all then I can save to
Cassandra okay now one more thing then
after I'm doing one more filter I want
only messages one from this data see
message one message one message one I
just get only message one and then I say
collect so I just want to see this but
again what will happen it will start
from the top so if you look at this
three actions you can see all three
actions depend on this rdd everything is
starting from here right that is where
you can cash an rdd it is possible to
Cache an rdd so when I say caching
whatever data you have in that rdd along
with rdd will be cached and then where
can you cache you can cach in Ram you
can cach in hard disk and you can also
say Ram plus hard disk like wherever you
want so there are multiple options to
cach it
and once you cash an rdd let's say you
say count it will count then you say
save to Cassandra it will start from
here because this rdd data is already
cached and don't think if you're caching
you're caching for one month once your
spark program is over caching is deleted
so this is caching is valid only till
your program completes all the action
once all the actions are over you exit
everything is gone then you are not so I
will show you tomorrow when you start a
spark spark application uh if I a
developer I will create something called
spark context object there's an object I
create and that represents my program
and ideally when my program finishes
I'll kill that object if I kill that
everything is gone even caching will be
gone because that caching is valid only
to speed up my existing program I'm not
caching it for a program I'll write next
month that is not possible during that
execution time I want to speed it up see
I I create this clean AIO whatever and I
say count it will count and delete all
this thing from memory right so it is
not in memory during the execution is in
memory after the execution is over
nothing is in memory ah then you should
keep the intermediate data in memory
right you should tell spark I will call
an action show me the output but this
rdd whatever data you have don't delete
from your pipeline keep it because I I I
want to reuse it for some other action
then it keep that's called caching
H so there is a command called UNP
persist UNP persist is an action it's
not a transformation if you say UNP
persist it will delete it from cach I'll
show you how to cach tomorrow it's very
easy and otherwise if you kill your Spar
context object like your session is gone
it'll automatically deleted so basically
so you may be thinking so how do I start
programming with spark right I want to
write a spark program so first thing you
should learn how to create an rdd if you
know how to create an rdd then you can
start with uh these simple simple
Transformations map filter now there is
also a flip side of all of this uh the
flip side is initially when Spar came
into the industry everybody was mad
about rdds okay transformation so I'll
write a map filter people were literally
dying on writing these things okay but
soon there was a problem with this see
the problem is if you write your code
using these Transformations and actions
there is is no way sparkk can optimize
your uh code why am I saying this is
that you say filter okay in the filter
you are saying that filter only error
messages right unless spark runs it it
doesn't know what you're talking about
and this these things do not have a very
strict
schema rdds are not having very strict
schema so when you want to process
structure data let's say like CSV file
okay I want to read a CSV file and I
want control I want schema everything
rdd is not a good way to process it you
can process it but spark internally will
not be able to optimize your code that
is where Spark SQL comes into picture or
data frames we call it so spark has a
module called spark SQL and Spark SQL is
much more powerful and optimized than
core
spark so when you write spark SQL you
should know how to create a table and
query using SQL it's end off a SQL only
so it's is easy for to program and more
than that it is much more optimized than
core rdds because you are using all
Lambda and Spark has no way to
understand what Lambda you are using
unless it runs it right I wrote a Lambda
code some weird code spark has no way to
understand what is the meaning of this
Lambda unless it see the data but if it
is a table I can say filter on this
column spark knows what is that column
what is that data type where it is right
it can avoid that column while loading
the data to optimize I'm saying right so
if I write a SQL query it has a filter
Group by then join and something spark
and read it apply the schema and
understand what you talking about I will
I will talk about in the data frame
class I will talk about it so any
questions on Cor spark like so far we
have discussed if anything is not clear
just ask me okay so don't think okay so
these are some of the common
Transformations map flat map filter
these are the name of the transformation
and these are some of the actions that
we have right yeah so um anyway we will
get started with the core hands on
tomorrow but uh we can look at some of
the things so when you have a cluster so
I'm just connected to the cluster right
now right spark gives you a shell to
work like the hive shell if you remember
so you started high and started typing
so very similar to that spark gives you
something called a spark shell
H and the spark shell is available in
Python Scala and R there is no Java
shell Java as such does not have a shell
functionality you have to write using ID
or something right now if you want your
uh spark shell in Scala we are going to
do python but I want the Scala shell I
can simply say spark
shell if I simply type spark hyph shell
uh oh I
think why spark shell is not
working I think I have to export a path
to get started spark shell okay so let
me try this let's try the python shell
ppar 2 this should
work H
99 so this is Java 8 as of now Java 8 is
supported okay I think in the cluster
they have disabled Scala shell access
maybe I have to export some
configuration to do that that's why I'm
not able to open the scalar shell but if
you want the python shell you type Pi
spark two that's the command why P spark
two because we are in spark two this
cluster has spark one and Spark two
installed you can try this we will just
uh you know some do some Basics not like
advanced stuff but
I don't know why it's taking this much
time usually the shell starts very
fast the shell is not starting let me
just
check yeah so started so very simple so
this is your python shell okay and what
it says when starting the shell it says
type help and blah blah blah uh logging
level for spark R and all it says Spark
version is
2.2.0 so on the cloudex lab we running
2.2.0 using python version 275 that's
okay okay spark session available as
spark I will talk about this later later
what is this spark session okay but it
just says spark session available as
spark okay and this is your spark shell
so now from here you can start creating
rdds and then writing Transformations
and everything that you want okay
now I will just see if I can show you
this
so if you type this command P spark
simply type P spark okay dash dash
help sorry sorry P spark 2 right py
spark 2 hyphen hyphen
help yeah so I want to show you these
options okay just have a look at
this so I'm just saying that I want to
start a spark shell okay and I need some
help in starting that right so what it
says what are the options you can have
so one is Master URL so this we don't
have to worry as of now okay if I scroll
down I can show you can you see this
driver
memory so when you start spark shell
okay a driver will be created okay and
then you can ask for how many executors
you want right how much memory you want
for the
driver right one question and then here
you see what is this executor memory I
told you right you can ask for how many
executors you want and how much memory
so by default I think it is 1 GB it is
written here default is 1 GB okay but
you can ask for executor memory right
and if I scroll down
uh see here executor core how many
processor cores you want for an Executor
right and see here driver core how many
processor core you want for the driver
number of executor default two so if you
starting the spark shell like this
interactive shell you will get two
executors one driver each executor has 1
GB Ram default setting but using these
arguments I can say uh how many
executors I want and what is their
memory and so on and so
forth now another important Point let me
try this okay just give me one moment I
will just do a p spark
two
okay
and let me let me open my
mail because uh the spark also has a UI
and that UI
was sent to me in my mail because for
every cluster it is
different sign
in
uh just check this
this is a yarn mode I think okay let me
just check
okay so one of the URL I think it is not
working probably it will work now I
don't
know yeah so can you guys see this uh GL
faculty great learning this is me okay
and it says 55 seconds before I started
as uh shell this is actually showing you
the history server that's what I'm
wondering okay 55 seconds before it
started let me just go to the
application
so you have to actually see this URL but
it is not
showing okay maybe I
think it's giving the old UI
let me exit from here
okay I
refresh so what you're looking at here
is this history server URL and this is
not really useful to us because once you
start a spark shell and then you exit it
it will display here you can see here P
spark shell it was launched by me 1.8
minute I exit from here and then you can
see here but this is not really useful
to us because uh this will come only in
the history server that means once you
exit from spark okay so there is a real
UI that port number is not working
currently when you are running the spark
it will show there but I can show you
something if so this session we I
stopped okay I didn't do anything I
stopped this session and if I go to this
application this is the spark UI
so here you can see the jobs and there
are no jobs and if I expand this can you
see executor one added and there is
Executor two added because default you
are having two executors uh and here you
can see
executors and there was one driver and
two executors so there was one driver
active and two executors we got as per
this right um and one more important
point is that
when you simply say p spark
2 if you simply say Pi spark 2 spark
will start in the local
mode that means you are not talking to
yarn or anything if you want to talk to
yarn you have to say hyphen hyphen
Master yarn this is the option you have
to use so whenever you are starting the
spark shell otherwise if you start in
the local mode what will happen driver
executor everything will be in a single
jvm and also the resources will be
allocated by your operating system not
by Yan in the local mode Yan has no role
in your local mode and that is good for
learning if you simply want to learn say
spy spark 2 and then you can just start
writing your code whatever ideally when
you're are launching the spark shell you
have to say py spark 2 and then you have
to say hyphen hyphen Master Yan meaning
uh I want to register with Yan and then
launch Spar so what this request will go
to Yan and Yan will allocate containers
to you now let's try to do one thing
let's try to launch it with our own
parameters right so what were the what
were the options we have P
spark to das Dash
help so let's look at what are the
options let's say we want more executor
I want three executor probably right so
what is the option num executor hyph
hyund num executors I will say three and
hyphen hyphen
execute course course let it be there I
want memory more so where is memory
hyphen hyphen executor memory right
default is 1 GB I want probably 2 GB
something like that right so what I can
do I can simply go to my
shell P spark 2 I can say master is Yar
and I can say number
of executors probably four I don't know
and then what is that
executor
memory why this is there okay executor
memory I need 2G 2GB
hitender
uh by spark does not support any
application option why is saying
that probably we are asking for
more let's
see let's see whether we can get three
executors okay so first we will try that
then we will see whether we can get more
memory or something so right now I'm
saying that hey launch a spark shell the
master is
y
sorry H not meem right
memory right yeah wrong command correct
so now I asked for three executors and
it started and we can't see it here
unless we exit so let me exit from here
so it will go to the history server
okay I'll refresh
here this is what we started
right can you see executor one added two
added three added okay can you see so
three executors were given to me and you
can actually go here
executors can you see this is one driver
these are executors right you get
executors and here you
see the memory right so how much memory
did you
ask
huh I think one default we selected
right one GB we selected actually so so
for rdd storage right now you are
getting what
384
.1 uh megabyte so how this rule actually
works like I told around roughly 50% of
the memory will be taken and remaining
it will give it to you you will actually
get around 600 MB in this but it'll
display only this much because you are
not loading any data as of now it is
simply running the executor is simply
running you don't have any anything
inside this right
right for the
rdd that's what you don't have any data
as such right when we load something we
can actually see the memory it will
occupy how much uh amount of rdd you
have it will
occupi huh so 384 right now it has
allocated but there is no data inside
that zero B is saying that but when you
actually load let's say 500 data it will
show 500 MB actually occupied in this
because up to maximum 600 MB I think you
will get because 40% anyway will be gone
for your system calls and
jvm right so up to 600 500 and something
uh 50 or something you will see here
okay so right now you can see only less
memory because we are not actually doing
anything with here so that is why it is
simply displaying uh uh you know this
much and uh I exited from here so what
if we try to change
this number of executors four hyphen
hyphen executor memory
right memory 2G there is a threshold
value I I think uh if it is more than 1
GB let's try this there is a threshold
value what is this ah it says an error
occurred while calling Javas Spar
context required executor memory is
above maximum threshold for this cluster
so the cluster uh uh administrator has
said that you can ask only maximum this
much so what they
say
248 + 384 MB is above so uh it is saying
that Max to Max you can ask only 248 2
GB of memory but here what will happen
when you launch an Executor is that
initially it will allocate a 384 MB okay
then it'll keep a 2GB as you know what
you say excess to be allocated so it is
saying that the total memories above the
cluster administrator what he has said
you cannot launch so we can try 1.5g or
something like that I think uh 1.5 gab
should work ideally let me check if that
will work okay I'll
exit
exit and we'll try this tomorrow don't
worry okay I just wanted to check
whether the cluster is running fine we
will try this together tomorrow don't
worry okay so how do you mention um so
see here size must be specified as bytes
kilobytes megabytes gigabytes terabytes
right so I think you cannot say
1.5 G that will not work right so how do
you mention uh megabytes M right
did I say g
h so what did I
type I typed G only right then why it
didn't
work M should
work M should work ideally right 1,500
M it should be whole number I think you
cannot say 1 Point 5G like you can say
1G or 2G okay so now it launched and I
will exit so but uh now what I'm showing
you here is not correct because I'm
showing you from the history
server like I'm starting spark shell
then exit from there then I'm showing
you that this is the UI that is not
correct you you should see live right if
you're if you're running the job you
should see live that UI is not working I
was asking for that and I don't know why
so the jobs will display anything which
is running why you are not seeing
anything here because we don't have any
rdd or any action or if you have
something you will see here okay and
stages I will talk the storage will show
you if you have cached an rdd if you
cach an rdd it will come here in the
storage section environment is where it
will show the Java libraries and all
executors will show how many executors
and driver that you have added jobs jobs
is where if you run a job it will come
like you said this thing do collect it
will display here like when you run an
actual job it will display here the dag
and
all stages it will show you in the job
how many stages are there and how those
stages are created we will see that
tomorrow anyway okay then you have
storage and then you have environment
and executors right so environment will
show you the different libraries and all
which are part of spark so if you go
here now you can't actually mess with me
what if I want to see this from the
shell right so I'm just
saying uh P spark I'm starting it again
okay and I don't have a GUI access now
but there is a command what
happened what it is saying hm spelling
mistake ah M
sorry so there was a command SC do
underscore
con there is a weird command which can
show you all this from the shell so now
you are seeing this in the GUI what if I
don't have a GUI so I can do an sc
doore do get
all perfect
so I remember the command I'm happy okay
so there is a weird command nobody
remembers this command actually SC doore
okay uh but the output is weird but you
can see something from the output what
is that you can
see can you see number of executor
somewhere it will be here is driver
memory right this is driver memory and
you can say master is
yarn somewhere number ah see here
executor memory how much it has
allocated 1,500 you can also see number
of
executed where do you
see it's very difficult to read from
this I can copy paste that's
better is not even allowing me to
copy H see here when I'm copying I'm
getting
so this command is like a million dollar
command I'm quite sure nobody will know
this command even I came to know very
recently how can you see The Spar
configs from the CLI SC doore conon get
all it's a shortcut actually I'll show
you the command this is the command
please make a note of this because
sometimes we will start working in a
cluster where they will not give you GUI
then how do you know how many executors
you launched or what happened you know
you have to look at this way this way so
this will give you an idea about the
number of executors everything launched
live live you're seeing like now you
launched now these many are there
score con. getet all now the good news
is we will try something in the Shell
the bad news is you will actually try
most of the things like this Jupiter
notebook so how do you create an rdd
from p park import SC blah blah blah uh
me just all output clear
okay so kdd cup data rad data
paraly this brings us to the end of this
big data analytics tutorial now before
you guys sign off I'd like to inform
that we have launched a completely free
platform called as great Learning
Academy where you have access to free
courses such as AI cloud and digital
marketing so guys thank you very much
for attending the session and have a
great learning
ahead e
