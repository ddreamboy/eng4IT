this is a cat
you know that
everyone watching this video knows that
even a toddler will point to this photo
and call it a cat
well not necessarily
sometimes children mix up cats and dogs
especially if they weren't exposed to
different breeds or colors or sizes of
animals before but as soon as they've
seen enough cats and dogs and other
furry friends they've learned the
difference
we apply similar logic when helping
machines understand the visual world
this technology helps autonomous
vehicles avoid pedestrians and other
cars as well as react to road signs and
that's how computer models can locate
tumors in mri images with up to 90
percent accuracy
the image recognition skill allows
computers to process more information
than the human eye often faster and more
accurately or simply when people are not
involved in looking
so how can machines see and interpret
the visual world
well let's talk about computer vision
and its applications
[Music]
in the first three years of our lives we
create one million neural connections
per second whenever we learn something
new a neuron in our brain lights up with
an electrical impulse and sends the
message about a new experience to other
neurons forming connections everything
we know is shaped by these neural
connections networks of them whenever we
see a new type of cat the same
connection strengthens making it easier
for us to recognize an animal as a cat
next time
with machines we use similar neural
networks but those networks are
artificial
just a disclaimer we will be using
simplified explanations and formulas to
make concepts more digestible
an artificial neural network is an
advanced machine learning algorithm that
can answer questions without any hints
from humans questions like how much will
it cost to fix a car or a roof after the
disaster tractable uses ai to study the
photos taken by insurance clients to
evaluate how much should be reimbursed
and in this demo nvidia applies image
recognition to satellite imagery to
detect destruction of homes in the
woolsely fire in 2018.
the neural network looks for specific
features that it knows must represent
damage and it does so by doing math
a neural net consists of many layers an
input layer that receives the data
the output layer that makes the final
forecast
and the hidden layers where the magic
happens
layers consist of nodes neurons that are
all interconnected each neuron is
performing a computation
just like biological neurons receive
signals about the world artificial ones
receive data
the difference it's represented in a
format understood by computers numbers
for images it's the color values of the
pixels composing the image
neurons in the input layer are not that
important because they simply receive
data and pass it on so let's focus on
this node in one of the hidden layers
connected to many nodes in the previous
layer it accepts one number from each
node but it doesn't treat all these
connections equally they have different
importance or weights also represented
as numbers the higher the weight the
more influence one node has on another
so a node multiplies inputs by their
corresponding weights and then adds them
all up to create a single number this
number will become an input for the
nodes in the next layer
so the weights basically help the net
look for the data that carries the
greatest importance for the task and
layer upon layer recognize more of the
details until the last node puts it all
together and says here are all the areas
hit by hail
yes an experienced evaluator can also do
this kind of job manually but they will
have to travel to the affected zone and
inspect large areas immensely extending
the claim processing time but a computer
can analyze data from photos without
workers moving around and wasting
precious time
of course it doesn't have to be just
static images amazon incorporated neural
networks to develop one of its most
groundbreaking products amazon go stores
every part of their unique just walk out
experience is run by computer vision
cameras and sensors keep track of every
person entering the store and what
they're doing whether picking up an item
or returning it to the shelf at the same
time every product sold at the store is
tracked by cameras too is it on the
shelf or in someone's hand
and finally those two instances are
combined to answer the ultimate question
which person took what product
with almost 30 locations in the u.s and
now a shop in london the automated
stores are no longer a concept but the
retail reality as amazon starts
providing the technology to whole foods
tesco and starbucks
this way computer vision is transforming
the way offline shopping works any
visual data that can provide useful
information can be processed by machines
saving time money and providing
competitive advantage
this was the basic principle that all
neural nets work by but they can be
different too
for image recognition tasks
convolutional neural networks are used
the most they were specifically created
to work with two-dimensional image data
and are extremely good at locating
important features of objects
here's how convolutional neural networks
work
computers see images as grids of pixels
as you may know each pixel is
represented and encoded as a combination
of red green and blue of different
intensity here's what a computer sees
when you input this picture
let's switch it to grayscale and reduce
the resolution for simplicity
from this color intensity map alone the
computer must determine how many cars
are parked in the parking lot this
information can be used to optimize
price or redirect traffic to less
populated areas
this will get a bit math heavy so be
prepared
in a convolutional neural network we do
almost the same thing the normal neural
network does but instead of multiplying
one pixel value by its weight we
multiply a patch of pixels by a set of
weights called a filter or a kernel
this is done because in images the
difference between neighboring pixel
values is important pixels with low
values are darker while high values are
lighter if the contrasting pixels are
located close together it's a good
indication that we're looking at an
object's outline
so we multiply this patch of pixels by
the filter and receive one pixel
the filter moves across the image patch
by patch until it transforms the whole
picture so the convolutional neural net
not only extracts value from the image
but also reduces its dimension making it
easier to process
depending on what features we want the
machine to find we apply a specific
filter
they're like templates helping bring out
needed features
this one can pinpoint the outlines other
kernels can filter out horizontal or
vertical lines there are kernels for
finding faces and separate face features
okay but how does a computer know which
weights to multiply by we're glad you're
catching on because this is kind of a
crucial part neural networks learn those
weights during the training process
want to know how to train a deep
learning model
let's start with something familiar how
do humans see
well to be honest we don't know for sure
how our brains apply meaning to the
visuals around us the most popular
theory is pattern recognition it states
that we rely on patterns or features to
figure out what objects we're seeing
a cat has its set of characteristics a
long tail
fur and big pointy ears to start with
but we don't teach our children about
cats by listing all those
characteristics to them we show them
pictures because it's easier and allows
for much more flexibility
this and this are both cats even though
they're different in many ways but a
child is capable of catching all the
things features that are common in them
so we do the same thing with machines we
show them pictures tell them what's in
them and hope they figure out all the
important features by themselves
say you want to build an app that will
calculate the probability of pneumonia
from x-ray images similar to the one we
prototype for decision making in the
hospital setting
as we already established you can't
simply tell the computer what disease
looks like on a radiograph moreover you
want to distinguish between different
conditions
so the best approach is to give the
computer a vast amount of labeled chest
x-rays to extract valuable features on
its own
this is one of over 112 000 training
images we use for our pneumonia scenario
its pixel map is fed to the input layer
which performs convolution except this
time all filter values are random
completely incidental the results are
passed to outgoing layers where they're
multiplied and added many times before
arriving at the output layer there the
resulting output is compared to the
desired one in case of an error the
system starts tweaking weights over and
over until it gives a correct result the
process of training is basically finding
appropriate weights from the constant
feedback loop
this video might not look like much but
here an aerospace manufacturer airbus is
making aviation history in this
demonstration they present a new
autonomous technology that will soon
assist pilots of commercial flights in
taxiing takeoffs and landings
notice the pilot nervously hovering a
hand over the stick while the aircraft
soars into the sky on its own accord
airbus uses cameras sensors and software
powered by computer vision to let the
jet navigate the runway independently
and take off at the appropriate time in
harsh weather conditions pilots will
have to manually correct the plane but
in ideal circumstances like this they
can delegate such operations to the
computer and get busy with navigational
tasks and communication
on the ground the transformation
promises to be even more drastic
currently over 6 000 pedestrians die in
traffic accidents every year in the u.s
a number that increased by almost 50
percent since 2010. over 90 percent of
collisions happen due to drivers getting
distracted
but if we remove human error from the
equation we can significantly improve
road safety
in the first quarter of 2021 tesla
vehicles operating with autopilot became
engaged in a single road accident for
every 4.19 million miles driven in the
same period tesla cars not using
autopilot experienced one accident for
every 978 000 miles traveled so it seems
like self-driving tesla cars can be four
times safer than the ones operated by a
regular human driver even though people
are wary of autonomous cars stats speak
for themselves tireless and watchful
autopilots are projected to make roads
safer and more walkable as we inevitably
move towards the autonomous future
one of the most popular uses of computer
vision is facial recognition the unique
architecture of our faces allows
machines to perform fantastically
accurate face matching to validate our
identity or find missing persons
since the launch of their facial
recognition system in 2016 interpol
identified almost 1500 terrorists
criminals persons of interest and
missing people
these applications are our triumph in a
decades-long effort to make computers
see the same way we do
that said it's not always that positive
it's one thing when technology tracks
humans on the road but a completely
different one when it recognizes their
identities logs details about their
personal lives or classifies
marginalized groups to mistreat them
deep fakes can be entertaining in one
context while harmful and destructive in
others
shockingly accurate face search engines
are available both to those who want to
stop misuse of their pictures and to
those who want to stalk other people
this is a common dilemma that almost
every technology becomes subject to
should we stall the progress to protect
our privacy it seems that computer
vision has advanced too far and promises
too many benefits to have us ever choose
to stop progress in its tracks our job
here is to provide regulations and rules
to make sure that progress and ethics go
hand in hand
[Music]
you
