okay so good morning everyone and thanks
for coming to my tutorial on modern time
series analysis this is sort of my
interpretation of modern time series
analysis I suppose everyone would have a
different interpretation but to me
mainly what that means is we're not
going to cover auto regressive models
we're going to talk about them briefly
and then we're gonna mainly talk about
methods that are more computationally
taxing some of them were invented many
decades ago but I would still classify
them as modern given that they've been
far more successful sort of with the era
of big data and far more successful with
sort of readily available computing
resources but that doesn't necessarily
mean the ideas are very new but I mean
that's also true for even neural
networks right the ideas are also not
new a lot of the modernity comes from
the computing resources so that's how
we'll be thinking about this a little
bit about me and my background in this
subject so a few years ago I actually
gave another tutorial
on time series analysis at Syfy and that
one covered more exploratory data
analysis data processing and also more
traditional methods like ARIMA models
auto regressive models that sort of
thing so if you're interested in that
and you don't have a background that is
one resource of many resources that's
just the one that I've prepared you
don't need to be familiar with that
material for today but it is definitely
material you should be familiar with and
that should really be your bread and
butter if you are working with
time-series professionally even though
those men tear up those methods are sort
of low-tech or less modern they're very
much relevant today they still perform
exceptionally well and they also give
you a really good conceptual foundation
and similarly it's good to be
comfortable with data cleaning of time
series and all of that so that's a
little bit about me I'm also just a
little bit curious about people in this
room so I guess I'll just do a quick
survey show of hands if you already work
with time series data at work okay I
hope there will still be stuff for you
if not no hard feelings and you can
certainly pipe in with your own
suggestions and input as well
and how many people have never worked
with time series data okay or very
limited how about very limited okay fair
enough and how many people work in a
scientific sector broadly defined okay
and how many people don't work in
something you would call remotely
scientific okay
fair enough so good for me to know and
then just out of curiosity who thinks
they came the farthest to get here can
anybody beat Singapore that's really
tough yeah that's okay there was someone
on my site who claimed they'd been
traveling for two days but they didn't
come to this conference they might have
you beat though as far as like time or
whatever okay okay so here's an outline
of what we're gonna be talking about
just a very brief overview of sort of
what makes time series data special some
things you want to keep in your head
when you're looking at time series data
especially if you're new to it or
haven't worked with it for a while and
then we're gonna divide our discussion
into three components we're going to be
looking at state state space models for
time series just an overview of what
those are and we're going to cover two
methods specific to state state state
space methods we're going to talk a bit
about applying traditional missional
machine learning methods specifically
for time series and what approaches you
can take there and what you want to look
out for and we're gonna wrap up looking
at some deep learning for time series
okay so time series generally what our
time series it seems like everybody has
a good idea of what those are given that
you work with them right so the classic
example I think in every time series
book almost always you will run into
many many examples of something like
stock prices there are readily available
usually free fairly chaotic time series
so they're really interesting to look at
and of course nobody is all that good at
modeling them because if they were
they'd be filthy rich so it remains sort
of an open problem which is also
interesting but I want to point out that
many other things are time series that
don't get called time series and even
get analyzed in their respective
disciplines without anyone using that
word even though that's what they are so
on the upper right hand
of this slide I have an EKG diagram I've
never heard a doctor talk about time
series data but actually that's often
what they're looking at right they are
taking some sort of measurement over
time when you're hooked up to any device
in the hospital it has some kind of
digital or analog curve that's a time
series right so I think data science or
time series analysis or whatever you
want to call it has a lot to contribute
to this field and while we keep hearing
about the benefits of healthcare for
trying it for for data science or deep
learning we don't hear about it so much
in the time series context even though
there's actually a lot of time series
data in the health field as well as many
others similarly on the bottom I think
this is an NMR but I might be wrong if a
chemist wants to correct me and tell me
this is some other kind of spectroscopy
but this is some kind of NMR looking at
molecular structure right this is also a
time series by my definition even though
here the x axis is not time it's
actually just wavelength because you can
use many of the same methods on it and
what you have is you have a well-ordered
well spaced temporal axis so we are even
going to be working with some data today
where your temporal axis is not time per
se but it's something with this sort of
well ordered metric between different
points in time right so that's what
makes time series different from
anything that's cross-sectional analysis
and here's an example so what might I do
with something like that right if I
don't have time as an axis am I really
gonna do prediction well probably not
right prediction might not be the most
useful thing although maybe you can
contrive an example but you can
certainly do something like time series
clustering right you might have multiple
NMR spectra and you want to find a way
to classify them without having hundreds
of grad students leaving over it right
if you can find a way to replace those
grad students even better okay so tasks
for time series analysis what are things
people do well as usual visualization
and exploratory data analysis when you
do this for time series often you have
specific time related temporal questions
such as understanding the temporal
behavior of the data
especially if your time axis is actually
a time axis such as seasonality right is
there some kind of recurrence in your
data some kind of seasonality you can
also look for cyclical data not quite
this time the same as seasonal data
right so cyclical data will have some
sort of recurrence but it can be damped
and it can also sort of change its
period over time so an example of
seasonal would be something like the
weather and an example of an example of
cyclical would be something like people
hypothesize there's a business cycle
right some sort of boom-and-bust cycle
and banking or stock prices that sort of
thing and what's another thing we
usually want to do is sort of identify
underlying distributions and the nature
of the temporal processes producing the
data right we want to get a sense of I
have a hypothesis that I see this sort
of river level over time and I want to
get at sort of what describes that river
level more fundamentally that's another
thing I might want to do not just
predict but also have a way of
describing the dynamics what else do we
need to do well we can do estimation of
past present and future values right so
there's also this distinction between
say something like filtering and
forecasting or also smoothing we'll talk
about all of that in the context of
state space but the idea is if you have
noisy measurements and you can think of
all sorts of things as a noisy
measurement such as stock prices where
maybe you think the market got it wrong
and there's a real price that's separate
from what the market says or it could be
a medical sensor that has some sort of
known plus or minus so let's say even if
you measure your blood sugar within a
minute on the same device you will still
have you know 5% discrepancy and that's
something that's actually accepted by
the FDA right now for devices used by
people with diabetes so there's an
example of you know you can't get it
perfect and so you could have a time
series with just lots of measurement
error so you can have tasks of figuring
out well what was the true value at all
these time steps right there was a value
I measured versus the value that is
actually true and is there a way of
getting at that
next up classification that's another
common time series task this could be
both in something like the medical field
right can I
to find what kind of EKG I'm looking at
is is showing me normal heart or some
kind of arrhythmia this could be
classification of NMR spectra like we
talked about this could be
classification of retail behavior do we
have sort of distinctive trajectories
through our website right things like
that can also be time series analysis
and then another major task is anomaly
detection right can we figure out which
points are the problematic points right
and the better we can do that the less
often say a bank will have to cancel
credit cards or a doctor will have to
run tests that weren't really necessary
versus run the ones that we really need
we don't always think of these things as
time series analysis but increasingly we
have panel data for all these things
right increasingly for things we think
of as cross-sectional data right surveys
or whatever we're better able to track
people and so we have a sense of how
things are evolving over time okay
time series data versus cross-sectional
data what are some of the challenges or
things to keep in mind well number one
there's more opportunities for missing
data points it can be quite challenging
following the same people or even
samples through time and I imagine
anyone who has worked especially in sort
of human related Sciences knows about
this problem right good luck finding the
same survey respondents year in and year
out as sort of an epic task even for
really simple scientific data that can
be a problem for example when I was
putting together the data for this
tutorial I was trying to use some
Colorado River data from the federal
government and I kept getting bombarded
with this sort of bright red text
indicating what measurements they had
discontinued due to funding cuts right
and so time series especially anything
that looks at government data sets is
even affected by politics to some extent
which is kind of funny right you'll just
run into it more than you might if you
were just doing panel data second major
point there's a high degree of
correlation between data points values
in the past almost always and we hope
predict values in the future right so
you have to think about what your errors
look like you have to think about what
your relation to points in time looks
like as opposed to correlations where
cross-sectional data where you tend to
throw things in and these are different
people and hopefully to first order
they're not super correlated with one
another so this is good for prediction
but it can be bad for models that assume
independent
inputs right so for example when we are
looking at some machine learning
techniques or when we're thinking about
how can I apply sort of non time-series
statistics to time series analysis we
might very readily violate the
assumptions whereas if we're sort of
careless in general about running linear
regression that's not so likely to be
true it's very likely to be true in time
series analysis and then there's also
just the the hassle of dealing with
timestamps or other measures of whatever
your distance metric is along your
temporal access right you have things
like time zones frequency irregularities
and so on even if you only work with
sort of scientific data where things
like the exact timestamp like day of the
week doesn't matter because as far as I
know that doesn't affect physics you
might somehow find that for example your
timing measurement is a little bit off
right you have some sort of bias problem
so it doesn't go away even in that case
okay
other characteristics data is collected
sequentially one axis is monotonically
increasing right so as I mentioned
that's my definition of a time series
it's got to have a meaningful axis with
measurable distance it doesn't have to
be time we do want to characterize
structure across data points right so
again seasonality in cycles
autocorrelation and trends who is
familiar with the concept of
autocorrelation I just want to make sure
that's fairly familiar so just a quick
overview for those who aren't
autocorrelation is just how a point at
time T is correlated say with at the
time T minus one for the entire time
series right so at time T how do I
relate like this value how does it
always relate to the value before it
right in a probabilistic sense that's
autocorrelation and then finally
stochastic behavior even as the
behavioral regime right so in
cross-sectional data we're used to
thinking about oh if I see sort of
qualitatively different behavior I might
be identifying sort of different
subgroups in my population right maybe
I'm looking at two molecules or sort of
two types of consumer or whatever but in
time series you can just have within the
same series some kind of stochastic
shift to a different regime and you need
to think about how to identify that that
sort of change point identification okay
here's a slide on autocorrelation and in
case anyone's not familiar
with it even if you are I guess the main
point I want to make is that you don't
always know from eyeballing your data
what it's going to look like right so we
might think oh I'm just sort of looking
at this top panel I have a sense of how
correlated these things are but I
personally when I then look at the lower
panels this series is much more sort of
self correlated and correlated than I
would have thought from just looking at
it right it looks kind of noisy and
maybe there's some kind of seasonality
but not to the extent then that I see
with the ACS so that could be good to
keep in mind to your data is never clean
enough or never sort of noisy enough or
white noise like enough or or beautiful
enough that you should not run basic
Diagnostics ok special concerns for time
series data so correlated errors right
and on the right-hand side on the upper
right-hand side this is actually the
errors or the residuals of a time series
model so at this point I say already
wrote a model that I thought I counted
for the seasonality and the self
correlation but what we can see is on
these blue lines give us the
significance levels you can see that
this model actually is failing to
describe all the autocorrelation of the
time series right so this is something
you need to keep in mind for time series
there's special Diagnostics you need to
do that our time aware in addition to
having time aware modeling we need to
have time aware Diagnostics
cross-validation also will usually look
different in a time series context right
so there I'll draw your attention to
this plot in the lower right hand corner
for cross-validation it depends on what
you are modeling and the nature of your
data but often if you are trying to
predict the future you want to make sure
that future information does not leak
backwards in time influence your model
and make your model look a lot better
than it does that sounds like common
sense right we're all professionals we
would never ever do that right and yet
many people myself included will find
themselves very embarrassed when
something does much better in training
and you think you properly cross
validated it and you roll it out to
production which is really the ground
truth and it doesn't and so you can
really never ever ever be careful enough
about the cross validation and the next
bullet
we'll look at so one thing you can do as
is illustrated in this lower right hand
corner is you sort of roll your training
data and your testing data forward in
time and the idea is is you never want
to test on data that is in any way older
than what you trained on because you
don't want information propagating
backwards that's another thing about
time series and temporal axes most of
the time right the information should
really only propagate in one direction
you can write models that are the
opposite right you can have causal time
series and you can have the opposite
where information goes backwards in time
that doesn't tend to be interesting for
modeling though right that doesn't tend
to be the questions we're trying to
answer at work or in research um few
other notes on look ahead right so this
is just for cross validation just
because you've covered this does not in
any way mean you've avoided look ahead
just because you've properly crossed
validated there are all sorts of other
things to keep in mind time stamping
right just because you have data
available time stamped at a particular
time as an input into your model doesn't
mean that that time stamp reflects say
when you would actually have that data
right so you might have for example
let's say you're trying to predict the
jobs numbers and you might have some
sort of survey of how people are feeling
and you have the date of the survey when
it was taken but you forget that maybe
it doesn't get input into the system
available to you for a week
and that's the sort of thing that can
happen when you move from research to
production and suddenly you realize that
your inputs that you thought were
available actually are it right and
maybe it took place in the past but it
hasn't been registered yet so there's
another example of look ahead there are
many more and as far as I know there's
not sort of an automated way to diagnose
look ahead you just have to really keep
your eyes open and it depends a lot on
what your field is and sometimes it's
not even an issue depending again on on
the field but especially tricky with
human data
okay so just brief overview any
questions or comments or anything people
with experience would want to throw in
on top of that I know it's like 8:30 in
the morning oh yes somebody does
only in the statistical sense so I have
no frequency domain examples here
although I can speak about it briefly
where we get to deep learning because
certainly there's a lot done there and
that's a good thing to highlight right
so there's a whole sort of other domain
of time-series analysis that looks at
time series in the frequency domain very
successful especially in certain fields
and actually for those who are
interested in a brief tutorial I have
that in my first tutorial posted so
that's good pointer is yes no I won't be
discussing data imputation although all
of the methods we use can be applied to
data imputation so I'll make sure to
comment on that as we go but there are
no examples and that's that's another
given that what I highlighted about the
missing data this can often be important
okay okay so let's talk about
state-space models we'll start with some
background on Box Jenkins ARIMA modeling
so ARIMA modeling if you haven't used it
you've probably at least seen this term
ARIMA or maybe you've seen ARMA or AR or
ma you see these are all sort of the
same class of models statistical models
developed a very long time ago at this
point sort of early early to mid 20th
century as you can see though they can
have despite you know sort of being old
traditional they have excellent
performance especially on small data
sets so here I have the example of the
airline passenger data set very famous
data set if you look at time series
textbooks this is always there you can
see so the forecast versus the observed
are pretty close right this is already
pretty good and so one of the
interesting things about modern time
series analysis right are sort of
non-traditional methods however you want
to call them is you always want to make
sure that you are actually contributing
something that you couldn't get say with
an ARIMA
and that is surprisingly difficult not
just that it's also surprisingly
difficult sometimes for an arena model
to those to beat an even simpler model
which is just sort of your know model
which could be something like I will
predict for time t plus 1 the value I
have at T and just leave it at that and
that itself can be an extremely good
model so I just want to point out that
we are starting with a fairly high bar
especially in certain domains right if
you have certain kinds of data sets that
don't have too much noise or they have
say a strong seasonal component you can
do very well with this sort of model so
what does an ARIMA model look like well
I just want to show you sort of one way
of thinking about it if L is your lag
operator a lag operator means if I apply
L to X sub T X at time T I get X at t
minus 1 if I apply l squared to X sub T
I get X sub t minus 2 right so it's just
a way of expressing that we're moving
something back in time in this case now
my alpha is my alpha sub I on the
left-hand side those are my auto
regressive coefficients and my theta is
on my right hand side are my moving
average components so this is sort of
the traditional way to express an ARIMA
model it's some sort of or this is an
ARMA model rather it's some sort of on
the left way of operating on the data
itself so this is going to have
coefficients that apply to pass values
and then on the right you're going to
have coefficients that apply to sort of
past errors right and those could be
noise and the system stochastic noise
they could be measurement noise you can
interpret it in a number of ways but the
idea is that your future value is in
some way going to depend on your past
values and your past errors and what
sort of values you choose for P and Q
your summation are part of how you do
your model tuning now if you go to an
ARIMA model now you have all of that
plus a difference so all that means is
you have this additional 1 minus L to
the D which means you're going to
difference your time series so that
means instead of drawing my original
time series like my air passengers with
the absolute number at each time stamp
I'm going to convert my time series for
a first difference to the Delta so the
time series becomes a series of delta T
Delta X's instead of X's right so you
have the exact same in
Meishan you haven't lost any information
except maybe your offset and what that
does is it often converts your time
series to a more tractable form maybe it
won't have such differences in
volatility over time maybe it won't have
the trend maybe it will now be eligible
for models to apply that wouldn't
otherwise right for example or EMI needs
stationary data so often you're going to
be different saying to get that
stationary data so that that's the model
that's as much as I'm going to get into
this model for those who don't know it
yet that all you sort of need to know is
this describes future data in terms of
past data and past errors it's as simple
as that and I should also mention
fitting these things is not easy fitting
these things can be a bit of an art or
it can be a matter of sort of optimizing
your aka information criterion depending
on who you talk to they're sort of
different schools of thought but
everyone agrees this is a fairly
difficult problem and it's easy to sort
of over fit it's easy to specify an
overly complicated model and so on and
that's the sort of thing where you can
then get embarrassed later when you roll
it into production and it turns out to
not be as good as you thought it was
okay so why are people not happy with
ARIMA
despite for example this beautiful plot
I showed you right to me this already
seems pretty good right I'd say if I
could get this on on everything I'm
trying to predict in the world that's
already you know enough to buy a house
or buy some stocks or something like
that but there's a lot plenty of
problems right so I think the first one
you probably noticed as I took you
through it is it's not especially
intuitive right I just showed you this
equation the equation itself is a little
confusing if you haven't worked with
these sort of forever but on top of that
when you get coefficients that's also
still kind of confusing right what does
it mean that you know X sub t minus 1
the coefficient is 0.3 or 0.7 I don't
like what what's the difference between
the point 3 and the point 7 I don't
really know and especially when you have
multiple inputs right so if I'm saying X
sub T is going to be a function of X sub
T minus 1 X sub 2 minus 2 X sub 2 minus
3 and they all have coefficients and it
really becomes kind of difficult to
think about what that dynamic looks like
and you'll see this in time series
textbooks mostly when they give you
examples they'll stick to something with
just one or two inputs right so like an
AR 1 or 2 model precisely because the
the more calm
it gets the more difficult it gets to
even think about where different
behaviors are coming from there's also
no way to build in an underlying
understanding about how it works right
so especially if you come from a
discipline with sort of a well developed
set of time series data maybe people
have been staring at it for five years
or a hundred years right depending on
what discipline you're in usually
there's some knowledge about that system
and there's also some theories people
want to test right so they might even
say well it's not enough to be able to
predict or describe this model with some
kind of dynamic that's a RMA whatever
that doesn't mean much to me but I'm
aware that say there's a random walk
element to this kind of data or I'm
aware that there's a cyclical element
which can't really be described with an
ARIMA or there are external regressors
I'd like to include and then you know
you get into more complicated models or
you start looking for models that are a
little bit intuitive also some systems
cycle more slowly or stochastically then
can be easily described with an ARIMA
model so ARIMA models do really well
with sort of short cycles right maybe
seven days on daily data things like
that but when you're already even
getting to sort of 24 hours and hourly
data
ARIMA models can get really ugly and
it's unlike machine learning where you
sort of throw more data at it and you
might get a better result
ARIMA models sort of level out so they
do really well with small data sets
arguably they still beat everything else
because they're just so you can give it
just a 40-point time series and it will
do really well and good luck doing that
with a tree or a neural network but on
the other hand when you give it 40,000
points instead of 40 it's not clear that
you get much better performance okay so
enter structural time series so these
have been around for quite a while
they're not new but they're sort of much
easier to implement them they used to be
a few decades ago right you get much
more computing power now and what that
means is you can have a way that's a
little bit more intuitive of
understanding your system so here's an
example of a model that has a level
component and a seasonal component and
then there's a component they call
irregular is just sort of everything
else that can't be explained that sort
of the error term we're going to and add
at the end and so he
arguably we already get a much better
sense right so compare this sorry to
flip back but compare this to that right
this I don't really have a sense of why
my models doing well or what's
contributing to it right I can see that
my model is describing the seasonal data
I could look at my coefficients from my
ARIMA model and try to puzzle it out but
there might be quite a few of them so
it's difficult to see where as here I
feel like I can look at this and say
well you know we have this underlying
level that sort of stays quite steady up
until like sort of 83 and then it takes
a dip and I feel like I know a little
bit more about what's going on and I can
possibly use this as supporting evidence
for a hypothesis or maybe evidence that
goes against my hypothesis about how my
data is behaving so these are how we use
structural time series interesting thing
they can also be expressed in a Rhema
form so in a way we're not actually
doing anything novel we're not actually
if you're going to be like sort of a
very formal mathematician about it
we're not contributing anything new but
to my mind we really are because we have
a way of inspecting and enhancing sort
of the human intelligence rather than
the statistics these are fit via maximum
likelihood or a common filter right so
you can have an MLA interpretation of
that you can have a Bayesian
interpretation of that interestingly
common filters again not super modern
I believe they go back to the time of
the Apollo mission at least that's the
story everyone likes to tell at
conferences that they actually wanted to
use a different way to estimate the
trajectory of the spaceship but they
realize they didn't have the computing
storage space where they'd need to store
many points in time to continue their
estimation so they had to roll out
something where you only need to store
one data point at a time and you just
update and so that's part of the beauty
of something like a common filter method
is you estimate your whole time series
and you just keep a couple of numbers
around so even if you're estimating a
time series that's millions of points or
you know 10,000 points something more
reasonable you're only keeping one
number you're not storing the whole
thing so this can be really good on on
small machineries such as the machinery
of you know the computers of the 60s and
70s still useful today these are largely
developed in econometrics so if you have
an econ background you probably have
seen the
quite a bit whereas they are otherwise
unfortunately fairly underutilized they
offer insights into the underlying
structure as we just illustrated and
it's also possible not in the package
we're using but there are packages where
you can inject Bayesian analysis via
priors onto your parameter so you can
sort of build these out quite
extensively state seus models generally
right what what's useful about states
state space model it's also compared to
traditional ARIMA models they don't just
offer forecasting and a set of
coefficients they sort of offer three
different stages and these are all
inherent to fitting them stage number
one is filtering filtering is this
remember I have this underlying
assumption I have some sort of noise in
my data or are these underlying
components but I only get to measure the
one thing right filtering is a way of at
time T I take all the data from zero up
to time T including time T and I use
that to make an estimate of sort of what
is the true value right now right
there's a value I'm seeing but that
might be fairly noisy versus a true
value and so you can see why that would
be useful on the Apollo mission right
we're actually is our spaceship right
are our sensors say this but is that
actually where they are because it's
pretty important to have a good idea of
where they are next step is prediction
right so in this case at time T I'm
gonna look T plus 1 or maybe even T plus
K into the future and that's something
else I can do with a state space model
right so I can both update where I think
I am now I can think about where I am in
the future and then finally I can do
smoothing which essentially is you have
all of your data or all the data of
relevance up to say Big T and I want to
go back to some earlier time point and
say well at time 10 whatever at time 10
I thought I was at you know this point
but now that I see where I ended up at
the end I realized I even get additional
information I can sort of back propagate
that and get an even better estimation
of where I was at that time so that's
called smoothing so this is all
functionality the filtering and the
smoothing that you don't get with an
ARIMA model right so you actually do
have new technology from having this
perspective common filter I thought
about having everyone code this up but I
decided this was not the best use of our
time so this is just to sort of give you
an overview
is a just sort of a high level overview
wiki graphic of how this works you start
with some sort of prior knowledge
usually just some sort of vanilla
Gaussian distribution you have your
underlying state and you have some model
for how you should update that
underlying state so you make a
prediction as to where you think is
going based on your knowledge of the
system dynamics and its current
trajectory once you make the prediction
you use that to do your update right you
compare how your prediction right at
time T before I measured at time T or at
time t minus 1 the state of the world I
saw was such that I predicted my time
series would go this way or my
measurement right my measurement would
go this way but at time T when I got to
time T and got my measurement my
measurement had actually gone the other
way right and so this becomes a way of
balancing well my forecast wasn't
totally meaningless right so I don't
just discard that because I have new
data what I do is balance why does what
I'm predicting versus what I actually
measured why are they so different and
how should I combine them to have
something reasonable rather than just
picking one or the other so you do your
update step and then you go back to do
your next time step and then so this is
sort of the forward propagation and then
going backward in time smoothing is very
similar computationally but it's a
separate cycle so there's your common
filter the underlying model for what
something looks like is something like
this so with a state space you have an
underlying state and then you have what
you can measure right so your underlying
state X is going to be some sort of
model based on on your right-hand side
of your equation you have F sub K times
X times K minus 1 so how do you update
your state based on what was earlier
that looks like a lot like an auto
regressive model but on the other hand
you also have the option for inputs such
as be K times UK that would be what to
your knowledge was happening at time T
not time t minus 1 right so for example
in the case of Apollo X sub K minus 1 is
where they thought Apollo was at time K
minus 1 but u sub K was well what are we
doing with our motor or our rocket like
how are we positioned in our propulsion
system at this moment or is it even on
that's what's described by u sub K right
so you can also if you know sort of what
your astron
they're doing up there you can factor
that in as well and then you have some
sort of error terms so there's there's
your description of your underlying
state and then you have your measurement
which will cause these sub K here right
you recognize that in some way your
measurement might not be directly the
same as what your X sub K is right so in
the Apollo example your state where you
are and your position that you measure
you know should be the same thing we
don't have some sort of translation but
it could also be that these things are
not in the same space right it could be
that I'm actually X maybe I'm measuring
only velocity and acceleration not
positioned and then somehow with my
observation I only get position and I'm
trying to find a way to translate
between them so the point I want to make
here is just that these are really
flexible you can describe all sorts of
dynamics but it does require that you
have some kind of hypothesis about the
dynamics of your system to really be
interesting right so you might not want
to throw this out a model of say really
noisy stock data and you're sort of like
I have no idea what this is so like
let's make up a bottle and just put it
in because you probably won't get
sensible outputs but a model like a
rocket where you have Newtonian physics
and you have a flight plan makes a
really good case for this okay and this
is just spelled out more in detail you
usually won't have to code this up
yourself but it's good to feel empowered
and able to do so it looks really awful
but actually it's just a bunch of matrix
multiplication and inversion so good to
remember that you can look it up and
code it if you want okay and then how do
these models tend to get evaluated
usually with your K key information
criterion your AIC so just for anyone
who's not familiar with this I want to
put it up it's two times your number of
parameters minus two times your log
likelihood right so you want it to be
more negative when you're evaluating
models this is just one way you would
might evaluate models such as
state-space models okay so here are some
models that we're going to look at in a
in a jupiter notebook right after this
so we're gonna look at things like local
linear trend or smooth trends so this I
pulled directly out of the stats models
package for the unobserved components
model so let's see can you see this no
yes here's my MA so okay so it can give
you either two ways to select these
either of these can be your inputs and
then
this tells you sort of the underlying
model that it's using so in this case we
have an underlying model where our
observable is some function of the
underlying state plus epsilon some kind
of error and the underlying state itself
is a function of its former state and
also some sort of velocity type terms
some kind of trend term of where it's
moving that's our beta plus some sort of
error and then our beta itself can
change so that's our local linear trend
and then take a look at the smooth trend
and see if you spot the difference they
look quite similar but there is one
difference so what's the difference
Yeah right so no no stochastic term here
and the smooth trend in the middle
equation there's no stochastic term
right so that's just to give you a sense
of the distinctions that people make
people sort of make these slight tweaks
and they have different names but it's
always best to just go look at the
equations to see what they look like
okay so now that I have bored your pants
off with a bunch of slides I think it's
time to open the first Jupiter notebook
okay so people should feel free to use
either the instructor or the student
version student version is if you don't
want to be able to cheat you know that
can be a good way to discipline but you
should you can be your own judge of what
makes the most sense okay so the first
thing we need to do is obtain and
visualize our data so in this case I I
had downloaded some two different
measurements of global temperature data
yes yes okay so thank you excellent
point
so from the base folder these are in the
state state state space models folder
okay I'm sorry so for anyone who walked
in late no worries the set of Jupiter
notebook slides are available on the
slack Channel and a zip file and
otherwise someone has helpfully posted
them here I see that's the older version
sorry about that okay I really
appreciate that so just want to confirm
do you most people have the student
instructor version let's see so if you
scroll up in the slack this was sent 913
p.m. last night it's the second version
rather than the one that's pinned and I
can upload it again so it will be the
newest thing
okay so that's the newest thing
so in that case you should email Syfy at
an calm and they will just tell them
that and they will almost instantly
reply
I think someone's just Manning that
continuously other questions okay
okay so for those downloading it's a
pretty small zip file so I think you
should have it pretty quickly I'm just
gonna just slowly look at this data and
you should be caught up pretty quick Oh
github is updated okay great thank you
so github is also updated if you find
that more useful
that's this okay okay so we're just at
obtain and visualize data so this CSV
should be available in the zip file and
I have the source listed here this is
just two sources of global temperature
data obviously climate change data is a
great source of arguing about time
series a great source of possibly
ambiguous time series depending on your
politics but it's a very rich source of
debate so I grabbed some of that and the
point of this data set is they have sort
of two sources of the data now we can
compare so if we load that data and take
a look at it the first thing I do is
just plot you know just basic time
series plot
I also recommend sometimes that you
don't plot the whole dataset all at once
right so for example here I just looked
at the first hundred data points anyone
have an idea why I recommend that or why
you might feel the same that versus
let's look at this one yes let's see
maybe I was a man how's that
okay so I'm not especially satisfied
with these plots does anyone feel
satisfied with these plots I hope not
okay so that's actually the first
exercise I have for you I'm not gonna
scroll up for those who want to have it
be a secret until you figure it out but
so its first coding exercise for
everyone I'd like you to take a couple
of minutes to figure out what's wrong
with the way the data looks now and the
plot and how can we fix it because it's
not even just an aesthetic problem
there's an error right now so this would
be one of these if I presented this to
my boss I should be really embarrassing
kind of situations so think about what
that is and how you could correct it
okay so I think we had already covered
the problems right we've got data types
that are not especially helpful we've
actually combined different measurements
into one time series when actually that
doesn't make any sense I should have
been a little wary of that given that
already even in my plot I don't have
time displaying right I have just sort
of a random index so the first thing
we're gonna do is we're just gonna pivot
our data and then if we plot our data
incidentally it seems like that has
reordered our date index appropriately
and now I'm seeing something more like
what I expect and now I'm looking only
at one type one time series instead of
two right I actually have two time
series in my data set
I should not plot them as one and as you
see here right unfortunately often and
this is something that's tricky about
time series right you can be plotting it
the wrong way and it will still look
reasonable right especially if I didn't
have domain knowledge about sort of
global warming that would have looked
fine to me and maybe I would have
foolishly carried on another problem
though is I still so now my my dates
seem to be an order right I could check
more extensively but they're still only
useful as an index whereas I would like
something a little more useful so let's
just go through this we won't do this as
an exercise how can we make the index
more time aware well nicely pandas
provides date time and also period time
indexes so we're gonna use date time and
if we do this we see we now have a date
time index instead of a base index class
so what do I get from that well already
notice compared to this plot which I
already thought was
okay I've now got a pot where these are
not being treated as strings they're
being treated as actual dates and so
pandas can fill in a little bit of
background knowledge about what sort of
labeling would be reasonable and then I
also get sort of Handy label indexing
like this so now I can now index a by
year right just the 1880 is going to
show me all the 1880s which also is just
going to make my life easier I can do
some plotting like this so now I plot
both series from 1880 to 1950 and get a
sense of how correlated they are or I
can just plot everything from 1950
onwards and again get all that nice
functionality with the date time so this
is just a reminder that that's available
to you and it can be very helpful and
obviously always look carefully at your
data before you get started okay so
quick exercise how strongly do these
measurements correlate contemporaneously
what about with a time lag so now we've
got two groups of data just quickly two
minutes how helpful are they for
predicting one another or for
understanding one another
we can it can depend a lot on from one
package to another I guess one critique
I would even have of the Python spaces
there's not a lot of uniformity around
this because the functionality is spread
out among many packages and not to be
anti Python because Python is my first
love but are actually does a better job
of having more of a unified interface
okay so how strongly do these
measurements correlate contemporaneously
there's all sorts of ways to think about
that but just want to point out we can
you know just do simple things like
scatter plots right so now I'm plotting
the two measurements the GC a G and the
G is temp against one another for the
exact same time period so at any given
time they seem to correlate pretty
strongly if I you know if I saw
something like that it might work more
often I would be delighted similarly we
can ask what about whether years offset
right what do these have any sort of
predictive value for each other so in
this case I'm plotting 1880 to 1899 for
one variable versus 1881 to 1900 for the
other so I've just offset it by a year
what does that look like having inste
look more like the real world and what I
see at my job anyway but is it terrible
well it depends on your domain right
maybe for predicting air passengers this
would be terrible maybe for predicting
housing prices it's not so bad we can
look at the Pearson R and we see it's
not zero right I mean actually this
probably looks worse than it is so it
can also be good to have different
measures visual and numeric and again
just as a reminder we can sort of look
at the data with standard standard tools
from pandas and then if we want to get
an idea of our date range we can look at
our min and Max for our index right so
all sorts of standard operations okay so
now we're going to throw this into an
unobserved component model I'm only
interested in training on the data from
1960 onwards just personal choice I
thought that's a little bit more
interesting that's sort of where we see
things taking off we're going to define
our model so we have our model per M
and all I'm gonna stay say is I want a
smooth trend for my level I want no
cycle and no seasonality that's sort of
my first pass based on these clots we
did here right so looking sort of at
1960 onwards I'm like eh I don't know if
I seem much cycling or seasonality
arguably I could be more careful about
this but this is gonna be my first pass
so I build my model with a dictionary I
fit my model so I'm gonna make an
unobserved components object from stats
models right so stats models has a whole
time series analysis API including a
state space API with unobserved
components I give it my data and I just
give it the model to unpack right and
there's other sort of parameters you can
tweak and you can read up about those in
the documentation but this is as simple
as it is for a first pass I'm then gonna
fit my model and I'm gonna plot the
components right because this is what I
was talking about as being like the
great thing as compared to an ARIMA
okay so I've plotted my components what
do I see here I've got my predicted
versus observed plot ah so how do we how
do we feel about this fill seems like
we're always within our confidence
interval it seems to follow pretty well
this is a one step ahead prediction
right so you also would want to think
about whether that's sort of interesting
to you is it interesting to predict
global temperatures one month in advance
or would we like to try a more will try
more below we also can look at our two
components right so we have a level
component and a trend component what do
we think about these how do these look
compared to what I showed you in the
PowerPoint I would say not great right
they they both sort of look like Wiggly
lines I would say that I don't get a
whole lot of insight out of this when I
look at it I say hmm level versus trend
I mean they both it looks like hmm okay
the model mostly mostly gives the
absolute value of the time series to the
level and then it's almost like the
trend is just whatever was left over but
it doesn't look like a meaningful trend
so thinking about that I think okay well
let me revisit
that and we will revisit that in a
couple of steps we might also want to
plot our predictions so to do that we
can also set a greater time horizon to
do at the end to do dynamic prediction
so if you move down to the next cell
I set the number of steps I want to
predict forward as being 20 so 20 months
arbitrarily chosen so now I want to get
predictions and I want to do the last 20
steps is dynamic meaning that now I'm
not going to update at each step right
that's when we do our one step ahead
forecasting now for my last 20 steps
it's just gonna roll with those 20 steps
it's just gonna take what it predicted
and that will be its new input take what
it predicted that will be its new input
it's gonna move forward so if I do that
this is what I get and I want to draw
your attention to the end what do you
notice at the end sort of this upper
right-hand corner it's just sort of a
flat line right so that's another thing
you're gonna notice with these models to
be aware of right when you're doing sort
of multi-step horizons they're not so
great in the sense of since there's no
error they'll just assume there is no
error and they're just gonna sort of
keep going along whatever smooth
trajectory is established by the
underlying model that can be a good
thing or a bad thing depending on what
you want to do this certainly will look
a little bit different say from an ARIMA
model where you will sort of see more
wiggles and things like that okay and
then how does it do how does this model
do well it looks pretty good for one
step ahead right but can I can I really
decide if it's really good just based on
one one plot well no especially if I'm
not comparing this to what would my null
model look like right I need to think
about what my null model would look like
you can also do a cleaner plot so this
is just to illustrate if you want to put
in some confidence bounds again this red
now is showing your dynamic prediction
your dotted lines are showing your
confidence bounds they don't look
fantastic either right so here's another
difference say it compared to if you're
used to an ARIMA model your confidence
bounds will rapidly diverge because of
this unincorporated error that you no
longer able to handle and we can look at
this also more up
to get a sense and as you can see what
we discussed right your your forward
moving prediction is just going to be a
flat line for these simple models okay
so next exercise for you all is consider
adding a seasonal term for twelve
periods to the model fit above and does
this improve the fit of the model so we
had initially rejected seasonality but
should we revisit that and how do you do
it if you're not sure go to the stats
models API you can google that and look
for the unobserved components
documentation so let's take a couple
minutes to look at that so also I've
just posted the slides I had a request
to post slides so those are now also in
the time series Channel ok so how do we
add a seasonal term here we keep a local
linear trend and now we add a seasonal
parameter which we set to twelve by
twelve its monthly data right so what
what if anything else would I expect
right so if the if the earth has a
seasonality that would be my guess as to
what it is and if we now apply our
components we see something interesting
now alright so we see what looks like a
very regular seasonal component which
again shows how silly I was not to have
done more exploration of my data at the
start when beginning to think about what
kind of model would be appropriate we
also see now that sort of the trend
component has gone to just this
negligible thing it's not really adding
to the model we have a level component
and a seasonal component and that
already seems to do a pretty good job so
how does this compare to the original
model if we wanted to compare them we
might think about for example comparing
the correlation of the two models with
the data and we see these are sort of
indistinguishable right so that to my
mind is not a very helpful metric and
shows how we should be skeptical because
visually clearly one was doing a better
job of describing the data are offering
more intuition to us than the other so
that shows sometimes that these
numerical measurements might not be very
informative
and what about if we look at the mean
absolute error in that case in both
cases we see that the new model does
slightly better but yes okay so if you
look at the I'll go up and hopefully you
have this in your notebook as well it's
this is the seasonal component so the
seasonal component is not zero it's it's
clearly significant and it's showing
sort of a pattern that makes sense based
on our domain knowledge of some kind of
12-month cycle of the earth's
temperature okay I'm glad people are
just interrupting me when whenever is
helpful because that's what this is
about
okay so point here is especially with
structural models people sometimes try
to optimize them just looking at
something like the AIC and that's great
and it's at least some sort of measure
but arguably especially because
structural models are mainly just sort
of helping your intuition and helping
you understand the underlying dynamics
you might find that a model that looks
much worse visually that isn't offering
intuition doesn't do that much worse
right but then it's not offering any
value so a big part of thinking about
when to use these models and how to use
them is what are they teaching you about
the data and it's really interesting to
me that in this case we see an
illustration of how just exploring
models pointed out something that we
sort of initially missed about this data
because we did insufficient exploration
that's another way this can be better
than a Rhema for example with inner
Rhema you might notice that your model
isn't fitting your data very well but
you won't get such easy insights into
why versus here we say oh look boom I
added a seasonal term and I can actually
see the seasonality rather than just
seeing a coefficient okay so now let's
explore the seasonality more what's
something else we see here we see that
our trend component wasn't especially
useful so why don't we get you why don't
we get rid of it all together how do we
do that we switched to a local level
model instead of a local trend model and
we keep our seasonality so if we do that
we really now we get something that
looks really beautiful in terms of not
having extraneous portions of our model
like a whole trend component that we
don't really need we also see the
seasonal component again very strong
signal interestingly it is not
I'm varying right into a sort of uniform
and we see a sort of a level drifting
attending to drift in the positive
direction right so to the extent that we
want to have debates about climate
change this is one insight of like oh if
I fit this kind of model I see gradually
that the level is going up and I see a
seasonality and that can be sort of
first-order way to talk about this data
when you're first getting into it
question in the back
No okay okay and then we can again sort
of look at the mean absolute error and
we see that we are continuing to improve
these are sort of small incremental
improvements not massive improvements
but mainly we are having a more
parsimonious model that gives us better
intuition into our data okay and then
once we see this we realize we really
should have revisited our data and
thought about the shape of it right so
if we sort of plot here we can see more
seasonality by looking more closely
although we see that this is not as
clean of seasonality picture as we saw
say with the air passengers data right
so it's a noisier seasonality and yet
our structural model did a pretty good
job of capturing it nonetheless okay so
final exercise and then we're going to
take a break a common null model for
time series is to predict the value at
time t minus one for the value at time T
so how does such a model compare to the
models we fit here just take two minutes
to compare those right have we actually
done anything with sort of our fancy
structural time series model or are we
just kidding ourselves something you
always have to be asking yourself with
that fancy methods
yeah exactly so basically it will just
have a coefficient 4 it will have 12 11
different coefficients right you don't
need 12 right you need n minus 1 for n
seasons and then you'll have a
coefficient per and you'll say Oh at
this point we add or subtract this much
and that's where you get a seasonal
component yeah so it can handle
something like an incomplete cycle well
I should I should really be saying an
incomplete seasonal cycle as compared to
a psycho because structural time series
can also handle a cycle what a cycle is
is when you have some sort of repeated
pattern but it's not as regular as a
season so for a season the way that
these models are fit is they're sort of
a different component per season so
there will be 11 components to describe
the 12 season model right because one of
them will just give a null component
like everything every other kind of
model that does this and then if you
want to fit cyclical data what that will
usually fit to is sort of a sine curve
where what it's then trying to fit to is
to determine the appropriate frequency
to model the data and you can also add
things like a damping parameter so you
have sort of a start with a cycle but
then usually what you want to do is you
want to both frequency will be time
dependent unlike a season where the
frequency is fixed and it will also have
the option for those who are either
physicists or remember of some sort of
like damping envelope is something else
you can fit too but that's not what we
see in our data right we in this data
arguably we see some kind of seasonal so
in that case so for seasonal you will
really have like S sub 1 S sub 2
S sub 3 and so on and it will just
assume that F sub 1 is what you start
with
it doesn't actually care what we
consider the first one it will just take
that and as long as it goes around in a
circle that's good
okay so how does our null model compare
to what we did so yes
so absolutely so this is even a problem
with ARIMA models right which seems sort
of more traditional statistical it's
what's interesting for example just to
back out for a Rhema and then I'll come
back to this case you can actually have
models that look different they look
like they have different parameters but
actually if you do the polynomial math
you can factor out and you realize you
have like sort of extra terms that
literally just cancel out and you are
literally fitting a model that math
tells you is too complicated like your
model makes no sense mathematically so
even with a simpler method like that
with structural time series I I consider
them more useful sort of as an
exploratory tool it's not something I
wouldn't fly an airplane or treat cancer
based on the outcome of this now that's
just me many Khan Academy Trish ins
might feel a little bit differently the
more data you have the better if you
move this into a Bayesian model where
you have a much stronger prior that's
also better and actually Google has a
great Bayesian structural time series
package because then you at least have
stronger inputs into what you want so if
you inject say a strong prior from a
Bayesian sense and I would trust these a
little bit more versus if you are just
using your default Gaussian and rolling
with what it is absolutely and these can
be very easy to overfit like many sort
of data intensive computation intensive
models and there is no guarantee also
that you get to the optimum fit for your
data but arguably that's even a good
thing right because getting to your
optimum fit for your data could be
overfitting as well yes
so this depends on the discipline
certain disciplines will have very
strong priors not in the Bayesian sense
of the shape of the distribution but in
the definition of the model so for
example if you're modeling the stock
market you have to make a very strong
case for using anything stronger than
say a random walk and you'd have to have
a justification another example I know
of is people modeling hydrology feel
very strongly that that should just be a
local level model without a trend so
that tends to come from other domain
knowledge or theories about causation of
the data you're seeing yeah so in this
case we're just exploring because we are
not climate change experts so for
example I would I would be very
skeptical if someone went to Congress
with this and said look I figured it out
for example okay okay so very quickly
and then we'll take a quick break so if
we're looking at our null model compared
to this model if we consider the
correlation they're not measurably
different right so we've done all this
work fancy model but it's not it's not
clear we've done much better maybe we've
done a tiny bit better and maybe that
matters but as we were just discussing
since it's possible to overfit our
models or there's so many knobs to turn
we have to look at skepticism right so
whether this is a good model will depend
on your purposes whether you're just
trying to interpret it are you trying to
make predictions and what sort of
accuracy do you need right is half a
percentage point improvement in
correlation meaningful in your field or
no it's very context dependent but
otherwise I mean if you're someone's
boss and they come to you with this
obviously the first thing you should do
is shoot them down and say do you really
need this because you've barely improved
on your null model so explain to me why
why you haven't and what about in terms
of me and absolute error well here we
have quite a difference right so so
maybe this is what's something we're
thinking about right and maybe we should
say okay well well why is that and and
how can we improve that and what
insights does that offer us right so
again just a reminder to use different
metrics and think about your context
okay so those are structural models
we're going to take a short break and
then we're going to come back and do a
hidden Markov model
and that will be a wrap on state space
so let's just call it a five-minute
break and come back at 9:22 okay so our
break is over I just dumped my email
address in the slack also in case that's
not available on Syfy's so if anyone has
questions after the tutorial or
follow-up comments great to hear from
you yes so the slides are in the slack
as a zip file and then for anyone who
wasn't here at the beginning I am
unfortunately locked out of github at
the moment
for very it's a very long story so as
soon as I'm back in github they'll also
be on git and and someone has nicely
posted them I get now but ultimately I
would like to you know have the sides
and they get the repository altogether
so and also if you email me once I get
that together hopefully in a few days
you can also have that does that answer
your question yes okay any other
questions okay okay and then just so
folks know we'll definitely do another
break before they stop serving breakfast
at 10:30 in case anyone needs to pick me
up okay so we're going to be talking
about hidden Markov models now this is
another technique it's not especially
novel or considered cutting-edge but
it's increasingly useful because now you
can actually get decent computing power
as opposed to when it was invented you
know and maybe if you were at IBM and
had a really fancy computer or whatever
you might be able to fit it now we can
oh you know just fit it on our laptops
so much more relevant than it used to be
what is a hidden Markov model why is
this the state's today's state space
model well it's the same idea in the
sense of having the idea that there is
an underlying state that you cannot
observe and there is the output of that
state which you can observe in the
structural time series models we tend to
have the model that the state and the
measured quantity are basically the same
thing with error but we can also broaden
our perspective with hidden Markov
models and think well maybe what we
observe might not even be in
same physical or conceptual category as
the underlying state that's producing it
so in this case what we have we have a
sequence of measurements as depicted
here where we envision I feel strongly
that people should be able to come and
go whenever they want so no one no one
is imprisoned here okay or locked out on
the way back so in this sense what we
envision is we have a sequence of states
and for whatever reason the dynamics of
that system the states are evolving over
time we conceptualize this in a
discrete-time framework but this can
also be in continuous time and you can
do hidden Markov models and continuous
time we won't look at those but if those
describe your data you can think about
that so we have a sequence of discrete
times where step the underlying state is
evolving we can't see that directly what
we can observe is whatever the
observable is why also at those times
and somehow X the underlying state is
what produces a certain kind of Y so how
do we make this more physical well one
example where this is widely used and
actually where you don't even need to
have the fiction of discrete time steps
is in DNA analysis so when people look
at DNA sequences and they're trying to
think about why do I see maybe certain
peptides or why do I see a certain
sequence and what does that tell me
about the underlying state of the DNA
I'm not a biologist so I'm probably
slaughtering this but the point is is
you have these DNA chunks that really
are coming in discrete measures and
there's an example where it is a time
series in the sense of having an axis
that is well ordered and has a good
definition of sort of moving forward or
backward along an axis so that's an
example where maybe your underlying
state again don't don't hold me to it on
the biology maybe your underlying state
is something like how methylated is this
chunk of DNA and what you observe maybe
is something like over time like what
kind of enzymes are being produced at
this time what kind of distribution so
notice that X itself the underlying
state that's usually going to be
as discreetly defined right I have a
countable number of states these are
discrete states I'm not going to be in
two at the same time although again you
can model anything right you can make
things infinitely complicated but that's
not the traditional hmm and then your
outputs these can either be discrete or
they can be continuous right so in the
discrete case it could be like what kind
of sequence am i observing where that
sequence is some other kind of indicator
but not the state or it can be some kind
of continuous variable so an example of
that and the one we're gonna model in
our notebook is we can think of rivers
as being sort of in high flow or low
flow States to first-order and then we
can think about well if you're in a high
flow or a low flow state what kind of
flow are you going to see that's the Y
and the Y will be a continuous variable
right so it's not like I'll see you
exactly the same number for low state
versus high state and there's a good
chance that the distributions will
overlap but if I sort of look at the
whole sequence I might get a sense of oh
these were low and these were high and
these were low so why might I use this
say as opposed to an ARIMA model and I
don't mean to say that every mamada
loves the be-all-end-all but we should
think well why would I choose this model
over say another one that's known to
perform well well there's a couple of
reasons so again just like with
structural time series this offers us
some kind of intuition and some kind of
sort of model of the underlying dynamic
so to the extent we have a model like
that for a particular system like this
if it fits then that's a good thing
because it can offer some kind of
insight another thing for example in the
case of river flow and this I learned
just from reading a hydrologist blog who
is blogging about HMMs is that they find
that ARIMA models don't capture these
sort of nonlinear dynamics of it's not
that you know time T minus one predicts
time T it's more of this sort of regime
switching where you know kind of the
probability of the regime switching but
when you last switch two regimes doesn't
necessarily tell you when you will next
mature regimes and that's a feature of
Markov processes in general right the
whole point of a Markov process is that
when I'm at t minus 1 if I know my state
at t minus 1 I don't even really care
about King before so that's quite
different from an ARIMA model where I
say oh I'm looking at my whole history
to determine where I'm going next with a
hidden Markov model I'll say as long as
I know XT minus 1
I don't need to look backwards in time
sort of all the information I need is in
that data point so those are just a few
observations about hidden Markov models
and when you might want to use them okay
so there are state space model
observations are an indicator of the
underlying state but we posit some
separate underlying state and in this
case you really wouldn't posit that your
underlying state is just your observable
with error that's not sort of what
you're doing with a hidden Markov model
unlike the structural time series in a
Markov process the past doesn't matter
if the present status is known right so
the most informative thing is just your
most recent measurement and nothing else
gives you new information so these
things the two aspects computationally I
want to talk a bit about before we fit
our parameter estimation with the bound
welch algorithm and smoothing / state
labeling with the Viterbi algorithm so
I'm guessing you've seen these words
floating around I just want to give you
an overview or an intuition for what
they are before we use them okay so the
bound Welch algorithm is how we
determine parameters so when would we
use this this is at the very start this
is like I have a time series I have a
sense that there's some kind of regime
switching but more than that I really
have no idea like I have no priors
there's nothing I know about sort of
whether you know the mean of one state
is 3 or 17 I have no idea so this is
really amazing in the sense of you can
just say here's here's my data and
here's how many sort of underlying
states I think I have go and and it will
sort of figure out a set of parameters
now of course what what does that entail
it's the same concern we talked about
earlier you have a lot of knobs to turn
this is a very complicated situations
right when you start thinking about how
you would code this up if you just had
to code this yourself it's sort of a
nightmare
so basically what that means it it's not
guaranteed to converge to a global
maximum you're not guaranteed to get the
absolute best parameters to describe
your data what this algorithm does
guarantee is that you will always get a
little bit better with each cycle of
this algorithm so it's a forward
backward expectation maximization
algorithm right which means you sort of
first figure out what your likelihood
expectation is given your data that's
your
expectation step and then your
maximization step is you sort of update
your estimate of your parameters to
maximize the likelihood given that
expression of the likelihood form and
then you do it again right and you just
sort of do it and do it and do it until
you get to what you think is an
acceptable convergence level or an
acceptable likelihood but that will only
get you to a local maximum so that's
another thing to keep in mind with these
models right again there there's not
some sort of unique well-defined closed
form solution and often what people will
recommend is that you run it many many
times before you sort of make any
pronouncements on what this is showing
you right so you don't just run it once
maybe you run it a hundred times and
look at what your your parameter
estimations are and see if some sort of
like narrative or some form is emerging
from your many many attempts and of
course keep in mind you can over fit the
data right so it's quite easy to maybe
choose a set of parameters that will do
much better on your training data than
on some sort of validation holdout so
that's always a risk with these sorts of
things okay so just a brief overview of
the details right what's the problem
you're trying to estimate basically
three things you're trying to estimate a
which is sort of your transition matrix
probability so what's that if we go back
you're at the next time step including
the same state so a describes from XT
minus 1 to XT what does my transition
look like am i likely to stay in the
same state am I likely to go to a
different state and let's quantify that
right it's not just you don't get an
answer of a little likely not likely
right you actually get a matrix saying
to go from from state I to state J your
probability is point four to go from
state I to state K your probability is
point two or whatever and so on so
that's the first thing you're trying to
fit your B is so you're seeing another
matrix there or another sort of set of
parameters indexed off your state how
likely is a particular Y assuming a
particular underlying state and then
what's the final thing you need to
describe this model pie for the priors
that's telling you how likely you are to
sort of begin in
a particular state so this model also
posits some sort of beginning which is
something you want to think about maybe
if you're taking the same time series
and just slicing it up over and over
again the idea of a prior is saying well
no we sort of think you're likely to
start in some states you need to think a
little bit about what is starting mean
and does my data sort of acceptable fit
that so on your forward step essentially
what you're doing is you're developing
the probability of being at time T where
T can be any time the probability of
being in state I at XT and at the same
time right so this is a combined
probability the probability of seeing
the whole sequence of your actual
observations at y1 all the way up to YT
so that's your forward step you're sort
of moving forward in time how probable
at XT is it that I'm in state I and that
I've seen all this data at the same time
and then you go backwards in the other
direction and then you say okay on the
other hand how probable is it if you
know so in this case conditional on
being in state I at time T how probable
is it that I see then the sequence from
t plus 1 all the way to t right so
you're sort of effectively picking a
point in the middle your forward step is
how probable is it that I am in steps
state I and I've seen this data and your
backward step is again I'm still a state
I how probable is the data that comes
after it given that I'm in step I right
so you can see if some sort of measure
of we're converging on this measure of
how probable is this whole sequence from
the point of view of this point in time
and then you estimate two other
parameters right which essentially
become our just expressions of these so
we calculated this Alpha and the beta
right the forward and the backward and
from these you can express so sort of
more generic quantities such as above
gamma sub I is the probability of being
in state I at time T even all the data
you've observed the full time series and
your estimation of theta right your
parameters and you can also estimate
final component of the behavior which is
the transitioning given the data you've
seen in the theta so it's a lot of
programming as you can
and this is just to make you aware of
sort of what's going on under the hood
and from this you can then estimate
those three quantities right the pie for
the prior how likely you are at the
beginning of a sequence to be in any
given starting state because that will
influence where you go from there
alpha sub IJ how likely you are to
transition from state I to state J at a
particular time step and finally beta
which is how likely you are to see a
particular observed value given you're
in a particular underlying state okay so
that's how we would estimate the
parameters this is really
computationally taxing it's really
difficult and like I said you're not
guaranteed to get to the perfect
parameters you will get you one
iteration of parameters that are a
global maximum optimization final slide
before we go to some code what's the
other thing we want to do so first we
want to have the parameters to sort of
explain our underlying sequence right if
we posit a certain number of states what
parameters can we come up with to
describe the behavior okay now we've got
those parameters let's say we accept
them and want to move forward then what
we need to do is figure out if we posit
these underlying parameters as
describing our state what actually were
my states in my time series at each time
step I can actually get a label and how
do I do this I do this via the Viterbi
Viterbi algorithm for determining
sequence so this has to come afterwards
to use this I need to have those
parameters describing my time series if
I have that this is a dynamic
programming problem for those who don't
know the term dynamic programming is
basically when you realize that your
larger problem is actually composed of a
smaller problem you can that sounds like
recursion but it's different because you
can also memorize it it's sort of a
limited countable ordered set of
problems right so why does that describe
this why is this not just recursion and
dynamic programming because I have a set
number of steps in my whole path I have
a set number of options I can explore so
if you think about it I could actually
summarize all of that in a matrix right
so to get from say
what's the best way to do that and I
could explore all of the possible states
permute those states versus the cost and
I can ultimately sort of come up with
the most probable way to go through
those states if that sounds complicated
and like I'm waving my hands I
definitely am you can read the code it's
actually not that bad but it's a pain in
the butt to code right but you sort of
get the sense of like all the things
you'd need to think about to code this
up properly and make sure you found the
most probable path through your data so
leave it at that this is a great example
though of something that could be
accessible if you did want to code it up
on your own okay so now we're gonna open
the second the second notebook as soon
as I find my notebook so again in state
space model is now we're looking at the
Gaussian hmm so a Gaussian hidden hidden
Markov model is where you're gonna have
continuous observables you can also have
a multinomial hidden markov model where
both your underlying state and your
observable are sort of these discrete
states so that's another thing if that
better describes your data you could
look into okay so in this case as I
mentioned I was trying to get Colorado
River data but the website wasn't
working so well I don't know if that's
all so budget cuts for whatever but
we're gonna look at the Nile instead
this is a very famous data set if you
read traditional stats textbooks this is
all over so if we look at the nile data
we see that we don't have the problem
hold on I need to clear this start over
we don't want to be giving away the
answers we don't have the problem of
sort of multiple groups of observations
we just have one value for each year
just the flow for that year we don't
even know when this was taken or if it's
a cumulative value but what we do see is
we see this plot right so this is kind
of interesting given what the
hydrologists told us based on the one
hydrology blog I've ever read they're
sort of a high flow and a low flow state
to first order if you want to model a
river system that could be due to things
like
El Nino or whatever the equivalent is in
the Nile region right but these sort of
global larger global cycles or patterns
or states that are affecting things like
River flow that are not things like just
one year predicts the next but more like
you are in one regime and at some point
you switch to another but it's not
deterministic some kind of nonlinear
stochastic switching so if I sort of
eyeball this to me I'm like wow this
already looks like sort of two states
right like I've sort of got this high
thing up here and this low thing up here
the one bit of background research I did
is I thought oh man is this one the
Soviets built that dam on the Nile maybe
that's just building the dam but then if
you look at the the time stamp it's
around 1900 which was definitely not
when the Soviets were building the the
dam on the Nile so we know it wasn't
that so we say okay I'm gonna assume
this is some sort of climatological
phenomenon we're looking at so let's
take a look at the API for hmm learn so
a little bit of history about hmm learn
I believe this used to be part of
scikit-learn and then it split off to be
its own project and was a continuation
so just be aware of that when you look
into hidden Markov models it was
sufficiently complex or interesting to
be its own thing although it has a
similar API so first tricky thing is
actually they just sort of require this
shape they require two dimensions for
their input so that's why there's this
expand ins if you run it without that
you'll get an error and you'll just have
to put it back in so I very generously
just gave that to you we're gonna start
with saying there's two states right
there's gonna be a low flow state and a
high flow state I just read that in my
hydrology blog so it must be good we set
up the model as a Gaussian hmm and very
nicely see we only have to set the
number of components the number of
iterations and then I just want to fit
it with my values and I want to get the
hidden state so I run a model predict so
let's just run that and let's see what
do we even get for hidden states we get
a numpy array and what's the shape the
shape is the same thing we put in and
what does it look like well it's it's a
bunch of numbers and actually they're
all the same number which is not
especially interesting so far if I look
at it sort of it looks like I have two
values one has 72 count the other has 20
account that's not telling me that much
what if I plot it so I'm plotting my
hidden state note that a hidden state is
not something it necessarily make sense
to plot in the sense of like the higher
state is not necessarily to higher value
so just keep in mind this is just to
show sort of a label and a really quick
and easy way it is the number on the
hidden state means absolutely nothing
so the fact that I sort of go from high
to low doesn't mean anything by itself
just looking at ones and zeros but what
do we notice here this verse is what I
saw above is it is it telling me
anything I didn't know none especially
right because all it said is oh yeah you
were in one state here and then it seems
like you took a nosedive and you're in
another state there's two states and it
sort of agrees with me there so we're
gonna talk a little bit about how to fix
this or what might be more interesting
and informative but before we do that I
would say since we now realize we're
gonna want to experiment a bit is I
would ask you to just take a couple of
minutes and think how can we package
this API a little more conveniently so
go ahead and read up on the hmm learn
Gaussian hmm API and also think
basically I want you to turn this into a
function so that I can just call it
really easily instead of having to sort
of write this over and over here's one
solution I imagine folks had other ideas
but the main goal would be you'd want
something that accepts both your values
and also your states it could be
convenient to have that reshape in case
you are often passing in data that lacks
that second dimension right better to
just get that stuff out of the way take
the mental tax off in this case I'm
gonna have the same the same model right
so the number of components which I now
want to make configurable I chose not to
make the number of iterations
configurable but that's sort of up to
you to decide number of iterations tends
to depend more on the quality of your
data rather than or the nature of your
data rather than the number of states to
first order so this wouldn't be
something you'd have to adjust that much
for the same data set we grab the hidden
states we also probably want to extract
some more information right just knowing
the state label by itself is not that
interesting I also want to know what the
BOM Welch algorithm found to be sort of
the mean and standard deviation for the
Gaussian model that I'm developing right
so in this case these these describe
sort of what I expect to see coming out
of my observable data and then so that's
the amuse and the Sigma's right are the
the Gaussian distributions for the Y
it's given the underlying state and then
transmat is the transition matrix this
is the thing that describes how likely
are you to go from state I to state J I
want to see that as well because that's
going to give me some information about
what the underlying dynamics look like
and does this match my theory of how
River flow works so if I were to only do
that that would be alright but I
actually also like personally I like to
reorder the output because the output
comes out and sort of an arbitrary order
right like as I said the hidden state
labels don't mean anything
they're just arbitrary labels they
continue to be arbitrary labels no
matter what or what I like to do is at
least label them in sort of an order
from like lowest to highest so lowest
mean has the lowest hidden state label
highest mean has the highest hidden
state label and not just at least to me
offer some way of saying oh that's a
bigger one that's a smaller one but it
doesn't mean more than that so I have
this code to just re permute everything
including the transition matrix and if I
do that we can now fit a method that
will give us the hidden states the Meuse
the Sigma is the transition matrix and
the model should we wish to do more with
the model so we run that again and then
I won't leave this as an exercise I
think we'll just go through it but how
might we want to plot this well what
things do we want to plot we want to
plot the real value and then we want to
plot the hidden state in some way right
I'd like to have these overlaid in some
way this is a matter of personal
preference some people will do things
like sort of color the background
according to the state you're in like I
said I have this idiosyncratic way of I
like to just plot the state as a line
graph so long as I preserve in my mind
the fact that this does not mean
anything except as an indicator of
different values right so it's not
actually indicating anything about the
size of those values
yes thank you
which part this one yes right because I
thought I would run that first thank you
okay so now my states are in are in the
order that I want them to be in right I
like just the highest mean to be the
highest labeled state again does not
mean anything more than that it just
means they're sort of an ordinal order
so here I still got I have the same
uninformative oh yeah you're in some
sort of like higher flow regime and now
you're in some sort of lower level or
lower flow regime but this hasn't taught
me much so the next exercise for you
guys now that we have this in the form
where we can experiment and plot our
results easily I'd like you to try two
different ways of maybe improving on
this that I would recommend one of them
is to cut off this sort of earlier set
of time points which might not be that
informative or might just be two
fundamentally different to be that
interesting because I'm not interested
in an outcome that just says oh for
twenty years you were in one state and
then for 80 years you're in another
state not super interesting so can I cut
off the region of interest and rerun the
analysis and then the second method is
to think well maybe I'm not looking at
two underlying States should I try it
with a different number of underlying
states and see what that looks like so
go ahead and take a few minutes to work
on those two options the first
possibility right we can cut off the
special region which to my mind means we
like this first state it seems to be so
different right it just takes over that
first day and I think well maybe that
was some sort of extraordinary event I
want to rule that out I'm interested in
sort of more typical dynamics of the
river so why don't I just cut off those
first 2728 values and model that and if
I do that I didn't put it into my fancy
plotting function but I can I can see
more switching now and this is sort of
more what I would expect based on the
very little I know about hydrology right
now I'm not looking for some regime
shift of like 20 years and then 80 years
right that's that sounds like almost
like a geological event or something I'm
interested in sort of more every few
years El Nino type stuff so that's one
way to do it but do I really need to do
that because that feels like
then maybe my model doesn't describe my
data very well if I have to sort of cut
off whole periods and and maybe that's
fine for my
purposes but I also want to see well
maybe can this just be a little more
robust so what if we instead just add
another hidden state right what if I say
well maybe there was this extraordinary
state and then there's the normal two
state dynamics so can this accommodate
that so if I put in three states
and plot that well this does seem to be
accommodating this right I have I have
my one state that's sort of the initial
extraordinary state I have another state
and another state so I have my three
states I seem to have gone wrong
somewhere in my ordering of my states so
I'll have to fix that bug
during our break ideally the first state
would sort of be up high and then it
would jump low and high and low so I
apologize for that but what we see is we
see more switching now right so we see
that this model is robust enough that it
can sort of describe a state that was
sort of very long and very different and
then accommodate regime shifting as well
and if I look at my muse what do I see I
see I know what I did sorry let me fix
my plotting if I can no sorry
so I have I have three sort of means for
my y observable right two of them are
low eight one around 800 880 and then I
have this one around 1100 which is sort
of this extraordinary regime and I can
also look at my transition matrix which
is going to describe how one state
travels to another and what do I see
here if I look at this right so why is
it three by three because I have three
states and the transition matrix
describes the probability of
transitioning from any one of those
three states to any one of the other
three states interestingly here I can
see I have a very challenging data set
because some of these transition
probabilities are zero right and in
particular the possibility of
transitioning from either of these top
two states to the third state is 0 why
is that because here's the third state
and it's it only observes in one
direction right it only observes going
from that top state down and sort of
exiting that regime it doesn't observe
an end
to that regime so we would need to have
domain knowledge to know is that
reasonable or if it's not reasonable is
there some sort of constraint on our
fitting outer then we would want to put
in to not permit things to go to zero so
those are sort of more advanced things
you might do on your own with domain
knowledge but that's not something
you're going to get out of the box so
I'll also keep in mind with all these
methods there are ways you can tweak
them to reflect your data dynamics but
otherwise as a general solution they're
gonna fit the data they see so if it
never ever ever sees that event it can
very well go to zero if you don't
constrain it not to and maybe that makes
sense so on the other hand you can say
well that's very flexible because what
it shows is it can accommodate both sort
of normal regime shifting and also
extraordinary events and maybe that's
interesting and helpful to know unlike
say in a Rhema model or even a
structural time series which will
struggle more with that sort of sudden
regime shift we can also see that from
there if we look at the first two rows
and columns we can see sort of the
probability of switching between the
sort of two more normal states and that
sort of seems to flicker back and forth
with you know directionality favoring
one more than the other it's not 50/50
and that's sort of an interesting
insight maybe if we are into hydrology
we might say oh yeah that reflects the
data and that's something I can pursue
in my research and so on so we get some
insights here what I talked about this
is just a local maximum not a global
maximum so you're not guaranteed to get
the same fit if you initialize
differently and so on it's just one set
of parameters you would want to run this
simulation many many times with
different starting conditions to really
have a better sense of how you're
describing this data and arguably with
just a hundred data points there's only
so much you can get out of it anyway
right and this is this is where some of
these modern methods they don't always
work as well as you might like with just
a hundred data points but it's a simple
simple toy example okay so we're gonna
go ahead and take a break we'll make
this I want people to have enough time
to go get some snacks so I think we'll
make it like a 10-minute break
it's the big snack break and there's
also an exercise should you wish to
attempt it so I will quickly show you
guys this when we get back and then we
will move on to machine learning methods
so let's see it's 10 o'clock exactly so
we'll restart at 10 10
and just to clarify the snacks are just
where breakfast was so across the
courtyard sort of that way okay so we're
gonna get started again first I'll just
quickly walk us through the last
exercise from the hidden Markov model
component and then we're gonna move on
to an overview of machine learning for
an hour and then last hour we'll be on
deep learning and I keep panicking when
I see my East Coast I'm Stan so psycho
crap it's 11:00 already no okay okay so
just one other function I wanted to make
you aware of is that you can sample from
a hidden Markov model once you have fit
it and you don't even need to fit one
you can also feed in parameters and
create a hidden Markov model with a
certain set of parameters so that can be
useful if you just want to see well what
would something look like and run many
iterations of it to test out a theory so
in this case I sample from the model we
just developed and then I can plot it
and think well that that's what I would
look like where that that's what one
sort of sampling of this would look like
but then if we were to refit it notice
that this looks very different from our
data right even though it's the same
hypothesized underlying process so keep
in mind that a process with a certain
set of Meuse and sigmaz to describe your
y sub t probabilities and your
transition matrix can still give you
data that looks very different so if we
were to then to refit we we see it like
a totally different model where the
three states are different as well and
in this case we're actually seeing what
it really looks like here's what
happened is that our two sort of normal
states were the ones that really got
repeated say sampled from repeatedly we
didn't really get to that third
extraordinary state that characterized
the beginning of our flow model so in
this case we're seeing a transition
matrix where the probabilities don't
look at all like what we saw in our
first case mainly because now it's
taking what was our two states out of
our three states and it's remodeling
just those two states as three so here's
an example of it's not necessarily wrong
or a bad thing but just to keep in mind
that the same hidden Markov model that
can describe one set of data can look
very different with a different set of
data
such as if one of your states is not
even there right so if you give it too
many states or too few states relative
to ground truth you can end up with
things that don't make a great deal of
sense there aren't very insightful so
just keep that in mind when you're
fitting these models especially if you
have these sort of outlier states that
you don't see very often okay
so any questions or comments on hidden
Markov models before we switch gears
okay so those are sort of your overview
of state space models
so as a recap
once I load this as a recap for our
state space models they can be really
good for when we have some sort of
underlying sense of what we think the
dynamics of the system are they can sort
of add evidence to those dynamics or at
the same time they can take away
evidence right they can tend to refute
what we think for example we saw that
having a trend in the earth temperature
data actually wasn't that interesting
and more just a local level model
sufficed and it also helped us discover
seasonality that we had been too lazy to
discover on our own and then with our
hidden Markov model we saw this model
can accommodate things like say an
extraordinary state change but in such
cases we will run into problems with
simulation so there are problems these
methods need to be treated carefully and
with knowledge of how they work you
can't just roll them out out of the box
but they do offer a lot of things that
save traditional statistical models
don't offer such as a way to sort of
break down the system and try to get
inside right so even though we only have
a univariate measurement we can actually
get at very complex dynamics underneath
that univariate method and also just to
point out all of these methods can also
accommodate multivariate time series
data as well so if you have parallel
temperature measurements and you want to
incorporate them or parallel
measurements and a hidden Markov process
that are all indicative of the same
underlying state or maybe a multivariate
state of some kind that can all be
accommodated as well with standard
software and then somebody was asking me
during the break well what about you
know your underlying restrictions such
as I mentioned you might want to have
you might want to have a transition
matrix that does not permit a zero
transition probability because you know
that's just not true
in your system you can build in things
that in my experience most of the time
that will require hacking the source
code or writing your own source code the
good news is that these are open source
tools so you have a lot of inspiration
slash code you can steal from to like
make your own thing and the other good
news is that the underlying algorithms
such as the Viterbi algorithm or the
Brown Welch algorithm I think they're
complicated in the sense that there are
many many indices but they're not rocket
science in the sense of it's all fairly
intuitive and easy to reason about so it
is fairly straightforward to hack this
once you've sort of developed that sense
of how it's working okay so now we're
going to talk about machine learning for
time series switching emphases a little
bit but a different way of understanding
our data or processing our data ok so
the first thing we need to think about
when we do machine learning for time
series is the fact that I'm not aware of
call my attention to if you can think of
one a machine learning method that's
really sort of developed for time series
and my experience when you apply machine
learning to time series you are applying
more general methods and finding a way
to make them reflect or accept time
series data as opposed to the models
we've just been looking at which were
built from their core around the idea of
a time series right so traditional
models like ARIMA but also models like
hidden Markov model and structural time
series at their core and vision sort of
temporarily ordered data on the other
hand when we get to things like decision
trees those do not fundamentally build
the model around this idea of temporal
notions right actually that is
completely absent from the model so if
we are going to use models like that we
need to provide a way of translating
this temporal data into something that
makes sense and can be accepted as an
input to that algorithm but also makes
sense right so we don't want to get
overly focused on the inputs we also
want to make sure that these are
sensible inputs and we're not just
throwing data at it so that's why for
example we would never just say well
let's make our time series the inputs to
our decision tree right let's have a
decision tree and maybe I have a really
short time series it's you know it's
just 10 points it's from time one to
time ten so I have plenty of machine
learning models that take more than 10
inputs so why don't I just put the
inputs
and see what happens it's very very rare
that that's going to get you anything
like what you want if you have that sort
of time series data arguably it's so
predictable and so clean and so easy
that you don't need machine learning
anyway you could probably just eyeball
it right so if you have that kind of
data with so little noise and such clear
structure your there's no reason to be
doing that so any case where that would
work it's not a case where you would
need to do it on the other hand what are
some things that could be difficult
right your time series can be different
lengths and also things don't sort of
got to say even a really simple example
maybe we always have many time series of
a volcano erupting so we know there's
always interruptions somewhere in the
data that's how we've defined this data
set that eruption won't occur at the
exact same moment though right in one
time series it might be at time step
three at another it might be a time step
13 so how could you expect a machine
learning algorithm with no notion of
time to sort of understand that this
eruption can occur at any of these times
but that once it does we expect whatever
collateral damage results or whatever
shows up in the time series so again
main message you can't just feed your
time series to a machine learning
network I have never seen a case where
that makes sense so though if someone
has one that would be great to know
about so what we do instead is we have
to do feature generation right and this
is no different you have to do that in
cross-sectional data too as well
sometimes you can generate you know data
off of your raw data but with time
series it's basically required and so
what sorts of features do you want right
what are we doing when we do feature
generation we're finding away to take
something like this curve and just have
a few numbers that just describe what it
looks like and that's no different from
any other time we'd want to do feature
generation it's no different from
whenever we want to summarize a complex
situation with a couple of words or a
couple of numbers or a mix and some
heterogeneous data so in this case we're
looking at a time series and what are we
going to use well we might use the
maximum value the minimum value the
median the mean the number of peaks as
you may be noticing these already if you
have sufficiently long time series and
now remember we're sort of each time
series is almost like its own data point
now so for each time series you're going
to need to go through each
timestep in that time series and compute
these things and if you think about that
that actually gets to be quite
computationally taxing right because to
even just look at the mean or the min or
the max I need to go through every data
point so if I have really long time
series the you know order N or order N
squared gets to be quite a drag and then
if I'm doing things like number of peeps
well that's this is one of these things
where you know a human eyeball is great
at this a deep learning network of the
right kind might be great at this but if
I'm just trying to write simple Python
routines and especially efficient ones I
don't know of a very quick and fast and
super reliable way to identify these
Peaks right so that's a sort of thing
that would be really helpful if we could
put it into a feature but we need to do
that sort of cost-benefit analysis of
how well for my data can I analyze this
you know and also the computational cost
so that's always something you're
thinking about with feature generation
is just how expensive it can be and
whether it's worth it I also want to
point out that times theory is well
studied very complicated domain
dependent sort of thing as far as what
turns out to be useful when let's go
back so this set I just wanted to
highlight it's one of many canonical
sets it's called the catch-22 canonical
set so this is sort of an ongoing really
interesting project it's part of this
project called the highly comparative
time series analysis project and what
they are doing is collecting every kind
of time series data they can get their
hands on at all frequencies from
whatever domains are available and you
can donate datasets and the idea is to
use machine learning itself to sort of
identify what features perform well
across many kinds of datasets for
classification and prediction tasks as
appropriate and so this is the list of
22 features that these researchers came
up with there's also sort of an
alternative version where it's down to
17 features rather than 22 but anyway
the point is is if you're interested in
sort of what's going on sort of
cutting-edge feature generation
research or what people might use out of
the box you could have something like
this now on the other hand I might say
22 is a really high number so sure if
you're doing sort of high-throughput
feature generation of many datasets just
to sort of try to develop insights about
what kind of features apply it's
universally great on the other hand if
you have a very specific youth case like
an EKG or some kind of seismological
measurement over time in LA when they
have a massive earthquake or whatever
this should not be your first go to
right so this is for a very general time
series but almost always if you have
domain knowledge or just have looked at
your data you can do better than just a
generic feature set but it's there if
you need it it's also as I was just
mentioning very disciplined specific and
sort of the more you know about the
underlying processes the more meaningful
you can identify features so on the Left
I have an EKG again I love this example
because it's periodic it has all sorts
of interesting features so these are
sorts of things for example that doctors
see in their medical textbooks and this
is how they learn to read this time
series right it's actually a time series
and part of what they're doing when they
do Diagnostics it's to do future
generation right effectively what
they're doing is identifying things like
oh the tall peak how tall is that and
you know how long before the first peak
and the tall peak and what's the
temporal distance between like that last
little dip and the first peak and so on
and that's part of how they diagnose
heart illnesses right so then they know
what's relevant and so if you were
working on data like this you would
probably want to work closely with a
physician rather than you know
developing all of these from scratch
right that would that would sort of be a
waste of time and then on the right I
have a case of astronomical data that's
another discipline where time series
features are actually very well
developed they're whole Python packages
just on astronomical time series feature
generations so if that's something you
do that's where you'd want to start out
looking so you should always start with
your discipline specific stuff once you
have your time series features and we're
going to compute some features what can
you do with them well one thing you can
do is classify your time series data so
here's an example from the time series
classification database they show sort
of twenty-four different classes of time
series data
and so one thing you can think of is if
I have a whole sample of different time
series how would I separate them out
like this and so we're gonna be looking
at two ways of thinking about that we're
gonna use random forests and we're gonna
use gradient boosted trees but just in
case anyone's not familiar what is a
decision tree a decision tree looks like
this right a decision tree is something
like you feed in your features of your
data and you look at one feature at a
time to sort of make decisions branching
on this decision tree branch down branch
down branch down and at the end after
you've made a series of decisions that
are sort of binary yes/no about various
features you get to either a
classification label or a regression
prediction right so these are why our
tree is helpful in general they can
capture nonlinear dynamics they are also
often fairly resistant sort of
extraneous garbage inputs so you can
also be a little bit less judicious
about sort of feeding everything in and
sometimes you won't be too punished but
of course no one's going to actively
advocate for that it's just good to
remember and they can also give you give
you a sense you can read them right and
start getting sort of a sense of how
things work so I imagine even a doctor
when they're looking at an EKG that's
sort of how they think right that's how
they're trained even they have these
heuristic decision trees and that's just
like our decision trees and machine
learning so the two techniques we're
going to use that are based on trees is
one the random forest right where we
just build a whole bunch of trees each
one sort of built off a subset of
features and a subset of data and we do
majority voting and the winner is you
know the majority winner and we're also
going to use XG boost
I'm guessing folks have at least heard
of XG boost or gradient boosted trees
the idea is that with a random forest
you build however many trees you're
gonna build and they're all in parallel
with some sort of random subset of the
data and the features and you build off
of that gradient boosted trees are built
sequentially so first you build your
best decision tree with whatever your
parameters are your second decision tree
is then built off the errors of the
first decision tree so each decision
tree is sort of trying to correct the
errors of the previous one with the idea
that you're going to add the inputs
right so it's not sort of a majority
consensus is that everyone contributes
there a little bit you add up the trees
and you're good
there's no reason at least in my mind
when I
about why this should work so well that
it does for time series specific data
but empirically in the last sort of
three years maybe even a little bit more
it's become clear that XG boost is very
successful at time series tasks much
more so than sort of earlier
applications of machine learning to time
series so that's why I highlight it it's
just empirically true when you look at
competitions or industry use cases also
clustering this is the other thing we're
gonna do in this notebook so again I'm
guessing folks are familiar with the
concept but just in case you're not
generally speaking what is clustering
look like it looks like you you look at
sort of the feature space of your data
be a time series data or cross-sectional
data and you hope somehow that the
features sort of cluster in this way
that indicate they're sort of specific
types of data points and you hope that
that's somehow means something more
fundamental about what has produced that
data that's a beautiful picture does
anyone actually see that picture at work
though I mean I I want your job if you
do right that's that's not really what
you see well you usually see is
something on the right and you're sort
of squinting at it and running your
analysis and that also tends to be true
for time series data so we also have to
keep in mind the messiness of real world
data this I think this picture is a
somewhat fair version of a really nice
time series right so if we were to
cluster this is what it looks like in
the time series case right like we can
see that those top two curves twelve and
eleven they look similar right they've
both got three bumps they're sort of in
the same space positionally they seem to
have approximately the same through a
volatility over time if we look at ten
and nine the gray that's an even better
case of yeah they look pretty similar
but you know some of them even have
features that the other one does in so
again it gets a little more complicated
and you start thinking well what's good
enough what isn't good enough if we look
at the green seven eight seven eight and
six those also are another good case
where you start wondering you know how
good is good enough that might be a
little bit more like real life where you
know there's there's there's some sort
of commonality you don't even know if
you could write code to identify what
that commonality is so you get you see
where it can be difficult to generate
features especially if we want to have
features that are robust for all of
these time series
the same time right because of course
all of these are jumbled together we
need to identify features that would
distinguish among these not really easy
right like number of Peaks well number
of Peaks my blue versus my brown they
might end up being the same or even my
brown versus my grey at the top you know
depending on what sort of peak finding
code I write the grey and the brown
could both have three peaks and come out
being very similar and what does that
mean so you have to have some domain
knowledge again and also sort of look at
your data so time series clustering is
surprisingly difficult right
conceptually I think it can also be
difficult sometimes because we're we
might be looking at it via the features
but ultimately I think what we most want
is we want to look at the original raw
series and make sure that looks similar
right we we have this intuitive sense of
if I can eyeball it and it looks similar
that's what I want to come up together
in a cluster it's very computationally
costly as I mentioned the feature
generation itself is very
computationally costly because you have
to go through the whole time series to
come up with a few features and then
that's just one data point because now
remember again your time series a whole
time series like one of those curves is
just one data point right so if each of
these curves is actually a thousand
measurements that's you know thousands
and thousands to only produce actually
twelve inputs right twelve samples so
keep that in mind as opposed to
cross-sectional data one pitfall I want
to point out is you might say well
forget feature generation why don't I
just measure the distance between curves
right and I could do something like use
Euclidean distance you almost never want
to use euclidean distance and we're
actually going to cover a really great
example of why and I'll introduce
another distance metric that works a lot
better and then time series clustering
is used across many disciplines so
that's good to be aware of as I
mentioned medicine right looking at an
EKG also in finance especially sort of
1980s style finance people used to plot
things and then say oh does this month
look like that month from the 60s and
maybe the same thing is going to happen
with that kodak stock or whatever that
definitely happens a lot less now but it
was a tried-and-true method back in the
day chemistry also like I talked about
the NMR spectra right it's effectively
when chemistry professors and there were
grad students out and tell them to go
take those spectra that's what they're
doing right does it look more like this
one or more like that one and how can we
cluster and and so on there are more
examples as well
here's a practical example this is I
don't even know this person but I've
always admired this blog post they did
so basically what they did is they
looked at Washington DC they looked at
the bike used per hour daily for all
days of the week for all bike rental
spots so I don't know I'm guessing
Austin has something like this too but
you know you sort of dock the bike
you're a member you take the bike to
another dock and they looked at the
traffic for one bikes were being taken
out of different spots and they
basically identified three kinds of
stations right so there was the one that
was busy both at the beginning of the
day and the end there was one that was
busy only at the end of the day and
there was one that was busy only at the
beginning of the day and what they
concluded was this sort of showed some
kind of pattern of people's movements
both to go to work and also sort of
where they go after work because
apparently often they don't go straight
home or we would have only the double
bump situation so there's plenty of
examples of fairly clean time-series
clustering in the wild I think that's
especially true for sort of human
behavior because we have these it turns
out really boring patterns like we go to
work we come home from work it's
actually really boring if you look at
human data a little bit noisier if you
look at things like astronomical data or
financial time series and that sort of
thing but sometimes you get really
beautiful clustering and then finally
yes okay finally last overview so I just
want to talk about the distance metric
we are going to look at for time series
which is called dynamic time warping
this is going to be an alternative
solution to that clustering problem
right so I highlighted how you might
generate features and features might
work well if you're looking at sort of
one particular class of time series but
it can be very difficult to generate
features that are robust across many
different classes of time series within
the same data set so another option that
people have come up with that's quite
computationally taxing but works pretty
well is called dynamic time warping so
what you can see here
imagine these solid curves are my two
time series I'm trying to line up
basically what it does is it tries to
find it tries to keep them parallel but
find a way to maximize we have the
goodness of fit so you can almost
and the nice thing about this is that
this tends to correspond pretty well
with what we do visually without having
to actually do the image analysis right
so this looks at it and just like we can
sort of squint and we can see the two
the two peaks at the beginning and in
our brains we want to match them up that
does that computationally so we're going
to revisit that and see what that looks
like so that's an overview of what we're
gonna do in the notebook so now let's go
to notebook number three so we're gonna
go out of the state space into the
machine learning and we'll start with
trees for clustering or classification
and prediction okay so the data set
we're going to start with is a data set
that's available via the cesium library
and it is an EEG data set so that's for
the brain rather than the heart and I'm
sure somebody in this room knows a ton
about it so feel free to pipe up if you
have any sort of input always
interesting alright so we visualized
this and we see that there we can even
sort of qualitatively see a difference
between three samples we can see at the
bottom there sort of one highly volatile
sample this is sort of all over the
place and then the top two are sort of
less volatile it's not just the
volatility though if we look we also
spot just differences in the values
right so the top plot looks like it
might be sort of Gaussian around zero
and it has sort of a range of like minus
one hundred to one hundred the second
plot if we were to just look at it
visually it doesn't look that different
from the top plot but if we look at the
values this one only ranges from zero to
minus a hundred right so the actual
values in this case seem to be some kind
of tip-off of the classes and then again
if we look at the last one this might be
sort of Gaussian and hard to know from
this but we see the range is also quite
different 500 to minus 500 right so
already we can spot that we'd want to
find ways to sort of preserve
information about the value
if we were thinking about generating
features as far as what kind of data
we're looking at we're looking at numpy
array is served within a dictionary and
they're giving us times measurements and
classes so that's something to be aware
of the measurements themselves are a
list and it looks like we've got 500
samples and then if we look at just one
of those samples it's four thousand and
ninety seven points long so you already
get a sense of oh my goodness so if I
have five hundred time series and each
of them has four thousand and ninety
seven points you see already how it gets
to be a little bit computationally
taxing just to go through all of that so
next if we generate features feel free
to go ahead and hit that button if you
want at least on my rinky-dink laptop
this takes a really long time so I'm
just gonna load it up from prepared data
so that's the code I did right and what
did I what did I extract I extracted the
amplitude the percent that's beyond one
standard deviation right so a measure of
like how many outliers we have and how
spread out they are the percent close to
the media and write some sort of measure
of how tightly is hewing to those
central values this skew and the
distribution and also the max slope
which is sort of how much it jumped up
or down absolute value from one data
point to another how did I find these
features well I just looked I looked at
my data a little bit saw what will fit
reasonably well and then I looked at the
caesium documentation and I generated
those so here you can see if you did
just load it up from the CSV this is
what you see and what do we see we see
some sort of reasonable variation and if
we look at the shape what is the shape
now it's just 500 by 6 and actually one
of those is just the channel so it's not
even relevant but I've digested 5,500 x
4,000 points down to 500 by 6 points
right so if I'm in a rinky-dink laptop
situation in particular or if I'm not
but I have some enormous time series
database and still need to compress it
down this is a way to do that right I've
really shrunk my data down we're gonna
skip this exercise of validating slash
calculating these features but I just
want
point out that you care right so cesium
cesium is this feature generation
library it is not the only one there are
plenty of Python feature generation
libraries some of them are
general-purpose like I talked about that
catch-22 dataset they have this
general-purpose library and some of them
are specialized such as I mentioned
there are astronomical time series
feature generation libraries it's good
to know these libraries are available
especially for things like these
features I can absolutely write out the
code for these features which I did here
but it's a pain in the butt right for
sort of standard things like calculating
the skew or calculating the max loop
why should I hand code those alright so
especially in the exploratory component
I might want to just farm that out to an
existing library so you can check on
your own time that indeed the features
we calculate match the features that are
produced by the algorithm I just check
for one data point and we can also sort
of think okay well are these meaningful
so one thing I did for example is did a
histogram of the amplitude feature by
class and it looks like at least for one
class right one class at least
distinguishes itself compared to the
other so okay there's at least some
preliminary evidence that some of these
features will help to distinguish
classes which is what I want to do okay
so these are these are some other
examples of plotting histograms to again
check are these features I generated
somewhat useful and in fact you can go
through quite a bit of feature
importance analysis by the time you're
doing that for time series it's just
like how you would do it for
cross-sectional data right do the
features I'm producing tend in some way
to correlate with the outcome I'm
interested in B that a classifier or a
forecast or whatever it is so you
definitely would want to go through and
check that they're sensible you also
ultimately if you have a well-developed
set of features you probably at some
point would want to code them yourself
right so I mentioned cesium or other
feature generation libraries are useful
in the sense of why should I code up the
same things repeatedly there's a few
solutions to that one is if you do this
enough you'll just have your own library
which is even better because you really
know it line by line and you know how it
works
ultimately if you have a really well
developed set of features you also don't
want to
out-of-the-box solution because that's
coded to be very general if I on the
other hand know the same three features
I always calculate one thing to keep in
mind is sometimes features can be
calculated together to cut down on
computational cost right so if I'm
looking at min and Max there's no reason
to do one pass through the data to find
the max and then another pass through
the data to find the men right so you
might want to group all things that go
through the whole data in the same way
and do them all together that's not
something a future generation library
will do for you I have not yet seen an
example though I love if anyone has one
of somebody sort of coding things up to
take advantage of those that would be a
great contribution to the open source
community if anyone wants to but in the
meantime be aware of that because even
in this really tiny data set I found I
had a weak quite a while to generate the
features right so this can blow up very
quickly and you don't want to spend
whole afternoons at work sort of sitting
there waiting for features that may not
even be very useful okay so that's the
future generation it can be quite
straightforward should be
domain-specific but in this case we sort
of went with a few common-sense features
and we're going to see how they work so
we prepare a training and a test set and
let's roll out a random forest
classifier we're going to use ten
estimators a max depth of three set a
seed for no particular reason we're just
saying okay how does this random forest
do so if we fit it and score it on our
training data we've got like a 62
percent sixty-three percent accuracy the
good news is our test data is not much
lower so it doesn't seem like we over
fit it seems like we have a reasonably
reasonably generalized model and if we
look sort of at what came back it also
looks reasonably well distributed so we
have five classes they're all sort of
distributed in our underlying test data
so that's something interesting to know
when we're thinking about our possible
null model right so when you think about
is 62% a good accuracy of course you
want to think about well what would even
a really dumb model do right so if we
have five five classes a dumb model
would not be doing 60% accuracy right so
is just some sort of sanity test that
our data is indeed doing something so
that's our random forest right it's just
to point out okay you can put some
feature
and get a classification that is far
better than a random classification with
just five features so I boil it down
essentially 4,100 data points to five
data points without even much
exploration and I could already do
something right so time series
classification is not that hard to get
some kind of result that is better than
random and that really boils a whole
dataset down to just a few numbers and
it depends of course on the quality of
the data and what you're looking at okay
so now we're going to run through the
exact same analysis just with XG boost
instead right so now I'm going to use an
extra boost classifier I'm going to
again use ten estimators and max depth
of three I'm gonna fit that and I'm
gonna score that and what I see now is I
have improved right away on my random
forest model this is as a rule of thumb
quite expected with time series so as I
was mentioning extra boost and gradient
boosted trees tend to do extremely well
on time series data relative to what had
been there before so that's something to
keep in mind although of course here we
do see a little bit of evidence of
overfitting so I might want to prune
that back and as a reminder what you get
with extra boost that you don't get
automatically with a random forest is
some measure of future importance so if
you were iterating and thinking about
okay well what features turned out to be
useful especially if you don't want to
go through all the time series and plot
them all if you don't have time for that
or if the time series are just so noisy
that the visual is not giving you much
information you can use something like
feature importance as one way of
thinking about what kinds of features
are useful and then you might look and
say okay so feature 0 is useful and what
was feature 0 anyway if I go all the way
back up what was that did it so that was
the amplitude right and that was
actually one of the things that jumped
out at us so if it says amplitude is
useful right these indicators of sort of
the numerical range is useful one thing
I might do to improve this model is say
okay well what other sorts of features
provide similar complimentary
information right so maybe in addition
to the amplitude maybe I need to add the
mean or something like that so provide
more information so you can use this as
a way of sort of learning also for this
kind of data what is useful
so that's classification we also want to
think about forecasting though right so
in this case we're going to take a look
at the air passengers data set which I
highlighted in a slide at the beginning
of the lecture so we're gonna load the
air passengers data set take a look at
what this looks like so here we can see
we've got monthly data going back to
1949 set our index and take a look at
what that looks like let's plot it and
so here we can see the plot there's
definitely some seasonal component and
there's also some trend component it's
going up now if we were doing say ARIMA
modeling we would definitely need to
make this stationary that's not a per se
requirement with something like machine
learning but it is generally a good idea
right so some of the same considerations
about what your data looks like for
traditional statistical models are still
things you can do to make your data
cleaner and easier to digest for a
machine learning model so that's thing
about log transforming this and also
dipping it so if we do that we get a
time series that is more uniform in its
variants and values right which has to
be a good thing because basically what
we're doing is making all the sub series
from this time series more comparable
right because what we're going to do
many different options and many
different samples by having a sliding
window right so this could be one sample
this could be another sample this could
be another sample because in this case I
have one baseline time series I'm gonna
convert that into many samples for my
machine learning algorithm so here's
where we see another difference compared
to a statistical model or a state space
model in the statistical or state space
model the model for inputting your data
is you have one long time series and you
throw that in on the other hand with
machine learning the model has nothing
to do with being aware of this temporal
component of the data so instead we take
different time windows we chop up our
time series and those become different
samples so that becomes a way to use one
time series and just model that one time
series for machine learning so why is
this different from what we just did my
classification because we
trying to forecast and we're trying to
forecast one series in particular if we
had many time series and that would be a
different consideration do we want each
time series to just be one sample or
even in that case do we want to chop up
each of those time series into sub
samples and use them all but in any case
what you want to make sure you do since
you're going to be doing a sliding
window across here is you would like to
make all these data points as comparable
to one another as possible because if
you think about a machine learning
perspective right the the machine
learning algorithm just sort of sees a
slice here and a slice here and it has
no way of sort of knowing which dynamics
should be carried around forward or
which dynamics are unique to a
particular aspect whereas if you give it
this they're all clearly more comparable
to one another thanks to those
transformations and then you would just
need to make sure to transform back at
the end when you wanted to check your
predictions so we're going to have a
time series that is the diff of the log
of these values and now there's an
exercise I'm going to ask you to think
about now that we have one time series
and we have it in a form where sort of
all components of the full range of time
look very similar how can we convert
that to many samples so take a couple of
minutes and think about how you would
chop this up to create different sub
time series as samples okay so let's
take a look at this together
what I decided to do and there's no
reason that this is especially great or
anything but I decided to break it up
into twelve months so to sort of look at
a year at a time a slice now arguably
that's not so great because you can't
really capture the seasonality if you
just have one sample of the season but I
said what the heck that's what I'm gonna
do so um twelve steps I think what
twelve steps does give you the
opportunity to do those maybe at least
be able to pinpoint where and where in
the seasonal cycle you are when it's
looking at many many examples whereas if
you have too few months you probably
can't spot that at all so that was part
of the rationale and then what I did was
I wanted first to just give it I pre
allocated this array of Val so if we
look at this it's a hundred and forty
three by twelve so basically I've just
stacked the time series twelve times
because I'm gonna want twelve time steps
and then what I do is I'm just gonna lag
it and I'm sure there's a more elegant
way to do this I used a for loop just
you know my Genki solution and I don't
have to think too hard about it and
basically I just for each column I shift
it up by however many lags I want right
because I want the first value to be 12
months in the past but relative to the
last value because I want to move
forward in time so if I do this then I
get this valve so now I have only 132 by
12 so why did I go from a hundred and
forty three samples to 132 samples well
I lost the samples where I don't have a
full 12 months right so at the very
beginning of the time series I don't
have those even of 12 months but I have
to go 12 months into the time series to
even have one sub sample of a time
series so if I look at that I can then
look at say the last and steps of my
time series right so that's that's this
time series I depicted up above right so
my last my last twelve steps are these
last values right the twelve last values
of the global time series and that
becomes the last sample now that I have
broken my time series into individual
windows right so vallis minus one is
just my last sample that I'm going to in
some way process for a machine learning
algorithm and notice these are the same
values right so I basically I converted
this box of say the last twelve samples
that's now sort of one sample time
series one thing I hope you noticed is
that I could actually have more samples
right now I only have a hundred and
thirty-two what would be some ways to
have more or at least I can think of one
way let's see so two they do overlap so
if you look see I have minus one six
seven - oh nine and then minus one six
seven - oh nine here but actually that's
a fantastic point and when I do my deep
learning example that was like my trick
there I was that we don't have
overlapping and we could
so if I have chopped it up to have
non-overlapping time windows that's kind
of silly if I think I want more time
series I should just have a sliding
window which is actually what I've done
in this case the way it was coded up
just by lagging one at a time rather
than chopping them up whereas if I had
done say a reshape then I certainly
wouldn't have had overlap and I would
have had to put it put that back in so I
do have I don't I do have overlap
already so I'm using each point as much
as I can I guess I think I want to point
out is if I reduce my number of steps I
could get a couple of more time points
out of there right so if I had only six
steps instead of twelve I would get six
more sequences most of the time that
won't matter maybe in such a small set
it might matter so that's something to
think about another thing you could
think about less used in machine
learning compared to deep learning but
you could think about doing some data
augmentation if you have some domain
knowledge to think about ways to augment
your data in a way that's realistic okay
so that's how I prepare my data so I've
just created shorter time series I
haven't actually created features yet
right so with forecasting we have this
additional step of first we take our
long series and we break it up into
smaller series I still need to feature
eyes right I still need to do the same
thing I did earlier for the
classification so I'm going to do that
here so I convert my measures to a list
because that's what cesium expects right
so I've got my list of measures and each
measure is this 11 times to 11 times
steps long and then I'm gonna feature
eyes this one I'm gonna run through just
so you guys can see this one is small
enough I can actually do it but as you
can see at least on my rinky-dink laptop
it still takes a while right so even
this really tiny data set that should be
instant but it isn't in part because
this is not a very general library and
in part because time series feature
generation is just that slow so if I
look at these columns what have I got I
think these are the same features I used
above so sort of a generic set of
features if I look at the histogram of
these let's see oh well I see it's sort
of multimodal I'm like wow that that's
cool maybe there's these underlying
populations I can get all excited about
some possible structure
similarly if I look at percent amplitude
I see something that might potentially
be multimodal so that seems like maybe
it could be good when I'm going to use a
tree to do some kind of regression task
I like multimodal it suggests there's
some sort of meaningful data underneath
so I'm gonna run now I'm gonna run
instead of an XG boost classifier I'm
gonna run a regresar so it's actually
the same process now that I have
converted my inputs to features it's the
same except now I'm looking for a
numerical output to forecast a value and
my Y instead of being a label is going
to be the forecasted value so if I do
that and I run my model oh well my RMS
he is dropping that's great on the other
hand our MSE can be hard to digest right
what does that what does that really
mean
so I can do with a few things I can do a
scatter plot of my test data notice I
have very little test data because this
is a very small data set you never
really use machine learning on such a
tiny data set right so keep in mind and
there you know your real-world
application you would have more points
but that also means you could
potentially be waiting many hours to
generate your feature so obviously we're
not doing that here but do keep in mind
your future generation will just sort of
blow up so if we look at our testing and
our even our training doesn't look
especially good here this is really bad
right I was just telling you guys how
amazing XG boost is and how it has
really contributed to machine learning
for time series I even have a negative
correlation between my prediction and my
actual value that's really bad right
that means I'm sort of actively doing
worse even than chance I would do better
to just sort of predict 0 always rather
than run my model I certainly don't want
to tell my boss this so exercise for the
reader although you can scroll down and
see the answer but just think for a
minute what what ones so terribly wrong
here is this remember this is this is
actually a really easy data set right I
showed you at the beginning of this
whole tutorial the air passengers data
and traditional ARIMA did great so why
can't I do anything so certainly
transforming makes it harder right and
that's actually it's a frequent sort of
mistake
see both in time-series research papers
and also blogs is sometimes people get
these amazing r-squared values or
amazing correlations but that's mainly
because they haven't transformed it and
sure like most of the prediction just
goes into the magnitude where is
actually the utility of the forecast is
sort of that Delta so especially when we
dipped it that makes it into a much
harder task because once you diff a time
series predicting the last value for the
next one is not a great strategy so we
have turned it into a harder problem but
actually when you transform it and do an
ARIMA model you'll still get great
results so that is not why that's maybe
why we don't get impressive sounding
numbers but that it still should do
better than it's doing so what what else
could have gone wrong so think about
what we did we sort of we took a sliding
window of basically a year's worth of
data and then we summarized it with
things like amplitude what else did we
use let's go back up percent beyond one
standard deviation skew max slope etc
what else could have gone wrong there
so I might buy that and this is where it
can be helpful to have an owl model I
might buy this thing I was like oh well
maybe this data is just not amenable to
this analysis like it's it's evolving so
much over time that it's not reasonable
to forecast but I guess I'd have to
rebuttals to that right one rebuttal is
even with our eyes we can sort of see
something so we should be able to find
some technique that can do that and also
as I mentioned the ARIMA can do it so if
we even though we have a model already
we know we can do it
that's not apparent once you have the
log transform but any any algorithm
worth its salt should not just not work
because we made a transform that makes
it more uniform but absolutely we should
think about those things I did de trend
it yes
so you could say removing the trend may
be removed all the signal I think that's
one way of framing what he said so
that's what I'm saying that could be one
possibility if we weren't aware that
there is a model that works and this is
especially if you work in fields where
actually even the best model is pretty
bad you can have these debates all day
and sometimes you just don't know right
is it that there's just no signal left
or is it that we have a bad model and
that you know for some people that's
their whole job is figuring that out or
trying to figure that out and it happens
a lot in time serious in this case
though there actually let me call your
attention to this so we we didn't really
look at our future set in this case once
we computed it so here's the feature set
for my first five data points and what
do you notice here a lot of the values
are the same right so I mean if this was
what I was looking at rather than let's
say the picture of the time series I
mean really what am I supposed to do
with this the percent amplitude only has
two different numbers here the max slope
has the same number throughout and if we
think about why right let's let's plot
three samples of the same data right
remember it's a sliding window that's
the problem is we have created these
points out of sliding windows but then
the data that
have produced tends to just look at
things like the amplitude the numerical
qualities rather than say positional
qualities right so if I look with my eye
at these three curves I can see they're
different they're sort of at different
phases of the same cycle so if I can see
sort of that phase information which is
positional information I'm in a great
point but my algorithm right now is not
seeing anything that sort of positional
information like telling me where a peak
is it's only seeing information about
numbers right but if I'm only looking at
these time series along this axis I'm
actually not seeing anything that
differentiates these three right along
this axis the value axis they're no
different and that's part of what I did
wrong here is I only used features that
describe sort of the underlying values
produced by the process without
providing features that describe the
temporal structure rate the structure
along this x-axis that turned out to be
reasonably okay for the EEG data because
there was already so much variation on
the numerical axis we didn't even really
need to treat it as a time series right
we could just sort of summarize the
values we saw and we got a reasonable
classification but in this data set
that's not going to cut it right the
different samples are not different
along their numerical axis their
different along their positional axis so
let's revisit this and generate features
that encode some kind of positional
information so this is just one stab at
doing that basically we're going to
generate six features and what are these
features that I generated let's just
read them okay so there the first one is
NP dot where the values is equal to the
max so now I want to know at which time
step does the max occur my next feature
is NP dot where values is equal to the
men so now I want to know at which time
step does the minimum occur right so now
going back here those two features are
going to give me information about this
axis rather than this axis because
that's what I need to differentiate
these samples what have I got here I've
got the distance between the min and the
max and notice it's actually more than
distance its positional right because if
the min occurs before the Max versus
after this will be positive or negative
right so this is giving me
directionality
the relationship between the men and the
max I'm also still gonna keep the max in
this might not be that useful but I
figure what the half just in case that
matters and now I'm gonna take what am I
taking here I'm taking the last one -
the next - last one
why am I doing that well in this case I
have a really short time series right
just 11 steps and it seems like if I
look at the end the end last few points
are where these three distinguished
themselves even though they're sort of
neighboring sliding windows the part
where they're most different is at the
end so I'm just saying I want some
features to sort of focus on the
behavior that occurs here at the end so
that's what I'm doing with these again
I'm sort of blindly throwing these in
other than this idea that I need to
understand this positional relevance so
let me do this and then let me make sure
these features are different because if
they're not I'm really in trouble but
these do appear to be at least a little
bit different actually frame it just
prints a little prettier so you know
it's not great I'd have ten nine eight
but ten nine eight is a heck of a lot
better than 10 10 10 right so we at
least go from having columns that are
just absolutely identical which are
total garbage for our decision tree into
one where there are at least some
meaningful differences even between
quite similar time series because at the
end those time series have different
outputs okay so here we do that so we're
gonna divide it up again let's fit this
again notice here we've still got our
rmse is about the same point 1 3 5 what
was it up here where is it dude - dude
0.138 so I'm not doing that much better
on my rmse front let's do a scatterplot
though so here's my test and let's look
at the correlation on my test okay so
how do I adjust this so this is my test
plot let's remember what the first test
plot looked like with the total garbage
features I mean clearly this plot is a
lot better right so our old plot with
the garbage features it's almost like
they're just on independent axes it's
almost like there is no relationship at
all between our prediction and our a
true value here that's not true at
all right so I think we're seeing a case
again where our MSE won't tell you the
full story and that's something you need
to keep in mind in general with your
time series models no particular metric
is going to get it right all the time
you want to take a more holistic view
and it will also depend on your
applications right so our MSE gives me
the same thing for a model that has
almost no relationship to my outcome
versus one that does right if I go by
plotting and I would also point out here
especially if you ignore these two
outlier points and you seem to have even
more of a relationship right so if I do
my Pearson R versus my Spearman are my
spearmint R to get rid of a bit of the
outliers the correlation is a little bit
less negative although it's still
negative right so this model still needs
work but at least now we're bringing
some sense to the data versus before
because our inputs were identical for
different outcomes there was just no way
to get started for the algorithm and if
we do a scatter plot for the training we
see that we do even better right so is
again if we ignore that outlier in
particular this begins to look like
something of a relationship and actually
if we ignore the outlier we might even
finally be into the realm of positive
correlation okay so not super impressive
results because again we're dealing with
a small data set so actually this
exercise is here to show you a few
things right we saw how we can take one
time series and turn it into many
sliding windows to produce enough
separate samples to fit an algorithm
safe for prediction but on the other
hand I think what we see here is a it
can be very easy to generate the wrong
feature set and even a nonsense feature
set which is a reason you should be
careful about using packages that are
automated feature generation it could be
that your features are not in any way
meaningfully distinguishing between say
neighboring time series which is
something you want to do but you also
want to remember that especially for low
data situations machine learning is
really not the way to go so if you're
doing some sort of forecasting that
involves low quantities of data that's
not really what machine learning is
built for right versus something like
the area my model is so part of using
machine learning for time series is
knowing when to use it judiciously and
when you just don't have enough data to
justify it so all of these techniques
should really be used on much much
larger data sets and if you use them on
the larger data sets that's where you're
going to see some really excellent
performance and the
your data set the more likely that
machine learning is the way to go rather
than the traditional statistical models
because as I mentioned those do well on
small data sets and then they sort of
max out your estimation of your
parameters can only get so good when you
don't have that money first is when you
have many many parameters in a more
complex model if you have the data to
justify it you can get really excellent
results and you can see that for example
in cago competitions and industry
research papers in academic modeling
competitions of timeseriesforecasting XG
boost is performing very well on large
volume data ok so any questions about
machine learning and time series ok so
can I look at your computer while we
take a break as we're about to take a
break ok great so we're gonna take a
break let's reconvene at 11:20 it's
11:11 now and we will cover deep
learning as our last component so we
have we're gonna cover two more
notebooks I do want to briefly cover
time series clustering so I'm going to
fly through this and then we'll get to
the deep learning example there are two
deep learning examples in the notebooks
one on electricity forecasting and one
on stock forecasting the stock
forecasting one is sort of supposed to
be like a you know do it on your own
after the tutorial thing that was
because I wanted you to have a tensor
flow example as well as a mixed net
example just because I like those two
libraries but the concepts are the same
so we're going to go through the more
interesting example and then the stock
prediction example is just a second
example with noisier data and with a
different framework so that's just the
background but let's talk about
clustering let me just again briefly
remind you of the dynamic time warping
just because that's the thing I want to
cover ok so what we just did we used we
looked at how we generate features for a
time series what can go wrong there what
can go right there or why we might not
want to do it sort of blindly why we
might want to put some thought into it
we saw that feature generation can be
really time consuming we learned how to
extract many samples from a machine
learning
perspective samples rather than just one
time series to accommodate the nature of
how much team learning works and how it
thinks in terms of samples rather than
in terms of time series so now briefly
we're gonna look at clustering and
mainly I want to show you the value of
dynamic time warping so again as a
reminder this is one way to map time
series to one another in a way that
looks sort of more at their shapes
rather than at features so we're gonna
compare those briefly and see see how
they work okay so in this case what we
are looking at this is actually from the
other tutorial I gave a few years ago
this is sort of a test subset of what
are some word projections this is based
off a paper from I think the 1990s or
early aughts before we had all this
lovely deep learning and where people
were thinking okay like we scan
historical documents is there a way to
sort of identify which words within the
documents are the same and what they
came up with is let's look at these
documents look at a word in handwriting
right we're looking at sort of
historical stuff let's project that onto
a 1d axis right so sort of all the
letters on the word we just do a bin
count of the density convert that to a
1d axis that is actually an ordered
evenly spaced axis so it's like a
temporal axis even though it's not per
se time so if we do that they basically
boiled it down to about two hundred and
seven data points per word and if we
looked at what some of the words looked
like and they don't tell us what word in
the classification corresponds to
certain handwriting but we can see there
are sort of distinctive shapes right
different numbers of Peaks different
locations of the peaks and I also just
wanted to point out sometimes you can
just have a different way of summarizing
the data and you almost create a new
time series so this on the left is sort
of the time series of the projection of
the word and then on the right I took a
histogram of the values so basically I
took this summarised it numerically but
you can even think of a histogram as a
time series in the sense of your x axis
is again evenly spaced right so once you
start realizing that time series
temporal axis can be a way to think
about shapes of curves more generally
you can think of all sorts of ways to
generate features one really nice
sort of visualize time series when you
want to see many examples of the same
sort of class of a time series is you
can think about things like producing a
2d histogram so in this case we're
looking at the word 23 so let's see I
believe this this is word 23 yeah this
is where 23 here but that's just one
example right what if we want to make
sure that the features we see here are
more general we can do something like a
2d histogram and we see oh yeah it looks
like there are sort of two peaks here
although the location is not set that
specifically and a couple of Peaks going
on here and this is actually a good
example of part of the reason we can't
just feed the raw time series as the
inputs or as the features right because
things like Peaks their location jumps
around whereas if we were to just feed
it in that behavior won't really be
documented so we're gonna just generate
some features again off of them off of
what we've seen recently briefly so
again I used cesium and again this is
sort of time consuming so I would
recommend just reading it off the CSV
you could on your own time possibly
consider generating them but it can be
quite time consuming and again we sort
of look at a histogram to get a sense of
what these features looks like we at
least want to make sure we've got some
sort of spread we're gonna generate some
features for the histogram itself as
well treating that as a sort of
directional time series we're also going
to find the location of the Max value so
we sort of learned our lesson from that
last example we did with the air
passengers let's put in some sort of
positional information that we found
ourselves in this case where the max is
and then finally for clustering we want
to make sure all the features sort of
count equally right so we want to make
sure that it's not like amplitude just
swapped out percent beyond one standard
D right so what's a quick way to do this
this is not time serious specific but in
general we want to pre-process our
features and then we're gonna cluster
them and in this case I'm using
hierarchical clustering why am i doing
that because I don't really have a good
sense of what my distribution looks like
so I don't want to use something like
k-means where I'm assuming that I
necessarily want to minimize variance
and that I have these sort of well
shaped spheres I don't want to
anything like that at all so if I do
that when I'm going to see this is just
sort of the result of my clustering it
looks pretty messy like I'm not really
finding a good pattern and one measure
of that is like the homogeneity score
right so like how homogeneous are any of
the clusters I made in terms of the
original class labels the punchline is
not very good right so the clustering I
do based on features of my time series
at least for these features not great
now I could put in more work and try to
find good features or I could remember
oh wait dynamic time warping I heard
about this magical distance metric
that's really fantastic let me check
that out right so let's first look at a
toy example here's my toy example where
I prove to you that euclidean distance
is so terrible right so here I have two
sine curves and just a flat line I think
we would all agree that we would rather
match the two sine curves even if they
have sort of different frequencies we'd
rather think of them as being more
similar to each other rather than
thinking that either is sort of more
similar to a flat line right so if I
were going to group these say into only
two groups we'd probably put the two
sine curves together right there's more
fundamental process and common for those
two relative to just the flat line so as
an exercise I would recommend on your
own time considering calculating these
but I'll just point out that if we do
calculate these both of the sine curves
end up having a shorter distance to the
flat line than they do to each other
that you know there's no magic behind it
that's just you know sum of squares and
take the square root that's the behavior
you're going to get super undesirable
behavior so this has two components that
concern me firstly it's not super
computationally efficient any way to do
Euclidean distance right you still need
to go through all the data points it's
not like it's really fast and it's
really crap right so there's two reasons
not to use it it's almost always true
that you wouldn't want to use Euclidean
distance another measure that has been
recommended is a correlation measure
right like how sort of similar are they
how correlated are they that seems like
maybe it could do better than Euclidean
distance so we're going to do the same
thing in this case you see that
I am gonna add a little bit of random
noise to my time series 3 which is the
flatline just so that I don't have an
undefined correlation right because if
there's no variants that measure will
just come back as an an so let's do the
same thing let's look at the correlation
so ha interesting again the sine curves
are sort of negatively correlated with
each other versus they both have a
positive correlation with the flatline
so again if I use correlation a it's not
a super efficient metric anyway and via
doesn't do an especially good job
relative to my desired behavior so what
I would recommend is that we do dynamic
time warping here is one simple
implementation you can definitely code
this on your own so I would definitely
recommend this if you're ever going to
work with dynamic time warping so that
you feel more comfortable with the
definition the meat of it is here right
so I'm using this this is the dynamic
programming aspect is I have a matrix
because basically what I'm trying to do
is for each set of points right for
going up to the eius point on one time
series and the J thought another and
they don't need to be unified I want to
figure out what's the way to align them
that results in the shortest difference
in their values right that best aligns
their difference so I do that by
iterating I start at the beginning of
time series 1 and the beginning of time
series 2 and then I calculate the
distance between them so this is for
example one thing you can play with when
you want an even dynamic time warping
can have many definitions right here I'm
just gonna use the square of the
difference as my distance and then I
want to think okay so the min preceding
distance would either be the distance
from I minus 1 and J or the distance
from I J minus 1 or the difference with
I minus 1 J minus 1 remember these eyes
and J's are how I'm moving along the
temporal axis and remember I have
disconnected the 2 axis of my curves so
I can sort of move along one curve but
not the other or move along both only
one step the way I've defined it but I'm
sort of trying to move along the axis in
a way having a path such that my square
distance here is at a minimum so it's
sort of this iterative way of working
through the data so what if I move one
here but not here
or to here and but not here and how can
I sort of warp these two curves on to
each other in the way that makes them
fit the best that's what we're doing and
then ultimately I returned that minimum
distance I take the square root of that
thing and that's what I'm defining as my
time dynamic time warming distance so
now I sure hope this fixes the problem
or I just wasted a lot of time
so now TS 1/2 T s 2 is 3.7 TS 1 2 ts 3
is also 3 point 7 and T s 2 2 TS 3 is 4
point 1 6 ah so notice it's still not
perfect right I still have a problem of
my flat line not being as different from
my sine curve as I would like but it's a
lot better at least it's not telling me
that my two sine curves are way
different from each other compared to
the flat line so firstly dynamic warm at
dynamic time warping has a lot of
parameters you could tweak so one thing
you might want to do for a particular
dataset is explore sort of different
ways of stepping back and forth right so
what sort of time steps you'll allow and
different distance metrics to see if
maybe you could improve the distinction
depending on your underlying data and
then the other thing you might want to
do is just accept it at some point in
the sense of it is extremely difficult
to define differences between time
series and this is where if if this is
not good enough you might even begin to
look into deep learning because at some
point you say okay it's just an image
analysis problem instead and lets you
know ratchet it up a notch so that's
something to keep in mind to to the
extent we want to keep this as a
clustering problem this is how we want
to do it I also wanted to point out that
there are libraries that code this up so
you won't find dynamic time warping in
any of the sort of major libraries like
SK learn or sci-fi unfortunately but
there are spelled all sorts of DIY
github such as this one here so you can
see that actually we have the same
definition and they have but this is a
much more sophisticated implementation
where they're more knobs to turn so
these things are available but you do
have to do a bit of work now if I were
to compute this pairwise distance it is
really computationally taxing so we
would be here awhile just sitting around
we're not going to do that but we are
going to run through the clustering and
if we look at how the clustering perform
now now we have fantastic
homogeneity and completeness scores
right so the extent to which one class
is covered by one cluster is really good
and the extent to which one cluster is
composed of only one class is also
really good so now we have a really nice
one-to-one mapping versus if you
remember the homogeneity score for just
the features was more like 50% so your
distance metric can make a tremendous
difference for time series clustering
and similarly you can also use these
sorts of things for forecasting or
classification right so for
classification I think it's obvious you
would just sort of map something on to
the cluster and if your clusters are
quite homogeneous that's already useful
you can also use this for forecasting in
the sense of if you have a partial curve
you can say what curve or what time
series is this closest to and then use
that time series and its outcome to
predict the outcome for your new feature
so that's something you see in finance
you see that in health as well where
you're just sort of saying for example
this patient up to time T who are they
most similar to and maybe that is you
know a likely outcome for that patient
too and that's really interesting
because then you can use quite
heterogeneous data you can use really
messy data and you'll get a pretty good
result here as compared to trying to
come up with a feature set which can be
quite challenging so this is an
alternative way to do both forecasting
and classification any questions okay
great so we are on to our last component
which is deep learning for time series
so we just have a couple of slides to
cover okay so she'll have hands if you
have worked with a deep learning
framework of some kind before at least
on a toy example okay so for those who
haven't I'm gonna give like a very brief
rundown but this is in no way a good
substitute for reading up on it and
learning these packages but I just want
to give you a sense of how this would
look in a time series context so we
start with the simplest of neural
network examples which would be
something like a fully connected model
so neural networks are really just a
form of machine learning in the sense
that they expect a series of inputs and
will give you a series of outputs and
they are not necessarily time aware
right
they're not looking for give me your
time series and I will fit a model to a
time series they are looking for samples
right X and outputs and the beautiful
thing just like machine learning is deep
learning is like fairly agnostic it
doesn't really care what inputs you give
it right to some extent these have been
built as you throw anything you want at
it and you may very well get a nice
answer if you sort of tweak your
parameters and you have your training
and you know that aspect is a bit of an
art but what does this look like in the
simplest model you have an input layer
which would be for example could be
features so this looks a lot like a
decision tree right where we would
create our features and then we would
input them and the difference between
say a decision tree and a neural network
it's in the neural network will look
like this it will for example if you
were using a fully connected model then
every input would go through some kind
of multiplication with a weight and
possibly some sort of activation
function an activation function is
essentially just a way of introducing
non-linearity so you would essentially
have like a matrix multiplication
coupled with a non-linearity to produce
your next set of inputs into this layer
this layer would do the exact same thing
right some sort of matrix multiplication
followed by some nonlinear activation
and output it and then at the end you
might have a bunch of new features that
have come out through your model and you
find some way of combining them to
produce your output that's like you know
the very very high-level view for those
who really have not encountered this
before and obviously there's a lot that
goes into figuring out how you should do
that properly how to initialize it you
very quickly get into the millions of
parameters so we're gonna leave all that
aside and there are great tutorials
outside Syfy this week on this topic so
you can learn more but the main point
here is that a fully connected model
from a time series perspective is just
like another machine learning model it
will still require future generation and
a lot of thought about how to sort of
translate your time series into
something that this kind of model can
digest so it may very well do better
with say than a decision tree right
that's sort of an empirical question for
a particular kind of data but this is
not in any way time aware just like
machine learning techniques are not in
any way time aware and we have to sort
of cut up and pre process our time into
a format that this
since there are other options though so
the classic example of how you would put
a time series into a neural network is
called a recurrent neural network or are
an annual mostly CR NN and the idea here
is what this does
recurrent recognizes that your data will
come again and again your data recurs so
you build one cell here we go one cell
but basically what you do is unroll it
and you reapply it at each stage with
your new data so this provides a
unifying slash temporally aware way of
looking at your data because you have a
model that understands it's going to be
rolled out and used on the same data so
what does this look like for a
univariate or multivariate case and each
either example doesn't matter you have
your existing neural network model first
you put in your first value of your time
series right chug-chug-chug it produces
a hidden state you don't need to worry
about that but that's part of it's sort
of internal accounting why does it need
a hidden State well that's what makes it
temporarily aware that's what gives it
the ability to sort of remember things
from state to state right rather than in
this example we're just get something
and chugs out one response and there's
no time here this model itself has a
temporal axis right it produces its
hidden state then you put in your next
value it remembers its hidden state that
affects what goes on in here and out
comes a new hidden state so your model
has some form of memory or some way of
evolving over time recognizing that your
data wants something that recognizes
dynamics that are evolving over time so
recurrent neural networks just like
everything else we've talked about today
not a new idea I don't know the exact
time but I believe it's early 1980s that
these were envisioned and even things
like you probably know the term grew or
LST M my understanding is even in LST M
was envisioned in the 80s although maybe
I'm wrong in it's the 90s but it was
certainly a long time ago even though
these are still considered relatively
cutting-edge exciting new technologies
what's really exciting is a we've
developed much more of an art as to how
to fit these you know how do we
initialize these things what does
backpropagation looks like and of course
much more exciting is that we've all
gotten now fancy GPUs and things that
can actually chug through this right so
the ability to imagine something
this was there in the 60s right but the
ability to actually do it is what's
fairly new and also the presence of
enough data to get good results is
fairly new so the the two of variants of
recurrent neural networks that have
turned out to be quite successful one is
called a GRU so you see an example here
so what we're looking at now is if we
were to actually unpack the guts of this
thing so if we unpack the guts this is
what we've got we've if we've got our
our XT that comes in we track our hidden
States it goes through several different
variants of transformation and people
have sort of liked the update gate the
forget gate the idea is is you have
different components one is sort of
supposed to help the model to decide how
much is to sort of update its hidden
state based on new information another
sort of looks at the new information
comes it coming in and sort of
transforms it before it even sort of
talks to the hidden state and so you
have all these parameters that can sort
of specialize to describe different
different properties of the temporal
dynamics of your data depending on how
they allow the updating now the
interesting thing is in a way this
sounds a bit like what we talked about
with structural time series and
state-space models generally right it is
developing some sort of internal model
just like the common filter of how it
should adapt to new information given
its prior expectations although in this
case it's prior expectations are a
hidden state that is not sort of a well
described statistical thing it's more
like a set of parameters and a matrix
and actually the the theory of this is
not as well developed as I'm sure it
will be you know in a decade from now
and we'll eventually figure out you know
these are these are still models they
can still have statistical properties
it's just so much harder to think about
them but that's what a grue is a
groovers is an LS TM you've probably
heard of these two and L STM tends to be
a little bit more complicated it has one
more gate compared to a grew in many
cases it doesn't perform any better but
it's sort of worth looking at both of
them depending on your dataset
LS TM people say have sort of a longer
memory compared to a grew but again
depends very much on what kind of data
you're looking at okay so convolutional
neural networks you might be surprised
to see this here because you're thinking
oh I heard orang ins are actually rnns
are time aware but
actually CNN's can also be time aware in
the sense that an image is very much
like some of the seeing of things we've
done today right such as time series
classification time series clustering
does not look all that different from
image analysis right so we can also use
convolutional neural networks as a way
of understanding our time series as a
picture that can be on its own already a
good way to classify a time series we
can also do modifications to
convolutional neural networks that make
them temporarily aware so one example of
that is what's called a causal
convolution so in a standard convolution
if you do image analysis it sort of
treats all directions as the same and
all areas are equally spaced but you can
do a very easy modification a causal
convolution where convolution sort of
only goes in one direction temporarily
and in that case you have convolution
that is also time aware that can be used
as an input to other components of a
model or it can just be its own model
which of these performs better will very
much depend on your data what kind of
patterns are in your data and what kind
of tasks you're trying to do are you
trying to predict or are you trying to
classify cnn's tend to be more
successful for time series
classification rather than prediction
but of course that's just a very broad
generalization in a field where we're
still sort of working out the science of
how these things work and so one
actually really cool architecture I
wanted to highlight this was published
about two years ago I think
Ellis T net so these researchers
actually said well why why do we have to
pick and choose between convolutional
and recurrent behaviors there's actually
no reason we shouldn't use both and in a
way that makes sense right because if we
think about something like the airline
passengers data they're sort of bold
aspects right there's this sort of
aspect of we see a trend and a
difference and that sort of more like
RNN ish right to see a trend I'm not
sure convolutional understands that but
on the other hand we have this seasonal
component right where they're sort of
winter fall summer etc etc and that part
seems a bit more like an image analysis
type of component right so they had this
idea they also said we can use the
convolutional bit to understand better
how to use multivariate time series
right so to the extent we want to relate
different inputs right if I have
different inputs into my model and it's
not just a univariate
time-series do I want to sort of
convolve these there are a few
advantages to that one is maybe you'll
discover their relationships between
them especially if we convolve on time
and input right and we sort of take
almost like a sliding window image of
our time series but we can also think
maybe the convolution itself might
identify some seasonality right because
that's sort of image like they also said
well why do we always do sort of
recurrent where we just sort of feed it
in one at a time what we might also want
to do to design our architecture to
better reflect certain realities is we
might want to have something that
reflects the reality of seasonality so
we might also want to have something
where we skip right so maybe if I have
24-hour data I might want to have both
an RNN that looks at all the hours but I
might want to have an RNN that in
addition to looking at all the hours
only looks at the same local hour for
previous day right so if I'm at 3 a.m.
trying to predict the electricity for 4
am maybe I want to look at 1 2 3 4 or
you know 0 whatever up to 3 a.m. you
know that sort of slice but I might want
to look at 3 a.m. from the previous day
and the previous day in previous day and
think of that as its own time series and
have a recurrent neural network that
runs only through that or if not that
maybe only look at the states of the
recurrent neural network at those points
right so for those of you who have
worked with neural networks you know
that there's actually sort of infinite
architectural permutations you can make
anything you can imagine you can build
in a fairly straightforward way
although figuring out how to train it
properly of course is always where you
run into trouble and figuring out if you
actually have enough data to train it
but the wonderful thing about this as
compared to some of the other models
we've looked at today is you can imagine
any sort of dynamics and describe it
with some kind of neural network and see
if you can improve your forecasting or
classification or what-have-you
ah so that that is prediction so let's
see hopefully it will come back up ok so
in the meantime I would ask everyone to
open your notebooks so we're gonna look
at notebook 5
supposed to come back on electricity
okay I'm going to see if I can Oh
excellent thank goodness okay remind me
not to move at all until for the next 13
minutes okay so we're gonna try to
forecast electricity this is one of the
original data sets that was used when
the LST night model was published the
oztent
t net model when it was published they
saw pretty big gains compared to say
something like just using a grue or just
using a convolutional neural network and
they made this point of you know
building a an architecture that reflects
especially three of the temporal
Cadence's of human activity works really
well and also they were making the point
that with multivariate time series it
can be interesting to find ways to
combine those information channels so
those those were what they pointed out
so one of the model one of the data sets
they use with this was this electric
data set that we're going to use now so
the first thing we're gonna do is look
at the data let's look at this together
so if we read it in and look at the head
as you can see this is like very
multi-channel this is three hundred and
twenty one parallel time series of
electric use hourly Electric use at 321
different sites I want to say it's the
state of Alabama but I don't remember
you'd have to check the reference to the
data they do mention but it's hourly
times hourly electric use in some state
in the US and if we plot this as always
there are you know sort of better and
worse ways to plot but if we look this
this is really interesting right this is
just that one site for the full range of
time it's interesting to me because you
have sort of drastically different
regimes so if I were doing say a
traditional statistical model or even a
machine learning model I would be really
hard-pressed to think oh my goodness
like how am I gonna make all of these
different points in time comparable
right I mean this this looks really
different and it would it would take a
lot of thought where's with a neural
network sometimes because we don't for
now have those assumptions built in we
can also expect it to more flexibly
handle really varying data and maybe
even notice things that we wouldn't
notice but this this long-term plot this
gives us one view it tells us that we've
gone through sort of different regimes
of electric use at this one site and
this is just one of 321 inputs and
unless I we're doing this as a full-time
job for years I'm not sure I'd be able
to look at all three
twenty-one and know them although great
if you can write but also let's remember
if we look at it at a smaller time scale
will spot other data so if we look at it
at this smaller time scale what we spot
is some kind of recurring pattern right
which makes sense because this is our
early electric use now you might be
thinking oh well I would have expected
to see better use than this and this is
because we have I have pre difference to
this data for you so actually the
original paper fit to the original time
series which looks more like the air
passengers it's sort of account but a
that's too easy and B that tends to over
represent how well you're doing you're
like oh man look at me I got 99%
correlation they match so well when in
fact most of that could just be done
with a null model so if we difference it
out we are being really strict with
ourselves that like the model has to add
something useful that can predict from
hour to hour rather than just sort of
globally getting it right so I encourage
you whenever you can to use difference
data
okay so handy data structure this is not
nothing to do with time series but I
like to use something like a ring buffer
when I'm sort of thinking about a way to
do early stopping so another thing for
those new to neural networks there's no
sort of predefined limit to how long you
should train right you sort of want to
train as long as you're getting better
but you want to make sure you don't over
fit so that's similar to other
techniques we've discussed today the
difference being is that now we could
potentially have way more parameters
than even the most complicated model
we've looked at right we talked about
how structural time series there's so
many knobs to turn looks like you could
really game it if you wanted to that is
like orders of magnitude different from
this this is really a lot of knobs to
turn so we need to make sure to be
disciplined in our fitting so that's
just one way that I did early stopping
ok so data preparation I want to call
your attention to this because this part
actually looks a little bit like what we
had done before right where we take one
long time series and we reshape it into
many slices of a time series in this
case we have multi channels so we're not
just slicing along one axis we actually
have to slice all of our multivariate
time series and make sure that all of
them sort of are these sliding windows
and keep them in sync so that's what
this data does I would encourage you to
look at it on your
as well to see how that works we also
build data iterators so that we can
prepare batches of data okay so here's
the interesting part
so I provide examples of how you could
use a fully connected model or a CNN
model I would encourage you to try those
out on your own you'll find that they
don't tend to do as well as the RNN
model or the simple LST net model that
I've introduced here so just briefly the
RNN model how do you build an RNN model
and mix nut well you need to have a cell
and then you need to add that cell you
decide how many hidden units you have
which is basically saying how large of
like a hidden state matrix do I want to
have right like how many different
parameters do I want to have to describe
this behavior of how it should update
and how it should retain information so
with an RNN model that's essentially
just one layer of this RNN rollout that
we did already now let's look at this
LST net model this is really interesting
and this corresponds to that
architecture I showed you the first
thing we're gonna do in this case is
we're going to apply a convolutional
filter to our inputs so we're going to
take our inputs which is multi-channel
right so actually in this case we have
time by batch number by tnc time by
batch number by channel where channel is
is like 320 right so the first thing
we're gonna do is run that through a
filter where we're basically like
looking at a sliding window as an image
so the window slides both over time but
also over all of our features we then
take the convolutional output and that
becomes the input to our neural network
RNN the recurrent neural network so in
this case we've said instead of just
feeding in our raw values we could also
consider processing those with other
neural components it's almost a way of
saying the convolutional component is
going to produce features from our raw
features it's going to boil those down
to fewer features and then those
features are going to go into the
recurrent bit so we're sort of divvying
up the task we're not going to do the
future generation but some portion of
our network is going to do the future
generation before we feed it into the
recurrent neural network and if you
think about that that makes sense right
because why should I be putting in 320
parallel measurements into the recurrent
neural network which then a has
decide how the measurements are related
to each other and how they're
interesting but B has to keep track of
the temporal component so if I can sort
of shrink down the multivariate aspect
so that the RN and can concentrate on
the temporal component that can be
really helpful now there are two RN n
there are 2 RN n models right there's
one that sort of looks at every data
point and then there's one that sort of
skips seasonality but here I have left
that out and I just have the regular RN
n and the reason for that was I just
found that for this particular data set
it wasn't especially important so that's
something else you want to keep in mind
is what architecture actually improves
your process for a particular model and
then finally what I didn't mention but
what's really interesting is the authors
included an autoregressive element this
is just an element that uses the passed
values to predict future values right
this is the same as an ARIMA model so
actually what we're seeing here also is
combining a statistical approach with
neural networks and this is something
that is increasingly common and that has
actually proven quite successful in time
series in particular much more than in
other areas of machine learning so those
are our models we can define our
training and we can run this let's see
am i running my screen is well my screen
is frozen so we must be running because
that's what happens on my rinky-dink
laptop obviously you don't usually
really want to do deep learning on a
laptop and you don't usually want to do
it on a small data set so keeping both
of those in mind let's see okay well I
would encourage you to check out the
output and experiment with this on your
own but the the punchline that I wish we
had time to train on is that the LSD nap
model does quite a bit better than the
recurrent neural network alone so it's
really important to design structures
that are like savvy about how they
allocate the labor so in this case the
labor of the convolutional bit the
future generation is separated out from
the temporal analysis that's one thing
that makes this model really successful
the other thing that makes this model
really successful is the inclusion of
this AR component the autoroute
recive component and in fact if you
remove that component you take an
enormous hit to performance so this is I
think is great inspiration for how you
can take your traditional statistical
knowledge and work it into your neural
networks and you will find that that
gets you a really great outcome and in
fact so most recently there was an
academic research competition on machine
on on timeseriesforecasting and both the
number one and number two winners of
this competition which took place on a
hundred thousand different time series
over many domains turned out to be
integrating machine learning and
statistical statistical analyses that
are quite traditional in one case
combining statistical analyses with deep
learning and then another case using X G
boost to choose the coefficients to
combine statistical models so also the
ways that you can sort of permute these
things are quite varied gosh I could
have sworn this this ran faster when I
was oh here we go here we go okay so in
this case you can see I'm printing out
the correlation which metric you like
can be a matter of personal preference
it can also be a matter of what is
meaningful for your problem but
basically what we want to see here is
that it's improving especially on the
validation I think we will stop it here
if we can yes okay so stopped it here I
also have the results from the last time
I ran this already loaded no no I'm
lying
suckster
oh I see because I only went up to okay
let's do this okay so in this case we
can see what we're looking at after only
four iterations and we can see so far
for this first column it doesn't look
especially fantastic let's look at this
column quite different right and this
this actually looks a little bit more
promising especially if we ignore
outliers but we can see here that we've
actually managed to fit many time series
right we're fitting 321 time series in
parallel with one generalized model and
so we can sort of expect results to vary
between one and the other so if you let
this go for about 20 to 25 iterations
you get really excellent results but not
after after 5 epochs so I'd encourage
you to keep this running on your own
computers but takeaways here are this is
not even an enormous time series data
set right I can fit this all in memory
on my rinky-dink laptop so I actually in
my experience have found that deep
learning works even better than like
sort of regression trees and things like
that for cases of smallish
but not tiny data sets for deep learning
compared to statistical models and also
that you want to be creative about
combining your statistical models and
your deep learning so this is where you
see especially for time series analysis
having a broad domain knowledge of many
different sort of classes of analysis
will also help you to build more
creative models so even when you want to
do something more cutting-edge like deep
learning having that traditional
knowledge will really help you ok so I
have one slide to wrap up because I did
want to talk about ok so just to wrap up
to to highlight some things that we
haven't talked about but that are
important also as being on the horizon
or just active areas in modern time
series analysis I think the huge one
that we didn't have time for but that
you can very much address with what we
talked about today is anomaly detection
right so maybe even some of you work in
this field
things like structural time series
hidden Markov models regression trees
excuse and deep learning can all be
applied to the question of anomaly
detection so anomaly detection is not
something where you need
one specific technique actually all of
these are available to you and may even
be combined successfully there's also so
many new and old libraries for time
series analysis especially for more
modern methods there are literally
hundreds of packages definitely this is
an environment where ours options are
richer than Python so you might also
want to get more comfortable especially
like with at least being able to access
our packages through Python to have
access to those time series analysis
obviously is an area of active research
both in industry and in academia but
most people who are doing cutting-edge
research work in our so I'm a bit of a
downer on that front that if you want to
have access to those methods you also
want to be looking at the our ecosystem
as well as the Python ecosystem and then
the two things I would highlight that we
didn't talk about but you would also
want to look into firstly as automated
forecasting at scale so somebody had
brought up Facebook's profit package
there's also a Google package there's
also a Twitter package I'm probably
forgetting some others but there are
some massive tech companies uber looking
at really massive data sets of
time-series developing ideas for their
own research and then sometimes either
open sourcing them or sometimes offering
forecasting as a service so an example
of that is Amazon now offers via AWS is
offering forecasting as a service using
sort of their expertise that they're at
their own data right so if you you have
data that looks like Amazon's data right
like many many parallel time series or
highly variable time series such as
rolling out new products you might want
to look into that the extent to which
they are open about what they're doing
versus a little bit cagey varies by
company so some of these packages are
completely open-source some of them are
completely closed off what is clear when
you read the literature is they're
barely using deep learning it's
primarily traditional ARIMA or
traditional models that have few
parameters because the goal for them is
to just have a fairly reliable
transparent forecast rather than the
perfect forecast and then finally as I
mentioned to some extent it looks like
the future is combining machine learning
and statistical approaches so you very
much do still want to have that
traditional timeseriesforecasting
background and ARIMA and that sort of
thing because that is not going away and
it continues to be part of even the most
cutting
results in time series analysis okay so
I'm available for questions afterwards
thanks very much for coming to the
tutorial and please be in touch if you
have questions or comments
[Applause]
