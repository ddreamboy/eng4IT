Principal component analysis is a
technique used in machine learning to
reduce dimension. In this video we are
going to look at what it is, we'll write
some python code, and in the end there
will be an interesting exercise for you
to work on.
Let's say you're working on a building a
machine learning model to predict
property prices.
Here all the columns on the left hand
side are the
features
or the attributes of the property that
decides the price. So the column in green
is actually your target variable.
You probably already know that the price
of the home is mainly dependent on area,
which town it is in, plot, etc.
It depends on how many bathrooms you
have in a property but not as much. For
example,
2600 square foot home with two bathroom
versus three bathroom, price won't be
that different.
But if you go from 2600 to 3000 square
feet price will be
significantly different. So clearly area
plot plays a more important role in
determining the last price,
bathroom plays a little bit of a role,
and how about this particular column
trees nearby!
Whether you have two trees nearby your
home or three trees nearby your home,
you don't care that much. So that column
probably doesn't impact
the last price at all or or it impacts
it only a little bit.
When you're solving
real life machine learning problems you
will have many columns- hundred, thousand,
two thousand columns or features,
and you need to
do something to identify the features
which are very important.
You're working on let's say handwritten
digit classification;
where you know you have digits which are
written by hand and then you try to
classify as
one of the numbers from 0 to 9.
Here
this image is represented as pixel. Let's
say this is a grade of 8 by 8 and every
number presents the
color. So 0 means black, 16 means highest
white you know
and since it's an 8 by 8 grid there are
total 64 pixels or numbers that helps to
determine what number it is. So this 64
pixels are called features and if you
represent....
Now if you think about uh
some of the pixels in these images you
will find that those pixels don't play
any role at all,
uh
in figuring what is it what digit is it.
For example, these two pixels
here or pretty much any pixel in this
last column.
No matter, what number it is these pixels
are always black. So we can say that
these pixels are not important feature.
And what if we get rid of these features?
We get two benefits out of that. First
the training is faster. You know machine
learning training takes a lot of time,
lot of compute resources. So you want to
save the training, you want to make it
little lightweight
and your inference can also be faster
and data visualization becomes easier.
Let's say you have 100 features and
somehow you reduce those 100 features
into only two features or three features,
then as a human
you can plot it on a 2D or 3D graph you
can visualize it and data visualization
helps
a great
Data visualization helps you a lot in
terms of your final decision making
uh with regards to what kind of model
you want to build.
PCA is a process of figuring out the
most important features
or the principal component that has a
maximum impact on a target variable.
PCA will
create in fact the new feature called
principal component. You know PC1, PC2, and
so on. 
So again going back to the digits
example,
Let's say I have 64
features out of that I am plotting only
two features corner pixel and central
pixel.
Now, here these cluster represent
different digits.
You immediately notice that the corner
pixel is not playing an important role.
Maximum variation is on the y-axis or
maximum variance is on the y-axis, which
is a central pixel. So if I ask you to
reduce this two-dimension into one
dimension, you can easily do so by
getting rid of corner pixel.
So this graph that I have drawn on the
right hand side it is one dimension. I
mean I know, the graph is not perfect
perfect it still looks like two
dimension but you get my point- to reduce
this from two dimension to one dimension
is
easier.
Let's look at the
iris flower data set where sepal width
and sepal height determine what kind of
flower it is.
If you have a scatter plot like this
you can draw a line, which covers the
maximum variance. So this line covers the
maximum variance or the
maximum information in terms of features,
and you can draw a perpendicular line
which covers the second most
variance you know
and these are called principal component.
So here PC1 is this
axis which covers the most of the
variance, PC2 is the axis that covers
the second most variance.
So when you apply PCA, 
you get a chart like this and I know
I have done only
this graph for two dimension but really
if you have 100 dimension, and if you
apply PCA you can figure out let's say
10 most important piece uh principal
components.
So for 100 features, you can actually
create 100 principal components in the
descending order of their impact on the
target variable.
So for digits
if I have to load this in a data frame,
panda data frame will look like this.
There's a little error it's like pixel
63 because I start with pixel 0,
but you already see
pixel 0 and 1 has
all values is 0 they are not kind of
important.
So when you use SKLearn library and
call PCA method where you say okay, n
components is six; you're basically
asking PCA to extract the sixth most
important component,
and that will look something like this.
So what this is doing is calculating new
features,
and these features are such that PC1
covers the most
variance in terms of features, you know
in terms of information extraction from
your dataset. PC2 is the second most
highest component
that contains a lot of information about
the features of your data set.
Here I give six, you can give it anything
you can have two three four five; you
know trial and error.
You can also
give a different
parameter to this method, which will be
like 0.95, which which means you know
get me 95 percent of information in
terms of features, you know get me 95
percent of variance. So you can do that
you will see that in the actual coding
section, but few things to keep in mind
before using PCA is you need to scale
the features. Because if you don't scale
it,
it's not going to work okay. For example,
here is a chart,
and if this graph is in millions and
this graph is in a normal value,
you know the graph might become skewed
and PC and the PCA will not work as per
your expectation.
Accuracy might drop. So it's a trade-off
when you're trying to reduce.
Let's say you have 100 features and if
all 100 features are important and
they're trying to contribute to
end target variable.
If you reduce from 100 to 5 you're going
to lose a lot of information. So we'll
see all of that in the coding session,
but just to summarize PCA is called a
dimensionality reduction technique
as
it helps us reduce dimension. We already
saw here in this case we had 64
dimension.
This helped us to
get to six dimension and when you have
less columns your
computation becomes much faster.
And it also helps you to address the
dimensionality curse problem. In machine
learning dimensionality curse is of
is a big problem so many data sets,
having so many
so many columns, you know so many
dimensions and that makes our model
really complex hard to visualize and PCA
is an excellent technique that help you
deal with dimensionality curse.
Let's get to python coding now.
I opened my Jupyter notebook and
imported some important libraries. I'm
going to use the handwritten digits
dataset from SKLearn
data set module.
Let's
load the data set by calling load digits
function.
When you do that,
it loads the data set and you can
call
dataset.keys to get an idea on you know
what the dataset is all about.
You can see that dataset.data will
contain
all the pixels
and these are the feature names and this
is the target array.
Right!
Now,
let's
look at dataset.data.shape.
So there are
17 almost 1700 or 1800 samples. Every
sample has
64 columns. So if you look at the
first sample
it's a flat one-dimensional array of 64
pixels and if you want to visualize this
data using matplotlib library, you need
to convert it into two dimensional array
and the way you do that is by calling
reshape function, which is because this
is numpy array and when you do eight by
eight it just converts one dimensional
array into
two dimensional
array of pixels.
Now
you can import
matplotlib
and
use
the plot, you know I'm gonna just plot a
gray image
and when you do plot dot mat show like
metric show,
it will
show that it will show a picture of that
array. What is my array? This is my array,
right! See two dimensional array so I'm
gonna just put ctrl c ctrl v
and my first digit
looks like this. This is clearly zero. If
I look at my
second data sample,
see
it is one,
then two
and so on. You know like 50th sample will
be
I think this is two.
Okay,
so this is how you visualize it.
Now,
let's look at
the target. When you do data set dot
target
uh
it's a huge array but it is your end
class you know if you do
unique,
you will find
the way you do unique is by calling
np.unique
and you see
the numbers are between range zero to
nine. 
We already saw let's say our
eighth number here
is 
which number it is okay I can check it
here.
So if you
do eight,
this is the eighth number I mean I know
it looks weird but let's do the tenth
number.
So then number is nine.
See nine.
So this dataset.data
is your feature and dataset.target is
your class basically. We are classifying
this
in 10 classes, 0
to 9.
Okay, let me create a data frame now, so I
can do pd dot DataFrame
and just supply data set dot data, and
that will create a data frame for
you. But
the
column
headers are you know randomly assigned.
So I'm going to
do
columns,
you can say data set dot feature names
and the feature names are all the pixels,
you know
like zero throw zero pixel zero throw
first pixel and so on
and I'll just save that
into a data frame
object.
Okay, my data frame object is ready.
Now
uh
I mean I know from this data set the
values are between 0 to 16, but if you
want to confirm you can do df dot
describe.
And for each of the columns if you look
at min and max;
I mean this column is useless but if you
look at mean and max for most of the
good pixels,
it will be between 0 and 16. 
Okay
um so
0 is my minimum value 16 is my highest
value.
Now I'm going to
store this in x and y. So df is my x and
my y
is my,
you know target set.
So again what is my x? x is my data frame.
what is my y? is my
target
classes, you know the digit which tells
you 0 to 9. 
Now,
I would like to scale these features
before building my machine learning
model, so I'm just going to use the
standard scalar. You know scaling is a
common thing that I do before training
my model,
and
skeleton provides very convenient API to
do that. When you do scalar dot
fit transform it will just scale that x
thing and you can
store this into x scale. So when you
print now
x
scale,
you will see the values
are
stand their scale you know using the
standard scalar. I think negative one to
one. You can use min max scalar as well.
Okay,
now we are ready to train the model. So
we are going to do our usual thing which
is train this split. You already know
this by this by this point
and
I'm going to
call train test split
on my x scale.
So x
scale
and
y, and I will keep my test size to be 20%,
you can do 30% whatever and I'm just
keeping random state for reproducibility.
If you don't want to keep it is fine
and in the result what I get is x train
x test
y train y test.
This should be in your muscle memory
friends, if you're doing machine learning
for some time. 
Now, 
I am ready to train my model. Well I'm
going to use logistic regression. You can
use
random forest decision tree. There are so
many classification techniques but i'll
start with logistic
regression. I will create a model, which
is an object of this class
and then model dot fit,
Again pretty standard
and model or score. So I do model score
to
measure the accuracy.
97 percent! See this is pretty amazing.
I get 97% accuracy
uh by training on all these
features. But now let's do PCA. So to do
PCA you can import PCA from SKLearn on
decomposition,
and then
I will create a PCA variable. So let me
increase my font size. It's very simple
PCA and here you supply two type of
arguments, you can either say
how many components you want. You can say
two five trial and error you can figure
it out. But what I'm going to do is I'm
going to supply 0.95.
What this will tell PCA is that,
retain 95 percent of useful features
and then
create new dimensions.
You're not explicitly saying I want only
two dimensional as an output, you're
saying
give me retain 95% of information,
retain 95 percent variation and then
whatever features you come up with I'm
happy with that,
and then
PCA dot
fit
transform.
The API is quite simple.
You supply your data frame here, you get
a new data frame with new feature, new
principal components
and when you do
xpca dot shape, see
now you've got only 29 columns.
If you
look at your original data frame,
you had 64 columns. So 64 columns to 29.
It probably got rid of
unnecessary things, such as this guy for
example.
Zeroth pixel! All values are zero. See
look at here this feature is not needed
at all. So it probably got rid of that
and it
it did not, by the way just to make it
clear it's not like it will pick 29
columns out of it. It will calculate
new columns okay. So if you do x
pca you see,
it
actually calculated
new columns
for you.
And if you do
PCA variable dot
explained
variance
ratio, 
it will tell you that my first column is
capturing you know 14 percent of
variation or 14 of
useful information from my data set.
PC2 second component
is capturing 13 percent of information
and so on.
So this is how
you
print explained plane variation ratio. There is
another thing called n components. It
will tell you how many components you
got, which is basically your columns you
know 29.
So here PCA help me reduce my columns
from 64 to 29. All these 29
columns are computed column. They're the
new feature,
but good news is you can now use
this
uh new data frame to train your model. So
I'm going to do trend test split once
again, but this time I will supply xpca,
y remains for high then my test size is
0.2
and I'll just
keep my random state to be
this
and as a result what you get is
your train and test
data sets.
And I'm going to use again same linear
regression
logistic regression sorry
and I will say
logistic regression.
This time I will fit on PCA
and then model
dots or
and y is
say. I got pretty good accuracy 97
percent but let's say
this failed to converge because
total number of iterations
limit reached. So I need to supply some
maximum iteration, so I will say
max
oter
so I need to increase my iteration
basically, to
so that the
gradient descent can converge.
See almost same accuracy. So you are
realizing that
despite dropping lot of features my
accuracy is almost same. Here is
97.22, here is
96.994. So it's
almost
similar.
Okay.
Now you can
um create PCA by supplying the
components explicitly. So I can say okay
PCA,
let's say
my
number of components is 2.
Now this is what is going to happen when
you explicitly say my components are 2.
So I'm going to do PCA transform with my
x
xpca.shape.
Now
we are telling
that
we want two components.
Okay so it
it will identify the two most important
features
but
here,
see let me see,
so see every data point is just two
features. Now did it capture enough
information.
We are not sure. When you print explain
variation variance ratio,
you are capturing 14% and 30% around 27 28
percent information. So naturally when
you use
this data set to train your model,
the accuracy is going to be low. So I'm
just going to
like same code but I'm using the new PCA,
and the new data frame and when you
train the model you see the accuracy is
reduced to 60, 60.83. So
you take a hit, but
the benefit you got here is
your
uh features are only two. So your
computers can be computation can be very
fast.
So you can decide what accuracy do you
want and then you can
reduce your dimensions accordingly.
So that's all I had for this video. Now
let's move on to the most important part
of
this
tutorial, which is an exercise. I'm going
to provide a link of this exercise below
but it's very important friends that you
work on exercise and I have given a very
good exercise, there is a data set
which you can download from Kaggle or you
can see if you click on here, there is
this heart csv and so there is a data
set for
heart disease and using this data set
you need to predict
if the person has heart disease or not
and you are going to use PCA. So work on
all these six aspects of
the exercise and let me know how it goes
in the comment post below; your solution,
your github link, whatever and if you
have any question
post in a comment below, and if you like
this video
share it with your friends. You know so
that they can learn, everything is for
free friends exercise explanation
all my youtube videos are free, so you
can have
learning at you know at almost no cost.
So please share it with your friends and
give it a thumbs up if you like this.
Thank you
