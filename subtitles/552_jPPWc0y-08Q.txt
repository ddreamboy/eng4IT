today we are going to discuss how
recurrent neural networks work I'm
assuming you know how a regular neural
network works in a regular neural
network the input passes through the
input layer to a hidden layer and then
to the output layer of course there can
be many hidden layers A recurrent neural
network or RNN differs from a regular
neural network the RN specifically
handles sequence data whereas a regular
neural network accepts all features of a
data point together at a time the
sequence or in what order the data
points are entered to the network does
not matter much in a regular neural
network so the RNN can take a sequence
one element after another and learn from
that sequence here is the abstract form
of an RNN as the input of an RNN first
one input element of the sequence comes
the element is passed through the hidden
layer the outcome of the Hidden layer is
passed through activation to process
output not only that a simple but
elegant thing is done with the outcome
of the Hidden layer whatever comes out
of the Hidden layer is fed back with the
next input that is the next input to the
RNN receives the hidden state of the
previous input let us go with an example
let us say that our sequence is I eat
rice the first word is the word I the
word I goes in as the input the hidden
layer output is passed through whatever
activation function you have but before
that the hidden state is fed back to the
cell with the word eat with the word I
the hidden state was either random or
all zeros it does not matter much
because I was the first word but now
with the next word eat the hidden state
of the word I is
processed after they go through the
hidden layer the output of the Hidden
layer contains processed information
about the word eat not only that it also
contains processed information about the
word I as the word I's hidden state was
fed with the word eat that means so far
the latest hidden state is keeping
information about I eat the next word is
rice rice goes in with the latest hidden
state which contains information about I
eat the new hidden State coming out of
the Hidden layer will contain
information about I eat rice now the
expected output eat for the input word I
and the output rice for the input word
eat will help determine the error of the
RNN which can be used for back
propagation let us discuss the math
which will clarify it further let us
call each word a Time step the word I is
at time step one word eat is at time
step two word rice is at time step three
let us say that the word at time step T
is represented by a vector XT
the word Vector XT becomes the input and
is Multiplied with the weight Matrix U
just like the regular neural network
what will be the size of the Matrix U it
is the weight Matrix of the rnn's input
element so the size of U will be the
length of the word Vector XT times the
number of neurons in the hidden State
the previous time step had an output
from the hidden layer which we call y t
minus1 which is another Vector that also
becomes an input the length of YT minus1
is equal to the number of neurons in the
hidden layer obviously because y t minus
one is the outcome of the Hidden layer
the vector y t minus1 is Multiplied with
another weight Matrix W the size of w is
the length of y t minus1 times the
number of neurons in the hidden layer
note again y t minus1 has a length equal
to the number of neurons in the h hidden
layer therefore the weight Matrix W is a
square Matrix W * previous hidden state
is added with u * input word vector and
it is added with the bias term
B the length of the bias term B in an
RNN is equal to the number of neurons in
the hidden layer during training the
network learns the optimal values for
the weights w u and B in rnns how you
process the final output for error
calculation depends on the specific
application and the nature of the task
for instance in a language modeling
scenario where the goal is to predict
the next word in a sequence the
Network's output at each time step is
often a probability distribution over
the vocabulary predicting the next word
based on the preceding context if the
task is to predict one specific word
given a sequence of a certain length
then the error calculation or loss
function would typically focus on the
accuracy of the prediction at the
specific time step relevant to the task
for example with a sequence length of
two words if the application is to
predict the third word the model is
trained to minimize the error in
predicting this third word the errors at
intermediate steps such as predicting
the second word given the first might
not be the primary focus unless they
contribute to the overall learning
objective the overall output is the
expected next word that comes after the
sequence so this part of the equation is
sent through a nonlinear activation
function here denoted as F which could
be tan H activation or relo function
while processing a single sequence in a
recurrent neural network the weights Wu
and the bias term B remain constant for
the sequence
the parameters u w and B are typically
updated via back propagation after
processing a sequence of data what if
you have several thousand words in a
sequence should we keep feeding the
words through the RN one after another
without updating w u and B no rnns have
serious issues rnns are particularly
susceptible to the problems of Vanishing
and exploding gradients especially with
long sequences in such cases the
gradients can either become too small or
too large as they are propagated back
through each time step of the sequence
making it difficult to train the network
effectively instead of repeatedly
feeding thousands of words without
updating the weights the common practice
is to create chunks of the sequence and
train for each chunk that is after each
chunk update the weights w u and B by
running the back propagation for the
next chunk use these updated Wu and B as
initialization keep doing this until
there is no more chunk left this
chunking approach makes the RNN more
manageable however this approach might
cause the loss of long-term dependencies
that span beyond the length of the
truncated sequences using more advanced
architectures like lstms or gruse is
generally recommended for tasks where
long range dependencies are crucial we
have yet to discuss one small item what
is
XT we said earlier in the video that XT
is a vector representation of a word yes
that is correct but where will we get
that Vector it is common to use an
embedding layer before the RN unit to
create the XT vectors which are the
input vectors for each time step to
incorporate embedding each word in the
text is mapped to a unique integer an
index in the embedding Matrix the
embedding layer uses these indices to
look up the corresponding embedding
Vector for each word these embedding
vectors xtes are then passed as inputs
to the RNN at each time step the
embeddings in the embedding layer will
be updated during the back propagation
process of an RNN provided that the
embedding layer is set to be trainable
when the RNN is trained on a task such
as language modeling or text
classification the gradients calculated
during back propagation are used not
only to update the weights and biases of
the RNN but also to adjust the
embeddings in the embedding layer note
that this RNN can not only be used for
text but also it can be used for time
series data both univariate and
multivariate for multivariate time
series the input Vector at each time
step can be conceptually similar to the
embedding Vector of a word in natural
language processing
applications that is the theory of r in
short see you in the next video for the
python implementation of RNN as a text
generator
