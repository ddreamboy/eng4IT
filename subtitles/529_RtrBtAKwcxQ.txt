Once I wanted to buy nest thermostat and
I wasn't sure if I should be buying that
or not.
I then called four of my friends who
already have that device and then asked
for their opinion. Three of them told me
I should buy it. One guy told me no I
shouldn't buy it, and I just took a
majority vote and I went ahead and
installed nest thermostat in my home.
We use ensemble learning in our real life,
where to make a decision we take opinion
from different people. Similarly in
machine learning
sometimes what happens is if you have
just one model and if you train
that model
using all your data set, that model might
get over fit or it might suffer from a
high variance problem. If you don't know
about bias and variance I have another
video, which you must check before
continuing on this video but to tackle
this high variance problem
we can use ensemble learning. In the case
of my nest thermostat why didn't I call
just one of my friend, because
that if I call only one person that
person might be biased. So I wanted to
make a good decision. Hence I talked to
multiple people and took a majority
vote. In ensemble learning we train
multiple models on a same data set and
then we
when we do prediction
we
do prediction on multiple models and
then whatever output, we get we combine
that result somehow to get our final
output.
Bagging and Boosting are the two main
techniques used in ensemble.. ensemble
learning and in this video we are going
to talk about Bagging. We'll also write
some python code. Let's get started
Let's say I have this data set of 100
samples and when I train a machine
learning model one of the problem I
might encounter is overfitting and
overfitting happens due to the nature of
the data set, your machine learning,
methodology, etc.,
and usually overfitting model has a
problem of high variance.
To tackle this problem one of the things
you can do is out of 100 samples, create
a small data set of 70 samples. Let's say
I'm just giving an example
and the way you create
this subset is by using resampling with
replacement. Now what is that exactly?
Let's first understand that. Let's say I
have this 10 samples,
out of which I want to create smaller
data set with four samples.
In resampling with replacement, we
randomly pick any data point; let's say
four
and then
when we go and pick second data point we
again randomly pick any data point from
1 to 10 with equal probability. We don't
look at what we already have in our
subset.
So second time I will let's say get 8.
Third time also we randomly pick any
data point from 1 to 10 and this time I
might get same sample again. So this is a
resampling with replacement where
in my subset I can get same data sample
multiple times.
So here,
from my original data set I created a
subset of 70 sample. I might create n
number of such
smaller data sets from my original data
set, using resampling with replacement
and then on individual data set I can
train my machine learning model. Let's
say I'm trying to classify if a person
should buy insurance or not and I'm
using logistic regression. So I will use
logistic regression model. So here M1 M2
M3 they all are logistic regression but
they are trained on a different
data set.
And when they're trained and now I have
to perform the prediction or inference
I will
perform that prediction on all three
models in parallel individually
and whatever result I get I just take a
majority vote. So here M1 and M3 is
saying person should buy insurance M2 is
saying
they should not.
Majority vote is clearly one and that is
my final outcome. The benefit you get
here is these individual models are weak
learners. Weak learners means
they are trained on a subset of data set
and hence
they
they will not overfit. You know it is
likely that they will generalize better,
they will not over fit and these
individual weak learners when you
combine the result you get
overall a good quality result. This was a
case of classification, same thing
applies for regression. Let's say you're
doing housing price prediction.
Here,
you take an average of
whatever is the prediction by individual
model.
Now
this technique is also called Bootstrap
Aggregation because
when you are creating this small
subset of data set using
resampling with replacement
that
procedure is called bootstrap, and when
you combine their results using either
an average or majority vote that is
called aggregation. So hence bagging is
also called bootstrap aggregation.
Many many times you hear all these terms
and jargons and you get worried what it
is, but really these concepts are very
very easy. You know you already
understood what bootstrap aggregation
means.
Now Random Forest is one of the bagging
technique with only one distinction,
which is
here we not only
sample
your your data rows but you also samples
your features. So basically you sample
your rows as well as your columns.
Let's look at our classical housing
price prediction example where town, area,
bedroom, etc are features and pricing
which is a green column is your target
variable. Here you will
sample
rows and columns both.
So here you can see
I don't have a bedroom column or a plot
column. I randomly resample out of see one
two three four five six out of seven
columns, I got only four columns. In the
second time
again I randomly sample
this column. So I did not get
uh for example here bedrooms in this
particular
data set. And in the third one for
example here I did not get
school rating. Okay.
So you are resampling
you're randomly picking rows as well as
columns then on individual data set you
train a decision tree model,
and then you aggregate the results. Here
I have decision tree regression
you can use it for classification
problem as well. But the point is very
simple random tree is basically a
bagging technique, but
here
we do one additional thing which is we
randomly pick
features as well.
And the difference between these terms
bagging and bagged trees is that, in
bagging individual model
can be svm knn logistic regression
pretty much any model. Whereas in bagged
trees, so random forest is a bag tree
example.
Here the every model that you're
training is
a tree. Alright so I hope
the theory is clear. Let's move on to
python coding using SKlearn.
I will be using this data set for the
coding today. The data set is about Pima
Indian Diabetes where
based on certain features you have to
predict whether the person has a
diabetes or not. By clicking on this link
I have downloaded this csv file, which
looks something like this. Here the
features are pregnancies, glucose, blood
pressure; these are all contributing
factors for diabetes.
Based on these you are deciding whether
the person has diabetes or not.
I have loaded this csv file in pandas
data frame as you can see here
and as soon as I load data in my data
frame, I like to do some basic
exploration.
So let's first start and find out if
there is any column which has null
values.
So the way you find it out is you will
do df dot is null dot sum.
This will tell you in this column, let's
say if the number is 5 which means it
has 5 null values. But we are lucky here
there are no null values, so no need to
worry about it.
Second thing that I do is df.describe.
This tells me the basic statistics for
each of the columns.
For pregnancies look at min and max. You
know max, I understand 17 is little high
but it's not unrealistic.
When I examine min max values in all
these columns they look normal and I
don't feel like we need to do any
outlier
detection or
outlier removal, etc. So I will
just go ahead
and assume
that there are no outliers.
The second thing I check is
whether there is any imbalance in the
data set. Because see
there's this outcome right. So here
if you do value counts what you will
find is
there are 500 samples
which which says no diabetes, 268 samples
which says yes person has a diabetes,
and if you look at the ratio
it's
0.53. So it looks like some imbalance but
it is not a major imbalance. Major
imbalance would be like you know 10 to 1
or 100 to 1 ratio.
This is more like
2 to 1 ratio, you know.
So I would
say that this
there is slight imbalance, but it's it's
not something that you should worry
about. So I would go ahead and
move on to the next stage, which is
creating x and y. So my x will be df dot
drop, because outcome is my target column.
I need to drop that
and the way you do that is
by using drop function in pandas
and in the
axis you will say columns.
And y would be df dot outcome.
Okay so my x and y is ready.
Now I will do scaling, because
the values are
on a different scale, here. You can see
this particular number you know 0 to 17
versus 0 to 199.
I mean it's not a huge difference in the
scale but still just to be on a safe
side
I will use standard scaling. You can use
min max scalar as well.
Let's create our scaler object, scaler
and if you have followed my previous
videos you know how to use scalar object
you can just say fit transform x
and what you get in return is your x
scale. This will be NumPy array. Hence I
will just print like first three rows
out of it.
You can see the values are scaled.
If you use min max scalar, you'll get
different set of values but both works
okay. Now
this should be in your muscle memory.
When you have x scale and y ready you
will do train test split
and for that I will import this method
in train test split what do you supply x
and y, right x and y. What is our x? x
scale
right we want to use the scale value and
then
in the output,
you know the standard output that I get
is x train x test y train and y test.
Now here,
I will use stratify argument because
there is slight imbalance. So I want to
make sure the test and train data set e
has equal number of samples like
equal proportion basically and I will
say y I mean it won't be equal but at
least the
this ratio should be maintained
and
random state I will set to
10 or maybe 20 let's go 10.
This is just a random number by the way
and
it allows you the
reproducibility. Every time you run this
method you will get same x train,
y train.
So if you do the shape okay 576 samples
in your train data set
and this data set has
192
and if you look at this.
Oh no actually you know what I should be
looking at
y
train dot value counts
and if you look at this
around same kind of ratio that you saw
here, okay.
Now,
we will
use decision tree classifier
to train a standalone model. You can use
svm kanye west neighbor any
classification model.
I am using decision tree to demonstrate
that decision tree is relatively
imbalanced classifier.
It can overfit and it will
you know it might generate an
um high variance model.
So
let's train decision tree
first and I will use cross validation
score here,
to just try it out on a different type
of data set rather than just x train
and you know x test.
So cross validation, if you don't know
about cross-validation score I have done
separate video on k-fold
cross-validation. You should watch that
if you are not aware. Otherwise
you know this thing will bounce off your
head.
So this isn't cross validation score
expects a classifier which is my
decision tree classifier then x and y
and then cv is equal to
5 10;
I'll give 5.
So what this will do is
uh
if I have
if I have um
768 samples.
It will divide them into five folds
and it will
try different
set of
x test and
you know y test to
x train and x test to try the model.
You should watch my video on k4 cross
validation. You will get a good
understanding of it. And this will return
this will do like five time training and
all those training results would be
inside the scores.
So see the score that you got
by running 5 iteration of training is
this and if you take a this is a NumPy
array, so just take a mean of it.
You'll find your model is giving you 71
percent accuracy, which is okay.
But now let me use bagging classifier
and
first thing you can do is
ask your friend sklearn bagging
classifier. Your friend is Google and
Google will tell you
which api you need to use.
So here see,
I will use the most important tool for
any programmer which is copy paste
and I
create a bagging classifier.
And backing classifier you can read all
the arguments but
I'm just going to use couple of
arguments. First of all okay which
estimator you are using. Well I am using
decision tree.
How many estimators like how many sub
groups of data set? 100,
trial and error, okay. You try 10 20
figure out, which one is giving you best
performance and this 100 is nothing but
this.
See in my presentation I said
3 model,
I am doing
100 models and 100 subset of data sets
and I will be training them in parallel
and how many samples. See here we used
70 out of 100.
So for sampling use 80%,
use 80% of my samples.
There is another thing called
oob score. So oob score is equal to.. now
what is oob score? Well
oob means out of bag.
When you are sampling,
because you are doing random
sampling by the law of probability
you are not going to cover all 100
samples in this subset. Let's say in this
subset,
all this subset
number
29 did not appear at all.
Okay, so number 29, let's say number 29 is
here right 1 to 100 number,
that 29 number sample
did not appear in any of this subset.
So now
all these models which are trained they
have not seen that data data point 29.
So you can use that 29
to test
the accuracy of these models. So you are
kind of treating that 29 sample as a
test data set.
Ideally what you do is you take your
data set, you split it into train
and test. So this
diagram that I have shown here this
block is actually your x train. So your x
test is separate already, which you will
be
using to test the performance of your
final model before before deploying that
into a wild.
But even within x train because of our
sampling strategy you might miss some
samples. Let's say you might you have
20 data samples, which which
which has not appeared in any of this
subset and those 20 samples after these
models are trained. You can use those 20
samples for prediction take the majority
vote
and figure out what was the accuracy, and
that accuracy is your oob score.
So you realize that okay okay let me
first do random state here.
Again random state is for predictability
and I will call this a bag model and
when I have a bag model
I will do
oob score.
Oob
score.
oob 
[Music]
Bagging
classifier.
Actually you know what
I have to fit
dot fit.
So I am doing x you know x and y train
fit
and then I get this.
You realize I did not try even x test
and y test,
on training data set when I did 80
samples. When I train 100 classifier, it
might have missed some of the samples
from my training data set and
on them I ran my
model prediction
and the accuracy I got is stored in this
oob score.
now I can do
regular scoring x test y test and you
see improvement right
77 percent versus standalone model
giving you 71 percent. Now I agree you
will tell me here you use cross
validation. Here, I did not use cross
validation. So let me use cross
validation then. So I'm going to do some
copy paste magic
and create the same bagging model and
then
use
cross validation scroll, okay. In cross
validation score what do you supply? You
supply first your model
then x
then y and how many
folds well five folds, okay.
You get scores back
and those scores
you can just take a mean.
You will find that
the base model gives you seventy one
percent accuracy
bagged model gives you seventy five
percent accuracy. So for
unstable classifier like decision tree,
bagging helps.
If you have a classifier
you know sometimes, you have unstable
classifier like decision tree and
sometimes your data set is such that
there are so many null values, you know
your columns are such that
your resulting model has high variance.
And whenever you have this high variance
it makes sense to use bagging classifier. 
Now,
we talked about random forest. So let's
let me try random forest as well on this
particular data set. So
I will try random forest here and I will
they say okay random forest classifier x
y cv equal to 5, I get scores
and
pretty
straightforward x
mean.
random forest classifier gives me one
little better performance.
Inside like underneath it will use
bagging.
It will not only sample your data rows
but it will sample your
feature columns as well as we saw in the
presentation.
Now comes the most important part of
this video, which is an exercise.
Learning coding or data science is like
learning swimming. By watching the
swimming video you are not going to
learn swimming obviously. Similarly
you need to work on this exercise,
otherwise
it will be hard to grasp the concepts
which I
just taught you.
Here I'm giving
a csv file, which I took from kaggle by
the way and it is about heart disease
prediction.
You have to load the data set apply some
outlier removal. I have given all the
information here. So
work on this exercise and once you have
put your sensor effort then only click
on solution link, because I have an AI
technology built into this video where
if you click on this link without trying
out on your own your laptop or computer
will get a fever
and it will not recover for next 10 days!
Okay, so you will miss all the fun. So
better you try first on your own and
then click on the solution link. I hope
this you like this video. If you did give
it a thumbs up!! at least
and share it with your friends. I wish
you all the best. If you have any
question,
there is a comment section below. Thank
you!
