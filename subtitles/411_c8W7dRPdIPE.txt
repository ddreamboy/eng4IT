have you ever been amazed by how google
home and amazon alexa assist in finding
information when asked over voice
or have you ever wondered how ecommerce
companies send you personalized emails
for shopping suggestions based on the
products that you bought online recently
well it's all because of machine
learning that these virtual assistants
and product recommendations work swiftly
in this video we're going to look at
machine learning roadmap for 2022
we will give you the step-by-step
process that you need to follow to build
a successful career in machine learning
so before we get started make sure that
you subscribe to the simply learn
channel and hit the bell icon to never
miss an update from simplylearn
so what is machine learning
machine learning is a subset of
artificial intelligence that uses data
algorithms and statistical techniques to
build intelligent systems
these systems learn by themselves and
improve with experience
the goal of machine learning is to
create computer models that can imitate
human behavior machine learning has
found its usage in almost every business
sector
from self-driving cars medical imaging
and diagnostics speech recognition
facial recognition to online fraud
detection all of these are possible
because of machine learning
in recent years ai and ml technologies
have made several breakthroughs and the
rising demand for ai applications across
different industries has led to the
significant growth of machine learning
as for
marketsandmarkets.com the machine
learning market is expected to grow to 9
billion dollars by 2022 at ac agr of 44
another report from
verifiedmarketresearch.com
suggests that the machine learning
market was valued at 2.4 billion us
dollars in 2019 and is projected to
reach 47.29 million usc by 2027
growing at a cagr of 44.9
from 2020 to 2027.
keeping these facts and forecasts in
mind let's look at the roadmap and
critical skills required to build a
career in machine learning
first and foremost is programming skills
for a machine learning engineer
programming languages form the building
blocks to develop complex machine
learning models
you need to learn at least one
programming language preferably python
or r you should be familiar with various
computer science concepts such as data
structures including stack queues trees
and graphs algorithms for searching and
sorting dynamic and greedy programming
space and time complexity etc you also
need to know libraries like numpy pandas
matplotlib cbon d-plure tidier and
ggplot for data analysis and
visualization the second skill you need
to possess is applied mathematics
while solving business problems using
machine learning you have to use machine
learning algorithms
to understand the mechanisms behind the
algorithms you need to have a good
knowledge of mathematical concepts such
as linear algebra calculus statistics
and probability
so mathematics and machine learning is
not just processing the numbers but
understanding what is happening why it's
happening and how to obtain the best
results the third skill to become a
machine learning expert is
data wrangling and sql
data analysts and machine learning
specialists often work with raw data
collected from various sources that are
not fit for analysis
it has been observed that eighty percent
of data analysis spend too much time on
data wrangling
so it's crucial for machine learning
experts to clean structure and enrich
raw data into desired format and make it
ready for analysis using data wrangling
techniques
sql is another crucial set that you
should carry
machine learning tasks involve using
data stored in the form of tables that
are present inside relational databases
a good understanding of sql commands
enables you to store manipulate retrieve
and handle structured data
the next skill set is machine learning
algorithms it's incredibly essential to
grasp all the standard machine learning
algorithms
implementing any machine learning
techniques requires choosing the
suitable model determining the correct
learning method and an in-depth
understanding of hyper parameter tuning
so you should be really good with
different supervised unsupervised and
reinforcement learning algorithms such
as linear regression logistic regression
svm knn decision trees k means
clustering etc now coming to the final
scale we have data modeling and
evaluation the objective of a machine
learning engineer is to train the best
performing model possible
depending on the problem at hand you
will need to choose a suitable error
measure and an evaluation strategy for a
machine learning model
the most vulnerable part of a machine
learning candidate's resume is the
absence of experience working on diverse
machine learning projects
with all the essential skills acquired
you can now build an impressive machine
learning portfolio highlighting some
exciting machine learning projects
machine learning engineers need a
portfolio that showcases their expertise
to implement machine learning techniques
to real-world problems
with your resume ready you can now look
for the best machine learning jobs in
top product companies and startups
you can become a machine learning
engineer a data scientist a data analyst
or a research scientist
some of the top companies hiring for
machine learning roles are google amazon
ibm uber grammar navidia and linkedin
machine learning engineers are some of
the highest paid professionals in the
world
according to glassdoor.com the national
average salary for machine learning
engineers in the united states is one
lakh 31 000 us dollars per year in india
you can earn nearly 8 lakh rupees per
annum
this salary may vary based on your
experience the industry you are applying
for and the company policy
there are immense opportunities in
machine learning for sectors such as
e-commerce manufacturing logistics
retail and healthcare
if you are interested in a career in
machine learning then start your journey
now with simplylearn's postgraduate
program in ai and machine learning that
is in partnership with purdue university
and in collaboration with ibm
you will learn all the in-demand machine
learning skills and work on programming
with hands-on examples
the program also offers job assistance
to know more about the learning part
tools covered and projects that you'll
be working on please click on the course
page link in the description below
in this video we will cover everything
you need to know to become an expert in
machine learning our experienced
instructors with good industry
experience will take you through this
course
first
you will understand the basics of
machine learning from a short animated
video
you will then know the essential
applications of machine learning you
will understand machine learning
concepts and learn why mathematics
statistics and linear algebra are
crucial then
we will focus on some vital machine
learning algorithms such as linear
regression logistic regression decision
trees random forest and k nearest
neighbors
you will also learn about regularization
dimensionality reduction and principal
component analysis we will perform a
prediction analysis on the recently held
u.s elections as well finally you will
study the machine learning roadmap for
2021 so let's begin
we know humans learn from their past
experiences
and machines follow instructions given
by humans
but what if humans can train the
machines to learn from the past data and
do what humans can do and much faster
well that's called machine learning but
it's a lot more than just learning it's
also about understanding and reasoning
so today we will learn about the basics
of machine learning
so that's paul he loves listening to new
songs
he either likes them or dislikes them
paul decides this on the basis of the
song's tempo
genre intensity and the gender of voice
for simplicity let's just use tempo and
intensity for now so here tempo is on
the x-axis ranging from relaxed to fast
whereas intensity is on the y-axis
ranging from light to soaring we see
that paul likes the song with fast tempo
and soaring intensity while he dislikes
the song with relaxed tempo and light
intensity so now we know paul's choices
let's say paul listens to a new song
let's name it as song a song a has fast
tempo and a soaring intensity so it lies
somewhere here looking at the data can
you guess whether paul will like the
song or not correct so paul likes this
song by looking at paul's past choices
we were able to classify the unknown
song very easily right let's say now
paul listens to a new song let's label
it as song b so song b
lies somewhere here with medium tempo
and medium intensity neither relaxed nor
fast neither light nor soaring now can
you guess whether ball likes it or not
not able to guess whether paul will like
it or dislike it are the choices unclear
correct we could easily classify song a
but when the choice became complicated
as in the case of song b yes and that's
where machine learning comes in let's
see how in the same example for song b
if we draw a circle around the song b we
see that there are four words for like
whereas one would for dislike if we go
for the majority votes we can say that
paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called k
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become complicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
predict for it more the data better the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you one
million coins of three different
currencies say one rupee one euro and
one dirham each coin has different
weights for example a coin of one rupee
weighs three grams one euro weighs seven
grams and one dirham weighs 4 grams your
model will predict the currency of the
coin here your weight becomes the
feature of coins while currency becomes
the label when you feed this data to the
machine learning model it learns which
feature is associated with which slave
for example it will learn that if a coin
is of 3 grams it will be a 1 rupee coin
let's give a new coin to the machine on
the basis of the weight of the new coin
your model will predict the currency
hence supervised learning uses labeled
data to train the model here the machine
knew the features of the object and also
the labels associated with those
features on this note let's move to
unsupervised learning and see the
difference suppose you have cricket data
set of various players with their
respective scores and wickets taken when
you feed this data set to the machine
the machine identifies the pattern of
player performance so it plots this data
with the respective wickets on the
x-axis while runs on the y-axis while
looking at the data you will clearly see
that there are two clusters the one
cluster are the players who scored
higher runs and took less wickets while
the other cluster is of the players who
scored less runs but took many wickets
so here we interpret these two clusters
as batsmen and bowlers the important
point to note here is that there were no
labels of batsmen and bowlers hence the
learning with unlabeled data is
unsupervised learning so we saw
supervised learning where the data was
labeled and the unsupervised learning
where the data was unlabeled and then
there is reinforcement learning which is
reward based learning or we can say that
it works on the principle of feedback
here let's say you provide the system
with an image of a dog and ask it to
identify it the system identifies it as
a cat so you give a negative feedback to
the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it will be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to a machine
learning model which then gives the
output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns i hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario 1 facebook recognizes your
friend in a picture from an album of
tagged photographs
scenario 2 netflix recommends new movies
based on someone's past movie choices
scenario 3 analyzing bank data for
suspicious transactions and flagging the
fraud transactions think wisely and
comment below your answers moving on
don't you sometimes wonder how is
machine learning possible in today's era
well that's because today we have
humongous data available everybody is
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis
also the memory handling capabilities of
computers have largely increased which
helps them to process such huge amount
of data at hand without any delay and
yes computers now have great
computational powers so there are a lot
of applications of machine learning out
there to name a few machine learning is
used in healthcare where diagnostics are
predicted for doctor's review the
sentiment analysis that the tech giants
are doing on social media is another
interesting application of machine
learning fraud detection in the finance
sector and also to predict customer
churn in the e-commerce sector while
booking a gap you must have encountered
surge pricing often where it says the
fare of your trip has been updated
continue booking yes please i'm getting
late for office
well that's an interesting machine
learning model which is used by global
taxi giant uber and others where they
have differential pricing in real time
based on demand the number of cars
available bad weather rush r etc so they
use the surge pricing model to ensure
that those who need a cab can get one
also it uses predictive modeling to
predict where the demand will be high
with the goal that drivers can take care
of the demand and search pricing can be
minimized great hey siri can you remind
me to book a cab at 6 pm today ok i'll
remind you thanks no problem comment
below some interesting everyday examples
around you where machines are learning
and doing amazing jobs so that's all for
machine learning basics today from my
site machine learning has improved our
lives in a number of wonderful ways
today let's talk about some of these i'm
rahul from simply learn and these are
the top 10 applications of machine
learning first let's talk about virtual
personal assistants google assistant
alexa cortana and siri now we've all
used one of these at least at some point
in our lives now these help improve our
lives in a great number of ways for
example you could tell them to call
someone you could tell them to play some
music you could tell them to even
schedule an appointment so how do these
things actually work first they record
whatever you're saying send it over to a
server which is usually in a cloud
decode it with the help of machine
learning in neural networks and then
provide you with an output so if you've
ever noticed that these systems don't
work very well without the internet
that's because the server couldn't be
contacted next let's talk about traffic
predictions now say i wanted to travel
from buckingham palace to lodge cricket
ground the first thing i would probably
do is to get on google maps so
search it
and let's put it here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicate that
they are slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as i told you
before red means heavily congested
yellow means slow moving and blue means
clear
so how exactly is google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
the help of machine learning and with
the help of two important measures first
is the average time that's taken on
specific days at specific times on that
route the second one is the real time
location data of vehicles from google
maps and with the help of sensors some
of the other popular map services are
bing maps maps dot me and here we go
next up we have social media
personalization so say i want to buy a
drone and i'm on amazon and i want to
buy a dji mavic pro the thing is it's
close to one lap so i don't want to buy
it right now but the next time i'm on
facebook i'll see an advertisement for
the product next time i'm on youtube
i'll see an advertisement even on
instagram i'll see an advertisement so
here with the help of machine learning
google has understood that i'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
gmail know what spam and what's not spam
so gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that gmail uses is content
filters header filters general blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values stock market
trading machine learning is used
extensively when it comes to stock
market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict datum when there are time lags
of unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3d models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and ischemic stroke lesions they
can also be used in fetal imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug discovery
clinical research and radiology and
finally we have automatic translation
now say you're a foreign country and you
see billboards and signs that you don't
understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence of sequence learning which is
the same thing that's used with chatbots
here the image recognition happens using
convolutional neural networks and the
text is identified using optical
character recognition furthermore the
sequential sequence algorithm is also
used to translate the text from one
language to the other hello and welcome
to machine learning tutorial part one
this is part one of a machine learning
series put on by simply learn my name is
richard kirschner i'm with the simply
learn team that's www.simplylearn.com
get certified get ahead what's in it for
you today well we'll start off with a
brief explanation of why machine
learning and what is machine learning
and then we'll get into a few of the
types of machine learning machine
learning algorithms linear regression
decision trees support vector machine
and finally we'll do a use case where
we're going to classify whether recipe
is of a cupcake or a muffin using the
svm or the support vector machine sounds
like a delicious way to explore machine
learning so why machine learning why do
we even care about having these
computers come up and be able to do all
these new things for us well because
machines can now drive your car for you
it's still very in the infant stage but
it's just exploding as we see with
google's waymo and then uber had their
program which unfortunately crashed they
know that this is huge this is going to
be the huge industry to change our whole
transportation infrastructure machine
learning is now used to detect over 50
eye diseases do you know how amazing
that is to have a computer that double
checks for the doctor for things they
might miss that's just huge in the
health industry pretty soon they
actually do already have that within
some areas where maybe not for eyes but
for other diseases where they're using
the camera on your phone to help
pre-diagnose before you go and see the
doctor and because the machine can now
unlock your phone with your face i mean
that's just cool having it being able to
identify your face or your voice and be
able to turn stuff on and off for you
depending on where you're at and what
you need talking about an ultimate
automation or world we live in and as we
dig in deeper we have a nice example of
facebook as you can see here they have
the facebook post with halloween comment
yes if you want it order here nobody
likes spam post on facebook that annoy
them into interacting with likes shares
comments and other actions i remember
the original ones were all if you don't
click on here you will have bad luck or
some kind of fear factor well this is a
huge thing in a social media when people
are getting spammed and so this tactic
known as engagement bait takes advantage
of facebook's news feed algorithm by
choosing engagement in order to get the
greater reach to eliminate engagement
bait the company reviewed and
categorized hundreds of thousands of
posts to train a machine learning model
that detects different types of
engagement bait so in this case we have
we're using facebook but this is of
course across all the different social
media they have different tools for
building and the facebook scroll gif
will be replaced kind of like a virus
coming in there and notices that there's
a certain setup with facebook and it's
able to replace it and they have like
vote baiting react baiting share baiting
they have all these different these are
kind of general titles but there
certainly are a lot of way of baiting
you to go in there and click on
something so they fed all this this data
was fed into the machine and then they
have the new post the new post comes up
that takes over part of the facebook
setup and that's what you're looking at
you're looking at this new post that's
replaced like a virus has replaced that
so what facebook do to eliminate this is
they start scanning for keywords and
phrases like this and checks the
click-through rate so it starts looking
for people who are clicking through it
without even looking at it or clicking
through it is not something that
normally would be clicked through once
facebook has scanned for these keywords
and phrases it is now able to identify
the spam coming in and this makes your
life easier so you're not getting
spammed it's not like walking through an
airport and a lot of countries you have
like hundreds of people trying to sell
you timeshare come join us sign up for
this eliminates that annoyingness so now
you can just enjoy your facebook and
your cat pictures or maybe it's your
family pictures mine is family certainly
people like their cat pictures too
another good example is google's
deepmind project alphago a computer
program that plays a board game go has
defeated the world's number one go
player and i hope i say his name right
kiji the ultimate go challenge game of
three of three was on may 27 2017 so
it's just last year that this happened
and what makes this so important is that
you know go is just is a game so it's
not like you're driving a car or
something in our real world but they are
using games to learn how to get the
machine learning program to learn they
want it to learn how to learn and that
is a huge step a lot of this is still in
its infant stage as far as development
as we saw what happened with the as i
referred to earlier the uber cars they
lost their whole division because they
jumped ahead too fast so still an infant
stage but boy is this like the beginning
of just an amazing world that is
automated in ways we can't even imagine
what tomorrow's going to look like we've
looked at a lot of examples of machine
learning so let's see if we can give a
little bit more of a concrete definition
what is machine learning machine
learning is the science of making
computers learn and act like humans by
feeding data and information without
being explicitly programmed
we see here we have a nice little
diagram where we have our ordinary
system uh your computer nowadays you can
even run a lot of the stuff on a cell
phone because cell phones advance so
much
and then with artificial intelligence
and machine learning it now takes the
data and it learns from what happened
before and then it predicts what's going
to come next and then really the biggest
part right now in machine learning
that's going on is it improves on that
how do we find a new solution so we go
from descriptive words learning about
stuff and understanding how it fits
together to predicting what it's going
to do to post scripting coming up with a
new solution and when we're working on
machine learning there's a number of
different diagrams that people have
posted for what steps to go through a
lot of it might be very domain specific
so if you're working on
photo identification versus language
versus medical or physics some of these
are switched around a little bit or new
things are put in they're very specific
to the domain this is kind of a very
general diagram first you want to define
your objective very important to know
what it is you're wanting to predict
then you're going to be collecting the
data so once you've defined an objective
you need to collect the data that
matches
you spend a lot of time in data science
collecting data and the next step
preparing the data you've got to make
sure that your data is clean going in
there's the old saying bad data in bad
answer out or bad data out and then once
you've gone through and we've cleaned
all this stuff coming in then you're
going to select the algorithm which
algorithm are you going to use you're
going to train that algorithm in this
case i think we're going to be working
with svm the support vector machine then
you have to test the model does this
model work is this a valid model for
what we're doing and then once you've
tested it you want to run your
prediction you want to run your
prediction or your choice or whatever
output it's going to come up with and
then once everything is set and you've
done lots of testing then you want to go
ahead and deploy the model remember i
said domain specific this is very
general as far as the scope of doing
something a lot of models you get
halfway through and you realize that
your data is missing something and you
have to go collect new data because
you've run a test in here someplace
along the line you're saying hey i'm not
really getting the answers i need so
there's a lot of things that are domain
specific that become part of this model
this is a very general model but it's a
very good model to start with and we do
have some basic divisions of what
machine learning does that's important
to know for instance do you want to
predict a category well if you're
categorizing thing that's classification
for instance whether the stock price
will increase or decrease so in other
words i'm looking for a yes no answer is
it going up or is it going down and in
that case we'd actually say is it going
up true if it's not going up it's false
meaning it's going down this way it's a
yes no zero one do you want to predict a
quantity that's regression so remember
we just did classification now we're
looking at regression these are the two
major divisions and what data is doing
for instance predicting the age of a
person based on the height weight health
and other factors so based on these
different factors you might guess how
old a person is
and then there are a lot of domain
specific things like do you want to
detect an anomaly that's anomaly
detection this is actually very popular
right now for instance you want to
detect money withdrawal anomalies you
want to know when someone's making a
withdrawal that might not be their own
account we've actually brought this up
because this is really big right now if
you're predicting the stock whether to
buy stock or not you want to be able to
know if what's going on in the stock
market is an anomaly use a different
prediction model because something else
is going on you got to pull out new
information in there or is this just the
norm i'm going to get my normal return
on my money invested so being able to
detect anomalies is very big in data
science these days
another question that comes up which is
on what we call untrained data is do you
want to discover structure in unexplored
data and that's called clustering for
instance finding groups of customers
with similar behavior given a large
database of customer data containing
their demographics and past buying
records and in this case we might notice
that anybody who's wearing certain set
of shoes go shopping at certain stores
or whatever it is they're going to make
certain purchases by having that
information it helps us to market or
group people together so that we can now
explore that group and find out what it
is we want to market to them if you're
in the marketing world and that might
also work in just about any arena you
might want to group people together
whether they're based on their different
areas and investments and financial
background
whether you're going to give them a loan
or not before you even start looking at
whether they're a valid customer for the
bank you might want to look at all these
different areas and group them together
based on unknown data so you're not you
don't know what the data is going to
tell you but you want to cluster people
together that come together
let's take a quick detour for
quiz time oh my favorite so we're going
to have a couple questions here under
quiz time and we'll be posting the
answers in this part 2 of this tutorial
so let's go ahead and take a look at
these quiz times questions and hopefully
you'll get them all right it'll get you
thinking about how to process data and
what's going on can you tell what's
happening in the following cases of
course you're sitting there with your
cup of coffee and you have your checkbox
and your pen trying to figure out what's
your next step in your data science
analysis
so the first one is grouping documents
into different categories based on the
topic and content of each document very
big these days
you know you have legal documents you
have maybe it's the sports group
documents maybe you're analyzing
newspaper postings
but certainly having that automated is a
huge thing in today's world
b identifying handwritten digits in
images correctly
so we want to know whether they're
writing an a or capital a b c
what are they writing out in their hand
digit their handwriting c behavior of a
website indicating that the site is not
working as designed
d predicting salary of an individual
based on his or her years of experience
hr hiring
set up there so stay tuned for part two
we'll go ahead and answer these
questions when we get to the part two of
this tutorial or you can just simply
write at the bottom and send a note to
simply learn and they'll follow up with
you on it
back to our regular content
now these last few bring us into the
next topic which is another way of
dividing our types of machine learning
and that is with supervised
unsupervised
and reinforcement learning supervised
learning is a method used to enable
machines to classify predict objects
problems or situations based on labeled
data fed to the machine and in here you
see we have a jungle of data with
circles triangles and squares then we
label them we have what's a circle
what's a triangle what's a square and we
have our model training and it trains it
so we know the answer very important
when you're doing supervised learning
you already know the answer to a lot of
your information coming in so you have a
huge group of data coming in and then
you have a new data coming in so we've
trained our model the model now knows
the difference between a circle a square
a triangle and now that we've trained it
we can send in in this case a square and
a circle goes in and it predicts that
the top one's a square and the next
one's a circle and you can see that this
is uh being able to predict whether
someone's going to default on a loan
because i was talking about banks
earlier supervised learning on stock
market whether you're going to make
money or not that's always important and
if you are looking to make a fortune in
the stock market keep in mind it is very
difficult to get all the data correct on
the stock market it is very it
fluctuates and weighs you really hard to
predict so it's quite a roller coaster
ride if you're running machine learning
on the stock market you start realizing
you really have to dig for new data so
we have supervised learning and if you
have supervised we should need
unsupervised learning in unsupervised
learning machine learning model finds
the hidden pattern in an unlabeled data
so in this case instead of telling it
what the circle is and what a triangle
is and what a square is it goes in there
looks at them and says for whatever
reason it groups them together maybe
they'll group it by the number of
corners and it notices that a number of
them all have three corners a number of
them all have four corners and a number
of them all have no corners and it's
able to filter those through and group
them together we talked about that
earlier with looking at a group of
people who are out shopping we want to
group them together to find out what
they have in common and of course once
you understand what people have in
common maybe you have one of them is a
customer at your store or you have five
of them our customer your store and they
have a lot in common with five others
who are not customers at your store how
do you market to those five who aren't
customers at your store yet they fit the
demographics of who's going to shop
there and you'd like them to shop at
your store not the one next door of
course this is a simplified version you
can see very easily the difference
between a triangle and a circle which
might not be so easy in marketing
reinforcement learning reinforcement
learning is an important type of machine
learning where an agent learns how to
behave in an environment by performing
actions and seeing the result we have
here where the in this case a baby it's
actually great that they used an infant
for this slide because the reinforcement
learning is very much in its infant
stages but it's also probably the
biggest machine learning demand out
there right now or in the future it's
going to be coming up over the next few
years is reinforcement learning and how
to make that work for us and you can see
here where we have our action in the
action on this one it goes into the fire
hopefully the baby didn't it was just a
little candle not a giant fire pit like
it looks like here when the baby comes
out and the new state is the baby is sad
and crying because they got burned on
the fire and then maybe they take
another action the baby's called the
agent because it's the one taking the
actions and in this case they didn't go
into the fire they went a different
direction and now the baby's happy and
laughing and playing reinforcement
learning is very easy to understand
because that's how as humans that's one
of the ways we learn we learn whether it
is you bring yourself on the stove don't
do that anymore don't touch the stove in
the big picture being able to have a
machine learning program or an ai be
able to do this is huge
because now we're starting to learn how
to learn that's a big jump in the world
of computer and machine learning and
we're going to go back and just kind of
go back over supervised versus
unsupervised learning understanding this
is huge because this is going to come up
in any project you're working on
we have in supervised learning we have
labeled data we have direct feedback so
someone's already gone in there and said
yes it's a triangle no that's not a
triangle and then you predict an outcome
so you have a nice prediction this is
this this new set of data is coming in
and we know what it's going to be and
then with unsupervised training it's not
labeled so we really don't know what it
is there's no feedback so we're not
telling it whether it's right or wrong
we're not telling it whether it's a
triangle or a square we're not telling
it to go left or right all we do is
we're finding hidden structure in the
data grouping the data together to find
out what connects to each other and then
you can use these together so imagine
you have an image and you're not sure
what you're looking for so you go in and
you have the unstructured data find all
these things that are connected together
and then somebody looks at those and
labels them now you can take that label
data and program something to predict
what's in the picture so you can see how
they go back and forth and you can start
connecting all these different tools
together to make a bigger picture there
are many interesting machine learning
algorithms let's have a look at a few of
them hopefully this gives you a little
flavor what's out there and these are
some of the most important ones that are
currently being used we'll take a look
at linear regression decision tree and
the support vector machine
let's start with a closer look at linear
regression linear regression is perhaps
one of the most well-known and
well-understood algorithms in statistics
and machine learning linear regression
is a linear model for example a model
that assumes a linear relationship
between the input variables x and the
single output variable y
and you'll see this if you remember from
your algebra classes y equals mx plus c
imagine we are predicting distance
traveled y from speed x our linear
regression model representation for this
problem would be y equals m times x plus
c or
distance equals m times speed plus c
where m is the coefficient and c is the
y-intercept and we're going to look at
two different variations of this first
we're going to start with time is
constant and you can see we have a
bicyclist he's got a safety gear on
thank goodness
speed equals 10 meters per second and so
over a certain amount of time his
distance equals 36 kilometers we have a
second bicyclist who's going twice the
speed or 20 meters per second and you
can guess if he's going twice the speed
and time is a constant then he's going
to go twice the distance and that's easy
to compute 36 times 2 you get 72
kilometers and so if you had the
question of
how fast would somebody is going three
times that speed or 30 meters per second
is you can easily compute the distance
in our head we can do that without
needing a computer but we want to do
this for more complicated data so it's
kind of nice to compare the two but
let's just take a look at that and what
that looks like in a graph
so in a linear regression model we have
our distance to the speed and we have
our m equals the ve slope of the line
and we'll notice that the line has a
plus slope and as the speed increases
distance also increases hence the
variables have a positive relationship
and so your speed of the person which
equals y equals mx plus c distance
traveled in a fixed interval of time and
we could very easily compute either
following the line or just knowing it's
3 times 10 meters per second that this
is roughly 102 kilometers distance that
this third bicyclist has traveled one of
the key
definitions on here is positive
relationship so the slope of the line is
positive as distance increase so does
speed increase let's take a look at our
second example where we put distance is
a constant so we have speed equals 10
meters per second they have a certain
distance to go and it takes him 100
seconds to travel that distance and we
have our second bicyclist who's still
doing 20 meters per second since he's
going twice the speed we can guess that
he'll cover the distance in about half
the time 50 seconds and of course you
could probably guess on the third one
100 divided by 30 since he's going 3
times the speed you can easily guess
that this is 33.333
seconds time we put that into a linear
regression model or graph if the
distance is assumed to be constant let's
see the relationship between speed and
time and as time goes up the amount of
speed to go that same distance goes down
so now your m equals a minus ve slope of
the line as the speed increases time
decreases hence the variable has a
negative relationship again there's our
definition positive relationship and
negative relationship dependent on the
slope of the line and with a simple
formula like this
and even a significant amount of data
let's see with the mathematical
implementation of linear regression and
we'll take this data so suppose we have
this data set where we have x y x equals
1 2 3 4 5 standard series and the y
value is 3
2 2 4 3. when we take that and we go
ahead and plot these points on a graph
you can see there's kind of a nice
scattering and you could probably
eyeball a line through the middle of it
but we're going to calculate that exact
line for linear regression and the first
thing we do is we come up here we have
the mean of x i and remember mean is
basically the average so we added five
plus four plus three plus two plus one
and divide by five and that simply comes
out as three and then we'll do the same
for y we'll go ahead and add up all
those numbers and divide by five and we
end up with the mean value of y of i
equals two point eight where the x i
references it's an average or means
value and the y i also equals a means
value of y and when we plot that you'll
see that we can put in the y equals 2.8
and the x equals 3 in there on our graph
we kind of give it a little different
color so you can sort it out with the
dashed lines on it and it's important to
note that when we do the linear
regression the linear regression model
should go through that dot
now let's find our regression equation
to find the best fit line remember we go
ahead and take our y equals mx plus c so
we're looking for m and c so to find
this equation for our data we need to
find our slope of m and our coefficient
of c
and we have y equals mx plus c
where m equals the sum of x minus x
average times y minus y average or y
means and x means over the sum of x
minus x means squared that's how we get
the slope of the value of the line and
we can easily do that by creating some
columns here we have x y computers are
really good about iterating through data
and so we can easily compute this and
fill in a graph of data and in our graph
you can easily see that if we have our x
value of 1 and if you remember the x i
or the means value is 3 1 minus 3 equals
a negative 2 and 2 minus 3 equals a
negative 1 so on and so forth and we can
easily fill in the column of x minus x i
y minus y i and then from those we can
compute x minus x i squared and x minus
x i times y minus y i and you can guess
it that the next step is to go ahead and
sum the different columns for the
answers we need so we get a total of 10
for our x minus x i squared and a total
of 2 for x minus x i times y minus y i
and we plug those in we get 2 10 which
equals 0.2 so now we know the slope of
our line equals 0.2 so we can calculate
the value of c that'd be the next step
is we need to know where it crosses the
y axis and if you remember i mentioned
earlier that the linear regression line
has to pass through
the means value the one that we showed
earlier we can just flip back up there
to that graph and you can see right here
there's our means value which is 3
x equals 3 and y equals 2.8 and since we
know that value we can simply plug that
into our formula y equals 0.2 x plus c
so we plug that in we get 2.8 equals 0.2
times 3 plus c and you can just solve
for c so now we know that our
coefficient equals 2.2 and once we have
all that we can go ahead and plot our
regression line y equals 0.2 times x
plus 2.2 and then from this equation we
can compute new values so let's predict
the values of y using x equals 1 2 3 4 5
and plot the points remember the 1 2 3 4
5 was our original x values so now we're
going to see what y thinks they are not
what they actually are and we plug those
in we get y of designated with y of p
you can see that x equals 1 equals 2.4 x
equals 2 equals 2.6 and so on and so on
so we have our y predicted values of
what we think is going to be when we
plug those numbers in and when we plot
the predicted values along with the
actual values we can see the difference
and this is one of the things that's
very important with linear aggression in
any of these models is to understand the
error and so we can calculate the error
on all of our different values and you
can see over here we plotted x and y and
y predict and we've drawn a little line
so you can sort of see what the error
looks like there between the different
points so our goal is to reduce this
error we want to minimize that error
value on our linear regression model
minimizing the distance there are lots
of ways to minimize the distance between
the line and the data points like sum of
squared errors sum of absolute errors
root mean square error etc we keep
moving this line through the data points
to make sure the best fit line has the
least square distance between the data
points and the regression line so to
recap with a very simple linear
regression model we first figure out the
formula of our line through the middle
and then we slowly adjust the line to
minimize the error keep in mind this is
a very simple formula the math gets even
though the math is very much the same it
gets much more complex as we add in
different dimensions so this is only two
dimensions y equals mx plus c but you
can take that out to x
z i j q all the different features in
there and they can plot a linear
regression model on all of those using
the different formulas to minimize the
error let's go ahead and take a look at
decision trees a very different way to
solve problems in the linear regression
model decision tree is a tree-shaped
algorithm used to determine a course of
action each branch of a tree represents
a possible decision occurrence or
reaction we have data which tells us if
it is a good day to play golf and if we
were to open this data up in a general
spreadsheet you can see we have the
outlook whether it's a rainy overcast
sunny temperature
hot mild cool humidity windy and did i
like to play golf that day yes or no so
we're taking a census and certainly i
wouldn't want a computer telling me when
i should go play golf or not but you
could imagine if you got up in the night
before you're trying to plan your day
and it comes up and says tomorrow would
be a good day for golf for you in the
morning and not a good day in the
afternoon or something like that this
becomes very beneficial and we see this
in a lot of applications coming out now
where it gives you suggestions and lets
you know what would be fit the match for
you for the next day or the next
purchase or the next uh whatever you
know next meal out in this case is
tomorrow a good day for playing golf
based on the weather coming in and so we
come up and let's uh determine if you
should play golf when the day is sunny
and windy so we found out the forecast
tomorrow is going to be sunny and windy
and suppose we draw our tree like this
we're going to have our humidity and
then we have our normal which is if it's
if you have a normal humidity you're
going to go play golf and if the
humidity is really high then we look at
the outlook and if the outlook is sunny
overcast or rainy it's going to change
what you choose to do so if you know
that it's a very high humidity and it's
sunny you're probably not going to play
golf because you're going to be out
there miserable fighting off the
mosquitoes that are out joining you to
play golf with you maybe if it's raining
you probably don't want to play in the
rain but if it's slightly overcast you
get just the right
shadow that's a good day to play golf
and be outside out on the green now in
this example you can probably make your
own creep pretty easily because it's a
very simple set of data going in but the
question is how do you know what to
split where do you split your data what
if this is much more complicated data
where it's not something that you would
particularly understand like studying
cancer they take about 36 measurements
of the cancerous cells and then each one
of those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but since the question is going to
come up is this the right decision tree
for that we should calculate entropy and
information gain two important
vocabulary words there are the entropy
and the information gain entropy entropy
is a measure of randomness or impurity
in the data set entropy should be low so
we want the chaos to be as low as
possible we don't want to look at it and
be confused by the images or what's
going on there with mixed data and the
information gain it is a measure of
decrease in entropy after the data set
is split also known as entropy reduction
information gain should be high so we
want our information that we get out of
the split to be as high as possible
let's take a look at entropy from the
mathematical side in this case we're
going to denote entropy as i of p of and
n where p is the probability that you're
going to play a game of golf and n is
the probability where you're not going
to play the game of golf now you don't
really have to memorize these formulas
there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log squared of p over p plus n minus n
over p plus n times the log squared of n
of p plus n but let's break that down
and see what actually looks like when
we're computing that from the computer
script side
entropy of a target class of the data
set is the whole entropy so we have
entropy play golf and we look at this if
we go back to the data you can simply
count how many yeses and no in our
complete data set for playing golf days
in our complete set we find we have five
days we did play golf and nine days we
did not play golf and so our i equals if
you had those together 9 plus 5 is 14.
and so our i equals 5 over 14 and 9 over
14 that's our p and n values that we
plug into that formula and you can go to
5 over 14 equals 0.36 9 over 14 equals
0.64 and when you do the whole equation
you get the minus 0.36 log root squared
of 0.36 minus 0.64 log squared root of
0.64 and we get a set value we get
0.94 so we now have a full entropy value
for the whole set of data that we're
working with and we want to make that
entropy go down and just like we
calculated the entropy out for the whole
set we can also calculate entropy for
playing golf and the outlook is it going
to be overcast or rainy or sunny and so
we look at the entropy we have p of
sunny times e of 3 of 2 and that just
comes out how many sunny days yes and
how many sunny days no over the total
which is five don't forget to put the
we'll divide that five out later on
equals p overcast equals four comma zero
plus rainy equals two comma three and
then when you do the whole setup we have
5 over 14. remember i said there was a
total of 5. 5 over 14 times the i of 3
of 2 plus 4 over 14 times the 4 comma 0
and 514 over i of 2 three and so we can
now compute the entropy of just the part
it has to do with the forecast and we
get .693 similarly we can calculate the
entropy of other predictors like
temperature humidity and wind and so we
look at the gain outlook how much are we
going to gain from this entropy play
golf minus entropy play golf outlook and
we can take the original 0.94 for the
whole set minus the entropy of just the
rainy day and temperature and we end up
with a gain of 0.247
so this is our information gain remember
we define entropy and we define an
information gain the higher the
information gain the lower the entropy
the better the information gain of the
other three attributes can be calculated
in the same way so we have our gain for
temperature equals
0.029 we have our gain for humidity
equals 0.152 and our gain for a windy
day equals 0.048 and if you do a quick
comparison you'll see the 0.247
is the greatest gain of information so
that's the split we want
now let's build the decision tree so we
have the outlook is it going to be sunny
overcast or rainy that's our first split
because that gives us the most
information gain and we can continue to
go down the tree using the different
information gains with the largest
information we can continue down the
nodes of the tree where we choose the
attribute with the largest information
gain as the root node and then continue
to split each subnode with the largest
information gain that we can compute and
although it's a little bit of a tongue
twister to say all that you can see that
it's a very easy to view visual model we
have our outlook we split it three
different directions if the outlook is
overcast we're going to play and then we
can split those further down if we want
so if the outlook is sunny but then it's
also windy if it's windy we're not going
to play if it's not windy we'll play so
we can easily build a nice decision tree
to guess what we would like to do
tomorrow and give us a nice
recommendation for the day so we want to
know if it's a good day to play golf
when it's sunny and windy remember the
original question that came out
tomorrow's weather report is sunny and
windy you can see by going down the tree
we go outlook sunny outlook windy we're
not going to play golf tomorrow so our
little smartwatch pops up and says i'm
sorry tomorrow is not a good day for
golf it's going to be sunny and windy
and if you're a huge golf fan you might
go uh-oh
it's not a good day to play golf we can
go in and watch a golf game at home so
we'll sit in front of the tv instead of
being out playing golf in the wind
now that we've looked at our decision
tree let's look at the third one of our
algorithms we're investigating support
vector machine support vector machine is
a widely used classification algorithm
the idea of support vector machine is
simple the algorithm creates a
separation line which divides the
classes in the best possible manner for
example dog or cat disease or no disease
suppose we have a labeled sample data
which tells height and weight of males
and females
a new data point arrives and we want to
know whether it's going to be a male or
a female so we start by drawing a line
we draw decision lines but if we
consider decision line one then we will
classify the individual as a male and if
we consider decision line two then it
will be a female so you can see this
person kind of lies in the middle of the
two groups it's a little confusing
trying to figure out which line they
should be under we need to know which
line divides the classes correctly but
how the goal is to choose a hyperplane
and that is one of the key words they
use when we talk about support vector
machines choose a hyperplane with the
greatest possible margin between the
decision line and the nearest point
within the training set
so you can see here we have our support
vector we have the two nearest points to
it and we draw a line between those two
points and the distance margin is the
distance between the hyperplane and the
nearest data point from either set so we
actually have a value and it should be
equal the distance between the two
points that we're comparing it to when
we draw the hyperplanes we observe that
line one has a maximum distance so we
observe that line one has a maximum
distance margin so we'll classify the
new data point correctly and our result
on this one is going to be that the new
data point is mel one of the reasons we
call it a hyperplane versus a line
is that
a lot of times we're not looking at just
weight and height we might be looking at
36 different features or dimensions and
so when we cut it with a hyperplane it's
more of a three-dimensional cut in the
data multi-dimensional it cuts the data
a certain way and each plane continues
to cut it down until we get the best fit
or match let's understand this with the
help of an example problem statement i
always start with a problem statement
when you're going to put some code
together we're going to do some coding
now classifying muffin and cupcake
recipes using support vector machines so
the cupcake versus the muffin let's have
a look at our data set and we have the
different recipes here we have a muffin
recipe that has so much flour i'm not
sure what measurement 55 is in but it
has 55 maybe it's ounces
but has a certain amount of flour
certain amount of milk sugar butter egg
baking powder vanilla and salt and so
based on these measurements we want to
guess whether we're making a muffin or a
cupcake and you can see in this one we
don't have just two features we don't
just have height and weight as we did
before between the male and female in
here we have a number of features
in fact in this we're looking at eight
different features to guess whether it's
a muffin or a cupcake
what's the difference between a muffin
and a cupcake turns out muffins have
more flour while cupcakes have more
butter and sugar so basically the
cupcake's a little bit more of a dessert
where the muffins a little bit more of a
fancy bread but how do we do that in
python how do we code that to go through
recipes and figure out what the recipe
is
and i really just want to say cupcakes
versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and jupiter nope the
anaconda navigator has all kinds of fun
tools once you're into the anaconda
navigator you can change environments i
actually have a number of environments
on here we'll be using python36
environment so this is in python version
3.6 although it doesn't matter too much
which version you use i usually try to
stay with the 3x because they're current
unless you have a project that's very
specifically in version 2x 27 i think is
usually what most people use in the
version 2. and then once we're in our
jupiter notebook editor i can go up and
create a new file and we'll just jump in
here
in this case we're doing svm muffin
versus cupcake
and then let's start with our packages
for
data analysis
and we almost always use a couple
there's a few very standard packages we
use
we use
import import import
numpy
that's for number python they usually
denote it as np that's very comma
that's very common
and then we're going to import pandas as
pd
and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy setup
as far as multiplying all the values in
an array in a numpy array data array
pandas
i can't remember if we're using it
actually in this data set i think we do
as an import it makes a nice data frame
and the difference between a data frame
and a numpy array is that a data frame
is more like your excel spreadsheet you
have columns you have indexes so you
have different ways of referencing it
easily viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so you need
them both in there
and then finally we're working with the
support vector machine so
from sk learn we're going to use the sk
learn model
import svm support vector machine
and then
as a data scientist you should always
try to visualize your data some data
obviously is too complicated or doesn't
make any sense to the human but if it's
possible it's good to take a second look
at it so you can actually see what
you're doing now for that we're going to
use two packages we're going to import
matplotlibrary.piplot as plt
again very common
and we're going to import seaborn as sns
and we'll go ahead and set the font
scale in the sns right in our import
line that's what this
semicolon followed by a line of data
we're going to set the sns and these are
great because the the seaborn sits on
top of map plot library just like pandas
hits on numpy so it adds a lot more
features and uses and control we're
obviously not going to get into matplot
library and see born that be its own
tutorial we're really just focusing on
the svm the support vector machine from
sk learn and since we're in jupiter
notebook
we have to add a special line in here
for our matplot library and that's your
percentage sign or ambersign matplot
library in line
now if you're doing this in just a
straight code project a lot of times i
use like notepad plus plus and i'll run
it from there you don't have to have
that line in there because it'll just
pop up as its own window on your
computer depending on how your computer
is set up because we're running this in
the jupyter notebook as a browser setup
this tells it to display all of our
graphics right below on the page so
that's what that line is for remember
the first time i ran this i didn't know
that i had to go look that up years ago
it was quite a headache so map plot
library inline is just because we're
running this on the web setup and we can
go ahead and run this make sure all our
modules are in they're all imported
which is great if you don't have an
import you'll need to go ahead and pip
use the pip or however you do it there's
a lot of other install packages out
there although pip is the most common
and you have to make sure these are all
installed on your python setup the next
step of course is we got to look at the
data you can't run a model for
predicting data if you don't have actual
data so to do that let me go and open
this up and take a look and we have our
cupcakes versus muffins
and it's a csv file or csv meaning that
it's comma separated variable
and it's going to open it up in a nice
spreadsheet for me and you can see up
here we have the type we have muffin
muffin muffin cupcake cupcake cupcake
and then it's broken up into flour milk
sugar butter egg baking powder vanilla
and salt
so we can do is we can go ahead and look
at this data also in our python
let us create a variable recipes equals
we're going to use our pandas module
dot read
csv remember is a comma separated
variable
and the file name happened to be
cupcakes versus muffins oops i got
double brackets there
do it this way
there we go cupcakes versus muffins
because the program i loaded or the the
place i saved this particular python
program is in the same folder we get by
with just the file name but remember if
you're storing it in a different
location you have to also put down the
full path on there
and then because we're in pandas
we're going to go ahead and you can
actually inline you can do this but let
me do the full print you can just type
in
recipes dot head
in the jupiter notebook but if you're
running in code in a different script
you need to go ahead and type out the
whole print recipes dot head
and panda's nose is that's gonna do the
first five lines of data and if we flip
back on over to the spreadsheet where we
opened up our csv file
uh you can see where it starts on line
two this one calls it zero and then
two three four five six is going to
match
i'm going to close that out because we
don't need that anymore
and it always starts at zero
and these are it automatically indexes
it since we didn't tell it to use an
index in here so that's the index number
for the left hand side and it
automatically took the top row
as labels so pandas
using it to read a csv is just really
slick and fast one of the reasons we
love our pandas
not just because they're cute and cuddly
teddy bears
and let's go ahead and plot our data
and i'm not going to plot all of it i'm
just going to plot the
sugar and flower
now obviously
you can see where they get really
complicated if we have tons of different
features
and so you'll break them up and maybe
look at just two of them at a time to
see how they connect
and to plot them we're going to go ahead
and use seaborne
so that's our sns
and the command for that is sns.lm plot
and then the two different variables i'm
going to plot is flour and sugar
data equals recipes
the hue equals type
and this is a lot of fun because it
knows that this is pandas coming in
so this is one of the powerful things
about pandas mixed with seaborne in
doing
graphing
and then we're going to use a pallet set
one there's a lot of different sets in
there you can go look them up for
seabourn or do a regular fit regular
equals false so we're not really trying
to fit anything
and it's a scatter kws
a lot of these settings you look up in
seabourn half of these you could
probably leave off when you run them
somebody played with this and found out
that these were the best settings for
doing a seaborn plot
let's go ahead and run that
and because it does it in line it just
puts it right on the page
and you can see right here that just
based on sugar and flour alone
there's a definite split
and we use these models because you can
actually look at and say hey if i drew a
line right between the middle of the
blue dots and the red dots
we'd be able to do an svm and a
hyperplane right there in the middle
then the next step is to
format or pre-process
our data
and we're going to break that up into
two parts
we need a type
label and remember we're going to decide
whether it's a muffin or a cupcake well
a computer doesn't know muffin or
cupcake it knows 0 and 1.
so what we're going to do is we're going
to create a type label
and from this we'll create a numpy array
and p where
and this is where we can do some logic
we take our recipes from our panda
and wherever type equals muffin it's
going to be 0 and then if it doesn't
equal muffin which is cupcakes is going
to be one so we create our type label
this is the answer so when we're doing
our training model remember we have to
have a training data this is what we're
going to train it with is that it's zero
or one it's a muffin or it's
not
and then we're going to create our
recipe
features
and if you remember correctly from right
up here the first column is type
so we really don't need the type columns
that's our muffin or cupcake
and in pandas we can easily sort that
out
we take our value recipes
dot
columns that's a pandas function built
into pandas
we've got values converting them to
values so it's just the column titles
going across the top
and we don't want the first one so what
we do is since it always starts at zero
we want one
colon till the
end and then
we want to go ahead and make this a list
and this converts it to a list of
strings
and then we can go ahead and just take a
look and see we're looking at for the
features make sure it looks right
let me go ahead and run that
and i forgot the s on recipes so we'll
go ahead and add the s in there and then
run that and we can see we have flour
milk sugar butter egg
baking powder vanilla and salt and that
matches what we have up here right where
we printed out everything but the type
so we have our features and we have our
label
now the recipe features is just the
titles of the columns and we actually
need
the ingredients
and at this point we have a couple
options one we could run it over all the
ingredients
and when you're doing this usually you
do but for our example we want to limit
it so you can easily see what's going on
because if we did all the ingredients
we have you know that's what
seven eight different hyper planes that
would be built into it we only want to
look at once you can see what the svm is
doing
and so we'll take our recipes and we'll
do just flour and sugar
again you can replace that with your
recipe features and do all of them but
we're going to do just flour and sugar
and we're going to convert that to
values
we don't need to make a list out of it
because it's not string values these are
actual
values on there and we can go ahead and
just print
ingredients and you can see what that
looks like
and so we have just the amount of flour
and sugar just the two sets of plots
and just for fun let's go ahead and take
this over here and
take our recipe features
and so if we decided to use all the
recipe features
you'll see that it makes a nice column
of different data so it just strips out
all the labels and everything we just
have just the values
but because we want to be able to view
this easily in a plot later on we'll go
ahead and take that and just do flower
and sugar
and we'll run that you'll see it's just
the two columns
so the next step is to go ahead and fit
our model
we're going to just call it model
and it's a svm we're using a package
called svc
in this case we're going to go ahead and
set the kernel equals linear so it's
using a specific set up on there and if
we go to the reference on their website
for the svm
you'll see that there's about there's
eight of them here
three of them are for regression
three are for classification the svc
support vector classification is
probably one of the most commonly used
and then there's also one for detecting
outliers
and another one that has to do with
something a little bit more specific on
the model but svc and sbr are the two
most commonly used standing for support
vector classifier and support vector
regression remember regression is an
actual value a float value or
whatever you're trying to work on and
sbc is a classifier so it's a yes no
true false
but for this we want to know zero one
muffin cupcake
if we go ahead and create our model and
once we have our model created we're
gonna do model.fit and this is very
common especially in the sk learn all
their models are followed with the fit
command
and what we put into the fit what we're
training with it is we're putting in the
ingredients which in this case we
limited to just flour and sugar
and the type label is it a muffin or
cupcake
now in more complicated data science
series you'd want to split into we won't
get into that today we split it into
training data and test data and they
even do something where they split it
into thirds where a third is used for
whether you switch between which one's
training and test there's all kinds of
things go into that it gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data
and let's go ahead and run this and now
we have our model fit
and i got an error here so let me fix
that real quick
capital svc it turns out
i did it lower case
support vector
classifier there we go let's go ahead
and run that
and you'll see it comes up with all this
information that it prints out
automatically
these are the defaults of the model
you notice that we change the kernel to
linear and there's our kernel linear on
the printout and there's other different
settings you can mess with
we're going to just leave that alone for
right now for this we don't really need
to mess with any of those
so next we're going to dig a little bit
into our newly trained model
and we're going to do this so we can
show you on a graph
let's go ahead and get the separating
and we're going to say we're going to
use a w for our variable on here
we're going to do model
dot coefficient underscore 0.
so what the heck is that again we're
digging into the model so we've already
got a prediction and a train
this is the math behind it that we're
looking at right now
and so
the w is going to represent two
different coefficients and if you
remember we had y equals mx plus c
so these coefficients are connected to
that
but in two dimensional it's a plane
we don't spend
too much time on this because you can
get lost in the confusion of the math so
if you're a math whiz this is great you
can go through here and you'll see that
we have a equals minus w0 over w of one
remember there's two different values
there
and that's basically the slope that
we're generating
and then we're going to build an xx what
is xx we're going to set it up to a
numpy array there's our np dot line
space so we're creating a line
plus the intercept well to make this
work we can do this as y y
equals the slope times each value in
that array that's a neat thing about
numpy so when i do a times x x which is
a whole numpy array of values it
multiplies a across all of them
and then it takes those same values and
we subtract the model intercept that's
here uh
where we had mx plus c so that'd be the
c from the formula y equals mx plus c
and that's where all these numbers come
from a little bit confusing because it's
digging out of these different arrays
and then we want to do is we're going to
take this and we're going to go ahead
and plot it
so plot the parallels to separating
hyperplane that pass through the support
vectors
and so we're going to create b equals a
model
support vectors
pulling our support vectors out there
here's our yy which we now know is a set
of data and we have we're going to
create yy down
equals a times xx plus
b1 minus a times b0
and then model support vector b is going
to be set that to a new value the minus
1 set up and y y up equals a times x x
plus b 1 minus a times b 0.
and we go ahead and just run this to
load these variables up if you want to
know understand a little bit more of
what's going on you can see if we print
y y let me just run that
you can see it's an array this is a line
it's going to have in this case between
30 and 60 so it's going to be 30
variables in here
and the same thing with yy up yy down
and we'll we'll plot those in just a
minute on the graph so you can see what
those look like
just go ahead and delete that out of
here and run that so it loads up the
variables nice clean slate
i'm just going to copy this from before
remember this our sns
our seaborn plot lm plot flower sugar
and i'll just go ahead and run that real
quick so you can see remember what that
looks like it's just a straight graph on
there
and then one of the new things is
because seaborn sits on top of pie plot
we can do the pie plot for the line
going through and that is simply plt dot
plot
and that's our xx
and yy our two corresponding values xy
and then somebody played with this to
figure out that the line width equals
two in the color black would look nice
so let's go ahead and run this whole
thing with the pie plot on there and you
can see when we do this it's just doing
flour and sugar on here
corresponding line between the sugar and
the flour and the muffin versus cupcake
and then we generated the
support vectors the yy down and yy out
so let's take a look and see what that
looks like
so we'll do our pl plot
and again this is all against xx
or our x value but this time we have yy
down
and let's do something a little fun with
this
we can put in a k dash dash
that just tells it to make it a dotted
line
and if we're going to do the down one
we also want to do the up one
so here's our yy
up
and when we run that it has both sets of
line
and so here's our support and this is
what you expect you expect these two
lines to go through the nearest data
point so the dashed lines go through the
nearest muffin and the nearest cupcake
when it's plotting it and then your svm
goes right down the middle so it gives
it a nice split in our data and you can
see how easy it is to see based just on
sugar and flour
which one's a muffin or a cupcake
let's go ahead and create a function
to predict
muffin or cupcake
i've got my
recipes i pulled off the
internet and i want to see the
difference between a
muffin or a cupcake
so we need a function to push that
through
and create a function with def
and let's call it muffin or cupcake now
remember we're just doing flour and
sugar today we're not doing all the
ingredients
and that actually is a pretty good split
you really don't need all the
ingredients you know it's flour and
sugar
and let's go ahead and do an if else
statement so if model predict
is a flower and sugar equals zero
so we take our model and we do it run a
predict it's very common in sk learn we
have a dot predict you put the data in
and it's going to return a value in this
case if it equals zero then print you're
looking at a muffin recipe
else if it's not zero that means it's
one then you're looking at a cupcake
recipe
that's pretty straightforward
for function or def for definition d e f
is how you do that in python
and of course we're going to create a
function you should run something in it
and so let's run a cupcake and we're
going to send it values 50 and 20 a
muffin or cupcake i don't know what it
is
and let's run this and just see what it
gives us
and it says oh it's a muffin you're
looking at a muffin recipe
so it very easily predicts whether we're
looking at a muffin or a cupcake recipe
let's
plot this
there we go plot this on the graph so we
can see what that actually looks like
and i'm just going to copy and paste it
from below where we're plotting all the
points in there
so this is nothing different than we did
before if i run it
you'll see it has all the points and the
lines on there and what we want to do is
we want to add another point
and we'll do plt plot
and if you remember correctly we did for
our test we did 50
and
20. and then somebody went in here and
decided we'll do uh y o for yellow or
it's kind of a oranges yellow color is
going to come out marker size nine those
are settings you can play with somebody
else played with them to come up with
the right setup so it looks good
and you can see there it is graphed
clearly a muffin
in this case in cupcakes versus muffins
the muffin has won
and if you'd like to do your own muffin
cupcake
contender series
you certainly can send a note down below
and the team at simply learn will send
you over the data they use for the
muffin and cupcake and that's true of
any of the data we didn't actually run a
plot on it earlier we had men versus
women you can also request that
information to run it on your data setup
so you can test that out
so to go back over our setup we went
ahead for our support vector machine
code we did a predict 40 parts flour 20
parts sugar i think was different than
the one we did whether it's a muffin or
a cupcake hence we have built a
classifier using svm which is able to
classify if a recipe is of a cupcake or
a muffin which wraps up our cupcake
versus muffin
today in our second tutorial we're going
to cover k-means and linear regression
along with going over the quiz questions
we had during our first tutorial
what's in it for you we're going to
cover clustering what is clustering k
means clustering which is one of the
most common use clustering tools out
there including a flow chart to
understand k-means clustering and how it
functions and then we'll do an actual
python live demo on clustering of cars
based on brands
then we're going to cover logistic
regression what is logistic regression
logistic regression curve and sigmoid
function and then we'll do another
python code demo to classify a tumor as
malignant or benign based on features
and let's start with clustering suppose
we have a pile of books of different
genres now we divide them into different
groups like fiction horror education and
as we can see from this young lady she
definitely is into heavy horror you can
just tell by those eyes and the maple
canadian leaf on her shirt but we have
fiction horror and education and we want
to go ahead and divide our books up well
organizing objects into groups based on
similarity is clustering and in this
case as we're looking at the books we're
talking about clustering things with
known categories but you can also use it
to explore data so you might not know
the categories you just know that you
need to divide it up in some way to
conquer the data and to organize it
better but in this case we're going to
be looking at clustering in specific
categories and let's just take a deeper
look at that we're going to use k means
clustering k means clustering is
probably the most commonly used
clustering tool in the machine learning
library k-means clustering is an example
of unsupervised learning if you remember
from our previous thing it is used when
you have unlabeled data so we don't know
the answer yet we have a bunch of data
that we want to cluster to different
groups define clusters in the data based
on feature similarity so we've
introduced a couple terms here we've
already talked about unsupervised
learning and unlabeled data so we don't
know the answer yet we're just going to
group stuff together and see if we can
find an answer of how things connect
we've also introduced feature similarity
features being different features of the
data now with books we can easily see
fiction and horror
and history books but a lot of times
with data some of that information isn't
so easy to see right when we first look
at it and so k-means is one of those
tools where we can start finding things
that connect that match with each other
suppose we have these data points and
want to assign them into a cluster now
when i look at these data points i would
probably group them into two clusters
just by looking at them i'd say two of
these group of data kind of come
together
but in k means we pick k clusters and
assign random centroids to clusters
where the k clusters represents two
different clusters we pick k clusters
and cyranium centroids to the clusters
then we compute distance from objects to
the centroids now we form new clusters
based on minimum distances
and calculate the centroids so we figure
out what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroid stop changing their
positions and become static
repeat previous two steps iteratively
till the cluster centroid stop changing
and the positions become static once the
clusters become static then k means
clustering algorithm is said to be
converged and there's another term we
see throughout machine learning is
converged that means whatever math we're
using to figure out the answer has come
to a solution or it's converged on an
answer so we see the flowchart to
understand make a little bit more sense
by putting it into a nice easy step by
step
so we start we choose k we'll look at
the elbow method in just a moment we
assign random centroids to clusters and
sometimes you pick the centroids because
you might look at the data in a graph
and say oh these are probably the
central points then we compute the
distance from the objects to the
centroids
we take that and we form new clusters
based on minimum distance and calculate
their centroids then we compute the
distance from objects to the new
centroids and then we go back and repeat
those last two steps we calculate the
distances so as we're doing it it brings
into the new centroid and then we move
the centroid around and we figure out
what the best which objects are closest
to each centroid so the objects can
switch from one centroid to the other
the centroids are moved around and we
continue that until it is converged
let's see an example of this
suppose we have this data set of seven
individuals and their score on two
topics a and b
so here's our subject
in this case referring to the person
taking the test
and then we have subject a where we see
what they've scored on their first
subject and we have subject b and we can
see what they score on the second
subject now let's take two farthest
apart points as initials cluster
centroids remember we talked about
selecting them randomly or we can also
just put them in different points and
pick the furthest one apart so they move
together either one works okay depending
on what kind of data you're working on
and what you know about it
so we took the two furthest points one
and one and five and seven and now let's
take the two farthest apart points as
initial cluster centroids each point is
then assigned to the closest cluster
with respect to the distance from the
centroids so we take each one of these
points in there we measure that distance
and you can see that if we measure each
of those distances and you use the
pythagorean theorem for a triangle in
this case because you know the x and the
y and you can figure out the diagonal
line from that or you just take a ruler
and put it on your monitor that'd be
kind of silly but it would work if
you're just eyeballing it you can see
how they naturally come together in
certain areas
now we again calculate the centroids of
each cluster so cluster one and then
cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes
1.8 comma 2.3 so remember is that one
and one well the very center of the data
we're looking at would put it at the one
point roughly 2 2 but 1.8 and 2.3 and
the second one if we wanted to make the
overall mean vector the average vector
of all the different distances to that
centroid we come up with four comma one
and five four so we've now moved the
centroids we compare each individual's
distance to its own cluster mean and to
that of the opposite cluster and we find
we can build a nice chart on here that
the as we move the centroid around we
now have a new different kind of
clustering of groups and using euclidean
distance between the points and the mean
we get the same formula you see these
new formulas coming up so we have our
individual dots distance to the mean
centroid of the cluster and distance to
the mean centroid of the cluster
only individual three is nearer to the
mean of the opposite cluster cluster two
than its own cluster one and you can see
here in the diagram where we've kind of
circled that one in the middle so when
we've moved the cluster the centroids of
the clusters over one of the points
shifted to the other cluster because
it's closer to that group of individuals
thus individual 3 is relocated to
cluster 2 resulting in a new partition
and we regenerate all those numbers of
how close they are to the different
clusters
for the new clusters we will find the
actual cluster centroids so now we move
the centroids over and you can see that
we've now formed two very distinct
clusters on here on comparing the
distance of each individual's distance
to its own cluster mean and to that of
the opposite cluster we find that the
data points are stable hence we have our
final clusters now if you remember i
brought up a concept earlier caming on
the k-means algorithm choosing the right
value of k will help in less number of
iterations and to find the appropriate
number of clusters in a data set we use
the elbow method and within sum of
squares wss is defined as the sum of the
squared distance between each member of
the cluster in its centroid and so you
see we've done here is we have the
number of clusters and as you do
the same k-means algorithm over the
different clusters and you calculate
what that centroid looks like and you
find the optimal you can actually find
the optimal number of clusters using the
elbow the graph is called as the elbow
method and on this we guessed it two
just by looking at the data but as you
can see the slope you actually just look
for right there where the elbow is in
the slope and you have a clear answer
that we want two different to start with
k means equals two a lot of times people
end up computing k-means equals two
three four five until they find the
value which fits on the elbow joint
sometimes you can just look at the data
and if you're really good with that
specific domain remember domain i
mentioned that last time you'll know
that where to pick those numbers or
where to start guessing at what that k
value is so let's take this and we're
going to use a use case using k-means
clustering to cluster cars into brands
using parameters such as horsepower
cubic inches make year etc so we're
going to use the data set cars data
having information about three brands of
cars toyota honda and nissan we'll go
back to my favorite tool the anaconda
navigator with the jupiter notebook and
let's go ahead and flip over to our
jupiter notebook and in our jupiter
notebook i'm going to go ahead and just
paste the basic
code that we usually start a lot of
these off with we're not going to go too
much into this code because we've
already discussed numpy we've already
discussed matplot library and pandas
they'll be being the number array pandas
being the pandas data frame and map plot
for the graphing and don't forget since
if you're using the jupiter notebook you
do need the map plot library inline so
that it plots everything on the screen
if you're using a different python
editor then you probably don't need that
because it'll have a pop-up window on
your computer
we'll go ahead and run this just to load
our libraries and our setup into here
the next step is of course to look at
our data which i've already opened up in
a spreadsheet and you can see here we
have the miles per gallon cylinders
cubic inches horsepower weight pounds
how heavy it is time it takes to get to
60
my card is probably on this one at about
80 or 90. what year it is so this is you
can actually see this is kind of older
cars and then the brand toyota
honda nissan so the different cars are
coming from all the way from 1971 if we
scroll down
to the 80s we have between the 70s and
80s the number of cars that they've put
out
and let's uh when we come back here
we're going to importing the data so
we'll go ahead and do data set equals
and we'll use pandas to read this in and
it's uh from a csv file remember you can
always post this in the comments and
request the data files for these either
in the comments here on the youtube
video or go to simplylearn.com and
request that the car csv i put it in the
same folder as the code that i've stored
so my python code is stored in the same
folder so i don't have to put the full
path if you store them in different
folders you do have to change this and
double check your name variables and
we'll go ahead and run this and we've
chosen set arbitrarily because you know
it's a data set we're importing and
we've now imported our car csv into the
data set
as you know you have to prep the data so
we're going to create the x data this is
the one that we're going to try to
figure out what's going on with and then
there's a number of ways to do this but
we'll do it in a simple loop so you can
actually see what's going on so we'll do
four i
and x dot columns so we're going to go
through each of the columns
and a lot of times it's important i'll
make lists of the columns and do this
because i might remove certain columns
or there might be columns that i want to
be processed differently but for this we
can go ahead and take x of i
and we want to go fill in a and that's a
pandas command but the question is what
are we going to fill the missing data
with we definitely don't want to just
put in a number that doesn't actually
mean something and so one of the tricks
you can do with this
is we can take x of i
and in addition to that we want to go
ahead and turn this into
an integer because a lot of these are
integers so we'll go ahead and keep it
integers and let me add the bracket here
and a lot of editors will do this
they'll think that you're closing one
bracket make sure you get that second
bracket in there if it's a double
bracket that's always something that
happens regularly so once we have our
integer of x of y this is going to fill
in any missing data with the average
and i was so busy closing one set of
brackets i forgot that the mean is also
has brackets in there for the pandas so
we can see here we're going to fill in
all the data with the average value for
that column so there's missing data is
in the average of the data it does have
then once we've done that we'll go ahead
and loop through it again
and just check and see
to make sure everything is filled in
correctly and we'll print and then we
take x
is null and this returns a set of the
null value or the how many lines are
null and we'll just sum that up to see
what that looks like and so when i run
this
and so with the x what we want to do is
we want to remove the last column
because that had the models that's what
we're trying to see if we can cluster
these things and figure out the models
there is so many different ways to sort
the x out for one we could take the x
and we could go data set our variable
we're using and use the i location one
of the features that's in pandas
and we could take that and then take all
the rows
and all but the last column of the data
set
and at this time we could do values we
just converted to values so that's one
way to do this and if i let me just put
this down here and print
x it's a capital x we chose and i run
this you can see it's just the values we
could also take out the values and
it's not going to return anything
because there's no values connected to
it what i like to do with this
is instead of doing the eye location
which does integers more common is to
come in here and we have our data set
and we're going to do data set
dot or data set dot columns and remember
that lists all the columns so if i come
in here
let me just mark that as red
and i print
data set dot columns
you can see that i have my index here i
have my mpg cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data columns of everything but
the last one minus one so now if i print
this you'll see the brand disappears
and so i can actually just take
data set columns minus one
and i'll put it right in here for the
columns we're going to look at
and let's un mark this
and unmark this
and now if i do an x dot head
i have a new data frame and you can see
right here we have all the different
columns except for the brand at the end
of the year
and it turns out when you start playing
with the data set you're going to get an
error later on and it'll say cannot
convert string to float value and that's
because if for some reason these things
the way they recorded them must been
recorded as strings so we have a neat
feature in here
on pandas to convert
and it is simply convert objects
and for this we're going to do convert
oops convert
underscore numeric numeric
equals
true and yes i did have to go look that
up i don't have it memorized the convert
numeric in there if i'm working with a
lot of these things i remember them but
depending on where i'm at what i'm doing
i usually have to look it up and we run
that oops i must have missed something
in here let me double check my spelling
and when i double check my spilling
you'll see i missed the first underscore
in the convert objects when i run this
it now has everything converted into a
numeric value because that's what we're
going to be working with is numeric
values down here
and the next part is that we need to go
through the data and eliminate null
values
most people when they're doing small
amounts working with small data pools
discover afterwards that they have a
null value and they have to go back and
do this
so you know be aware
whenever we're formatting this data
things are going to pop up and sometimes
you go backwards to fix it
and that's fine that's just part of
exploring the data and understanding
what you have
and i should have done this earlier but
let me go ahead and increase the size of
my window one notch
there we go easier to see
so we'll do 4i in
working with x dot columns we'll page
through all the columns and we want to
take
x of i and we're going to change that
we're going to alter it
and so
with this we want to go ahead and fill
in
x of i pandas has
the fill in a
and that just fills in any non-existent
missing data
then we'll put my brackets up and
there's a lot of different ways to fill
this data
if you have a really large data set some
people just void out that data because
and then look at it later in a separate
exploration of data
one of the tricks we can do is we can
take our column
and we can find the means
and the means is in other or quotation
marks
so we take the columns we're going to
fill in the non-existing one with the
means the problem is that returns a
decimal float so some of these aren't
decimals
certainly let me be a little careful of
doing this but for this example we're
just going to fill it in with the
integer version of this it keeps it on
par with the other data that isn't a
decimal point
and then what we also want to do is we
want to double check ways you can do
that is simply go in here
and take our x of i
column so it's going to go through the x
of i column it says is null
so it's going to return any any place
there's a null value it actually goes
through all the rows of each column is
null and then we want to go ahead and
sum that so we take that and we add the
sum value and these are all pandas so is
null is a panda command and so is sum
and if we go through that we go ahead
and run it
and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number value we turned it into
numeric
and we've removed the last column in our
data
and at this point we're actually going
to
start using the elbow method to find the
optimal number of clusters so we're now
actually getting into the sk learn part
uh the k means clustering on here
i guess we'll go ahead and zoom it up
one more not so you can see what i'm
typing in here
and then from sk learn we're going to or
sk learn
cluster
i'm going to import
k
means
i always forget to capitalize the k and
the m when i do this so it's capital k
capital m k means
and we'll go and create a um
array wcss equals when we get an empty
array if you remember from the elbow
method from our slide
within the sums of squares wss is
defined as the sum of square distance
between each member of the cluster in
the centroid so we're looking at that
change in differences as far as a
squared distance and we're going to run
this over a number of k-mean values
in fact let's go for i in range we'll do
11 of them
0 11.
and the first thing we're going to do is
we're going to create the actual
blue it all lower case
and so we're going to create this object
from the k-means that we just imported
and the variable that we want to put
into this is in
clusters
we're going to set that equals to i
that's the most important one because
we're looking at how increasing the
number of clusters
changes our answer there are a lot of
settings to the k-means
our guys in the back did a great job
just kind of playing with some of them
the most common ones that you see in a
lot of stuff is how you init your
k-means so we have k-means plus plus
this is just a tool to let the model
itself be smart how it picks its
centroids to start with its initial
incentroids we only want to iterate no
more than 300 times we have a max
iteration we put in there
we have the infinite the random state
equals zero you really don't need to
worry too much about these when you're
first learning this as you start digging
in deeper you start finding that these
are shortcuts that will speed up the
process
as far as a setup but the big one that
we're working with is the in clusters
equals i so we're going to literally
train our k-means 11 times we're going
to do this process 11 times
and if you're working with big data you
know the first thing you do is you run a
small sample of the data so you can test
all your stuff on it and you can already
see the problem that if i'm going to
iterate through a terabyte of data 11
times and then the k means itself is
iterating through the data multiple
times that's a heck of a process
so you got to be a little careful with
this a lot of times though you can find
your elbow using the elbow method find
your optimal number on a sample of data
especially if you're working with larger
data sources
so we want to go ahead and take our
k-means and we're just going to fit it
if you're looking at any of the sk-learn
very common that you fit your model and
if you remember correctly our variable
we're using is the capital x
and once we fit this value we go back to
the
array we made we want to go and just
depend that value on the end
and it's not the actual fit we're
pinning in there it's when it generates
it it generates the value you're looking
for is inertia so k means dot inertia
will pull that specific value out that
we need
and let's get a visual on this we'll do
our plt plot
and what we're plotting here
is first the x axis which is range 0
11 so that will generate a nice little
plot there and the wcss for our y-axis
it's always nice to give our plot a
title
and let's see we'll just give it the
elbow method for the title and let's get
some labels so let's go ahead and do plt
x label
and what we'll do we'll do number of
clusters for that
and plt y label
and for that we can do oops there we go
wcss since that's what we're doing on
the plot on there and finally we want to
go ahead and display our graph which is
simply plt dot oops
dot show there we go and because we have
it set to inline it'll appear in line
hopefully i didn't make a type error on
there
and you can see we get a very nice graph
you can see a very nice elbow joint
there at
two and again right around three and
four and then after that there's not
very much now as a data scientist if i
was looking at this
i would do either three or four and i'd
actually try both of them to see what
the
output look like and they've already
tried this in the back so we're just
going to use three as a setup on here
and let's go ahead and see what that
looks like when we actually use this
to show the different kinds of cars
and so let's go ahead and apply the
k-means to the cars data set
and basically we're going to copy the
code that we loop through up above
where k means equals k means number of
clusters and we're just going to set the
number of clusters to 3.
since that's what we're going to look
for and you can do 3 and 4 on this and
graph them just to see how they come up
differently be kind of curious to look
at that but for this we're just going to
set it to 3. go ahead and create our own
variable y
k means for our answers
and we're going to set that equal to
whoops a double equal there 2k means
but we're not going to do a fit we're
going to do a fit predict is the setup
you want to use and when you're using
untrained models you'll see a slightly
different because usually you see fit
and then you see just the predict but we
want to both fit and predict the k-means
on this and that's fit underscore
predict and then our capital x is the
data we're working with
and before we plot this data we're going
to do a little pandas trick we're going
to take our x value and we're going to
set x as matrix
so we're converting this into a nice
rows and columns kind of set up but we
want the we're going to have columns
equals none so it's just going to be a
matrix of data in here
and let's go ahead and run that
a little warning you'll see these
warnings pop up because things are
always being updated so there's like
minor changes in the versions and future
versions
instead of matrix now that it's more
common to set it dot values instead of
doing as matrix but math matrix works
just fine for right now and you'll want
to update that later on but let's go
ahead and dive in and plot this and see
what that looks like
and before we dive into plotting this
data i always like to take a look and
see what i am plotting so let's take a
look at y
k means i'm just going to print that out
down here
and we see we have an array of answers
we have 2 1 0 2 1 2.
so it's clustering these different
rows of data
based on the three different spaces it
thinks it's going to be
and then let's go ahead and print x and
see what we have for x and we'll see
that x is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions 0
and one
and if you were doing a full
presentation in front of the board
meeting
you might actually do a little different
than and dig a little deeper into the
different aspects because this is all
the different columns we looked at but
when we look at columns one and two for
this to make it easy
so let's go ahead and clear this data
out of here and let's bring up our plot
and we're going to do a scatter plot
here so plt scatter
and
this looks a little complicated so let's
explain what's going on with this
we're going to take the x values
and we're only interested in y of k
means equals 0 the first cluster okay
and then we're going to take value 0 for
the x-axis and then we're going to do
the same thing here
we're only interested in
k means equals 0 but we're going to take
this second column so we're looking at
the first two columns in our answer or
in the data
and then the guys in the back played
with this a little bit to make it pretty
and they discovered that it looks good
with as a size equals 100 that's the
size of the dots
we're going to use red for this one and
when they were looking at the data and
what came out it was definitely the
toyota on this we're just going to go
ahead and label it toyota again that's
something you really have to explore in
here as far as playing with those
numbers and see what looks good we'll go
ahead and hit enter in there and i'm
just going to paste in the next
two lines which is the next two cars
and this is our nissa and honda and
you'll see with our scatter plot we're
now looking at where y underscore k
means equals one
and we want the zero column and y k
means equals two again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to nissan and honda
and i'll go ahead and hit enter on there
and uh finally let's take a look and put
the centroids on there
again we're going to do a scatter plot
and on the centroids you can just pull
that from our k-means the model we
created
dot cluster centers
and we're going to just do
all of them in the first number and all
of them and the second number which is 0
1 because you always start with 0 and 1.
and then they were playing with the size
and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them it's always good to have some good
labels centroids
and then we do want to do a title
plt title
and pop up there
plt title cause you always want to make
your graphs look pretty and we'll call
it clusters of car make
and one of the features of the
plot library is you can add a
legend it'll automatically bring in it
since we've already labeled the
different aspects of the legend with
toyota nissan and honda
and finally we want to go ahead and show
so we can actually see it and remember
it's in line so if you're using a
different editor that's not the jupiter
notebook you'll get a pop-up of this
and you should have a nice set of
clusters here so we can look at this and
we have a clusters of honda and green
toyota and red nissan and purple and you
can see where they put the centroids to
separate them
now when we're looking at this we can
also plot a lot of other different data
on here as far because we only looked at
the first two columns this is just
column one and two or zero one as you
label them in computer scripting but you
can see here we have nice clusters of
car make and we were able to pull out
the data and you can see how just these
two columns
form very distinct clusters of data so
if you were exploring new data you might
take a look and say well what makes
these different
almost going in reverse you start
looking at the data and pulling apart
the columns to find out
why is the first group set up the way it
is maybe you're doing loans and you want
to go well why is this group not
defaulting on their loans and why is the
last group defaulting on their loans and
why is the middle group 50 defaulting on
their
bank loans and you start finding ways to
manipulate the data and pull out the
answers you want
so now that you've seen how to use
k-mean for clustering
let's move on to the next topic
now let's look into logistic regression
the logistic regression algorithm is the
simplest classification algorithm used
for binary or multi-classification
problems and we can see we have our
little girl from canada who's into
horror books is back that's actually
really scary when you think about that
with those big eyes in the previous
tutorial we learned about linear
regression dependent and independent
variables so to brush up y equals mx
plus c
very basic algebraic function of
y and x the dependent variable is the
target class variable we are going to
predict
the independent variables x1 all the way
up to xn are the features or attributes
we're going to use to predict the target
class
we know what a linear regression looks
like but using the graph we cannot
divide the outcome into categories
it's really hard to categorize 1.5 3.6
9.8
for example a linear regression graph
can tell us that with increase in number
of hours studied the marks of a student
will increase but it will not tell us
whether the student will pass or not in
such cases where we need the output as
categorical value we will use logistic
regression and for that we're going to
use the sigmoid function so you can see
here we have our marks 0 to 100 number
of hours studied that's going to be what
they're comparing it to in this example
and we usually form a line that says y
equals mx plus c and when we use the
sigmoid function we have p equals 1 over
1 plus e to the minus y
it generates a sigmoid curve and so you
can see right here when you take the ln
which is the natural logarithm i always
thought it should be nl not ln that's
just the inverse of
e your e to the minus y and so we do
this we get ln of p over one minus p
equals m times x plus c that's the
sigmoid curve function we're looking for
and we can zoom in on the function and
you'll see that the function as it
derives goes to 1 or to 0 depending on
what your x value is and the probability
if it's greater than 0.5 the value is
automatically rounded off to 1
indicating that the student will pass so
if they're doing a certain amount of
studying they will probably pass then
you have a threshold value at the 0.5 it
automatically puts that right in the
middle usually and your probability if
it's less than 0.5 the value rendered
off to zero indicating the student will
fail so if they're not studying very
hard they're probably going to fail this
of course is ignoring the outliers or
that one student is just a natural
genius and doesn't need any studying to
memorize everything that's not me
unfortunately i have to study hard to
learn new stuff
problem statement to classify whether a
tumor is malignant or benign and this is
actually one of my favorite data sets to
play with because it has so many
features and when you look at them you
really are hard to understand you can't
just look at them and know the answer so
it gives you a chance to kind of dive
into what data looks like when you
aren't able to understand the specific
domain of the data but i also want you
to remind you that in the domain of
medicine if i told you that my
probability
was really good it classified things
that say 90 or 95
and i'm classifying whether you're going
to have a malignant or a b9 tumor i'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so it might help them or aid them
in something they missed from before so
let's go ahead and dive into the code
and i'll come back to the domain part of
it in just a minute so use case and
we're going to do our normal imports
here we're importing numpy
pandas seaborn the matplot library and
we're going to do matplot library inline
since i'm going to switch over to
anaconda so let's go ahead and flip over
there and get this started
so i've opened up a new window in my
anaconda jupiter notebook
and by the way jupiter notebook you
don't have to use anaconda for the
jupiter notebook i just love the
interface and all the tools and anaconda
brings so we got our import numpy as in
p
for our numpy number array we have our
pandas pd we're going to bring in
seaborn to help us with our graphs as
sns
so many really nice tools in both
seaborne and matplot library and we'll
do our
matplotlibrary.pipelot as plt and then
of course we want to let it know to do
it in line and let's go and just run
that so it's all set up
and we're just going to call our data
data
not creative today
equals pd and this happens to be in a
csv file
so we'll use the pd.read underscore csv
and i happen to name the file i renamed
it
dataforp2.csv you can of course write in
the comments below the youtube and
request for the dataset itself or go to
the simplylearn website and we'll be
happy to supply that for you
and let's just open up the data before
we go any further and let's just see
what it looks like in a spreadsheet
so when i pop it open in a local
spreadsheet this is just a csv file
comma separated variables we have an id
so i guess the
categorizes for reference or what id
which test was done the diagnosis m for
malignant b for b9 so there's two
different options on there and that's
what we're going to try to predict is
the m and b and test it and then we have
like the radius
mean or average the texture average
perimeter mean area mean smoothness i
don't know about you but unless you're a
doctor in the field most of stuff i mean
you can guess what concave means just by
the term concave but i really wouldn't
know what that means in the measurements
they're taking so they have all kinds of
stuff like how smooth it is uh the
symmetry and these are all float values
let me just page through them real quick
and you'll see there's i believe 36 if i
remember correctly in this one
so there's a lot of different values
they take all these measurements they
take when they go in there and they take
a look at the different growth the
tumorous growth
so back in our data and i put this in
the same folder as a code so i saved
this code in that folder obviously if
you have it in a different location you
want to put the full path in there and
we'll just do
pandas
first five lines of data with the
data.head
we run that we can see that we have
pretty much what we just looked at we
have an id we have a diagnosis
if we go all the way across you'll see
all the different columns coming across
displayed nicely
for our data
and while we're exploring the data our
seaborn which we referenced as sns
makes it very easy to go in here and do
a joint plot you'll notice that
very similar to because it is sitting on
top of the plot library so the joint
plot does a lot of work for us and we're
just going to look at the first two
columns that we're interested in the
radius mean and the texture mean we'll
just look at those two columns
and data
equals data so that tells it which two
columns we're plotting and that we're
going to use the data that we pulled in
let's just run that
and it generates a really nice graph on
here
and there's all kinds of cool things on
this graph to look at i mean we have the
texture mean and the radius mean
obviously the axes you can also see
and one of the cool things on here is
you can also see the histogram they show
that for the radius mean whereas the
most commons radius mean come up and
where the most common texture is so
we're looking at the tech the
on each growth its average texture and
on each radius its average uh radius on
there it's a little confusing because
we're talking about the individual
objects average and then we can also
look over here and see the the histogram
showing us the median or how common each
measurement is
and that's only two columns so let's dig
a little deeper into seaborn they also
have a heat map and if you're not
familiar with heat maps a heat map just
means it's in color that's all that
means heat map i guess the original ones
were plotting heat density on something
and so ever since then it's just called
a heat map and we're going to take our
data and get our corresponding numbers
to put that into the heat map and that's
simply data dot c-o-r-r
for that that's a pandas expression
let's remember we're working in a pandas
data frame so that's one of the cool
tools in pandas for our data and this is
pull that information into a heat map
and see what that looks like
and you'll see that we're now looking at
all the different features we have our
id we have our texture we have our area
our compactness concave points
and if you look down the middle of this
chart diagonal going from the upper left
to bottom right
it's all white that's because when you
compare texture to texture they're
identical so they're 100 percent or in
this case a perfect one in their
correspondence
and you'll see that when you look at say
area or right below it it has almost a
black on there when you compare it to
texture so these have almost no
corresponding data they don't really
form a linear graph or something that
you can look at and say how connected
they are they're very scattered data
this is really just a really nice graph
to get a quick look at your data doesn't
so much change what you do but it
changes verifying so when you get an
answer or something like that or you
start looking at some of these
individual pieces you might go hey that
doesn't match according to showing our
heat map this should not correlate with
each other and if it is you're going to
have to start asking well why what's
going on what else is coming in there
but it does show some really cool
information on here and we can see from
the id
there's no real one feature that just
says if you go across the top line that
lights up there's no one feature that
says hey if the area is a certain size
then it's going to be b9 or malignant it
says there's some that sort of add up
and that's a big hint in the data that
we're trying to id this whether it's
malignant or b9 that's a big hint to us
as data scientists to go okay
we can't solve this with any one feature
it's going to be something that includes
all the features or many of the
different features to come up with the
solution for it and while we're
exploring the data let's explore one
more area and let's look at data dot is
null we want to check for null values in
our data if you remember from earlier in
this tutorial we did it a little
differently where we added stuff up and
sum them up you can actually with pandas
do it really quickly data. is null and
summit and it's going to go across all
the columns so when i run this
you're going to see all the columns come
up with no null
data
so we've just just to re hash these last
few steps
we've done a lot of exploration we have
looked at the first two columns
and seen how they plot with the seaborn
with a joint plot which shows both the
histogram
and the data plotted on the xy
coordinates
and obviously you can do that more in
detail
with different columns and see how they
plot together
and then we took and did the seaborne
heat map the sns
dot heat map of the data and you can see
right here where it did a nice job
showing us some bright spots where stuff
correlates with each other and forms a
very nice combination or points of
scattering points and you can also see
areas that don't
and then finally we went ahead and
checked the data is the data null value
do we have any missing data in there
very important step because it'll crash
later on
if you forget to do this step it will
remind you when you get that nice error
code that says no values okay
so not a big deal if you miss it but
it's no fun having to go back when
you're you're in a huge process and
you've missed this step and now you're
10 steps later and you've got to
remember where you were pulling the data
in
so we need to go ahead and pull out our
x
and our y let me just put that down here
and we'll set the x equal to and there's
a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the id
and the diagnosis so that certainly
would be an option
but we're going to do is we're actually
going to focus on the worst the worst
radius the worst texture parameter area
smoothness compactness and so on one of
the reasons to start dividing your data
up when you're looking at this
information
is
sometimes the data will be the same data
coming in so if i have two measurements
coming in to my model it might overweigh
them it might overpower the other
measurements because it's measuring it's
basically taking that information in
twice
that's a little bit past the scope of
this tutorial i want you to take away
from this though is that we are dividing
the data up into pieces and our team in
the back went ahead and said hey let's
just look at the worst so i'm going to
create a an array
and you'll see this array radius worst
texture worse perimeter worst we've just
taken the worst of the worst and i'm
just going to put that in my x so this x
is still a pandas data frame but it's
just those columns and our y if you
remember correctly
it's going to be oops hold on one second
it's not x it's data there we go so x
equals data and then it's a list of the
different columns the worst of the worst
and if we're going to take that then we
have to have our answer for our y for
the stuff we know and if you remember
correctly we're just going to be looking
at
the diagnosis that's all we care about
is what is it diagnosed is it b9 or
malignant
and since it's a single column we can
just do diagnosis oh i forgot to put the
brackets or the there we go okay so it's
just diagnosis on there and we can also
real quickly do like x dot head if you
want to see what that looks like
and y dot head
and run this and you'll see
it only does the last one i forgot about
that if you don't do print you can see
that the the y dot head is just mmm
because the first ones are all malignant
and if i run this the x dot head is just
the first five values of radius worst
texture worse parameter worst area worst
and so on
i'll go ahead and take that out
so moving down
to the next step we've built our two
data sets our answer and then the
features we want to look at
in data science it's very important to
test your model
so we do that by splitting the data
and from sk learn model selection we're
going to import train test split so
we're going to split it into two groups
there are so many ways to do this i
noticed in one of the more modern ways
to actually split it into three groups
and then you model each group and test
it against the other groups so you have
all kinds and there's reasons for that
which is past the scope of this and for
this particular example isn't necessary
for this we're just going to split it
into two groups one to train our data
and one to test our data and the
sklearn
dot model selection we have train test
split you could write your own quick
code to do this we just randomly divide
the data up into two groups but they do
it for us nicely
and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables
capital x train capital x test so we
have our training data we're going to
use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model
and we'll go ahead and take our train
test split that we just imported
and we're going to do x and our y are
two different data that's going in for
our split
and then the guys in the back came up
and wanted us to go ahead and use a test
size equals 0.3 that's test underscore
size random state it's always nice to
kind of switch your random state around
but not that important
what this means is that the test size is
we're going to take 30 percent of the
data and we're going to put that into
our test variables our y test and our x
test and we're going to do 70 into the x
train and the y train so we're going to
use 70 of the data to train our model
and 30 to test it
let's go ahead and run that and load
those up
so now we have all our stuff split up
and all our data ready to go and now we
get to the actual logistics part we're
actually going to do our create our
model
so let's go ahead and bring that in from
sklearn we're going to bring in our
linear model and we're going to import
logistic regression that's the actual
model we're using and this we'll call it
log model
stereo model and let's just set this
equal to our logistic regression that we
just imported
so now we have a variable log model set
to that class for us to use and with
most the
models in the sk learn we just need to
go ahead and fix it fit do a fit on
there and we use our x train that we
separated out with our y
train and let's go ahead and run this so
once we've run this we'll have a model
that fits this data that 70 percent of
our training data
and of course it prints this out that
tells us all the different variables
that you can set on there there's a lot
of different choices you can make but
for word though we're just going to let
all the defaults sit we don't really
need to mess with those on this
particular example and there's nothing
in here that really stands out as super
important until you start
fine tuning it but for what we're doing
the basics will work just fine and then
let's we need to go ahead and test out
our model is it working so let's create
a variable y predict and this is going
to be equal to
our log model
and we want to do a predict again very
standard format for the sk learn library
is taking your model and doing a predict
on it and we're going to test why
predict against the y test so we want to
know what the model thinks it's going to
be that's what our y predict is and with
that we want the capital x x
test
so we have our train set and our test
set and now we're going to do our y
predict and let's go ahead and run that
and if we print
y
predict
let me go ahead and run that
you'll see it comes up and it presents
and i prints a nice array of uh b and m
for b9 and malignant
for all the different test data we put
in there
so that's pretty good we're not sure
exactly how good it does but we can see
that it actually works and it's
functional was very easy to create
you'll always discover with our data
science that as you explore this you
spend a significant amount of time
prepping your data
and making sure your data coming in is
good
there's a saying
good data in good answers out bad data
in
bad answers out that's only half the
thing that's only half of it
selecting your models becomes the next
part as far as how good your models are
and then of course fine-tuning it
depending on what model you're using
so we come in here we want to know how
good this came out so we have our y
predict here log model dot predict x
test
so for deciding how good our model is
we're going to go from the
sklearn.metrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the model
data let's just print this out
and we'll take our classification report
and we're going to put into there
our test our actual
data so this is what we actually know is
true
and our prediction what our model
predicted for that data on the test side
and let's run that and see what that
does
so we pull that up you'll see that we
have
a precision for b9 and malignant b and m
and we have a precision of 93 and 91 a
total of 92 so it's kind of the average
between these two 92 there's all kinds
of different information on here your f1
score
your recall your support coming through
on this
and for this i'll go ahead and just flip
back to our slides that they put
together for describing it and so here
we're going to look at the precision
using the classification report and you
see this is the same printout i had up
above some of the numbers might be
different because it does randomly pick
out which data we're using so this model
is able to predict the type of tumor
with 91 percent accuracy
so we look back here let's you'll see
where we have uh b9 in england it
actually is 92 coming up here but we're
looking about a 92 91 precision and
remember i reminded you about domains
and we're talking about the domain of a
medical domain with a very catastrophic
outcome you know at 91 or 92 percent
precision you're still going to go in
there and have somebody do a biopsy on
it very different than if you're
investing money and there's a 92 percent
chance you're going to earn 10
and 8 chance you're going to lose 8
you're probably going to bet the money
because at that odds it's pretty good
that you'll make some money and in the
long run you do that enough you
definitely will make money and also with
this domain i've actually seen them use
this to identify different forms of
cancer that's one of the things that
they're starting to use these models for
because then it helps the doctor know
what to investigate
so that wraps up this section we're
finally we're going to go in there and
let's discuss the answer to the quiz
asked in machine learning tutorial part
1.
can you tell what's happening in the
following cases grouping documents into
different categories based on the topic
and content of each document this is an
example of clustering where k-means
clustering can be used to group the
documents by topics using bag of words
approach so if you've gotten in there
that you're looking for clustering and
hopefully you had at least one or two
examples like k-means that are used for
clustering different things then give
yourself a two thumbs up
b identifying handwritten digits and
images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you were able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
c behavior of a website indicating that
the site is not working as designed this
is an example of anomaly detection in
this case the algorithm learns what is
normal and what is not normal usually by
observing the logs of the website give
yourself a thumbs up if you got that one
and just for a bonus can you think of
another example of anomaly detection one
of the ones i use it for my own business
is detecting anomalies in stock markets
stock markets are very fickled and they
behave very radical so finding those
erratic areas and then finding ways to
track down why they're erratic was
something released in social media was
something released you can see where
knowing where that anomaly is can help
you to figure out what the answer is to
it in another area d predicting salary
of an individual based on his or her
years of experience this is an example
of regression this problem can be
mathematically defined as a function
between independent years of experience
and dependent variables salary of an
individual and if you guessed it this
was a regression model give yourself a
thumbs up and if you were able to
remember that it was between independent
and dependent variables and that terms
give yourself two thumbs up we're going
to cover mathematics for machine
learning so today's agenda is going to
cover data and its types and we're going
to dive into linear algebra and its
concepts
calculus statistics for machine learning
probability for machine learning
hands-on demos and of course throwing in
there in the middle is going to be your
matrixes and a few other things to go
along with all this
data and as types
data denotes the individual pieces of
factual information collected from
various sources it is stored processed
and later used for analysis
and so we see here just a huge grouping
of information a lot of tech stuff money
dollar signs numbers
and then you have your performing
analytics to drive insights and
hopefully you have a nice share your
shareholders gather it at the meeting
and you're able to explain it in
something they can understand
so we talk about data types of data
we have in our types of data we have a
qualitative categorical
you think nominal or ordinal
and then you have your quantitative or
numerical which is discrete or
continuous
and let's look a little closer at those
data type vocabulary always people's
favorite is the vocabulary words okay
not mine
uh but let's dive into this what we mean
by nominal
nominal they are used to label various
uh label our variables without providing
any measurable value
country gender race hair color etc
it's something that you either mark true
or false this is a label it's on or off
either they have a red hat on or they do
not
so a lot of times when you're thinking
nominal data
labels
think of it as a true false kind of
setup and we look at ordinal this is
categorical data with a set order or a
scale to it
and you can think of salary range as a
great one
movie ratings etc
you see here the salary rains if you
have ten thousand to twenty thousand
number of employees earning that rate is
a hundred and fifty twenty thousand to
thirty thousand a hundred and so forth
some of the terms you'll hear is bucket
this is where you have 10 different
buckets and you want to separate it into
something that makes sense into those 10
buckets
and so when we start talking about
ordinal a lot of times when you get down
to the brass bones again we're talking
true false so if you're a member of the
10 to 20 k rains
so forth those would each be either part
of that group or you're not but now
we're talking about buckets and we want
to count how many people are in that
bucket
quantitative numerical data
falls into two classes discrete or
continuous
and so data with a final set of values
which can be categorized class strength
questions answered correctly and runs
hit and cricket
a lot of times when you see this you can
think integer
and a very restricted integer i.e
you can only have 100 questions
on a test so you can it's very discreet
i only have 100 different values that it
can attain
so think usually you're talking about
integers but within a very small range
they don't have an open end or anything
like that
so discrete is very solid
simple to count
set number
continuous on the other hand continuous
data can take any numerical value within
a range so water pressure weight of a
person etc
usually we start thinking about float
values where they can get phenomenally
small in their in what they're worth
and there's a whole series of values
that falls right between discrete and
continuous
you can think of the stock market you
have dollar amounts it's still discrete
but it starts to get complicated enough
when you have like you know jump in the
stock market from
525.33 cents to
580.67 cents there's a lot of point
values in there it'd still be called
discrete but you start looking at it as
almost continuous because it does have
such a variance in it now we talk about
no we did we went over nominal and
ordinal
almost true false charts and we looked
at quantitative and numerical data which
were starting to get into numbers
discrete you can usually a lot of times
discrete will be put into it could be
put into true false but usually it's not
so we want to address this stuff and the
first thing you want to look at is the
very basic which is your algebra so
we're going to take a look at linear
algebra you can remember back when your
euclidean geometry we have a line well
let's go through this we have a linear
algebra is the domain of mathematics
concerning linear equations
and their representations in vector
spaces and through matrixes i told you
we're going to talk about matrixes
so a linear equation
is simply
2x plus 4y minus 3z equals 10. very
linear 10x plus 12.4 y equals z and now
you can actually solve these two
equations by combining them
and that's we're talking about a linear
equation
in the vectors we have a plus b equals c
now we're starting to look at a
direction
and these values usually think of an x y
z plot
so each one is a direction and the
actual
distance of like a triangle a b is c
and then your matrix can describe all
kinds of things
i find matrixes
confuse a lot of people
not because they're particularly
difficult but because of the magnitude
and the different things they're used
for
and a matrix is a chart
or a
you know think of a spreadsheet but you
have your rows and your columns
and you'll see here we have a times b
equals c
very important to know your counts
so depending on how the math is being
done what you're using it for making
sure you have the same rows and number
of columns or a single number there's
all kinds of things that play in that
that can make matrixes confusing
but really it has a lot more to do with
what domain you're working in
are you adding in
multiple polynomials where you have like
a x squared plus b y plus you know you
start to see that it can be very
confusing versus a very straightforward
matrix and let's just go a little deeper
into these because these are such
primary this is what we're here to talk
about is these different math uh
mathematical computations that come up
so we're looking at linear equations
let's dig deeper into that one an
equation having a maximum order of one
is called a linear equation
so it's linear because when you look at
this we have ax plus b equals c which is
a one variable
we have two variable ax plus b y equals
c a x plus b y plus z c z equals d
and so forth but all of these
are to the power of one you don't see x
squared you don't see x cubed so we're
talking about linear equations that's
what we're talking about in their
addition
if you have already dived into say
neural networks you should recognize
this ax plus by plus cz
setup plus the intercept
which is basically your your neural
network each node adding up all the
different inputs and we can drill down
into that most common formula is your y
equals mx plus c
so you have your y
equals the m which is your slope your x
value plus c
which is your
y-intercept you kind of labeled it wrong
here
threw me for a loop but the the c would
be your y-intercept so when you set x
equal to zero y equals c and that's
that's your y-intercept right there
uh and that's they just had a reverse
value of y when x equals zero
equals the y-intercept which is c and
your slow gradient line which is your m
so you get your y equals 2x plus 3.
and there's lots of easy ways to compute
this this way this is why we always
start with the most basic one when we're
solving one of these problems and then
of course the
one of the most important takeaways is
the slope gradient of the line
so the slope is very important that m
value
in this case we went ahead and solved
this
if you have y equals 2 x plus 3 you can
see how it has a nice line graph here on
the right
so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and rows
so we're talking m rows by n columns
here a11 is denotes the element of the
first row in the first column similarly
a12 and it's really pronounced a11 in
this particular setup so it's a row one
column one a 12 is a
row one column two
first row and second column and so on
and there's a lot of ways to denote this
i've seen these as like a capital letter
a smaller case a for the top row or i
mean you can see where they can go all
kinds of different directions as far as
the value
you just take a moment to realize
there's need to be some designation as
far as what row it's in and what column
it's in
and we have our basic operations we have
addition so when you think about
addition you have uh
two matrixes of two by two and you just
add each individual number in that
matrix and then when you get to the
bottom you have uh in this case the
solution is twelve ten plus two is
twelve five plus 3 is 8 and so on and
the same thing with subtraction
now again your counting matrix is you
want to check your
dimensions of the matrix
the shape you'll see shape come up a lot
in programming so we're talking about
dimensions we're talking about the shape
if the two shapes are equal
this is what happens when you add them
together or subtract them
and we have multiplication when you look
at the multiplication you end up at the
very a slightly different setup going
now
if we look at our last one we're
we're like why
this always gets to me when we get to
matrixes they don't really say why you
multiply matrixes
know my first thought is one times two
four times three but if you look at this
we get one times two plus four times
three
one times three plus four times five
uh six times two plus three times three
six times three plus three times 5. if
you're looking at these matrixes
think of this more as an equation
and so we have if you remember we went
back up here for our multiple line
equations let's just go back up a couple
slides where we were looking at
a two variable so this is a two variable
equation ax plus b y equals c
and this is a way to make it very quick
to solve these variables and that's why
you have the matrix and that's why you
do
the multiplication the way they do
and this is the dot product of uh one
times two
plus four times three
one times three plus four times five
uh six times two plus three times three
six times three plus three times five
and it gives us a nice little
14 23 21 and 33 over here which then can
be used and reduced down to a sample
formula as far as solving the variables
as you have enough inputs
uh and then in matrix operations when
you're dealing with a lot of matrixes uh
now keep in mind multiplying matrixes is
different than finding the product of
two matrixes okay so we're talking about
multiplication we're talking about
solving
uh for equations when you're finding the
product you are just finding one times
two keep that in mind because that does
come up i've had that come up a number
of times where i'm altering data and i
get confused as to what i'm doing with
it
uh transpose flipping the matrix over is
diagonal comes up all the time where you
have you still have 12 but instead of it
being 12 8 it's now 12 14
8 21 you're just flipping the columns
and the rows
and then of course you can do an inverse
changing the signs of the values across
this main diagonal
and you can see here we have the inverse
a to the minus 1 and ends up with
instead of 12 8 14 12 it's now minus 22
minus 12.
vectors uh vector just means we have
a value and a direction
and we have down four numbers here on
our vector
in mathematics a one dimensional matrix
is called a vector uh so
if you have your x plot and you have a
single value that values along the x
axis and it's a single dimension
if you have two dimensions you can think
about putting them on a graph you might
have x and you might have y and each
value denotes a direction and then of
course the actual distance is going to
be the hypothesis of that triangle
and you can do that with three
dimensionals x y and z
and you can do it all the way to nth
dimensions so when they talk about the k
means
uh for categorizing and how close data
is together they will compute that based
on the pythagorean theorem so you would
take
the square of each value add them all
together and find the square root and
that gives you a distance
as far as where that point is where that
vector exists or an actual point value
and then you can compare that point
value to another one it makes a very
easy comparison versus comparing 50 or
60 different numbers
and that brings us up to i gene vectors
and i gene values
hygiene vectors the vectors that don't
change their span while transformation
and i gene values the scalar values that
are associated to the vectors
conceptually
you can think of the vector as your
picture you have a picture it's
two dimensions x and y
and so when you do those two dimensions
and those two values or whatever that
value is
that is that point but the values change
when you skew it and so
if we take and we have a vector a
and that's a set value
b is
your is your you have a and b which is
your i gene vector 2 is the i gene value
so we're altering
all the values by two that means we're
maybe we're stretching it out one
direction making it tall if you're doing
picture editing
that's one of the places this comes in
but you can see when you're transforming
uh your different information how you
transform it is then your hygiene value
and you can see here vector after line
transit transition uh we have 3a a is
the hygiene vector 3 is the aging value
so a doesn't change that's whatever we
started with that's your original
picture and 3
is skewing it one direction and maybe
a b is being skewed another direction
and so you have a nice tilted picture
because you altered it by those by the
aging values
so let's go ahead and pull up a demo on
linear algebra and to do this i'm going
to go through my trusted anaconda into
my jupiter notebook
and we'll create a new
notebook called linear algebra
since we are working in python we're
going to use our numpy i always import
that as np or numpy array probably the
most popular
module for doing matrixes and things in
given that this is part of a series i'm
not going to go too much into numpy we
are going to go ahead and create two
different variables a for a numpy array
10 15 and b29
we'll go ahead and run this and you can
see there's our two arrays 10 15 29 and
i went in added a space there in between
so it's easier to read
and since it's the last line we don't
have to put the print statement on it
unless you want we can simply but we can
simply do a plus b so when i run this
we have 10 15 29 and we get 30 24 which
is what you expect 10 plus 20 15 plus 9
you could almost look at this addition
as being
just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do a dot t
plus b dot t
remember that t flips them and so if we
do that we now get them uh we now have
30 24 going the other way
we could also do something kind of fun
there's a lot of different ways to do
this
as far as a plus b i can also do a plus
b
dot t
and you're going to see that that will
come out the same the 30 24 whether i
transpose a and b or transpose them both
at the end
and likewise we can very easily subtract
two vectors i can go a minus b
and we run that and we get minus 10 6.
now remember this is the last line in
this particular section that's right not
to put the print around it
and just like we did before
we can transpose either the individual
or we can transpose the main set up and
then we get a -10 six 6 going the other
way
now we didn't mention this in our notes
but you can also do a scalar
multiplication
and just put down scalar so you can
remember that
uh what we're talking about here is i
have
this
array here u
and if i go a
times u
we'll take the value 2 we'll multiply it
by every value in here so 2 times 30 is
60 2 times 15
and just like we did before
this happens a lot because when you're
doing matrixes you do need to flip them
you get 60 30 coming this way
so in numpy uh we have they called dot
product
and uh with this this in a two
dimensional vectors it is equivalent of
two matrix multiplication remember we
were talking about matrix multiplication
uh where it is the
well let's walk through it
we'll go ahead and start by defining two
um
numpy arrays we'll have uh 10 20 25 6 or
our u and our v
and then we're going to go ahead and do
if we take
the values
and if you remember correctly
an array like this would be 10 times 25
plus 20 times 6.
we'll go ahead and
print that
there we go
and then we'll go ahead and do the np
dot dot
of u comma
v
and we'll find when we do this we go and
run this
we're going to get
370
370.
so this is a strain multiplication where
they use it to solve
linear algebra when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
value for our dot multiplication
and we did
addition earlier which is just your
basic addition
and of course the matrix you can get
very complicated on these or
in this case we'll go ahead and do
let's create two
complex matrixes
this one is a matrix of
12 10 4 6 4 31.
we'll just print out a so you can see
what that looks like here's print
a
we print a out you can see that we have
a
two by three
layer matrix for a and we can also put
together
always kind of fun when you're playing
with print values
we could do something like this we could
go in here
there we go
we could print a we have it end with
equals a
run and this kind of gives it a nice
look uh here's your matrix that's all
this is comma n means it just tags it on
the end that's all all that is doing on
there
and then we can simply add in what is a
plus b and you should already guess
because this is the same as what we did
before there's no difference uh we do a
simple vector addition we have 12 plus 2
is 14 10 plus 8 is 18 and so on
and just like we did the matrix addition
we can also do a minus b
and do our matrix subtraction
and we look at this we have what 12
minus 2 is 10 10 minus 8
where are we
oh there we go eight minus uh
confusing what i'm looking at i should
have reprinted out the original numbers
uh but we can see here 12 minus 2 is of
course 10 10 minus 8 is 2
4 minus 46 is minus 42 and so forth so
same as the subtraction as before we
just call it matrix subtraction it's
identical
now if you remember up here we had a
scalar addition we're adding just one
number to a matrix you can also do
scalar multiplication
and so simply if you have a single value
a and you have b which is your array we
can also do a times b
when we run that
you can see here we have 2 times 4 is 8
5 times 4 is 20 and so forth you're just
multiplying the 4 across each one of
these values
and this is an interesting one that
comes up
a little bit of a brain teaser is uh
matrix and vector multiplication
and so when we're looking at this
uh we are
just doing regular arrays it doesn't
necessarily have to be a numpy array
we have a which has our
array of arrays and b which is a single
array and so we can from here
do the dot
a b
and this is going to return two values
and the first value is that is you could
say it's like uh
we're doing the
this array b array
first with a and then with a second one
and so it splits it up so you have a
matrix of vector multiplication and you
can mix and match
when you get into really complicated uh
back-end stuff this becomes more common
because you're now you've got layers
upon layers of data and so you you'll
end up with a matrix and a set of bolt
vector matrices do you want to multiply
now keep in mind that if you're doing
data science a lot of times you're not
looking at this this is what's going on
behind the scenes so if you're in
the scikit looking at sk learn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on
other times you might find yourself
having to do part of this and manipulate
the data around so it fits right and
then you go back in and you run it
through the psi kit and if we can do
up here where we did a
matrix and vector multiplication we can
also do matrix two matrix multiplication
and if we run this we have the two
matrixes
you can see a very complicated array
that of course comes out on there for
our dot
and just to reiterate it we have our
transpose matrix which is your dot t
and so if we create a matrix a and we do
transpose it you can see how it flips it
from
5 10 15 20 25 30 to 5 15 25
10 20 30
rows and columns
and certainly with the math this comes
up a lot
it also comes up a lot with xy plotting
when you put in the pi plot you have one
format where they're looking at pairs
and numbers and then they want all of
x's and all y's so you know the
transpose is an important tool both for
your math and for plotting and all kinds
of things
another tool that we didn't discuss uh
is your identity matrix
uh and this one is more definition
uh the identity matrix um we have here
one where we just did two
so it comes down as one zero zero one uh
one zero zero zero one zero it creates a
diagonal of one and what that is is when
you're doing your identities you could
be comparing
all your different features to the
different features and how they
correlate
and of course when you have feature one
compared to feature one to itself it is
always
one
where
usually it's between zero one depending
on how well correlates so when we're
talking about identity matrix that's
what we're talking about right here is
that you create this preset matrix and
then you might adjust these numbers
depending on what you're working with
and what the domain is
and then another thing we can do
to kind of wrap this up we'll hit you
with the most complicated
piece of this
puzzle here is an inverse
a matrix
and let's just go ahead and put the um
it's a lengthy description
let's go and put the description this is
straight out of the
the website for
numpy
so given a square matrix a here's our
square matrix a which is 2 1 0 0 1 0 1 2
1. keep in mind 3 by 3 is square it's
got to be equal it's going to return the
matrix a inverse satisfying dot a
a inverse so here's our matrix
multiplication
and then of course it equals the dot
yeah a inverse of a
with an identity shape of
a dot shape 0. this is just reshaping
the identity
that's a little complicated there
so we're going to have our here's our
array
we'll go ahead and run this
and you can see what we end up with
is we end up with an array 0.5 minus 0.5
and so forth with our 2 1 1 going down 2
1 0 0 1 0 1 2 1.
getting into a little deep on the math
understanding
when you need this is probably really is
is what's really important when you're
doing data science
versus
handwriting this out and looking up the
math and handwriting all the pieces out
you do need to know about the linear
algorithm inverse of a
so if it comes up you can easily pull it
up or at least remember where to look it
up you took a look at the algebra side
of it let's go ahead and take a look at
the calculus side of what's going on
here with the machine learning so
calculus oh my goodness and differential
equations you got to throw that in there
because that's all part of the
bag of tricks especially when you're
doing large neural networks but also
comes up in many other areas the good
news is most of it's already done for
you in the back end
so when it comes up you really do need
to understand from the data science not
data analytics data analytics means
you're digging deep into
actually solving these math equations
and a neural network is just a giant
differential equation so we talk about
calculus
we're going to go ahead
and understand it by talking about cars
versus time and speed
so helps to calculate the spontaneous
rate of change
so suppose we plot a graph of the speed
of a car with respect to time
so as you can see here going down the
highway probably merged into the highway
from an on-ramp so i had to accelerate
so my speed went way up uh stuck in
traffic merged into the traffic traffic
opens up and i accelerate again up to
the speed limit
and uh maybe peter's off up there so you
can look at this is as
the speed versus time i'm getting faster
and faster because i'm continually
accelerating and if i hit the brakes you
go the other way
so the rate of change of speed with
respect of time is nothing but
acceleration how fast are we
accelerating
the acceleration is the area between the
star point of x and the endpoint of
delta x
so we can calculate a simple
if you had x and delta x we could put a
line there
and that slope of the line is our
acceleration
now that's pretty easy when you're doing
linear algebra but i don't want to know
it just for that line and those two
points i want to know what across the
whole of what i'm working with
that's where we get into calculus
so we talk about the distance between x
and delta x it has to be the smallest
possible near to zero in order to
approximate the acceleration
so the idea is instead of i mean if you
ever did took a basic calculus class
they would draw bars down here and you
would divide this area up
let's go back up the screen you divide
this area of this time period up into
maybe 10 sections and you'd use that and
you could calculate the acceleration
between each one of those 10 sections
kind of thing
and then we just keep making that space
smaller and smaller until delta x is
almost
infinitesimally small
and so we get a function of a
equals a limit as h goes to 0 of a
function of a plus h minus a function of
a over h and that is you're
computing the slope of the line
we're just computing that slope under
smaller and smaller and smaller samples
and that's what calculus is calculus is
the integral you can see down here we
have our nice uh integral sign looks
like a giant s
and that's what that means is that we've
taken this down to as small as we can
for that sampling
so we're talking about calculus we're
finding the area under the slope is the
main process in the integration
similar small intervals are made of the
smallest possible length of x plus delta
x where delta x approaches almost an
infinitesimally small space
and then it helps to find the overall
acceleration by summing up all the links
together
so we're summing up all the
accelerations from the beginning to the
end
and so here's our integral we sum of a
of x times d of x
equals a plus c
that is our basic calculus here
so when we talk about multivariate
calculus
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations
change in w over change of time
equals change of w over change of z
the differential of z to dx differential
of x to dt it gets pretty complicated uh
it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f of x of y of d of a equals
the sum from c to d and a to b of f of x
y d x d y equals
uh the sum of a to b sum of c to d of f
x y d y d x
understanding the very specifics of
everything going on in here and actually
doing the math is usually calculus 1
calculus 2 and differential equations
so you're talking about three
full-length courses to dig into
and solve these math equations
what we want to take from here is we're
talking about calculus uh we're talking
about summing of all these different
slopes and so we're still solving a
linear expression we're still solving
y equals m x plus b
but we're doing this for infinitesimally
small x's and then we want to sum them
up that's what this integral sign means
the sum of a of x d of x equals a plus c
and when you see these very complicated
multivariate differentiation using the
chain rule
when we come in here and we have the
change of w to the change of t equals
the change of w dz
and so forth that's what's going on here
that's what these means we're basically
looking for the area under the curve
which really comes to
how is the change changing speeds going
up how is that changing and then you end
up with a multiple layer so if i have
three layers of neural networks how is
the third layer changing based on the
second layer changing which is based on
the first layer changing and you get the
picture here that now we have a very
complicated
multivariate integration
with integrals
the good news is we can solve this
mathematically and that's what we do
when you do neural networks and reverse
propagation
so the nice thing is that you don't have
to solve this on paper unless you're a
data analysis and you're working on the
back end of integrating these formulas
and building the script to actually
build them so we talk about applications
of calculus it provides us the tools to
build an accurate predictive model
so it's really behind the scenes we want
to guess that what the change of the
change of the change is
that's a little goofy i know i just
threw that out there it's kind of a meta
term but if you can guess how things are
going to change then you can guess what
the new numbers are
multivariate calculus explains the
change in our target variable in
relation to the rate of change in the
input variables
so there's our multiple variables going
in there if one variable is changing how
does it affect the other variable
and then in gradient descent calculus is
used to find the local and global maxima
and this is really big uh we're actually
going to have a whole section here on
gradient descent because it is
really i mean i talked about neural
networks and how you can see how the
different layers go in there but
gradient descent is one of the most key
things for trying to guess the best
answer to something so let's take a look
at the code behind gradient descent
and before we open up the code let's
just do real quick
gradient descent
let's say we have a curve like this and
most common
is that this is going to represent your
error oops
error there we go error
ah hard to read there and i want to make
the error as low as possible
and so what i'm looking at it is i want
to find this line here
which is the
minimum value so we're looking for the
minimum
and it does that by
uh sampling there
and then it based on this it guesses it
might be someplace here and it goes hey
this is still going down
it goes here and then goes back over
here and then goes a little bit closer
and it's just playing a high low until
it gets to that spot that bottom spot
and so we want to minimize the error and
on the flip note you could also want to
be maximizing something you want to get
the best output of it
that's simply
minus the value
so if you're looking for where the peak
is
this is the same as a negative
for where the valley is looking for that
valley
that's all that is and this is a way of
finding it so
the cool thing is um all the heavy
lifting's done
i actually
ended up putting together one of these a
while back as when i didn't know about
sidekick and i was just starting
boys a long while back
and uh
is playing high low how do you play high
low not get stuck in the valleys uh
figure out these curves and things like
that well you do that and the back end
is all the calculus and differential
equations to calculate this out
the good news is you don't have to do
those
so instead we're going to put together
the code and let's go ahead
and see what we can do with that
so uh guys in the back put together a
nice little piece of code here which is
kind of fun
uh some things we're gonna note and this
is this is really important stuff
because when you start doing your data
science and digging into your machine
learning models
you're going to find
these things are stumbling blocks
the first one is current x where do we
start at
keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start
and we started at this because the
algorithm starts at x equals three
so we arbitrarily picked five learning
rate is uh how many bars to skip going
one way or the other i'm in fact i'm
going to separate that a little bit
because these two are really important
if we're dealing with something like
this where we're talking about
well here's our here's the function
we're going to use our
gradient of our function
2 times x plus 5 keep it simple so
that's a function we're going to work
with so if i'm dealing with increments
of a thousand point one is going to be a
very long time
and if i'm dealing with increments of
0.001
0.1 is going to skip over my answer so i
won't get a very good answer
and then we look at precision this tells
us when to stop the algorithm so again
very specific to what you're working on
uh if you're working with money and
you don't convert it into a float value
uh you might be dealing with 0.01 which
is a penny that might be your precision
you're working with
and then of course the previous step
size max iterations we want something to
cut out at a certain point usually
that's built into a lot of minimization
functions
and then here's our actual
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
it is less than max and max iters
say that 10 times fast
we're just saying if it's uh if we're if
we're still greater than our precision
level we still got to keep digging
deeper
um and then we also don't want to go
past a thou or whatever this is a
million or ten thousand uh running
that's actually pretty high i almost
never do max iterations more than like
100 or 200
rare occasions you might go up to four
or 500 if it's depending on the problem
you're working with uh so we have our
previous equals our current that way we
can track timewise
the current now equals the current minus
the rate times the formula of our
previous x
so now we've generated our new version
previous step size equals the absolute
current previous
so we're looking for the change in x
errors equals iterations plus one that's
so we know to stop if we get too far
and then we're just going to print the
local minimum occurs at
x on here and if we go ahead and run
this
you can see right here it gets down to
this point and it says hey
local minimum is minus
3.3222 for this particular series we
created
this is created off of our formula here
lambda x two times x plus five
now
when i'm running this stuff uh you'll
see this come up a lot
in uh with the sk learn kit
and one of the nice reasons of breaking
this down the way we did
is i could go over those top pieces uh
those top pieces are everything when you
start looking at these minimization tool
kits in built-in code and so from um
we'll just do it's actually
docs
dot
scipy.org
and we're looking at
the scikit
there we go
optimize minimize
you can only minimize one value
you have the function that's going in
this function can be very complicated
so we used a very simple function up
here
it could be
there's all kinds of things that could
be on there and there's a number of
methods to solve this as far as how they
shrink down
and your x naught there's your there's
your start value so your function your
start value
um
there's all kinds of things that come in
here that you can look at which we're
not going to
optimization automatically creates
constraints bounds
some of this it does automatically but
you really the big thing i want to point
out here is you need to have a starting
point you want to start with something
that you already know is mostly the
answer
if you don't then it's going to have a
heck of a time trying to calculate it
out
or you can write your own little script
that does this and does a high low
guessing and tries to find the max value
that brings us to
statistics what this is kind of all
about is figuring things out a lot of
vocabulary and statistics ah so
statistics well i guess it's all
relative it's definitely not an edel
class
so a bunch of stuff going on statistics
statistics concerns with the collection
organization analysis interpretation
and presentation of data
that is a mouthful
so we have from end to end
where does it come from is it valid what
does it mean how do we organize it um
how do we analyze it then you gotta take
those analysis and interpret it into
something that people can use kind of
reduce it to
understandable
and nowadays you have to be able to
present it if you can't present it then
no one else is going to understand what
the heck you did
so we look at the terminologies
there is a lot of terminologies
depending on what domain you're working
in so clearly if you're working in
a domain that deals with
viruses and t cells and and
how does you know where does that come
from you're studying the different
people then you can have a population
if you are working with
mechanical gear
you know a little bit different if
you're looking for the wobbling
statistics uh to know when to replace a
rotor on a machine or something like
that
that can be a big deal you know we have
these huge fans that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit
an interesting project that came up for
our city a while back
so population all objects are
measurements whose properties are being
observed
so that's your population all the
objects it's easy to see it with people
because we have our population in large
but in the case of the sewer fans we're
talking about the fan units that's the
population of fans that we're working
with
you have a parameter a matrix that is
used to represent a population or
characteristic
you have your sample a subset of the
population studied you don't want to do
them all because then you don't have a
if you come up with a conclusion for
everyone you don't have a way of testing
it so you take a sample
sometimes you don't have a choice you
can only take a sample of what's going
on you can't
study the whole population and a
variable a metric of interest for each
person or object in a population
types of sampling
we have a probabilistic approach
selecting samples from a larger
population using a method based on the
theory of probability
and we'll go into a little bit more
deeper on these we have random
systematic stratified
and then you have a non-probabilistic
approach selecting samples based on the
subjective judgment of the researcher
rather than random selection
it has to do with convenience trying to
reach a quota
or snowball
and they're very biased that's one of
the reasons you'll see this big stamp on
that says biased so you gotta be very
careful on that
so probabilistic sampling uh when we
talk about a random sampling we select
random size samples from each group or
category so we it's as random as you can
get we talk about systematic sampling
we're selecting random size samples from
each group or category with a fixed
periodic interval
uh so we kind of split it up this would
be like a time set up or our different
categories
and you might ask your question what is
a category or a group
if you look at i'm going to go back a
window let's say we're studying
economics of different of an area
we know pretty much that based on their
culture where they came from
they might need to be separated and so
uh and when i say separated i don't mean
separated from their
place where they live i mean as far as
the analysis we want to look at the
different groups and make sure they're
all represented
so if we had like an eighty percent uh
of a group that is uh say hispanic and
or indian and also in that same area we
have twenty percent twenty percent who
are uh let's call our expatriates they
left america and they're nice and
your caucasian group we might want to
sample a group that is representative of
both uh so we're talking about
stratified sampling and we're talking
about groups those are the groups we're
talking about and that brings us to
stratified sampling selecting
approximately equal size samples from
each group or category
this way we can actually separate the
categories and give us an insight into
the different cultures and how that
might affect them in that area
so you can see these are very very
different kind of
depends on what you're working with
as far as your data and what you're
studying
and so we can see here just a little bit
more we have selecting 25 employees from
a company of 250 employees randomly
don't care anything about them what
groups are in which office they're in
nothing
and we might be selecting one employee
from every 50 unique employees in a
company of 250 employees
and then we have selecting one employee
from every branch in the company office
so we have all the different branches
there's our group or categories by the
branch and the category could depend on
what you're studying so it has a lot of
variation on there
you see this kind of grouping and
categorizing is also used to generate a
lot of misinformation
so if you only study one group and you
say this is what it is
then everybody assumes that's what it is
for everybody and so you've got to be
very careful of that it's very unethical
thing to kind of do
so types of statistics we talk about
statistics
we're going to talk about descriptive
and inferential statistics
there are so many different terms and
statistics to break it up uh so we so
we're talking about a particular
setup
so we're talking about descriptive and
inferential
statistics
the base of the word describe
is pretty solid you're describing the
data what does it look like with
inferential statistics we're going to
take that from the small population to a
large population so if you're working
with a drug company you might look at
the data and say these people were
helped by this drug
they did
80 percent better
as far as their health or 80 percent
better survival rate than the people
who did not have the drug so we can
infer that that drug will work in the
greater populace and will help people so
that's where you get your inferential so
we are
predicting how it's going to affect the
greater population
so descriptive statistics it is used to
describe the basic features of data and
form the basis of quantitative analysis
of data
so we have a measure of central
tendencies we have your mean median and
mode
and then we have a measure of spread
like your range your interquartile range
your variance and your standard
deviation
and we're going to look at all these a
little deeper here in a second
but one of them you can think of is
how the data difference
differences you know what's the max min
range all that stuff is your spread and
anything that's just a single number is
usually your central
tendencies measure of central tendencies
so we talk about the mean it is the
average of the set of values considered
what is the average outcome of
whatever's going on
and then your median
separates the higher half and the lower
half of data
so where's the center point of all your
different data points
so your mean might have some a couple
really big numbers that skew it
so that the average is much higher than
if you took those outliers out where the
median would by separating the high from
the low might give you a much lower
number you might look at it and say oh
that's that's odd why is the average so
much higher than the median well it's
because you have some outliers or why is
it so much lower
and then the mode is the most frequent
appearing value this is really
interesting if you're studying economics
and how people are doing you might find
that the most common
income like in the u.s was
1.24 000 a year
where the average was closer to 80 000
and it's like wow what a difference well
there's some people have a lot of money
and so that skews that way up so the
average person is not making that kind
of money and then you look at the median
income and you're like well the median
income is a little bit closer to the
average uh so it does create a very
interesting way of looking at the data
again these are all uh central
tendencies single numbers you can look
at for the whole spread of the data
and we look at the measure of central
tendencies the mean is the average marks
of a students in a classroom so here we
have the mean sum of the marks of the
students total number of students and as
we talked about the median
we have
0 through 10
and we take half the numbers and put
them on one side of the line half the
numbers on the other side of the line
we end up with five in the middle and
then the mode what mark was scored by
most of the students in a test
in a simple case where most people
scored like an 82 percent and got
certain problems wrong easy to figure
out
not so easy when you have different
areas where like you have like the um oh
let's go back to economy a little bit
more difficult to calculate if you have
a large group that scores that makes 30
000
and a slightly bigger group that makes
26 000. so what do you put down for the
mode
uh certainly there's a number of ways to
calculate that and there's actually
different variations depending on what
you're doing
so now we're looking at a measure of
spread uh range what's the difference
between the highest and the lowest value
first thing you want to look at you know
it's uh we had everybody in the test
score between 60 and 100 percent so we
got 100 or maybe 60 to 90 it was so hard
that a lot of people could not get a
hundred percent
um you have your
inter-quartile range quartiles divide a
rank ordered data set into four equal
parts
very common thing to do as part of all
the basic packages whether you're
working in
data frames with pandas whether you're
working in scala whether you're working
in r
you'll see this come up where they have
range your min your max and then it'll
have your interquartile range how does
it look like in each quarter of data
variance measures how far each number in
the set is from the mean and therefore
from every other number in the set
so you have like how much turbulence is
going on in this data
and then the standard deviation it is to
measure the variance or the dispersion
of a set of values from the mean
and you'll usually see if i'm doing a
graph i might have the value graphed
and then based on the the error i might
gra graph the standard deviation in the
error on the graph as a background so
you can see how far off it is
so standard deviation is used a lot
so measurement of spread uh marks of a
student out of a hundred we have here
from 50 to 63 or 50 to 90.
so the range maximum marks minimum marks
we have 90 to 45 and the spread of that
is 45 90 minus 45. and then we have the
interquartile range using the same marks
over there you can see here where the
median is
and then there's the first quarter the
second quarter and the third quarter
based on splitting it apart by those
values
and to understand the variance and
standard deviation we first need to find
out the mean
so here's our you know calculating the
average there we end up approximately 66
for the average and then we look at that
the variance once we know the means we
can do equals the marks minus the mean
squared
why is it squared
because one you want to make sure it's
you don't have like if you if you're
putting all this stuff together you end
up with an error as far as one's
negative one's positive one's a little
higher one's a little lower
so you always see
the squared value and over the total
observations
and so the standard deviation equals the
square root of the variance which is
approximately 16.
and if you were looking at
a predictable model you would be looking
at the deviation based on the error how
much error does it have
that's again
really important to know if your if your
prediction is predicting something
what's the chance of it being way off or
just a little bit off
now that we've looked at the
tools as far as some of the basics for
doing your statistics and we're talking
about
let's go ahead and pull up a little demo
and show you what that looks like in
python code so you can get some little
hands on here for that let's go back
into our jupiter notebook in python now
almost all of this you can do in numpy
last time we worked in numpy this time
we're going to go ahead and use pandas
and if you remember from
pandas on here
this is basically a data frame rows
columns let's just go ahead and do a
print
df.head
and run that
and you can see we have the name jane
michael william rosie hannah sat on
their salaries on here and of course
instead of having to do all those hand
calculations and add everything together
and divide by the total
we can do something very simple on this
like use the command mean in pandas and
so if i go ahead and do this print df
pick our column salary because we want
to find the means of that calorie
we want to find the means of that column
and we go and print this out and you can
see that the
average income on here is 71 000.
and let's just go ahead and do this
we'll go ahead and put in
means
and if we're going to do that we also
might want to find the median
and the median is
very similar
except it actually is just median we're
used to means and average is kind of
interesting that those are they use the
two different words
uh there can be in some computation
slight differences but for the most part
the means is the average uh and then the
median
oops let's put a
median here
do you have salary that way it displays
a little better we can see the median is
54
000. so the halfway mark is
significantly below the average
why because we have somebody in here
makes 189 000. darn you rosie for
throwing off our numbers
but that's something you'd want to
notice this is this is the difference
between these is huge and so is what is
the meaning behind that when you're
studying a populist and looking at
the different data coming in and of
course we also want to find out hey
what's the most uh common
income that people make
in this little tiny sample and so we'll
go ahead and do the mode
and you can see here with the mode uh
it's at 50 000.
so this is this is very telling that
most people are making 50 000.
the middle point is at 54 000. so half
the people are making more than that
what that tells me is that if the most
common income is below the median
then
there's a few there's a scale there's a
lot of high salaries going up but
there's some really low salaries in
there
and so this trend which is very common
in statistic when you're analyzing
the economy and different people's
income is pretty common and the bigger
difference between these is also
very important when we're studying
statistics
and when you hear someone just say hey
the average income was you might start
asking questions at that point why
aren't you talking about the median
income why aren't you talking about the
mode the most common income what are you
hiding
uh and if you're doing these analysis
you should be looking at these saying
hey why are these discrepancies why are
these so different and of course with
any analysis it's important to find out
the minimum
and the maximum so we'll go ahead it's
just simply
dot min will pull up your minimum and
then dot max pulls up the maximum
pretty straightforward on as far as um
translating it and knowing which you
know what the
lowest value what your highest value is
here
which you'll use to generate like a
spread later on and real quick on no
mode uh note that it puts mode zero like
i said there's a couple different ways
you can compute the mode
um although the standard one's pretty
good we can of course do the range
which is your max minus your min so now
we have a range of 149 000 between the
upper end and the lower end and you
might want to be looking up the
individual values on all of these but
turns out there is a describe
feature in pandas
and so in pandas we can actually do df
salary describe and if we do this you
can see we have that there's seven uh
setups here's our mean
our standard deviation which we didn't
compute yet which would just be a dot
std
and you gotta be a little careful
because when it computes it it looks for
axes and things like that
we have our minimum value and here's our
quartiles
our maximum value and then of course the
name salary
so these are the these are the basic
statistics you can pull them up and like
just describe
this is a dictionary so i could actually
do something like um
in here i could actually go uh count
and run
and now it just prints the count
so because this is a dictionary you can
pull any one of these values out of here
it's kind of a quick and dirty way to
pull all the different information
and then split it up and depending on
what you need
now if i just walked in and gave you
this information
in a meeting
at some point you would just kind of
fall asleep
that's what i would do anyway um
so we want to go ahead and see about
graphing it here and we'll go ahead and
put it into a histogram and plot that
graph on it
of the salaries and let's just go ahead
and put that in here so
we do our map plot inline remember
that's a jupiter's notebook thing
a lot of the new version of the matplot
library does it automatically
but just in case i always put it in
there import map plot library pi plot is
plt that's my plotting
and then we have our data frame i don't
i guess i really don't need to respell
the data frame
maybe we could just remind ourselves
what's in it so we'll go ahead and just
print
df
that way we still have it
and then we have our salary df salary
salary.plot history title salary
distribution color gray
plot ax v line salary the mean value so
we're going to take the mean value
color violet line style dash this is
just all making it pretty
uh what color dashed line line width of
two that kind of thing
and the median and let's go ahead and
run this just so you can see what we're
talking about
and so up here we are taking on our plot
um so here's the data here's our our
data frame print it out so you can see
it with the salaries we'll look at the
salary distribution and just look at
this the way the is distributed um
you have our
in this case we did
let's see we had red for the median
we have violet
for our average or mean
and you can just see how it really
i mean here's our outlier here's our
person who makes a lot of money here's
the
average and here's the median
and so as you look at this you can say
wow
based on the average it really doesn't
tell you much about what people are
really taking home all it does is tell
you how much money is in this you know
what the average salary is
so
some of the things you want to take away
in addition to this is that it's very
easy to plot
an axv line
these are these up and down lines for
your markers
and as you just display the data i mean
you can add all kinds of things to this
and get really complicated keeping it
simple is pretty straightforward i look
at this and i can see we have a major
outlier out here we can definitely do a
histogram and stuff like that
but you know pictures worth a thousand
words
what you really want to make sure you
take away is that we can do a basic
describe
which pulls all this information out and
we can print any of the individual
information from the describe
because this is a dictionary
and so if we want to go ahead and look
up
the mean value we can also do describe
mean so if you're doing a lot of
statistics uh being able to
it doesn't have the print on there so
it's only going to print the last one
which happens to be the mean uh you can
very easily reference any one of these
and then you can also if you're doing
something a little bit more complicated
and you don't need just the basics you
can come through and pull any one of the
individual
references from the from the pandas on
here so now we've had a chance to
describe our data
let's get into inferential statistics
inferential statistics allows you to
make predictions or inferences from data
and you can see here we have a nice
little picture movie ratings and
um
if we took this group of people and said
hey how many people like the movie
dislike it can't say and then you ask
just a random person who comes out of
the movie who hasn't been in this study
you can infer that 55 chance of saying
liked
35 chance of saying disliked or a 10 or
11 percent chance of can't say
so that's real basics of what we're
talking about you're going to infer that
the next person is going to follow these
statistics
uh so let's look at point estimation uh
it is a process of finding an
approximate value for a population's
parameter like mean
or average from random samples of the
population let's take an example of
testing vaccines for covid19
vaccines and flu bugs all that it's a
pretty big thing of how do you test
these out and make sure they're going to
work on the populace
a group of people are chosen from the
population medical trials are performed
results are generalized for the whole
population so here's a protected there's
our small group up here where we've
selected them we run medical trials on
them and then the results work for the
population
nice diagram with the arrows going back
and forth and the very scary coveted
virus in the middle of one
and let's take a look at the
applications of inferential statistics
very central is what they call
hypotheses testing
and the confidence interval which go
with that and then as we get into
probability we get into our binomial
theorem our normal distribution in
central limit theorem hypothesis testing
hypothesis testing is used to measure
the plausibility of a hypothesis
assumption by using sample data
now
when we talk about theorems
theory hypothesis uh keep in mind that
if you are in a philosophy class
theory
is the same as hypothesis where theorem
is a scientific uh statement that is
something that has been proven
although it is always up for debate
because in science we always want to
make sure things are up to debate so
hypothesis is the same as a
philosophical class calling a theory
where theory in science is not the same
theory in science says this has been
well proven gravity is a theory
so if you want to debate the theory of
gravity try jumping up and down if you
want to have a theory about why the
economy is collapsing in your area
that is a philosophical debate
very important i've heard people mix
those up and it is a pet peeve of mine
when we talk about hypotheses testing
the steps involved in hypotheses testing
is first we formulate a hypothesis
we figure out the right test to test our
hypothesis we execute the test and we
make a decision and so when you're
talking about hypothesis you're usually
trying to disprove it if you can't
disprove it
and it works for all the facts then you
might call that a theorem at some point
so in a use case let's consider an
example we have four students we're
given a task to clean a room every day
sounds like working with my kids they
decided to distribute the job of
cleaning the room among themselves they
did so by making four chits which has
their names on it and the name that gets
picked up has to do the cleaning for
that day
rob took the opportunity to make chits
and wrote everyone's name on it so
here's our four people nick rob imlia
imlia and summer
now rick emilia and summer are asking us
to decide whether rob has done some
mischief in preparing the chits i.e
whether rob has written his name on one
of the chit
for that we will find out the
probability of rob getting the cleaning
job on first day second day third day
and so on till 12 days
the probability of rob getting the job
decreases every day i.e his turn never
comes up then definitely he has done
some mischief while making the chits
so the probability of rob not doing work
on day one is a three out of four
there's a 0.75 chance that he didn't do
work
2 days 3 4 times 3 4 equals 0.56
three days you have three-fourths
three-fourths three-fourths which equals
0.42
when you get to day 12 it's 0.032 which
is less than 0.05
remember this 0.05 that comes up a lot
when we're talking about
certain values when we're looking at
statistics
rob is cheating as he wasn't chosen for
12 consecutive days that's a very high
probability when
on day 12 he still hasn't gotten the job
cleaning the room
so we come up to our important important
terminologies
we have null hypothesis
a general statement that states that
there is no relationship between two
measured phenomenon or no association
among the groups
alternative hypothesis
contrary to the null hypothesis it
states whenever something is happening a
new theory is preferred instead of an
old one and so the two hypotheses go
hand in hand uh so your null this is
always interesting in in talking about
data science and the math behind it
it's about proving that the things have
no correlation null hypothesis says
these two have zero relation to each
other where the alternative hypothesis
says hey we found a relation this is
what it is
we have p-value the p-value is a
probability of finding the observed or
more extreme results when the null
hypothesis of a study question is true
and the t value it is simply the
calculated difference represented in
units of standard error the greater the
magnitude of t the greater the evidence
against the null hypothesis and you can
look at the t values being specific to
the test you're doing
where the p value is derived from your t
value and you're looking for what they
call the five percent or the 0.05
showing that it has a high correlation
so digging in deeper let's assume that a
new drug is developed with the goal of
lowering the blood pressure more than
the existing drug
and this is a good one because the null
value here isn't that you don't have any
drug the null value here is that it's
better than the existing drug the new
drug doesn't lower the blood pressure
more than the existing drug
now if we get that
that says our null hypothesis is correct
there is no correlation and the new drug
is not doing its job the alternative
hypothesis the new drug does
significantly lower the blood pressure
more than the existing drug
uh yay we got a new drug out there and
that's our alternative hypothesis or the
h1 or ha
and we look at the p value results from
the evidence like medical trial showing
positive results which will reject the
null hypothesis
and again they're looking for
a 0.05 or 5 percent and the t value
comparing all the positive test results
and finding means of different samples
in order to test hypothesis so this is
specific to the test how what percentage
of increase did they have
and this leads us to the confidence
intervals
a confidence interval is a range of
values we are sure our true values of
observations lie in
let's say you asked a dog owner around
you and asked them how many cans of food
do you buy for your per year for your
dog through calculations you got to know
that the on an average around 95 percent
of the people bought around 200 to 300
cans of food hence we can say that we
have a confidence interval of 2 300
where 95 percent of our values lie in
that sprint data spread
uh and this the graph really helps a lot
so you can start seeing what you're
looking at here we have the 95 percent
you have your peak in this case it's a
normal distribution so you have a nice
bell curve equal on both sides it's not
asymmetrical and 95 percent of all the
values lie within a very small range and
then you have your outliers the 2.5
percent going each way
so we touched upon hypothesis uh we're
going to move into probability so you
have your hypothesis once you've
generated your hypothesis we want to
know the probability of something
occurring probability is a measure of
the likelihood of an event to occur any
event can be predicted with total
certainty and can only be predicted as a
likelihood of its occurrence so any
event cannot be predicted with total
certainty can only be predicted as a
likelihood of its occurrence
uh score prediction how good you're
going to do in whatever
sport you're in weather prediction stock
prediction
if you've studied physics and chaos
theory even the location of the chair
you're sitting on has a probability that
it might move three feet over
granted that probability is one in like
uh i think we calculated as under one in
trillions upon trillions so it's
the better the probability the more
likely it's going to happen there are
some things that have such a low
probability
that we don't see them
so we talk about random variable a
random variable is a variable whose
possible values are numerical outcomes
of a random phenomena so uh we have the
coin tossed how many heads will occur in
the series of 20 coin flips probably you
know the on average they're 10 but you
really can't know because it's very
random how many times a red ball is
picked from a bag of balls if there's
equal number of red balls and blue balls
and green balls in there how many times
the sum of digits on two dice results
are five each
so you know there's how often you're
going to roll two fives on your pair of
dives
so in a use case uh let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values 2 3 4 5 6 7 8 9 10 11 12.
so we have a random variable and a
combination of dice
and instead of looking at how many times
both dice for roll 5 let's go ahead and
look at total sum of five and you have
in as far as your random variables you
can have a one four equals five four one
two three three two
so four of those roles can be four if
you look at all the different options
you have four of those random rolls can
be a five
and if we look at the total number
which happens to be 36 different options
uh you can see that we have four out of
36 chance every time you roll the dice
that you're gonna roll a total of five
you're gonna have an outcome of five
and uh we'll look a little deeper as to
what that means but you could think of
that at what point if someone never
rolls a five or they always roll a five
can you say hey that person's probably
cheating
we'll look a little closer at the math
behind that but let's just consider this
is one of the cases is rolling two dice
and gambling
there's also binomial distribution is
the probability of getting success or
failure as an outcome in an experiment
or trial that is repeated multiple times
and the key is is by meaning two
binomial so passing or failing an exam
winning or losing a game and getting
either head or tails so if you ever see
binomial distribution it's based on a
true false kind of setup you win or lose
let's consider a use case and let's
consider the game of football between
two clubs
barcelona and dortmund the teams will
have to play a total of four matches and
we have to find out the chances of
barcelona winning the series so we look
at the total games and we're looking at
five different games or matches
let's say that the winning chance for
barcelona is 75 or 0.75
that means that each game they have a 75
chance that they're going to win that
game and losing chances are 25 or 0.25
clearly 0.75 plus 0.25 equals 1. so that
accounts for 100 of the game probability
for getting k wins in in matches is
calculated
and we we're talking like so if you have
five games and you want to know if i
play
how many wins in those five games should
i get what's a percentage on those and
the probability for getting k wins and
in matches is calculated by p x equals k
equals n c k p to the k q to the n minus
k
here p is the probability of success and
q is the probability of failure and so
we can do total games of n equals five
where k equals zero one two three four
five
p which is the chance of winning is
point seven five q the chance of losing
equals one minus p which equals one
minus point o seven five which equals
point two five the probability that
barcelona will lose all of the matches
can then just plug in the numbers
and we end up with a .0009765625
so very small chance they're going to
lose all their matches
and we can plug in uh the value for two
matches probability that barcelona will
win at least two matches is
0.0878 and of course we can go on to the
probability that barcelona will win
three matches the 0.26 and course four
matches and so on and it's always nice
to take this information
and let's find the accumulated discrete
probabilities for each of the outcomes
where barcelona has won three or more
matches x equals three x equals 4 x
equals 5.
and we end up with the p equals 0.264
plus 0.395 plus 237 which equals 0.89
in reality
the probability of barcelona winning the
series is much higher than 0.75
and it's always nice to
put out a nice graph so you can actually
see the number of wins to the
probability and how that pans out
with our binomial case
continuing in our important terminology
location the location of the center of
the graph depends on the mean value and
this is some very important things so
much of the data we look at and when you
start looking at probabilities almost
always has a normalized look like the
graph in the middle
but you do have left skewed where the
data is skewed off to the left and you
have more stuff happening off to the
left and you have right skewed data
and so when this comes up and these
probabilities come up where they're
skewed it's really important to take a
closer look at that
mostly you end up with a normalized set
of data but you got to also be aware
that sometimes it's a skewed data
and then the height height of the slope
inversely depends upon the standard
deviation
so you can see down here the standard
deviation is really large it kind of
squishes it out and if the standard
deviation is small then most of your
data is going to hit right there in the
middle you can have a nice peak
and so being aware of this that you
might have a probability that fits
certain data but it has a lot of
outliers so you're if you have a really
high standard deviation
if you're doing stock market analysis
this means your predictions are probably
not going to make you much money
where if you have a very small deviation
you might be right on target and set to
become a millionaire
which leads us to the z-score z-score
tells you how far from the mean a data
point is it is measured in terms of
standard deviations from the mean around
68 percent of the results are found
between one standard deviation
around 95 percent of the results are
found between two standard deviations
and you read the symbols of course they
love to throw some greek letters in
there we have a mu minus two sigma
mu is just a quick way it's a kind of
funky u it just means the mean
uh and then the sigma is the standard
deviation and that's the o with the
little arrow off to the right or the
little
wagy tail going up the o with it with
the line on it
so mu minus two sigma
is your
uh 95 percent of the results are found
between two standard deviations
central limit theorem
this goes back to the skew if you
remember we were looking at the skew
values on this previous slide have left
skewed normalized and right skewed when
we're talking about it being skewed or
not skewed the distribution of the
sample means will be approximately
normally distributed evenly distributed
not skewed
if you take large random samples from
the population with the mean mu and the
standard deviation sigma with
replacement
and you can see here
of course we have our
mu minus 2 sigma and the spread down
here the mean the median and the mode
and so we're talking about very large
populations
these numbers should come together and
you shouldn't have a skewed value if you
do that's a flag that something's wrong
that's why this is so important to be
aware of what's going on with your data
where your samples are coming from and
the math behind it
and if we're going to do all this we got
to jump into conditional probability
the conditional probability of an event
a is the probability that the event will
occur
given the knowledge that an event to be
has already occurred
and you'll see this as bayes theorem
b-a-y-e-s base
uh and this is red
i mean you have these funky looking
little
p
brackets a b
this is the probability of a being true
while b is already true
and you have the probability of b being
true when a is already true so p
b of a
probability of a being true divided by
the probability of b being true
and we talk about bayes theorem which
occurred back in the 1800s when he
discovered this this is such an
important formula and it's really it's
not if you actually do the math you
could just kind of do
x y equals j k and then you divide them
out and you're going to see the same
math but it works with probabilities
which makes it really nice
and so if you have a set you might have
eight or nine different studies going on
in different areas different people have
done the studies they brought them
together
if we look at today's covered virus the
virus spread
certainly the studies done in china
versus the studies the way they're done
in the u.s
that data is different in each of those
studies but if you can find a place
where it overlaps
where they're studying the same thing
together you can then compute the
changes that you need to make in one
study to make them equal and this is
also true if you have a study of
one group and you want to find out more
about it so this formula is very
powerful and it really has to do with
the data collection part of the math and
data science and understanding where
your data is coming from and how you're
going to combine different studies in
different groups
and we're going to go into a use case
let's find out the chance of a person
getting lung disease due to smoking
this is kind of interesting the way they
word this
let's say that according to medical
report provided by the hospital states
that around 10 percent of all patients
they treated suffered lung disease
so we have kind of a generic medical
report
they further found out
by a survey that 15 percent of the
patients that visit them smoke
so we have 10 percent that are lung
disease and
15 of the patients smoke
and finally five percent of the people
continued smoke even when they had lung
disease uh not the brightest choice um
but you know it is an addiction so it
can be really difficult to kick and so
we can look at the probability of a
prior probability of 10 people having
lung disease
and then probability b probability that
a patient smokes is 15 percent
uh and the probability of b
if b then a the probability of a patient
smokes even though they have lung
disease is five percent
and probability of a is b probability
that the patient will have lung disease
if they smoke and then when you put the
formulas together you get a nice
solution here you get the probability of
a of b probability that the patient will
have lung disease if they smoke
and you can just plug the numbers right
in and we get a 3.33 percent chance
hence there is a 3.33 chance that a
person who smokes will get a lung
disease
so we're going to pull up a little
python code i'm always my favorite roll
up the sleeves
keep in mind we're going to be doing
this
kind of like the back end way
so that you can see what's going on and
then later on we're going to create
we'll get into another demo which shows
you some of the tools are already
pre-built for this
let's start by creating a set so we're
going to create a set with curly braces
this means that our set has
only unique values so you have a list
you have your tuples which can never
change and then you have um in this case
the the set so four seven you can't
create a four seven comma four it'll
delete the four outs it's only unique
values
and if you use dictionaries
quick reminder this should look familiar
because it is a dictionary we have a
value and that value is assigned to or
that key is assigned to a value
so you could have a key value set up as
a dictionary so it's like a dictionary
without the value it's just the keys and
they all have to be unique
and if we run this we have a
set of four seven
we can also take a list a regular
setup and i'm going to go ahead and just
throw in another number in here four
and run it
and you can see here if i take my list
one two three four four
and i convert it to a set and here it is
my set from list equals set my list
the result is one two three four so it
just deletes that last four right out of
there
and with the sets you can also go in
there and
print here is my set my set
three is in the set and then if you do
three in my set
that's going to be a logic function
uh and one in my set six is not in the
set and so forth if we run this
we get three is in the set true one is
in the set false because three five
seven is another one six is in the set
six is not in the set so not in my set
you can also use this with the list we
could have just used three five seven
and it would have
the same response on there is three and
i usually do if three is in but three in
my set is still works on just a regular
list
and we'll go ahead and do a little
iteration we're going to do kind of the
dice one remember
one two three four five six and so we're
going to bring in the iteration tool and
import product as product
and i'll show you what that means in
just a second so we have our two dice we
have dice a
and it's going to be a set of values
you can only have one value for each one
that's why they put it in a set and if
you remember from range it is up to
seven so this is going to be one two
three four five six it will not include
the seven and the same thing for our
dice b
and then we're gonna do is we're gonna
create a list
which is the product
of a and b so what's uh a plus b
and if we go ahead and run this it'll
print that out and you'll see
in this case when they say product
because it's an iteration
tool we're talking about creating a
tuple of the two so we've now created a
tuple of all possible outcomes of the
dice where dice a is
one two three one to six and dice b is
one to six and you can see one to one
one to two one to 3 and so forth
you remember we had a slide on this
earlier where we talked about
the different all the different outcomes
of a dice
we can play around with this a little
bit we can do n dice equals two
dice faces one two three four five six
uh another way of doing we did before
and then we can create an event space
where we have a set which is the product
of the dice faces
repeat equals indice and we'll go and
just run this
and you can see here it just again puts
it through all the different possible
variables we can have
and then if we wanted to take the same
set on here and print them all out like
we had before
we can just go through for outcome and
event space outcome and equals
so the event space is creating
a sequence and as you can see here when
we print it out it stacks some versus
going through and putting them in a nice
line
and we'll go ahead and do something
let's go print
since we have the end printing with a
comma that just means it's just gonna
it's not gonna hit the return going down
to the next line
and we'll go ahead and do the length
of our event space that'll be an
important variable we're going to want
to know in a minute
and of course if i get carried away with
my typing of length we'll print it twice
and it'll give me an error
so we have 36 different possible
variations here
and we might want to calculate something
like
what about the multiple of 3 what if we
want to have
the probability of the multiple of 3 in
our setup
and so we can put together the code for
the outcome and event space of x y
equals outcome
if x plus y
remainder 3 so we're going to divide by
3 and look at the remainder and it
equals 0
then it's a favorable outcome we're
going to pop that outcome on the in
there
and we'll turn it into a set so the
favor outcome equals a set
not necessary uh because we know it's
not going to be repeating itself but
just in case we'll go ahead and do that
and if we want to print out the outcome
we can go ahead and see what that looks
like and you can see here these are all
multiples of three one plus two is three
five plus four is nine which divided by
three is three and so forth
just like we looked up the length of the
one before let's go ahead and print
the length
of our
f outcome
so we can see what that looks like
there we go
and of course i did forget to add the
print in the middle because we're
looping through and putting an end on
the on the setup on there so we're going
to put the print in there and if i run
this you can see
we end up with 12. so we have 36 total
options
we have 12 that are multiple that add up
to a multiple of
3. and we can easily conv compute the
probability of this
by simply taking the length
of our favorable outcome over the length
of the event space
and if we print it out let me put that
in there
probability
last line so we just type it in we end
up with a 0.3333 chance
that's roughly a third
and we want to make this look nice so
let's go ahead and put in another line
there the probability of getting the sum
which is a multiple of three is
point three three three three
we can compute the same thing for five
dice
and if we do this for five dice and go
ahead and run it yeah you can see we
just have a huge amount of choices
so it just goes on and on down here and
we can look at
the length of the event space
and we have over 7
76
choices that's a lot of choices
and if we want to ask the question like
we did above uh
what is the sum where the sum is a
multiple of five but not a multiple of
three
we can go through all of these different
options and then
you can see here
d1 d2 d3 d4 d5 equals the outcome
and if you add these all together and
the
division by 5 does not have a remainder
of 0
but the remainder is also of a division
by three is not equal to zero
so the multiple of five is equal to zero
but the multiple three is not we can
just append that on here and then we can
look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length
of our favorable outcome
it's always good to see what we're
working with and so we have 904 out of
776
and then of course we can just do a
simple division to get the probability
on here what's the probability that
we're going to roll
a multiple of 5 when you add them
together
but not a multiple of three
and so we're just going to divide those
two numbers and you can see here we get
0.11625 or 11.62 percent
and so you can really have a nice visual
that this is not really complicated math
right here on probabilities
it's just how many options do you have
and how many of those are you possibly
going to be able to come up with with
the solution you're looking for
and this leads us to a confusion matrix
a confusion matrix is a table which is
used to describe the performance of a
classification model on a set of test
data for which the true values are known
and so you'll see in the left we have
the predicted and the actual
and we have a negative uh false negative
positive true positive
and then we have false positive and true
negative
and you can think of this as your
predicted model what does that mean that
means if you divided your data and you
used two-third of this to create the
model
you might then test it against an actual
case for the last third to see how well
it comes out how many times was it
true positive versus a
false positive it gave a false positive
response
and you can imagine in medical
situations this is a pretty big deal you
don't want to give a false positive so
you might adjust your model accordingly
so you don't have a false positive say
with a covet virus test it'd be better
to have a false negative and they go
back and get retested than to have 30
percent false positives where then the
test is pretty much invalid
so in a use case like cancer prediction
let's consider an example where a cancer
prediction model is put to the test for
its accuracy and precision
actual result of a person's medical
report is compared with the prediction
made by the machine learning model
and so you can see here here's our
actual predicted whether they have
cancer or not you know cancer a big one
you don't want to have a
false positive i mean a false negative
in other words you don't want to have it
tell you that you don't have cancer when
you do so that would be something you'd
really be looking for in this particular
domain you don't want a false negative
and this is again you know you've
created a model you have hundreds of
people or thousands of pieces of data
that come in
there's a real famous case study where
they have the imagery and all the
measurements they take and there's about
36 different measurements they take
and then if you run the
a basic model you want to know just how
accurate it is how many negative results
do you have that are either telling
people they have cancer that don't or
telling people that don't have cancer
that they do and then we can take these
numbers and we can feed them into our
accuracy our precision and our recall
so accuracy precision and recall
accuracy metric to measure how
accurately the results are predicted
and this is your
total
true where you got the right results you
add them together the true positive the
true negative
over all the results so what percentage
of them were accurate versus what were
wrong
we talked about precision is a metric to
measure how many of the correctly
predicted cases are actually turned out
to be positive uh so we have a precision
on
true positive
again if you're talking about like covid
testing with the viruses
you really want this to be a high number
you want this true
that to be the center point where you
might have the opposite if you're
dealing with a cancer where you want no
false negatives
so this is your metric on here precision
is your test positive
true positive plus
false positive
and then your recall how many of the
actual positive cases we were able to
predict quickly with our model
uh so test positive is the test positive
plus the
false negative on there and we'll want
to go ahead and do a demo on the naive
bayes classifier before i get too far
into uh naive bayes classifier because
we're going to pull it from the sk learn
or the scikit um let's go ahead kind of
an interesting page here for classifiers
when you go into the sk learn kit
there's a lot of ways to do
classification and i'll just zoom up in
here so you can see some of the titles
there's everything from the nearest
neighbor linear
but we're going to be focusing on the
naive bayes over here
and this is just a sample data set that
they put together and you can see how
some of these have a very different
output the naive bayes remember is set
up as probably the most simplified
calculator or set of predictions out
there
and so what we've been talking about
with the true false and stuff like that
where there's
a then a belief that there is a
independent assumption between the
features where the features are very
assumed to have some kind of connection
uh then we can go ahead and use that for
the prediction and so that's what we're
using is a naive bayes classifier versus
many of the other classifiers that are
out there
for this we're going to use the social
network ads it's a little data set on
here
and let me go and just open that up the
file
here we go it has user id gender age
estimated salary uh purchased
and so we have you can see the user id
mail 19
estimated salary 19 000 and purchased
zero so it's either going to make a
purchase or not
so look at that last one zero one we
should be thinking of binomials we
should be thinking of a simple naive
bayes classifier kind of set up
so if we close this out we're going to
go ahead and import our numpy as np
we're going to nice to have a good
visual of our data so we'll put in our
matplot library
here's our pandas our data frame
uh and then we're going to go ahead and
import the data set and the data set's
going to be we're going to read it from
the social network ads.csv then we're
going to print the head just so you can
see it again
even though i showed you it in the file
and x equals the data set i location
uh two three values and y is going to be
the four uh column four let me just run
this it's a little easier to go over
that
you can see right here we're going to be
looking at
0 1 2 as age
and estimated salary so 2 3
and that's what i location just means
that we're
looking at the number versus a regular
location a regular location you'd
actually say age and estimated salary
and then column four is did they make a
purchase they purchased something
so those are the three columns we're
going to be looking at when we do this
and we've gone ahead and imported these
and
imported the data so now our data set is
all set with this information in it
and we'll need to go ahead and split the
data up so we need our from the sk learn
model selection we can import train test
split
this does a nice job we can set the
random state so randomly picks the data
and we're just going to take uh 25 of
it's going to go into the test our x
test and our y test
and the 75 will go to x train and y
train
that way once we
create our model we can then have data
to see just how accurate or how well it
has performed with our prediction
the next step in pre-processing our data
is to go ahead and do feature scaling
now a lot of this is start to look
familiar if you've done a number of the
other modules and setup you should start
noticing that we
bring in our data we take a look at what
we're working with
we go ahead and split it up into
training and testing
in this case we're going to go ahead and
scale it scale it means we're putting it
between a value of minus 1 and 1
or some place in the middle ground there
this way if you have any huge set you
don't have this huge
setup if we go back up to here where
salary the salary is
20 000 versus age 35.
well there's a good chance with a lot of
the back end math that 20 000 will skew
the results and the estimated salary
will have a higher impact than the age
instead of balancing them out and
letting the calculations weigh them
properly
and finally we get to actually create
our naive bayes model
and then we're going to go ahead and
import the gaussian naive bayes
and the gaussian is is the most basic
one that's what we're looking at now it
turns out though if you go to the sk
learn
kit
they have a number of different ones you
can pull in there there's a
bernoulli i know i've never used that
one categorical
um complement and here's our gaussian
so there's a number of different options
you can look at
gaussian when you come to the naive
bayes is the most commonly used
so we're talking about the naive bayes
that's usually what people are talking
about when they when they're pulling
this in
and one of the nice things about the
gaussian if you go to their website
to sk learn the naive bayes gaussian
there's a lot of cool features one of
them is you can do partial fit on here
that means if you have a huge amount of
data you don't have to process it all i
want you once you can batch it into the
gaussian nb model and there's many other
different things you can do with it as
far as fitting the data and how you
manipulate it we're just doing the
basics so we're going to go ahead and
create our classifier we're going to
equal the gaussian in b
and then we're going to do a fit we're
going to fit our training data and our
training solution so x train y train
and we'll go ahead and run this uh it's
going to tell us that it ran the code
right there
and now we have our trained classifier
model
so the next step is we need to go ahead
and run a prediction we're going to do
our y predict equals the
classifier.predict
x test so here we fit the data and now
we're going to go ahead and predict
and now we get to our confusion matrix
so from
the sk learn matrix metrics you can
import your confusion matrix
just as saves you from doing all the
simple math that does it all for you
and then we'll go ahead and create our
confusion metrics with the y test and
the y predict so we have our actual
and we have our predicted value
and you can see from here this is the
chart we looked at here's predicted so
true positive false positive
false negative true negative
and if we go ahead and run this there we
have it 65 3 7 25
and in this particular prediction we had
65 or predicted the truth as far as a
purchase they're gonna make a purchase
and we guessed three wrong
and then we had 25 we predicted would
not purchase and seven of them did so
there's our our confusion matrix
at this point if you were with your
shareholders or a board meeting
you would start to hear some snoozing if
they were looking at the numbers and you
say hey here's my confusion
matrix
so let's go ahead and visualize the
results
we're going to pull from the matplot
library colors import listed color map
and this is actually my machine is going
to throw an error because this is being
because of the way the setup is i have a
newer version on here than when they put
together the demo
and we need our x set and our y set
which is our x train and y train
and then we'll create our x1 x2
and we'll put that into a grid
uh and we set our x set minimum stop and
our x at max stop
and if you come all the way over here
we're going to step .01 this is going to
give us a nice line
is what that's doing and we're going to
plot the contour
plot the x limit plot the y limit
and put the scatter plot in there let's
go ahead and run this uh to be honest
when i'm doing these graphs there's so
many different ways to do that there's
so many different ways to put this code
together
to show you what we're doing it's a lot
easier to pull up the graph and then go
back up and explain it
so the first thing we want to note here
when we're looking at the data
is this is the training set
and so we have those who didn't make a
purchase we've drawn a nice area for
that
that's defined by the naive bayes setup
and then we have those who did make a
purchase the green and you can see that
some of the green drops fall into the
red area and some of the red dots fall
into the green
so even our training set isn't going to
be a hundred percent
we couldn't do that
and so we're looking at our different
data coming down
we can kind of arrange our x1 x2 so we
have a nice plot going on and then we're
going to create the
contour
that's that nice line that's drawn down
the middle on here with the red green
that's where that's what this is doing
right here with the reshape and notice
that we had to
do the dot t if you remember from numpy
if you did the numpy module
you end up with pairs you know x
x1 x2 x1 x2 next row and so forth you
have to flip it so it's all one row you
have all your x1's and all your x2s
so this is what we're kind of looking
for right here on this setup
uh and then the scatter plot is of
course um your scattered data across
there we're just going through all the
points that puts these nice little dots
on to our setup on here and we have our
estimated salary and our h and then of
course the dots are did they make a
purchase or not
and just a quick note this is kind of
funny you can see up here where it says
x set y set equals x train y train
which seems kind of a little weird to do
this is because this is probably
originally a definition
so it was its own module that could be
called over and over again
and which is really a good way to do it
because the next thing we're going to
want to do is do the exact same thing
but we're going to visualize the test
set results
that way we can see what happened with
our test group our 25 percent
and you can see down here we have the
test set uh and it
if you look at the two
graphs next to each other this one
obviously has
75 percent of the data so it's going to
show a lot more
this is only 25 of the data you can see
that there's a number that are kind of
on the edge as to whether they could
guess by age and income they're going to
make a purchase or not
but that said it still is pretty clear
it's pretty good as far as how much the
estimate is and how good it does
now
graphs are really effective
for showing people what's going on but
you also need to have the numbers and so
we're going to do from sklearn we're
going to import metrics
and then we're going to print our
metrics classification port from the y
test and the y predict
and you can see here we have precision
precision of 0s is 90 there's our recall
0.96 we have an f1 score and a support
and we have our precision the recall on
getting it right
and then we can do our accuracy the
macro average and the weighted average
so you can see it it pulls in
pretty good as far as how accurate it is
you could say it's going to be about 90
percent is going to guess correctly
that that they're not going to purchase
and we had an 89
chance that they are going to purchase
and then the other numbers as you get
down
have a little bit different meaning but
it's pretty straightforward on here
here's our accuracy and here's our micro
average and the weighted average and
everything else you might need and if
you forgot the exact definition of
accuracy it is the
true positive true negative over all of
the different setups
precision is your true positive over all
positives true and false and recall is a
true positive over true positive plus
false negative
and we can just real quick flip back
there
so you can see those numbers on here
here's our precision here's our recall
and here's our accuracy on this
multiple linear regression let's take a
brief look at what happens when you have
multiple inputs so in multiple linear
regression we have well we'll start with
the simple linear regression where we
had y equals m plus x plus c and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just x we have x1 x2 x3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have m1 m2 m3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
m1 times x1 plus m2 times x2
so on all the way to m to the nth x to
the nth and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in python if you
remember before we were looking at a
company and just based on its rnd trying
to figure out his profit we're going to
start looking at the expenditure of the
company we're going to go back to that
we're going to predict his profit but
instead of predicting it just on the r d
we're going to look at other factors
like administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company is going to
be to start our coding we're going to
begin by importing some basic libraries
and we're going to be looking through
the data before we do any kind of linear
regression we're going to take a look at
the data to see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now i'm going to be
doing this in anaconda jupiter notebook
a very popular ide i enjoy it's such a
visual to look at and so easy to use
just any id for python will work just
fine for this so break out your favorite
python ide so here we are in our jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as np and then i want you to skip
one line and look at import pandas as pd
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as np and you have to
almost have that for your sk learn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
i'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import
matplotlibrary.pipelot as plt and then
seaborn as sns seaborn works with the
matplot library so you have to always
import matplot library and then seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the seaborne is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the amber scion map plot library
inline that is only because i'm doing an
inline ide my interface in the anaconda
jupiter notebook requires i put that in
there or you're not going to see the
graph when it comes up let's go ahead
and run this it's not going to be that
interesting so we're just setting up
variables in fact it's not going to do
anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in the slide you'll
see companies equals pd.read csv and it
has a long line there with the file at
the end 1000 companies dot csv you're
going to have to change this to fit
whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that i have companies equals
pd.reader underscore csv and i've
changed this path to match my computer c
colon slash simply learn slash 1000
underscore companies.csv and then below
there we're going to set the x equals to
companies under the i location and
because this is companies as a pd data
set i can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is or we have a colon minus
one and we want the values set into
there so x is no longer a data set a
pandas data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies dot head which
lists the first five rows of data and
i'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when i
hit run you'll see it breaks it out into
a nice setup this is what panda is one
of the things pandas is really good
about is it looks just like an excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns r and d
spend administration marketing spend
state profit it even notes that the top
are column names it was never told that
but pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a csv so you can
actually see the raw data so here i've
opened it up as a text editor and you
can see at the top we have rnd spend
comma administration comma marketing
spin comma state comma profit carriage
return i don't know about you but i go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where i can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and investments and you understand what
uh 165
349 dollars and 20 cents compared to the
administration cost of a hundred and
thirty six thousand eight hundred ninety
seven dollars and eighty cents so on so
on helps to create the profit of 192 261
and 83 cents that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
notebook and see what we got going and
so we have the visualization and again
we're using sns which is pandas as you
can see we imported the map plot library
dot pi plot as plt which then the
seaborn uses and we imported the seaborn
as sns and then that final line of code
helps us show this in our inline coding
without this it wouldn't display and you
could display it to a file and other
means and that's the matplot library in
line with the amber sign at the
beginning so here we come down to the
single line of code seaborn is great
because it actually recognizes the panda
data frame so i can just take the
companies dot core for coordinates and i
can put that right into the seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one we have the columns and the
rows we have r and d spending we have
administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so r d spending is going
to be the same as r d spending and the
same thing with administration costs
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with r d spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression model so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupiter notebook and what we're bringing
in is we're going to bring in the sk
learn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital l label capital e encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what i'm talking about here if you
remember when we did the companies dot
head and we printed the top five rows of
data we have our columns going across we
have column zero which is r and d
spending column one which is
administration column two which is
marketing spending and column three is
state
and you'll see under state we have new
york california florida now to do a
linear regression model it doesn't know
how to process new york it knows how to
process a number so the first thing
we're going to do is we're going to
change that new york california and
florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
to take the label coder and we're going
to fit and transform the x also the
third row so we're going to take that
third row we're going to set it equal to
a transformation and that transformation
basically tells it that instead of
having a uh new york it has a zero or a
one or a two and then finally we need to
do a one hot encoder which equals one
hot inc or categorical features equals
three and then we take the x and we go
ahead and do that equal to one hot
encoder fit transform x to array this
final transformation preps our data for
us so it's completely set the way we
need it is just a row of numbers even
though it's not in here let's go ahead
and print x and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if i go ahead
and just do row 0 you'll see i have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computer
has automatically transformed our header
into the setup and it's automatically
transferring all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data
now whenever we create a predictive
model of data we always want to split it
up so we have a training set and we have
a testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and i'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital x test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models you
usually see that a capital x and you see
the train and the test and the lower
case y what this is is x is our data
going in that's our rnd spin our
administration our marketing and then
why which we're training is the answer
that's the profit because we want to
know the profit of an unknown entity
that's what we're going to shoot for in
this tutorial the next part train test
split we take x and we take y we've
already created those x has the columns
with the data in it and y has a column
with profit in it and then we're going
to set the test size equals 0.2 that
basically means 20 percent
so twenty percent of the rows are going
to be tested we're going to put them off
to the side so since we're using a
thousand lines of data that means that
200 of those lines we're going to hold
off to the side to test for later and
then the random state equals zero we're
going to randomize which ones it picks
to hold off to the side we'll go ahead
and run this it's not overly exciting so
setting up our variables but the next
step is the next step we actually create
our linear regression model now that we
got to the linear regression model we
get that next piece of the puzzle let's
go ahead and put that code in there and
walk through it so here we go we're
going to paste it in there and let's go
ahead and since this is a shorter line
of code let's zoom up there so we can
get a good look and we have from the
sklearn dot linear underscore model
we're going to import linear regression
now i don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
where we had this long formula on the
bottom and we were doing all this
summarization and then we also looked at
uh setting it up with the different
lines and then we also looked all the
way down to multiple linear regression
where we're adding all those formulas
together all of that is wrapped up in
this one section so what's going on here
is i'm going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor dot fit in this case we do x
train and y train because we're using
the training data x being the data in
and y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when i run the regressor it gives an
output linear regression it says copy x
equals true fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the x test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor dot
predict x test going in and this gives
us y predict now because i'm in jupiter
inline i can just put the variable up
there and when i hit the run button
it'll print that array out i could have
just as easily done print y predict so
if you're in a different ide that's not
an inline setup like the jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients in the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when i run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into where it's
coming from you can see the formula down
here where y equals m1 times x1 plus m2
times x2 and so on and so on plus c the
coefficient so these variables fit right
into this formula y equals slope 1 times
column 1 variable plus slope 2 times
column 2 variable all the way to the m
and to the n and x to the n plus c the
coefficient or in this case you have
minus 8.89 to the power of 2 etc etc
times the first column and the second
column and the third column and then our
intercept is the minus 1 0 3 0 0 9 point
boy it gets kind of complicated when you
look at it this is why we don't do this
by hand anymore this is why we have the
computer to make these calculations easy
to understand and calculate now i told
you that was a short detour and we're
coming towards the end of our script as
you remember from the beginning i said
if we're going to divide this
information we have to make sure it's a
valid model that this model works and
understand how good it works so
calculating the r squared value that's
what we're going to use to predict how
good our prediction is and let's take a
look what that looks like in code and so
we're going to use this from
sklearn.metrics we're going to import r2
score that's the r squared value we're
looking at the error so in the r2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us that we know are true the y
predict of those 200 values is what we
think it was true and when we go ahead
and run this we see we get a 0.9352
that's the r2 score now it's not exactly
a straight percentage so it's not saying
it's 93 correct but you do want that in
the upper 90s o and higher shows that
this is a very valid prediction based on
the r2 score and if r-squared value of
0.91 or 9-2 as we got on our model
remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression let's take an
example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo i will take
you into jupiter notebook and
show the code but before that let me
take you through a couple of slides to
explain what we are trying to do so
let's say you have an eight by eight
image and there the image has a number
one two three four and you need to train
your model to predict what this number
is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set and
test set with the training set we train
our model and then with the test set we
test the model then we get good accuracy
and then we use it for for inference
right so that is typical methodology of
training testing and then deploying of
machine learning models so let's take a
look at the code and see what we are
doing so i will not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images are
a sample of these images and
then we split the data into training and
test as i mentioned earlier
and we can do some exploratory analysis
and
then we build our model we train our
model with the training set and then we
test it with our test set and find out
how accurate our model is using the
confusion matrix the heat map and use
heat map for visualizing this and i will
show you in the code what exactly is the
confusion matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about 0.94
which is pretty good or 94 which is
pretty good all right so what is the
confusion matrix this is an example of a
confusion matrix and
this is used for identifying the
accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion matrix is that first
of all this as you can see this is a
matrix and the size of the matrix
depends on how many outputs we are
expecting right
so
the most important part here is that the
model will be most accurate when we have
the maximum numbers in its diagonal like
in this case that's why it has almost 93
94 percent because the diagonals should
have the maximum numbers and the others
other than diagnose the cells other than
the diagonals should have very few
numbers so here that's what is happening
so there is a two here there are there's
a one here but most of them are along
the diagonal this what does this mean
this means that the number that has been
fed is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed or rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two misclassifications
okay so that is the meaning of saying
that the maximum number should be in the
diagonal so if you have all of them so
for an ideal model which has let's say
hundred percent accuracy everything will
be only in the diagonal there will be no
numbers other than zero in all other
cells so that is like a hundred percent
accurate model okay so that's uh just of
how to use this matrix uh how to use
this confusion matrix i know the name is
a little funny sounding confusion matrix
but actually it is not very confusing
it's very straightforward so you are
just plotting what has been predicted
and what is the labeled information or
what is the actual data that's also
known as the ground truth sometimes okay
these are some fancy terms that are used
so predicted label and the actual name
that's all it is okay yeah so we are
showing a little bit more information
here so 38 have been predicted and here
you will see that all of them have been
predicted correctly there have been 38
zeros and the predicted value and the
actual value is exactly the same whereas
in this case
right it has there are i think 37 plus
five yeah 42 have been fed the images 42
images are of digit 3 and
the accuracy is only 37 of them have
been accurately predicted three of them
have been predicted as number seven and
two of them have been predicted as
number eight and so on and so forth okay
all right so with that let's go into
jupiter notebook and see how the code
looks so this is the code in
in jupiter notebook for logistic
regression in this particular demo what
we are going to do is train our model to
recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and
then the last line in this block is to
load the digits so let's go ahead and
run this code then
here we will visualize the shape of
these digits so we can see here if we
take a look this is how the shape is
1797
by 64. these are like 8x8 images so
that's that's what is reflected in this
shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and matplot and we will take a
look at uh some of the sample images
that we have unloaded so this one for
example creates a figure and then we go
ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at zero one two three four so this is
how the image is this is how the data is
okay and uh based on this we will
actually train our logistic regression
model and then we will test it and see
how well it is able to recognize so the
way it works is the pixel information so
as you can see here this is an 8 by 8
pixel kind of a image and
the each pixel whether it is activated
or not activated that is the information
available for each pixel now based on
the pattern of this activation and
non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn
for which a number what is the pattern
of the activation of the pixels right so
that is what we are going to train our
model okay so the first thing we need to
do is to split our data into training
and test data set right so whenever we
perform any training we split the data
into training and test so that the
training data set is used to train the
system so we pass this probably multiple
times
and then we test it with the test data
set and the split is usually in the form
of the and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20 0.23 that means 23 percent of that
entire data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is uh called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and
we pass the data that we want to split
and the the results will be stored in x
underscore train and y underscore train
for the training data set and what is x
underscore train these are these are the
features right which is like the
independent variable and why underscore
train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in x underscore train and
since this is the labeled data
for each of them each of the
observations we already have
the label information saying whether
this digit is a zero or a one or a two
so that this this is what will be used
for comparison to find out whether the
the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way x underscore train y underscore
train is for the training data set x
underscore test y underscore test is for
the test data set okay so let me go
ahead and execute this code as well and
then we can go and check quickly what is
the how many entries are there and in
each of these so x underscore train the
shape is 13 83
by 64. and y underscore train has 1383
because there is nothing like the second
part is not required here and then x
underscore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the
logistic regression library and which is
a part of scikit-learn so
we we don't have to implement the
logistic regression process itself we
just call these the function and let me
go ahead and execute that so that
we have the logistic regression library
imported now we create an instance of
logistic regression right so logistic
regr is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line
is where we are passing
our data the training data set right
this is our the the predictors and uh
this is our target we are passing this
data set to train our model all right so
once we do this in this case the data is
not large but by and large the training
is what takes usually a lot of time so
we spend in machine learning activities
and machine learning projects we spend a
lot of time for the training part of it
okay so here the data set is relatively
small so it was pretty quick so all
right so now our model has been trained
using the training data set and
we want to see how accurate this is so
what we'll do is we will test it out in
probably faces so let me first try out
how well this is working for
one image okay i will just try it out
with one image my the first entry in my
test data set and see whether it is
correctly predicting or not so
and in order to test it so for training
purpose we use the fit method there is a
method called fit which is for training
the model and once the training is done
if you want to test for a particular
value new input you use the predict
method okay so let's run the predict
method and we pass this particular image
and
we see that the
shape is or the prediction is 4. so
let's try a few more let me see for the
next 10
seems to be fine so let me just go ahead
and test the entire data set okay that's
basically what we will do so now we want
to find out how accurately this has
performed so we use the score method to
find what is the percentages of accuracy
and we see here that it has performed up
to 94 percent accurate okay so that's on
this part now what we can also do is we
can um also see this accuracy
using what is known as a confusion
matrix so let us go ahead and try that
as well
so that we can also visualize how well
this model has uh done so let me execute
this piece of code which will basically
import some of the libraries that are
required and
we we basically create a confusion
matrix and instance of confusion matrix
by running confusion matrix and passing
these values so we have so this
confusion underscore matrix method takes
two parameters one is the y underscore
test and the other is the prediction so
what is the y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us
and this is what the system has
the model has generated so we kind of
create the confusion matrix and we will
print it and this is how the confusion
matrix looks as the name suggests it is
a matrix
and
the key point out here is that the
accuracy of the model is determined by
how many numbers are there in the
diagonal the more the numbers in the
diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of
observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers uh which means the accuracy is
very low the diagonal indicates a
correct prediction that this means that
the actual value is same as the
predicted value here again actual values
same as the predictor value and so on
right so the moment you see a number
here that means the actual value is
something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that is basically how we read the
confusion matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 plus 44 plus 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
matrix now let us visualize this
confusion matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
we'll add some colors as well it's uh
it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks
and as you can see here the diagonals
again are all the values are here most
of the values so which means reasonably
this seems to be reasonably accurate and
yeah basically the accuracy score is 94
percent this is calculated as i
mentioned by adding all these numbers
divided by the total test value so the
total number of observations in test
data set
okay so this is the confusion matrix for
logistic regression
all right so now that we have seen the
confusion matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we we picked up randomly a few of them
so
this is number four which is the actual
value and also the predicted value both
are four
this is an image of zero so the
predicted value is also zero actual
value is of course zero then this is the
image of nine
so this has also been predicted
correctly nine and actual value is nine
and this is the image of one and again
this has been predicted correctly as
like the actual value okay so this was a
quick demo of logistic regression how to
use logistic regression to identify
images
need for confusion matrixes
classification models have multiple
output categories
most error measures will tell us the
total error in our model but we cannot
use it to find out individual instances
of errors in our models
so you have your input coming in you
have your classifier
it measures the error and it says oh
53 of these are correct
but we don't know which 53 are correct
is it 53 correct uh on guessing on the
spam is it 23 guessing on spam and
27 guessing on what's not spam
this is where the confusion matrix comes
in
so during the classification we also
have to overcome the limitations of
accuracy accuracy can be misleading for
classification problems
if there is a significant class
imbalance a model might predict the
majority class for all cases and have a
high accuracy score
and so you can see here we have our
email coming in and there's two spams
the classifier comes in and goes hey it
only catches one of those spams and it
misclassifies one that's not spam
so our model predicted eight out of ten
instances and will have an accuracy of
80 percent but is it classifying
correctly
a confusion matrix represents a table
layout of the different outcomes of
prediction
and results of a classification problem
and helps visualize its outcomes
and so you see here we have our
simple chart predicted and actual
the confusion matrix
helps us identify the correct
predictions of a model for different
individual classes as well as the errors
so you'll see here that the values
predicted by our classifier are along
the rows this is what we're going to
guess it is or our model is guessing
what this is based on its training so
we've already trained the model
to guess whether it's spam or not spam
or whatever it is you're working on
and then the actual values of our data
set are along the columns
so this is the actual value that's
supposed to be
people who can speak english will be
classified as positives so because they
have a remember 001 do you speak english
yes no and you could extend this that
they might have do you speak
french do you speak whatever languages
and so you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
english will be classified as negatives
so there'll be a zero so you know zero
ones
the number of times our actual positive
values are equal to predicted positive
values gives us true positive tp
the number of times our actual negative
values are equal to predictive negative
values gives us true negative tn
the number of times our model wrongly
predicts negative values as positives
gives us a false positive
fp
and you'll see when you're working with
these a lot you know memorizing that is
false positive you can easily figure out
what that is and pretty soon you're just
looking at the fp or the tp depending on
what you're working on and the number of
times our model wrongly predicts
positive values as negatives gives us a
false negative fp
now i'm going to do a quick step out
here
let's say you're working in the medical
and we're talking about cancer
do you really want a bunch of false
negatives
you want zero under false negative
uh so when we look at this confusion
matrix if you have five percent false
positives and five percent false
negatives it'd be much better to even
have twenty percent false positives
because they go in and test it and zero
false negatives
the same might be true if you're working
on uh say a car driving is this a safe
place for the car to go
well you really don't want any false
positives you know yes this is safe
right over the cliff
so again when you're working on the
project or whatever it is you're working
on this chart suddenly has huge value
we were talking about spam email
how many important emails say from your
banking overdraft charge coming in that
you want to be a
a true a false negative you don't want
it to go in the spam folder
likewise you want to get as much of the
spam out of there but you don't want to
miss anything really important
confusion matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics
accuracy
precision recall and f1 score the f1
score is the one i usually hear the most
and accuracy is usually what you put on
your chart
when you're sending in front of the
shareholders how accurate is it people
understand accuracy
um
f1 score is a little bit more on the
math side and so you got to be a little
careful when you're quoting f1 scores in
the when you're sitting there with all
the shareholders because a lot of them
will just glaze over so confusion matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right
it is the sum of all true values divided
by the total values
and this makes sense uh
again it's one of those things
i don't want to
you know what depends on what you're
looking for are you looking for
not to miss any spam mails are you
looking to drive down the road and not
run anybody over
precision
is used to calculate the model's ability
to classify positive values correctly it
answers the question when the model
predicts a positive value how often is
it right
it is the true positive divided by the
total number of predicted positive
values again this one
depends on what project you're working
on whether this is what you're going to
be focusing on
so recall it is used to calculate the
model's ability to predict positive
values
how often
does the model actually predict the
correct positive values
it is the true positives divided by the
total number of actual positive values
and then your f1 score it is the
harmonic mean of recall and precision it
is useful when you need to take both
precision and recall into account
consider the following two confusion
matrix derived from two different
classifier
to figure out which one performs better
we can find the confusion matrix for
both of them
and you can see we're back to
does it classify whether they can speak
english or
or non-speaker they speak some they
don't know the english language and so
we put these two uh confusion matrixes
out here we can go ahead and do the math
behind that we can look up the accuracy
that's a tpn plus tn over the tf plus tn
plus fp
plus fn and so we get an accuracy of
0.8125
and we have a precision
if you do the precision which is your tp
truth positive over tp plus fp
we get
0.891
and if we do the recall we'll end up
with the 0.825 that's your tp over tp
plus fn
and then of course your f1 score which
is 2 times precision times recall over
precision plus recall
and we get the 0.857
and if we do that with another model
let's say we had two different models
and we're trying to see which one we
want to use
for whatever reason
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our f1
score
and as we're looking at this we might
look at the accuracy because that's
really what we're interested in is
how many people
are we able to
classify as being able to speak english
i really don't want to know
if i'm you know i i really don't want to
know if they're non-speakers
i'd rather miss 10 people speaking
english instead of 15. and so you can
see from these charts we'd probably go
with the first model because it does a
better job
guessing who speaks english and has a
higher accuracy because in this case
that is what we're looking for
so uh with that we'll go ahead and pull
up a demo so you can see what this looks
like in the python
setup in in the actual coding for this
we'll go into anaconda navigator if
you're not familiar with anaconda it's a
really good tool to use as far as doing
display
in demos
and for quick development as a data
scientist i just love the package
now if you're going to do something
heavier lifting
there's some limitations with anaconda
and with the setup but in general you
can do just about anything in here with
your python
and for this we'll go with jupiter
notebook
jupiter lab is the same as jupiter
notebook you'll see they now have
integration with pi charm if you work in
pycharm
certainly there's a lot of other
integrations that
anaconda has and we've opened up
my simply learn files i work on and
create a new file called confusion
matrix demo
and the first thing we want to note is
the data we're working with
here i've opened it up in a wordpad or
notepad or whatever
you can see it's got a row of
headers
comma separated and then all the data
going down below
and then i save this in the same file so
i don't have to remember what path i'm
working on
of course if you have your data
separated and you're working with a lot
of data you probably want to put it into
a different folder or file depending on
what you're doing
and the first thing we're going to do is
go ahead and import our tools
we're going to use the pandas that's our
data frame
if you haven't had a chance to work with
the data frame please review panda's
data frame and go into simply learn you
can pull up the pandas data frame
tutorial on there
and then we're going to use
the the scikit framework which is all
denoted as sklearn
and i can just pull this in you can see
here's the
scikit-learn.org
with the stable version that you can
import into your
python and from here we're going to use
the train test split
for splitting our data we're going to do
some pre-processing
we're going to do use the logistic
regression model that's our actual
machine learning model we're using and
then what this court this particular
setup is about is we're going to do the
accuracy score the confusion matrix and
the classifier report
so let me go ahead and run that and
bring all that information in
and just like we open the file we need
to go ahead and load our data in here
so we're going to go ahead and do our
pandas read csv
and then just because we're in jupyter
notebook we can just put data to read
the data in here a lot of times we'll
actually let me just do this i prefer to
do the
just the head of the date or the top
part
and you can see we have age sex
i'm not sure what cp stands for
test bps cholesterol
so a lot of different measurements
if you were in this domain you want to
know what all these different
measurements mean
i don't want to focus on that too much
because
when we're talking about data science a
lot of times you have no idea what the
data means if you've ever looked up the
breast cancer measurement it's just a
bunch of measurements and numbers
unless you're a doctor you have no idea
what those measurements mean
but if it's your specialty and your
domain you better know them so we're
going to go ahead and create y and it's
going to we're going to set it equal to
the target
so here's our target value here
and there's either 1 or 0.
so we have a classifier if you're
dealing with one zero true false what do
you have you have a classifier
and then our x is going to be uh
everything except for
the target
so we're going to go ahead and drop the
target axis equals 1. remember that's
columns versus the index or rows axis
equals 0 would would give you an error
but you would drop like row 2.
and then we'll go ahead and just print
that out so you can see what we're
looking at and
here we have
y data x data and you can see from the x
data we have the x head and we can go
ahead and just do print
the
y head data
and run that
so this is all loading the data that
we've done so far if there's a confusion
in there go back and rewind the tape and
review it
and then we need to go ahead and split
our data into our x train
x test y train y test
and then keep in mind you always want to
split the data before we do the scalar
and the reason is is that
you want the scalar on the training data
to be set on the training data data or
fit to it
but not on the test data
think of this as being out in the field
you're not it could actually alter your
results
so it's always important to do make sure
whatever you do to the training data
or whatever
fit you're doing is always done on the
training not on the test and then we
want to go ahead and scale the data now
we are working with a
linear regression model and i'll mention
this here in a minute when we get to the
actual model
uh so some sometimes you don't need to
scale when you're working with linear
regression models it's not going to
change your result as much as say a
neural network where it has a huge
impact
but we're going to go ahead and take
here's our x train x test y train y test
we create our scalar we go ahead and
scale
the scale is going to fit the x train
and then we're going to go ahead and
take our x train and transform it
and then we also need to take our x test
and transform it based on the scale on
here so that our x is now between that
nice minus 1 to one and so this is all
uh our pre
data setup
and
hopefully
uh all of that looks fairly familiar to
you if you've done a number of our other
classes and you're up to the setup on
here
and then we want to go ahead and do is
create our model and we're going to use
the logistic regression model
and from the logistic regression model
we're going to go ahead and fit our x
train and y train
and then we'll run our predicted value
on here
and so let's go ahead and run that and
so now we are we actually have like our
x test and our prediction so if you
remember from
our matrix we're looking for the actual
versus the prediction and how those
compare
and
if i take us back up here you're going
to notice that we imported the accuracy
score
the confusion matrix and the
classification report
and there's of course our logistic
regression the model we're using for
this
and i did mention it's going to talk a
little bit about scalar and the
regression model
the scalar on a lot of your regression
models your basic mass standard
regression models and i'd have to look
it up for the logistic regression model
when you're using a standard regression
model you don't need to scale the data
it's already just built in by the way
the model works
in most cases uh but if you're in a
neural network and you're there's a lot
of other different setups then you
really want to take this and fit that on
there
and so we can go in and do the accuracy
uh
and this is if you remember correctly we
were looking at the accuracy
with the english speaking
so this is saying our accuracy
as to whether this person is i believe
this is the heart data set
it's going to be accurate about 85
percent of the time as far as whether
it's going to predict the person's going
to have a heart condition
or the one as it comes up with the zero
one on there
which would mean at this point that you
have an 85 percent uh being correct on
telling someone they're extremely high
risk for a heart attack kind of thing
and so we want to go ahead and create
our confusion matrix and we just do that
of course the software does everything
for us so we'll go ahead and run this
and you can see right here um
here's our 25
prediction uh correct predictions right
here
and if you remember from our slide i'll
just bring this over says a nice visual
we have our true positive
false positive
uh so we had 25 which were true that it
said hey this person is going to be high
risk at heart and we had four
that were still high risk that has said
we're false
so out of these 25 people or out of
these 29 people and that makes sense
because you have 0.85
out of 29 people it was correct on 25 of
them
and so uh here's our accuracy score we
were just looking at that our accuracy
is your true positive and your true
negative over all of them so how true is
it there's our accuracy coming up here
0.85
and then we have our nice matrix
generated from that
and you can see right here is a similar
matrix we had going for the from the
slide and this starts to this should
start asking questions at this point
so if you're in a board meeting or
you're working with this you really want
to start looking at this data here and
saying well
is this good enough is uh this number of
people and hopefully you'd have a much
larger data set it might is my confusion
matrix showing for the true positive and
false positive
is that acceptable for what we're doing
uh and of course if you're going to put
together whatever data you're putting
out you might want to separate the
true negative false positive false
negative true positive and you can
simply do that
by doing the confusion matrix
and then of course the ravel part lets
you
set that up so you can just split that
right up into a nice tuple and the final
thing we want to show you here in the
coding
on this part
is the confusion matrix metrics
and so we can come in here and just use
the matrix equals classification report
the y test and the predict and then
we're going to take that classification
report and go ahead and print that out
and you can see here it does a nice job
giving you your accuracy
your micro average your weighted average
you have your precision
your recall your f1 score and your
support all in one window so you can
start looking at this data and saying oh
okay
our precision's at .83
uh
.87 for getting a positive and 0.83 for
the negative side for a zero
and we start talking about whether this
is a valid information or not to use
and when we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not
you might have something where we went
back to the languages maybe you also
want to know whether they speak english
or hindi
or french and you can see right here
that we can now
take our confusion matrix and just
expand it as big as we need to depending
on how many different classifiers we're
working on decision tree important terms
before we dive in further we need to
look at some basic terms we need to have
some definitions to go with our decision
tree and the different parts we're going
to be using we'll start with entropy
entropy is a measure of randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of
entities so as we start splitting it
into subgroups we come up with our
second definition which is information
gain information gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
ten and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
information gain is simply the entropy
e1 from the top and how it's changed to
e2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
python because they'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went on
the different parts of our tree and they
call the leaf node leaf node carries the
classification or the decision so it's
the final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node
how does a decision tree work wonder
what kind of animals all get in the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and there of different colors
and shapes let's see what that looks
like and how do we split the data we
have to frame the conditions that split
the data in such a way that the
information gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a e funky e of k where i equals
1 to k k would represent the number of
animals the different animals in there
where value or p
value of i would be the percentage of
that animal times the log base two of
the same the percentage of that animal
let's try to calculate the entropy for
the current data set and take a look at
what that looks like and don't be afraid
of the math you don't really have to
memorize this math just be aware that
it's there and this is what's going on
in the background and so we have three
giraffes two tigers one monkey two
elephants a total of eight animals
gathered and if we plug that into the
formula we get an entropy that equals
three over eight so we have three
giraffes a total of eight times the log
usually they use base two on the log so
log base two of three over eight plus in
this case it says yellow fence two over
eight two elephants over total of eight
times log base two two over eight plus
one monkey over total of eight log base
two one over eight and plus two over
eight of the tigers log base two over
eight and if we plug that into our
computer our calculator i obviously
can't do logs in my head we get an
entropy equal to 0.571
the program will actually calculate the
entropy of the data set similarly after
every split to calculate the gain now
we're not going to go through each set
one at a time to see what those numbers
are we just want you to be aware that
this is a formula or the mathematics
behind it gain can be calculated by
finding the difference of the subsequent
entropy values after a split now we will
try to choose a condition that gives us
the highest gain we will do that by
splitting the data using each condition
and checking that the gain we get out of
them the condition that gives us the
highest gain will be used to make the
first split can you guess what that
first split will be just by looking at
this image as a human is probably pretty
easy to split it let's see if you're
right if you guessed the color yellow
you're correct let's say the condition
that gives us the maximum gain is yellow
so we will split the data based on the
color yellow if it's true that group of
animals goes to the left if it's false
it goes to the right the entropy after
the splitting has to decreased
considerably however we still need some
splitting of both the branches to attain
an entropy value equal to zero so we
decide to split both the nodes using
height as a condition since every branch
now contains single label type we can
say that entropy in this case has
reached the least value and here you see
we have the giraffes the tigers the
monkey and the elephants all separated
into their own groups this tree can now
predict all the classes of animals
present in the dataset with a hundred
percent accuracy that was easy
use case loan repayment prediction let's
get into my favorite part and open up
some python and see what the programming
code in the scripting looks like in here
we're going to want to do a prediction
and we start with this individual here
who's requesting to find out how good
his customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from python and we're going to load
up our data and take a look at what the
data looks like so the first thing i
need is i need something to edit my
python and run it in so let's flip on
over and here i'm using the anaconda
jupiter notebook now you can use any
python ide you like to run it in but i
find the jupiter notebook is really nice
for doing things on the fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as np that's very standard
when we're dealing with mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numbers it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as pd
that's also a standard the pandas is a
data frame setup and you can liken this
to
taking your basic data and storing it in
a way that looks like an excel
spreadsheet so as we come back to this
when you see np or pd those are very
standard uses you'll know that that's
the pandas and i'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so i'm going to bring
in our train test and split and this is
coming from the sk learn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk.tree
import decision tree classifier that's
the actual tool we're using remember i
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have
sklearn.metrics
for accuracy score we need to score our
setup that's the whole reason we're
splitting it between the training and
testing data and finally we still need
the sklearn import tree and that's just
the basic tree function is needed for
the decision tree classifier and finally
we're going to load our data down here
and i'm going to run this and we're
going to get two things on here one
we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says i need to read a file and when this
was written
the person who wrote it this is their
path where they stored the file
so let's go ahead and fix that
and i'm going to put in here my file
path i'm just going to call it full file
name
and you'll see it's on my c drive and
this very lengthy setup on here where i
stored the data2.csv file
don't worry too much about the full path
because on your computer will be
different
the data.2 csv file was generated by
simplylearn
if you want a copy of that you can
comment down below and request it here
in the youtube
and then if i'm going to give it a name
full file name
i'm going to go ahead and change it here
to full
file name so let's go ahead and run it
now and see what happens
and we get a warning
when you're coding
understanding these different warnings
and these different errors that come up
is probably the hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a opportunity to
understand what's going on here
if you read the warning it says the
cross validation is depreciated so it's
a warning on it's being removed
and it's going to be moved in favor of
the model selection
so if we go up here we have sklearn dot
cross validation and if you research
this and go to the sklearn site you'll
find out that you can actually just swap
it right in there with
model selection
and so when i come in here and i run it
again
that removes a warning
what they've done is they've had two
different developers develop it in two
different branches
and then they decided to keep one of
those and eventually get rid of the
other one that's all that is and very
easy and quick to fix
before we go any further i went ahead
and opened up the data
from this file remember the the data
file we just loaded on here the data
underscore2.csv
let's talk a little bit more about that
and see what that looks like both as a
text file because it's a comma separated
variable file and in a spreadsheet
this is what it looks like as a basic
text file you can see at the top they've
created a header and it's got one two
three four five columns and each column
has data in it and let me flip this over
because we're also going to look at this
in an actual spreadsheet so you can see
what that looks like and here i've
opened it up in the open office calc
which is pretty much the same as excel
and zoomed in and you can see we've got
our columns and our rows of data a
little easier to read in here we have a
result yes yes no we have initial
payment last payment credit score
house number
if we scroll way down
we'll see that this occupies a thousand
and one lines of code or lines of data
with the first one being a column and
then one thousand lines of data
now as a programmer
if you're looking at a small amount of
data i usually start by pulling it up in
different sources so i can see what i'm
working with
but in larger data you won't have that
option it'll just be too too large so
you need to either bring in a small
amount that you can look at it like
we're doing right now or we can start
looking at it through the python code so
let's go ahead and move on and take the
next couple steps to explore the data
using python let's go ahead and see what
it looks like in python to print the
length and the shape of the data so
let's start by printing the length of
the database we can use a simple lin
function from python
and when i run this
you'll see that it's a thousand long and
that's what we expected there's a
thousand lines of data in there if you
subtract the column head
this is one of the nice things when we
did the uh balance data from the panda
read csv
you'll see that the header is row zero
so it automatically removes a row
and then shows the data separate it does
a good job sorting that data out for us
and then we can use a different function
and let's take a look at that
and again we're going to utilize the
tools in panda
and since the balance underscored data
was loaded as a panda data frame
we can do a shape on it and let's go
ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using python
and now that we've taken a look at the
length and the shape let's go ahead and
use the pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data dot head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of dataset just a
simple print statement
and we run that
let's just take a closer look at that
let me zoom in here
there we go
pandas do such a wonderful job of making
this a very clean
readable data set so you can look at the
data you can look at the column headers
you can have it when you put it as the
head
it prints the first five lines of the
data
and we always start with zero so we have
five lines we have zero one two three
four instead of one two three four five
that's a standard scripting and
programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
excel spreadsheet or open office cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
date ahead you can actually see what
this data looks like the initial payment
last payment credit scores house number
so let's take this now that we've
explored the data and let's start
digging into the decision tree so in our
next step we're going to
train and build our data tree and to do
that we need to first
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces
so first we have our x and y
where do x and y come from well x is
going to be our data
and y is going to be the answer or the
target you can look at source and target
in this case we're using x and y to
denote the data in and the data that
we're actually trying to guess what the
answer is going to be and so to separate
it we can simply put in x equals the
balance of the data.values the first
brackets
means that we're going to select all the
lines in the database so it's all the
data
and the second one says we're only going
to look at columns one through five
remember i always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house number
well if we want to look at one through
five we can do the same thing for y
which is the answers and we're going to
set that just equal to the zero row so
it's just the zero row and then it's all
rows going in there so now we've divided
this into two different data sets
one of them with the
data going in and one with the answers
next we need to split the data
and here you'll see that we have it
split into four different parts
the first one is your x training your x
test your y train your y test
simply put we have x going in where
we're going to train it and we have to
know the answer to train it with
and then we have x test where we're
going to test that data and we have to
know in the end what the y was supposed
to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to
0.3 so that's roughly 30 percent will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we get to actually build
our decision tree and they've called it
here clf underscore entropy that's the
actual decision tree or decision tree
classifier and in here they've added a
couple variables which we'll explore in
just a minute and then finally we need
to fit the data to that so we take our
clf entropy that we created and we fit
the x train and since we know the
answers for x trade or the y train we go
ahead and put those in and let's go
ahead and run this and what most of
these sklearn modules do is when you set
up the variable in this case we set the
clf entropy called decision tree
classifier it automatically prints out
what's in that decision tree there's a
lot of variables you can play within
here and it's quite beyond the scope of
this
tutorial to go through all of these and
how they work but we're working on
entropy that's one of the options we've
added that it's completely a random
state of 100 so 100 percent and we have
a max depth of three now the max depth
if you remember above when we were doing
the different graphs of animals means
it's only going to go down three layers
before it stops and then we have minimal
samples of leaves as five so it's going
to have at least five leaves at the end
so i'll have at least three splits i'll
have no more than three layers and at
least five end leaves with the final
result at the bottom now that we've
created
our decision tree classifier not only
created it but trained it let's go ahead
and apply it and see what that looks
like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here
and before we run it let's just take a
quick look at what it's doing here
we have a variable y predict that we're
going to do
and we're going to use our
variable clf entropy that we
created and then you'll see dot predict
and it's very common in the sk learn
modules
that their different tools have to
predict when you're actually running a
prediction
in this case we're going to put our x
test data in here
now if you delivered this for use in
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess
whether the person is going to be uh pay
them back or not
in this case though we need to test out
the data and just see how good our
sample is how good of our tree does at
predicting the loan payments and finally
since anaconda jupiter notebook is works
as a command line for python we can
simply put the y predict e in to print
it i could just as easily have put the
print
and put brackets around y predict en to
print it out we'll go ahead and do that
it doesn't matter which way you do it
and you'll see right here that runs a
prediction this is roughly 300 in here
remember it's 30 percent of a thousand
so you should have about 300 answers in
here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit
there we go
so you have a nice full picture
and we'll see here we're just going to
do a print accuracy is
and then we do the accuracy score
and this was something we imported
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from
that's coming from here down here from
sklearn.metrics import accuracy score
and you could probably run a script make
your own script to do this very easily
how accurate is it how many out of 300
do we get right and so we put in our y
test that's the one we ran the predict
on and then we put in our y predict e n
that's the answers we got and we're just
going to multiply that by a hundred
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage
and let's run that and see what it looks
like
and if you see here we got an accuracy
of 93.66667
so when we look at the number of loans
and we look at how good our model fit we
can tell people it has about a 93.6
fitting to it so just a quick recap on
that we now have accuracy set up on here
and so we have created a model that uses
the decision tree algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about 94.6 percent the bank can now use
this model to decide whether it should
approve the loan request from a
particular customer or not and so this
information is really powerful we may
not be able to as individuals understand
all these numbers because they have
thousands of numbers that come in but
you can see that this is a smart
decision for the bank to use a tool like
this to help them to predict how good
their profit's going to be off of the
loan balances and how many are going to
default or not how does a random forest
work as a whole so to begin our
random forest classifier let's say we
already have built three trees and we're
going to start with the first tree that
looks like this just like we did in the
example this tree looks at the diameter
if it's greater than or equal to 3
it's true otherwise it's false so one
side goes to the smaller diameter one
side goes to larger diameter and if the
color is orange it's going to go to the
right true we're using oranges now
instead of lemons and if it's red it's
going to go to the left false we build a
second tree very similar but split
differently instead of the first one
being split by a diameter this one when
they created it if you look at that
first bowl it has a lot of red objects
so it says is the color red because
that's going to bring our entropy down
the fastest and so of course if it's
true it goes to the left if it's false
it goes to the right and then it looks
at the shape false or true and so on and
so on and tree three is a diameter equal
to one and it came up with this because
there's a lot of cherries in this bowl
so that would be the biggest split on
there is is the diameter equal to one
that's going to drop the entropy the
quickest and as you can see it splits it
into true if it goes false and they've
added another category does it grow in
the summer and if it's false
it goes off to the left if it's true it
goes off to the right let's go ahead and
bring these three trees you can see them
all in one image so this would be three
completely different trees categorizing
a fruit and let's take a fruit now let's
try this
and this fruit if you look at it we've
blackened it out you can't see the color
on it so it's missing data remember one
of the things we talked about earlier is
that a random forest works really good
if you're missing data if you're missing
pieces so this fruit has an image but
maybe the person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have
to put the color in there so ignore the
color down there but the diameter equals
3 we find out it grows in the summer
equals yes and the shape is a circle and
if you go to the right you can look at
what one of the decision trees did this
is the third one
is the diameter greater than equal to
three is the color orange well it
doesn't really know on this one but if
you look at the value it's a true and
you go to the right tree two classifies
it as cherries is a color equal red is
the shape a circle true it is a circle
so this would look at it and say oh
that's a cherry and then we go to the
other classifier and it says is the
diameter equal one well that's false
does it grow in the summer true so it
goes down and looks at as oranges so how
does this random forest work the first
one says it's an orange
the second one said it was a cherry and
the third one says it's an orange
and you can guess if you have two
oranges and one says it's a cherry
when you add that all together the
majority of the vote says orange so the
answer is it's classified as an orange
even though we didn't know the color and
we're missing data on it i don't know
about you but i'm getting tired of fruit
so let's switch and i did promise you
we'd start looking at a case example and
get into some python coding today we're
going to use the case the iris flower
analysis
this is the exciting part as we roll up
our sleeves and actually look at some
python coding before we start the python
coding we need to go ahead and create a
problem statement wonder what species of
iris do these flowers belong to let's
try to predict the species of the
flowers using machine learning in python
let's see how it can be done so here we
begin to go ahead and implement our
python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case i'm going to be using the
anaconda jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our jupiter
notebook and i've already opened up a
new page for python 3 code and i'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets
import load iris now this isn't the
actual data this is just the module that
allows us to bring in the data the load
iris and the iris is so popular it's
been around since 1936 when ronald
fisher published a paper on it and
they're measuring the different parts of
the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random forest classifier we need to
go ahead and import a random forest
classifier from the sk learn module so
dot ensemble import random force
classifier and then we want to bring in
two more modules
and these are probably the most commonly
used modules in python and data science
with any of the
other modules that we bring in and one
is going to be pandas we're going to
import pandas as pd pd is a common term
used for pandas and pandas is basically
creates a data format for us where when
you create a pandas data frame it looks
like an excel spreadsheet and you'll see
that in a minute when we start digging
deeper into the code panda is just
wonderful because it plays nice with all
the other modules in there and then we
have numpy which is our numbers python
and the numbers python allows us to do
different mathematical sets on here
we'll see right off the bat we're going
to take our np and we're going to go
ahead and seed the randomness with it
with 0. so np.random.seed
is seating that is 0. this code doesn't
actually show anything we're going to go
ahead and run it because i need to make
sure i have all those loaded and then
let's take a look at the next module on
here the next six slides including this
one are all about exploring the data
remember i told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our jupyter notebook and
here we go we've gone ahead and run the
imports and i'm going to paste the code
down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the iris
data now we're actually assigning that
data to the variable iris and then we're
going to go ahead and use the df to
define data frame
and that's going to equal pd and if you
remember that's pandas as pd so that's
our pandas
and panda data frame and then we're
looking at iris data
and columns equals iris feature names
and we're going to do the df head and
let's run this you can understand what's
going on here
the first thing you want to notice is
that our df has created what looks like
an excel spreadsheet
and in this excel spreadsheet we have
set the columns so up on the top you can
see the four different columns and then
we have the data iris.data down below
it's a little confusing without knowing
where this data is coming from so let's
look at the bigger picture and i'm going
to go print i'm just going to change
this for a moment and we're going to
print all of iris and see what that
looks like
so when i print all of iris i get this
long list of information
and you can scroll through here and see
all the different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary
you'll have a key
or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of sepal length sepal width petal
length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back to the head
and this is one of the neat features of
pandas and panda data frames
is when you do df.head or the
pandadataframe.head
it'll print the first five lines of the
data set in there along with the headers
if you have it in this case we have the
column header set to iris features and
in here you'll see that we have zero one
two three four in python most arrays
always start at zero so when you look at
the first five it's going to be zero one
two three four not one two three four
five so now we've got our iris data
imported into a data frame let's take a
look at the next piece of code in here
and so in this section here
of the code
we're going to take a look at the target
and let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our jupyter notebook
i'm going to put the code in here and
before i run it i want to look at a
couple things going on so we have df
species
and this is interesting because right
here you'll see where i have df species
in brackets which is uh the key code for
creating another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one i could have just as
easily done iris and then in brackets
target depending on what i'm working on
both are
acceptable
let's go ahead and run this code and see
how this changes and what we've done is
we've added the target from the iris
data set
as another column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our neural network to
look for these this different data and
know what a setosa is or a vera color
which we'll see in just a minute or
virginica those are the three that are
in there and now we're going to add one
more column i know we're organizing all
this data over and over again it's kind
of fun there's a lot of ways to organize
it what's nice about putting everything
onto one
data frame is i can then do a print out
and it shows me exactly what i'm looking
at and i'll show you that where you
where that's different where you can
alter that and do it slightly
differently but let's go ahead and put
this into our script up to that now and
here we go we're going to put that down
here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data
and
one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it df remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals np remember
that stands for numpy.random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length df comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take
75
of the data roughly because there's a
randomness involved and we're going to
use that to train it and then the other
25 we're going to hold off to the side
and use that to test it later on so
let's flip back on over and see what the
next step is so now that we've labeled
our database for which is training and
which is testing
let's go ahead and sort that into two
different variables train and test and
let's take this code and let's bring it
into our project and here we go let's
paste it on down here and before i run
this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def dot
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train
one's called test 75 and train 25
percent in test
and then to sort that out
we're going to do that by doing df our
main original data frame with the iris
data in it
and if df is trained equals true
that's going to go in the train and if
df is trained equals false it goes in
the test
and so when i run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and that's make it readable to
humans
i don't know about you but i hate
looking at zeros and ones
so let's start with the features and
let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the df columns
and we know the first four of them zero
one two three that'd be the first four
are going to be the features or the
titles of those columns and so when i
run this
you'll see down here that it creates an
index sepa length sepa width pedal
length and pedal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing i want you to notice here is
that when you're in a command line
whether it's jupiter notebook or you're
running command line in the terminal
window
if you just put the name of it it'll
print it out this is the same as doing
print
features
and the shorthand is you just put
features in here
if you're actually writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when i run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about jupiter notebooks
is i'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that
for our final step in prepping our data
before we actually start running the
training and the testing is we're going
to go ahead and convert the species on
here into something the computer
understands so let's put this code into
our script and see where that takes us
all right here we go we've set y equal
to pd dot factorize
train species of zero
so let's break this down just a little
bit we have our pandas right here pd
factorize what's factorized doing i'm
going to come back to that in just a
second
let's look at what train species is
and why we're looking at the group 0 on
there
and let's go up here and here is our
species
remember this on that we created this
whole column here for species
and then it has setosa setosa setosa
setosa and if you scroll down enough
you'd also see virginica and vera color
we need to convert that into something
the computer understands zeros and ones
so the train species
of zero because this is in the format of
a of an array of arrays so you have to
have the zero on the end
and then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when i run this you'll see that y
generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go
uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our full script that we're running
and let's paste this in here and let's
take a look and see what this is we have
we're creating a variable clf and we're
going to set this equal to the random
forest classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard
in jobs all that does is to prioritize
it not something to really worry about
usually when you're doing this on your
own computer you do in jobs equals two
if you're working in a larger or big
data and you need to prioritize it
differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts
zero's fine for here
but let's go ahead and run this
we also have clf dot fit train features
comma y and before we run it let's talk
about this a little bit more clf
dot fit so we're fitting we're training
it we are actually creating our random
forest classifier right here this is a
code that does everything and we're
going to take our training set remember
we kept our test off to the side and
we're going to take our training set
with the features and then we're going
to go ahead and put that in and here's
our target the y
so the y is 0 1 and 2 that we just
created and the features is the actual
data going in that we put into the
training set
let's go ahead and run that
and this is kind of an interesting thing
because it printed out the random force
classifier
and everything around it and so when
you're running this in your terminal
window or in a script like this
this automatically treats us like just
like when we were up here and i typed in
y and i printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals two
you can see the random state equals zero
those are the two that we sent in there
you would really have to dig deep to
find out all these the different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like max features
as auto so all the features that we're
putting in there is just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process and leaf nodes that's
the end nodes remember we have the fruit
and we're talking about the leaf nodes
like i said there's a lot in this we're
looking at a lot of stuff here so you
might have in this case there's probably
only think three leaf nodes maybe four
you might have thousands of leaf nodes
at which point you do need to put a cap
on that and say okay it can only go so
far and then we're going to use all of
our resources on processing this and
that really is what most of these are
about is limiting the process and making
sure we don't overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false warm start
is if you're programming it one piece at
a time externally since we're not we're
not going to have like we're not going
to continually train this particular
learning tree and again like i said
there's a lot of things in here that
you'll want to look up more detail from
the sk learn
and if you're digging in deep and
running a major project on here for
today though all we need to do is fit
our train our features and our target y
so now we have our training model what's
next if we're going to create a model
we now need to test it remember we set
aside the test feature test group 25 of
the data so let's go ahead and take this
code and let's put it into our script
and see what that looks like okay here
we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the setosa the
virginica and the versacolor and what
we're putting into our predict is the
test features
and i always kind of like to know what
it is i am looking at
so real quick we're going to do test
features and remember features is an
array
of sepal length simple width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing what
features looks like
this is just playing with the with
pandas data frames you'll see that it's
an index so when you put an index in
like this
into
test features into test
it then takes those columns and creates
a panda data frames from those columns
and in this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5 0.2 and we're going to
put those in and we're going to predict
what our new
forest classifier is going to come up
with and this is what it predicts it
predicts 0 0 0 1 2 one one two two two
and and uh again this is the
flower type setosa virginica and
versacolor so now that we've taken our
test features
let's explore that let's see exactly
what that data means to us so the first
thing we can do with our predicts is we
can actually generate a different
prediction model when i say different
we're going to view it differently it's
not that the data itself is different so
let's take this next piece of code and
put it into our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf.predict
probability so we're running it just
like we ran it up here but this time
with this we're going to get a slightly
different result and we're only going to
look at the first 10.
so you'll see down here instead of
looking at all of them
which was what 27 you'll see right down
here
that this generates a much larger field
on the probability and let's take a look
and see what that looks like
and what that means
so when we do the predict underscore
prabha
for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did
this is the predictors the first one is
predicting a one
for setosa it predicts a 0 for virginica
and it predicts a 0 for versa color and
so on and so on and so on and let's um
you know what i'm going to change this
just a little bit let's look at 10
to 20 just because we
can we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5
and so if we're going to vote and we
have two equal votes it's gonna go with
the first one so it says uh setosa gets
zero votes virginica gets point five
votes versacolor gets point five votes
but let's just go with the virginica
since these two are equal and so on and
so on down the list you can see how they
vary on here so now we've looked at both
how to do a basic predict of the
features and we've looked at the predict
probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like and let's go ahead and paste
that code in here and run it and this
goes along with the next piece of code
so we'll skip through this quickly and
then come back to a little bit so here's
iris
dot target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the setosa
virginica versus color and then we're
going to go ahead and do the prediction
again we've run we could have just set a
variable equal to this instead of
re-running it each time but we're going
to run it again clf dot predict test
features remember that returns the zeros
the ones and the twos and then we're
going to set that equal to predictions
so this time we're actually putting it
in a variable and when i run this
it distributes it it comes out as an
array and the array is setosa satosa
satosa satosa setosa we're only looking
at the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts mapping it to all the
different flower types the versa color
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it
and let's talk about both these sections
of code here
and how they go together
the first one is our predictions and i
went ahead and did predictions through
25 let's just do five
and so we have setosa satoshi satosa
satoshi that's what we're predicting
from our test model
and then we come down here we look at
test species i remember i could have
just done test dot species dot head and
you'll see it says setosa satosa setosa
setosa and they match
so the first one is what our
forest is doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our jupiter notebook here
let's see let's go ahead and paste that
in and then i'm gonna because i'm on the
jupiter notebook i can do a control
minus so you can see the whole line
there
there we go resize it
and let's take a look and see what's
going on here we're gonna create in
pandas remember pd stands for pandas and
we're doing a cross tab this function
takes two sets of data
and creates a chart out of them so when
i run it you'll get a nice chart down
here
and we have the predicted species
so across the top you'll see the setosa
versus color virginica and the actual
species setosa versacolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versus color where they meet and you
have virginica where they meet and
they're meeting where the actual and the
predicted agree
so this is the number of accurate
predictions so in this case it equals
30. if you had 13 plus 5 plus 12 you get
30. and then we notice here where it
says virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 divided by 32 and if we multiply
it by a hundred we can say that it is 93
percent accurate so we have a 93 percent
accuracy with our model i did want to
add one more quick thing in here on our
scripting before we wrap it up so let's
flip back on over to my script in here
we're going to take this line of code
from up above i don't know if you
remember it but predix equals the iris
dot target underscore names
so we're going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point i
would go ahead and change this
and this is an array of arrays this is
really important when you're running
these to know that
so you need the double brackets and i
could actually create data maybe let's
just do two flowers so maybe i'm
processing more data coming in and we'll
put two flowers in here
and then i actually want to see what the
answer are is so let's go ahead and type
in preds and print that out and when i
run this
you'll see that i've now predicted two
flowers that maybe i measured in my
front yard as versacolor and versacolor
not surprising since i put the same data
in for each one
this would be the actual
end product going out to be used on data
that you don't know the answer for
so that's going to conclude our
scripting part of this today we're going
to cover the k nearest neighbors let's
refer to as k n n and k n n is really a
fundamental place to start in the
machine learning it's the basis of a lot
of other things and just the logic
behind it is easy to understand and
incorporated in other forms of machine
learning so today what's in it for you
why do we need k n n
what is k n
how do we choose the factor k
when do we use k n n
how does k n algorithm work and then
we'll dive in to my favorite part the
use case predict whether a person will
have diabetes or not that is a very
common and popular used data set as far
as testing out models and learning how
to use the different models in machine
learning by now we all know machine
learning models make predictions by
learning from the past data available so
we have our input values our machine
learning model builds on those inputs of
what we already know and then we use
that to create a predicted output
is that a dog little kid looking over
there watching the black cat cross their
path no dear you can differentiate
between a cat and a dog based on their
characteristics
cats
cats have sharp claws uses to climb
smaller lengths of ears meows and purrs
doesn't love to play around dogs they
have dull claws bigger length of ears
barks loves to run around you usually
don't see a cat running around people
although i do have a cat that does that
where dogs do and we can look at these
we can say we can evaluate the sharpness
of the claws how sharper their claws
and we can evaluate the length of the
ears and we can usually sort out cats
from dogs based on even those two
characteristics
now tell me if it is a cat or a dog not
a question usually little kids know cats
and dogs by now
unless they live a place where there's
not many cats or dogs so if we look at
the sharpness of the claws the length of
the ears and we can see that the cat has
smaller ears and sharper claws than the
other animals its features are more like
cats it must be a cat
sharp claws length of ears and it goes
in the cat group because knn is based on
feature similarity we can do
classification using knn classifier so
we have our input value the picture of
the black cat it goes into our trained
model and it predicts that this is a cat
coming out so what is knn what is the k
n algorithm
k nearest neighbors is what that stands
for it's one of the simplest supervised
machine learning algorithms mostly used
for classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified knn stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine
another favorite of mine k n stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride k and k n is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equals 5.
we'll talk about k in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose k
how do we know k equals 5 i mean that's
what's the value we put in there so
we're going to talk about it how do we
choose a factor k k n algorithm is based
on feature similarity choosing the right
value of k
is a process called parameter tuning and
is important for better accuracy so at k
equals three we can classify we have a
question mark in the middle as either a
as a square or not is it a square or is
it in this case a triangle and so if we
set k equals to three we're going to
look at the three nearest neighbors
we're going to say this is a square and
if we put k equals to 7 we classify as a
triangle depending on what the other
data is around and you can see as the k
changes depending on where that point is
that drastically changes your answer and
we jump here we go how do we choose the
factor of k you'll find this in all
machine learning choosing these factors
that's the face you get he's like oh my
gosh did i choose the right k did i set
it right my values in whatever machine
learning tool you're looking at so that
you don't have a huge bias in one
direction or the other and in terms of k
n n the number of k if you choose it too
low the bias is based on it's just two
noises it's right next to a couple
things and it's going to pick those
things and you might get a skewed answer
and if your k is too big then it's going
to take forever to process so you're
going to run into processing issues and
resource issues so what we do the most
common use and there's other options for
choosing k is to use the square root of
n so it is a total number of values you
have you take the square root of it in
most cases you also if it's an even
number so if you're using uh like this
case squares and triangles if it's even
you want to make your k value odd that
helps it select better so in other words
you're not going to have a balance
between two different factors that are
equal so usually take the square root of
n and if it's even you add one to it or
subtract one from it and that's where
you get the k value from that is the
most common use and it's pretty solid it
works very well when do we use knn we
can use knn when data is labeled so you
need a label on it we know we have a
group of pictures with dogs dogs cats
cats data is noise free and so you can
see here when we have a class and we
have like underweight 140 23 hello kitty
normal that's pretty confusing we have a
high variety of data coming in so it's
very noisy and that would cause an issue
data set is small so we're usually
working with smaller data sets where you
might get into a gig of data if it's
really clean doesn't have a lot of noise
because k n is a lazy learner i.e it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the knn but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knn and also just
using for smaller data sets k n works
really good how does a k n algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we see right here we have two variables
you know true false they're either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using knn so if
we have new data coming in this says 57
kilograms and 177 centimeters is that
going to be normal or underweight to
find the nearest neighbors we'll
calculate the euclidean distance
according to the euclidean distance
formula the distance between two points
in the plane with the coordinates x y
and a b is given by distance d equals
the square root of x minus a squared
plus y minus b squared and you can
remember that from the two edges of a
triangle we're computing the third edge
since we know
the x side and the y side let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance d1 is the square
root of 170 minus 167 squared plus 57
minus 51 squared which is about 6.7 and
distance 2 is about 13. and distance 3
is about 13.4 similarly we will
calculate the euclidean distance of
unknown data point from all the points
in the data set and because we're
dealing with small amount of data that's
not that hard to do it's actually pretty
quick for a computer and it's not a
really complicated mass you can just see
how close is the data based on the
euclidean distance hence we have
calculated the euclidean distance of
unknown data point from all the points
as shown where x1 and y1 equal 57 and
170 whose class we have to classify so
now we're looking at that we're saying
well here's the euclidean distance who's
going to be their closest neighbors now
let's calculate the nearest neighbor at
k equals three and we can see the three
closest neighbors puts them at normal
and that's pretty self-evident when you
look at this graph it's pretty easy to
say okay what we're just voting normal
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per k n
algorithm the class of 57 170 should be
normal so a recap of knn positive
integer k is specified along with a new
sample we select the k entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in python
so let's dive into the predict diabetes
use case so use case predict diabetes
the objective predict whether a person
will be diagnosed with diabetes or not
we have a data set of 768 people who
were or were not diagnosed with diabetes
and let's go ahead and open that file
and just take a look at that data and
this is in a simple spreadsheet format
the data itself is comma separated very
common set of data and it's also a very
common way to get the data and you can
see here we have columns a through i
that's what one two three four five six
seven eight
eight columns with a particular
attribute and then the ninth column
which is the outcome is whether they
have diabetes as a data scientist the
first thing you should be looking at is
insulin well you know if someone has
insulin they have diabetes that's why
they're taking it and that could cause
issue in some of the machine learning
packages but for a very basic setup this
works fine for doing the knn and the
next thing you notice is it didn't take
very much to open it up i can scroll
down to the bottom of the data there's
768.
it's pretty much a small data set you
know at 769 i can easily fit this into
my ram on my computer i can look at it i
can manipulate it and it's not going to
really tax just a regular desktop
computer you don't even need an
enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what ide i'm using
certainly you can use any particular
editor for python but i like to use for
doing uh very basic visual stuff the
anaconda which is great for doing demos
with the jupiter notebook and just a
quick view of the anaconda navigator
which is the new release out there which
is really nice you can see under home i
can choose my application we're going to
be using python36 i have a couple
different versions on this particular
machine if i go under environments i can
create a unique environment for each one
which is nice and there's even a little
button there where i can install
different packages so if i click on that
button and open the terminal i can use a
simple pip install to install different
packages i'm working with let's go ahead
and go back under home and we're going
to launch our notebook and i've already
you know kind of like
the old cooking shows i've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case it's going
to open up chrome because that's my
default that i use and since the script
is pre-done you'll see i have a number
of windows open up at the top the one
we're working in and since we're working
on the k n predict whether a person will
have diabetes or not let's go and put
that title in there
and i'm also going to go up here and
click on cell actually we want to go
ahead and first insert a cell below and
then i'm going to go back up to the top
cell and i'm going to change the cell
type to markdown that means this is not
going to run as python it's a markdown
language so if i run this first one it
comes up in nice big letters which is
kind of nice remind us what we're
working on and by now you should be
familiar with doing all of our imports
we're going to import the pandas as pd
import numpy is np pandas is the pandas
data frame and numpy is a number array
very powerful tools to use in here so we
have our imports so we've brought in our
pandas our numpy our two general python
tools and then you can see over here we
have our trained test split by now you
should be familiar with splitting the
data
we want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data to see
how good it is pre-processing a standard
scalar preprocessor so we don't have a
bias of really large numbers remember in
the data we had like number of
pregnancies isn't going to get very
large where the amount of insulin they
take can get up to 256 so 256 versus 6
that will skew results so we want to go
ahead and change that so they're all
uniform between minus 1 and 1. and then
the actual tool this is the k neighbors
classifier we're going to use
and finally the last three are three
tools to test all about testing our
model how good is it we just put down
test on there and we have our confusion
matrix our f1 score and our accuracy so
we have our two general python modules
we're importing and then we have our six
modules specific from the sk learn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
pd and we'll take a look at the data in
python we looked at it in a simple
spreadsheet but usually i like to also
pull it up so we can see what we're
doing so here's our data set equals
pd.read csv that's a pandas command and
the diabetes folder i just put in the
same folder where my ipython script is
if you put in a different folder you
need the full length on there we can
also do a quick links of the data set
that is a simple python command len for
length we might even let's go ahead and
print that we'll go print and if you do
it on its own line link that data set
the jupyter notebook it'll automatically
print it but when you're in most of your
different setups you want to do the
print in front of there and then we want
to take a look at the actual data set
and since we're in pandas we can simply
do data set head and again let's go
ahead and add the print in there
if you put a bunch of these in a row you
know the data set one head data set two
head it only prints out the last one so
i use i always like to keep the print
statement in there but because most
projects only use one data frame pandas
data frame doing it this way doesn't
really matter the other way works just
fine and you can see when we hit the run
button we have the 768 lines which we
knew and we have our pregnancies it's
automatically given a label on the left
remember the head only shows the first
five lines so we have zero through four
and just a quick look at the data you
can see it matches what we looked at
before we have pregnancy glucose blood
pressure all the way to age and then the
outcome on the end and we're going to do
a couple things in this next step we're
going to create a list of columns where
we can't have zero there's no such thing
as zero skin thickness or zero blood
pressure zero glucose uh any of those
you'd be dead so not a really good
factor if they don't if they have a zero
in there because they didn't have the
data and we'll take a look at that
because we're going to start replacing
that information with a couple of
different things and let's see what that
looks like so first we create a nice
list as you can see we have the values
talked about glucose blood pressure skin
thickness and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
panda tools that will do a lot of this
where we can replace the n a but we're
going to go ahead and do it as a data
set column equals dataset column.replace
this is this is still pandas you can do
a direct there's also one that's that
you look for your nan a lot of different
options in here but the nan in numpy nan
is what that stands for is none it
doesn't exist so the first thing we're
doing here is we're replacing the zero
with a numpy none there's no data there
that's what that says that's what this
is saying right here so put the zero in
and we're going to play zeros with no
data so if it's a zero that means the
person's well hopefully not dead hope
they just didn't get the data the next
thing we want to do is we're going to
create the mean which is the integer
from the data set from the column dot
mean where we skip n a's we can do that
that is a pandas command there the skip
n a so we're going to figure out the
mean of that data set and then we're
going to take that data set column and
we're going to replace all the n p n a n
with the means why did we do that and we
could have actually just taken this step
and gone right down here and just
replaced zero and skip anything were
except you could actually there's a way
to skip zeros and then just replace all
the zeros but in this case we want to go
ahead and do it this way so you could
see that we're switching this to a
non-existent value and we're going to
create the mean well this is the average
person so if we don't know what it is if
they did not get the data and the data
is missing one of the tricks is you
replace it with the average what is the
most common data for that this way you
can still use the rest of those values
to do your computation and it kind of
just brings that particular value of
those missing values out of the equation
let's go ahead and take this and we'll
go ahead and run it doesn't actually do
anything so we're still preparing our
data if you want to see what that looks
like we don't have anything in the first
few lines so it's not going to show up
but we certainly could look at a row
let's do that let's go into our data set
with printed data set
and let's pick in this case let's just
do
glucose and if i run this this is going
to print all the different glucose
levels going down and we thankfully
don't see anything in here that looks
like missing data at least on the ones
it shows you can see it skipped a bunch
in the middle that's what it does if you
have too many lines in jupiter notebook
it'll skip a few and and go on to the
next in a data set let me go and remove
this and we'll just zero out that
and of course before we do any
processing before proceeding any further
we need to split the data set into our
train and testing data that way we have
something to train it with and something
to test it on and you're going to notice
we did a little something here with the
pandas database code there we go my
drawing tool we've added in this right
here
of the data set and what this says is
that the first one in pandas this is
from the pd pandas it's going to say
within the data set we want to look at
the eye location and it is all rows
that's what that says so we're going to
keep all the rows but we're only looking
at 0 column 0 to 8. remember column
this is actually 0 to 7 it doesn't
include the last one and then we go down
here to y which is our answer and we
want just the last one just column eight
and you can do it this way with this
particular notation and then if you
remember we imported the train test
split that's part of the sk learn right
there and we simply put in our x and our
y
we're going to do random state equals
zero you don't have to necessarily seat
it that's a seed number i think the
default is one when you seed it i'd have
to look that up and then the test size
test size is 0.2 that simply means we're
going to take 20 percent of the data and
put it aside so that we can test it
later that's all that is and again we're
going to run it not very exciting so far
we haven't had any printout other than
to look at the data but that is a lot of
this is prepping this data once you prep
it the actual lines of code are quick
and easy and we're almost there with the
actual writing of our knn we need to go
ahead and do a scale the data if you
remember correctly we're fitting the
data in a standard scalar which means
instead of the data being from you know
five to 303 in one column and the next
column is one to six we're going to set
that all so that all the data is between
minus one and one that's what that
standard scalar does keeps it
standardized and we only want to fit the
scalar with the training set but we want
to make sure the testing set is the x
test going in is also transformed so
it's processing it the same so here we
go with our standard scalar we're going
to call it sc underscore x for the
scalar and we're going to import the
standard scalar into this variable and
then our x train equals sc underscore x
dot fit transform so we're creating the
scalar on the x train variable and then
our x test we're also going to transform
it so
we've trained and transformed the x
train and then the x test
isn't part of that training it isn't
part of that of training the transformer
it just gets transformed that's all it
does and again we're going to and run
this and if you look at this we've now
gone through these steps all three of
them we've taken care of replacing our
zeros for key
columns that shouldn't be zero and we
replace that with the means of those
columns that way that they fit right in
with our data models we've come down
here we split the data so now we have
our test data and our training data and
then we've taken and we scaled the data
so all of our data going in no no we
don't tr we don't train the y part the y
train and y test that never has to be
trained it's only the data going in
that's what we want to train in there
then define the model using k neighbors
classifier and fit the train data in the
model so we do all that data prep and
you can see down here we're only going
to have a couple lines of code where
we're actually building our model and
training it that's one of the cool
things about python and how far we've
come it's such an exciting time to be in
machine learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of y
and we get 768 and if we import math we
do math dot square root let's do y
train there we go it's actually supposed
to be x train
before we do this let's go ahead and do
import math and do math square root
length of y test and when i run that we
get 12.409
i want to show you where this number
comes from we're about to use 12 is an
even number so if you know if you're
ever voting on things remember the
neighbors all vote don't want to have an
even number of neighbors voting so we
want to do something odd and let's just
take one away we'll make it 11. let me
delete this out of here that's one of
the reasons i love jupiter notebook is
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
k neighbors classifier n neighbors equal
11. remember we did 12 minus 1 for 11 so
we have an odd number of neighbors p
equals 2 because we're looking for is it
are they diabetic or not and we're using
the euclidean metric there are other
means of measuring the distance you
could do like square square means values
all kinds of measure this but the
euclidean is the most common one and it
works quite well it's important to
evaluate the model let's use the
confusion matrix to do that and we're
going to use the confusion matrix
wonderful tool and then we'll jump into
the f1 score
and finally accuracy score which is
probably the most commonly used quoted
number when you go into a meeting or
something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion matrix why
test y predict so those are the two
values we're going to put in there and
let me go ahead and run that and print
it out and the way you interpret this is
you have the y predicted which would be
your title up here you could do let's
just do p r e d
predicted across the top
and actual going down actually it's
always hard to to write in here actual
that means that this column here down
the middle that's the important column
and it means that our prediction said 94
and prediction in the actual agreed on
94 and 32. this number here
the 13 and the 15 those are what was
wrong so you could have like three
different if you're looking at this
across three different variables instead
of just two you'd end up with the third
row down here in the column going down
the middle so in the first case we have
the the and i believe the zero has a 94
people who don't have diabetes the
prediction said that 13 of those people
did have diabetes and were at high risk
and the 32 that had diabetes it had
correct
but our prediction said another 15 out
of that 15 it classified as incorrect so
you can see where that classification
comes in and how that works on the
confusion matrix then we're going to go
ahead and print the f1 score let me just
run that and you see we get a 0.69 in
our f1 score
the f1 takes into account both sides of
the balance of false positives where if
we go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're a data
scientist and you're talking to other
data scientists they're going to ask you
what the f1 score the f score is if
you're talking to the general public or
the decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the f1 score but the f1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82 percent not
too bad for a quick flash look at
people's different statistics in running
an sk learn and running the k n n the k
nearest neighbor on it so we have
created a model using knn which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accuracy score we got the
0.818 was pretty close to what we got
and we can pretty much round that off
and just say we have an accuracy of 80
tells us it is a pretty fair fit in the
model so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it looked like this where
we have two sets of data but one of them
occurs in the middle of another set you
can see here where we have the blue and
the yellow and then blue again on the
other side of our data line in this data
set we can't use a hyperplane so when
you see data like this it's necessary to
move away from a 1d view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1d input and
transfer it to a two-dimensional output
as you can see in this picture here the
1d when transferred to a 2-dimensional
makes it very easy to draw a line
between the two data sets what if we
make it even more complicated how do we
perform an svm for this type of data set
here you can see we have a two
dimensional data set where the data is
in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2d to a 3d
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support vector machine we'll
start with high dimensional input space
or sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words and documents so we can run our
machine learning algorithms over them
i've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the realization
parameter or lambda is a parameter that
helps figure out whether we're going to
have a bias or overfitting of the data
whether it's going to be overfitted to a
very specific instance or is going to be
biased to a high or low value with the
svm it naturally avoids the overfitting
and bias problems that we see in many
other algorithms these three advantages
of the support vector machine make it a
very powerful tool to add to your
repertoire of machine learning tools now
we did promise you a used case study
we're actually going to dive into some
python programming and so we're going to
go into a problem statement and start
off with the zoo so in the zoo example
we have family members going to the zoo
we have the young child going dead is
that a group of crocodiles or alligators
well that's hard to differentiate and
zoos are a great place to start looking
at science and understanding how things
work especially as a young child and so
we can see the parents sitting here
thinking well what is the difference
between a crocodile and an alligator
well one crocodiles are larger in size
alligators are smaller in size snout
width the crocodiles have a narrow snout
and alligators have a wider snout and of
course in the modern day and age the
father is sitting here thinking how can
i turn this into a lesson for my son and
he goes let a support vector machine
segregate the two groups i don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script
so here we arrive in our actual coding
and i'm going to move this into a python
editor in just a moment but let's talk a
little bit about what we're going to
cover first we're going to cover in the
code the setup how to actually create
our svm and you're going to find that
there's only two lines of code that
actually create it and the rest of it is
done so quick and fast that it's all
here in the first page and we'll show
you what that looks like as far as our
data because we're going to create some
data i talked about creating data just a
minute ago and so we'll get into the
creating data here and you'll see this
nice correction of our two blobs and
we'll go through that in just a second
and then the second part is we're going
to take this and we're going to bump it
up a notch we're going to show you what
it looks like behind the scenes but
let's start with actually creating our
setup i like to use the anaconda jupiter
notebook because it's very easy to use
but you can use any of your favorite
python editors or setups and go in there
but let's go ahead and switch over there
and see what that looks like so here we
are in the anaconda
python notebook or anaconda jupiter
notebook with python we're using python
3. i believe this is 3.5 but it should
be work in any of your 3x versions and
you'd have to look at the sklearn and
make sure if you're using a 2x version
an earlier version let's go and put our
code in there and one of the things i
like about the jupiter notebook is i go
up to view and i'm going to go ahead and
toggle the line numbers on to make it a
little bit easier to talk about and we
can even increase the size because this
is edited in this case i'm using google
chrome explorer and that's how it opens
up for the editor although anyone any
like i said any editor will work now the
first step is going to be our imports
and we're going to import four different
parts the first two i want you to look
at are line one and line two are numpy
as np and matplot library dot pi plot as
plt now these are very standardized
imports when you're doing work the first
one is the numbers python we need that
because part of the platform we're using
uses that for the numpy array and i'll
talk about that in a minute so you can
understand why we want to use a numpy
array versus the standard python array
and normally it's pretty standard setup
to use np for numpy the map plot library
is how we're going to view our data so
this says you do need the np for the sk
learn module but the map plot library is
purely for our use for visualization and
so you really don't need that for the
svm but we're gonna put it there so you
have a nice visual aid and we can show
you what it looks like that's really
important at the end when you finish
everything so you have a nice display
for everybody to look at and then
finally we're gonna i'm gonna jump one
ahead to line number four that's the
sklearn.dataset
import make blobs and i told you that we
were going to make up data and this is a
tool that's in the sk learning to make
up data i personally don't want to go to
the zoo get in trouble for jumping over
the fence and probably get eaten by the
crocodiles or alligators as i work on
measuring their snouts and width and
length
instead we're just going to make up some
data and that's what that make blobs is
it's a wonderful tool if you're ready to
test your setup and you're not sure
about what data you're going to put in
there you can create this blob and it
makes it real easy to use and finally we
have our actual svm the sklearn import
svm on line three so that covers all our
imports we're going to create remember i
used to make blobs to create data and
we're going to create a capital x and a
lowercase y equals make blobs in samples
equals 40. so we're going to make 40
lines of data it's going to have two
centers with a random state equals 20.
so each
each group is going to have 20 different
pieces of data in it and the way that
looks is that we'll have under x
an x y plane so i have two numbers under
x and y will be 0 1 that's the two
different centers so we have yes or no
in this case alligator or crocodile
that's what that represents and then i
told you that the actual sk learner the
svm is in two lines of code and we see
it right here with clf equals svm dot
svc kernel equals linear and i set sql
to 1 although in this example since we
are not regularizing the data because we
want to be very clear and easy to see i
went ahead you can set it to a thousand
a lot of times when you're not doing
that but for this thing linear because
it's a very simple linear example we
only have the two dimensions and it'll
be a nice linear hyperplane will be a
nice linear line instead of a full plane
so we're not dealing with a huge amount
of data and then all we have to do is do
clf dot fit x comma y and that's it clf
has been created and then we're going to
go ahead and display it and i'm going to
talk about this display here in just a
second but let me go ahead and run this
code and this is what we've done is
we've created two blobs you'll see the
blue on the side and then kind of an
orangish on the other side that's our
two sets of data they represent one
represents crocodiles and one represents
alligators and then we have our
measurements in this case we have like
the width and length of the snout and i
did say i was going to come up here and
talk just a little bit about our plot
and you'll see plt that's what we
imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
i have the capital x and then in
brackets i have a colon comma 0. that's
from numpy if you did that in a regular
array you'll get an error in a python
array you have to have that in a numpy
array it turns out that our make blobs
returns a numpy array and this notation
is great because what it means is the
first part is the colon means we're
going to do all the rows that's all the
data in our blob we created under
capital x and then the second part has a
comma 0. we're only going to take the
first value and then if you notice we do
the same thing but we're going to take
the second value remember we always
start with zero and then one so we have
column zero and column one and you can
look at this as our x y plots the first
one is the x plot and the second one is
the y plot so the first one is on the
bottom 0 2 4 6 8 and 10 and then the
second one x of the one is the 4 5 6 7 8
9 10 going up the left hand side s
equals 30 is just the size of the dots
we can see them it says real tiny dots
and then the c map equals plt.cm.paired
and you'll also see the c equals y
that's the color we're using two colors
zero one and that's why we get the nice
blue and the two different colors for
the alligator and the crocodile now you
can see here that we did this the actual
fit was done in two lines of code a lot
of times will be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case i am giving it a width and
length 3 4 and a width and length 5 6.
and note that i put the data as a set of
brackets and then i have the brackets
inside and the reason i do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case i'm processing two lines and
then i'm just going to print and you'll
see clf dot predict new data so the clf
and the dot predict part is going to
give us an answer and let's see what
that looks like and you'll see 0 1. so
predicted the first one the 3 4 is going
to be on the one side and the 5 6 is
going to be on the other side so one
came out as an alligator and one came
out as a crocodile now that's pretty
short explanation for the setup but
really we want to dug in and see what
it's going on behind the scenes and
let's see what that looks like
so the next step is to dig in deep and
find out what's going on behind the
scenes and also put that in a nice
pretty graph
we're going to spend more work on this
and we did actually generating the
original model and you'll see here that
we go through a few steps and i'll move
this over to our editor in just a second
we come in we create our original data
it's exactly identical to the first part
and i'll explain why we redid that and
show you how not to redo that and then
we're going to go in there and add in
those lines we're going to see what
those lines look like and how to set
those up and finally we're going to plot
all that on here and show it and you'll
get a nice graph with the what we saw
earlier when we were going through the
theory behind this where it shows the
support vectors and the hyperplane and
those are done where you can see the
support vectors as the dashed lines and
the solid line which is the hyperplane
let's get that into our jupiter notebook
before i scroll down to a new line i
want you to notice line 13 has plot show
and we're going to talk about that here
in just a second but let's scroll down
to a new line down here and i'm going to
paste that code in and you'll see that
the plot show has moved down below let's
scroll up a little bit and if you look
at the top here of our new section one
two three
and four is the same code we had before
and let's go back up here and take a
look at that we're going to fit the
values on our svm and then we're going
to plot scatter it and then we're going
to do a plot show so you should be
asking why are we redoing the same code
well when you do the plot show that
blanks out what's in the plot so once
i've done this plot show i have to
reload that data now we could do this
simply by removing it up here re-running
it and then coming down here and then we
wouldn't have to rerun these first four
lines of code now in this it doesn't
matter too much and you'll see the plot
show was down here and then removed
right there on line five i'll go ahead
and just delete that out of there
because we don't want to blank out our
screen we want to move on to the next
setup so we can go ahead and just skip
the first four lines because we did that
before and let's take a look at the ax
equals plt.gca
now right now we're actually spending a
lot of time just graphing that's all
we're doing here okay so this is how we
display a nice graph with our results in
our data ax is very standard used
variable when you talk about plt and
it's just setting it to that axis the
last axis in the plt it can get very
confusing if you're working with many
different layers of data on the same
graph and this makes it very easy to
reference the ax so this reference is
looking at the plt that we created and
we already mapped out our two blobs on
and then we want to know the limits so
we want to know how big the graph is we
can find out the x limit and the y limit
simply with the get x limit and get y
limit commands which is part of our met
plot library and then we're going to
create a grid and you'll see down here
we have we've set the variable xx equal
to np.line space x limit 0 x limit 1
comma 30 and we've done the same thing
for the y space and then we're going to
go in here and we create a mesh grid and
this is a numpy command so we're back to
our numbers python let's go through what
these numpy commands mean with the line
space in the mesh grid we've taken x x
small x x equals np line space and we
have our x limit 0 and our x limit 1 and
we're going to create 30 points on it
and we're going to do the same thing for
the y axis now this has nothing to do
with our evaluation it's all we're doing
is we're creating a grid of data and so
we're creating a set of points between 0
and the x limit we're creating 30 points
and the same thing with the y and then
the mesh grid loops those all together
so it forms a nice grid so if we were
going to do this say between the limit 0
and 10 and do 10 points we would have a
0 0 1 1 0 1 0 2 0 3 0 4 to 10 and so on
you can just imagine a point at each
corner one of those boxes and the mesh
grid combines them all so we take the yy
and the xx we created and creates the
full grid and we've set that grid into
the yy coordinates and the x coordinates
now remember we're working with numbi
and python we like to separate those we
like to have instead of it being x comma
1 you know x comma y
and then x2 comma y2 and this in the
next set of data it would be a column of
x's and a column of y's and that's what
we have here is we have a column of y's
and we put it as a capital yy and a
column of x's capital x with all those
different points being listed and
finally we get down to the numpy v stack
just as we created those in the mesh
grid we're now going to put them all
into one array x y array now that we've
created the stack of data points we're
going to do something interesting here
we're going to create a value z and the
z equals the clf that's our that's our
support vector machine we created and
we've already trained and we have a dot
decision function we're going to put the
x y in there so here we have all this
data we're going to put that x y in
there that data and we're going to
reshape it and you'll see that we have
the x x dot shape in here this literally
takes the xx resets it up connected to
the y and the z value lets us know
whether it is the left hand side is
going to generate three different values
the z value does and it'll tell us
whether that data is a support vector to
the left the hyperplane in the middle or
the support vector to the right so it
generates three different values for
each of those points and those points
have been reshaped so they're right on a
line on those three different lines so
we've set all of our data up we've
labeled it to three different areas and
we've reshaped it and we've just taken
30 points in each direction if you do
the math you have 30 times 30 so it's
900 points of data and we separated
between the three lines and reshaped it
to fit those three lines we can then go
back to our map plot library we've
created the ax and we're going to create
a contour and you'll see here we have
contour capital xx capital yy these have
been reshaped to fit those lines z is
the labels so now we have the three
different points with the labels in
there and we can set the colors equals k
and i told you we had three different
labels but we have three levels of data
the alphas just makes it kind of
see-through so it's only 0.5 of the
value in there so when we graph it the
data will show up from behind it
wherever the lines go and finally the
line styles this is where we set the two
support vectors to be dash dash lines
and then a single one is just a straight
line that's what all that setup does and
then finally we take our ax dot scatter
we're going to go ahead and plot the
support vectors but we've programmed it
in there so that they look nice like the
dash dash line and the dashed line on
that grid and you can see here when we
do the clf dot support vectors we are
looking at column zero and column one
and then again we have the s equals one
hundred so we're gonna make them larger
and the line width equals one face
colors equals none let's take a look and
see what that looks like when we show it
and you can see we get down to our end
result it creates a really nice graph we
have our two support vectors and dashed
lines and they have the near data so you
can see those two points or in this case
the four points where those lines nicely
cleave the data and then you have your
hyper plane down the middle which is as
far from the two different points as
possible creating the maximum distance
so you can see that we have our nice
output for the size of the body and the
width of the snout and we've easily
separated the two groups of crocodile
and alligator congratulations you've
done it we've made it of course these
are pretend data for our crocodiles and
alligators but this hands-on example
will help you to encounter any support
vector machine projects in the future
and you can see how easy they are to set
up and look at in depth regularization
in machine learning
so our agenda on this one is fitting the
data
understanding linear regression
bias and variance
what is overfitting
what is underfitting and those are like
the biggest things right now in data
science is overfitting and underfitting
what does that mean
and what is regularization and then
we'll do a quick hands-on demo to take a
look at this
so fitting the data let's start with
fitting the data and we talk about
what is data fitting it's a process of
plotting a series of data points and
drawing the best fit line to understand
the relationship between the variables
and this is what we called data fitting
and you can see here we have a couple of
lines we've drawn on this graph we're
going to go in a little deeper on there
so we might have in this case just the
two dimensions we have an efficiency of
the car and we have the distance
traveled in 1000 kilometers
and so what is data fitting well it's a
linear relationship and a linear
relationship
very specifically linear means line uh
the line used to represent the
relationship is a straight line that
passes through the data points and the
variables have linear relationship
linear regression
so let's start with uh how linear
regression works a linear regression
finds a line that best fits the data
point and gives a relationship between
the two variables
and so you can see here we have the
efficiency of the car
versus the distance traveled and you can
see this nice straight line drawn
through there
and when you talk about multiple
variables all you're doing is putting
this instead of a line it now becomes a
plane
it gets a little more complicated with
multiple variables but they all come
down to this linear kind of drawing a
line through your data and finding what
fits the the data the best
and so we can consider an example uh
let's say that we want to find the
relationship between the temperature
outside versus the sales of ice cream
and so we start looking at that we're
looking at the how many ice cream cones
we're selling or how much money we sold
in ice cream and we're looking at how
warm it is outside which would hopefully
draw a lot of people into the ice cream
store
and suppose we have two lines we're
going to draw l1 and l2 and we're going
to kind of guess which one we think is
the best fit
and which claim to describe the
relationship between the variables
and so first we find the square of the
distance between the line l1
and each data point and add them all and
find the mean distance
and i want you to think about that when
we
square something if it's a negative or
positive number it no longer matters
because a minus 2 squared is 4 2 squared
is 4. so we're removing what side of the
line is on and we're just looking for
the error in this case the mean distance
of each of the little dotted lines you
see here
this way of calculating the square of
the distance adding them and then taking
the mean is called mean square error or
loss function
and we talk about loss how far off are
we that's what we're really talking
about what did we miss when we have a
positive distance and a negative
distance and of course when we square it
it is neither it just becomes a positive
error
and so we take the mean
square error and a lot of times you'll
see it referred to as mse
if i look in the code and i'm going
through my python code and i see mse i
know that's a mean squared error
and we take all the dotted lines and we
calculate this error we add them all
together and then we average it or find
the means
and in this case
they ran a demo on this and it was uh
1127.27
for our l1 line
now we find the loss function for line
l2 in a similar fashion and we get the
mean square error to be 6397
and it's computed the same way so maybe
you put this line just way outside the
data range and this is the error you get
by analyzing our results we find that
the loss function or the mean square
error is less for l1 than l2 hence l1 is
the best fit line
this process describes a lot of machine
learning processes it was we're going to
keep guessing and get as close as we can
to find the right answer we have to have
some way to invite to calculate this and
figure out which one's the best and the
mean square error is one of the better
fits to doing for doing this and most
commonly used
we really want to talk about bias and
variance very important terms to know in
machine learning
and with linear regression
so bias
bias occurs when an algorithm has
limited flexibility to learn from data
variance defines the algorithm's
sensitivity to specifics sets of data
let's start with bias and variance you
can see here we have the two different
setups
bias you can think is very generalized
where variance is very specific
and so we talk about bias such models
pay very little attention to the
training data and over simplify the
model
therefore the validation error or
prediction error and training error
follow similar trends
and uh with bias if you over simplify it
so much you're going to miss
your
local
if if you have like a really
good fit you're going to miss it you're
going to just kind of
guess what the average is and that's
what your answer is going to be
with variance a model with a high
variance pays a lot of attention to
training data and does not generalize
therefore the validation error or
prediction error are far apart from each
other
such models always lead to a high error
on training and test data as a bias does
where variants such models usually
perform very well on training data but
have high error rates on test data
and i want you to think about this
when we're talking about a bias
the error is going to be high both when
you're training it and you're testing it
why because we're just kind of
getting an average we're not really
fitting it close
with variance we're fitting it so close
that the test data does really good it's
going to nail it every time if you're
doing categorical testing that's a car
that's a truck that's a bicycle
but with variants
suddenly a
truck has to have certain features
and it might have to be red
because you had so many red pictures so
if it has if it's an 18 wheeler it has
to be red if it's blue then it has to be
a bicycle
that's the kind of variance we're
talking about where it picks up on
something and it cannot get the right
answer unless it gets a very specific
data
and we see that so that as you're
testing it your models
and you programmed it you got to look
for how i trained it what is coming out
and if it's not if it's not looking good
on either bias or if it's not looking
good on the training
or on the test data
then your bias then your bias in your
data
if it really looks good on the training
data then that's going to be your
variance you've over fitted the data
and those are very important things to
know when you are building your models
in regression of any kind or any kind of
setup for predicting
so in dark games if all the data found a
particular pointer this can be
considered as a biased throw and the
player aims for the particular score
for variance if all the darts fall on
different pointers and no two darts fall
on the same pointer then this can be
considered as a varied throw and the
player aims for various scores
again the bias sums everything up in one
point kind of averages it together where
the variance really looks for the
individual
predictions coming out
so let's go ahead and talk about
overfitting
when we talk about overfitting it's a
scenario where the machine learning
model tries to learn from the details
along with the noise and the data tries
to fit each data point on the curve
you can see that
um
if you plug in your coordinates you're
just going to get the whatever it's
fitted every point on the data stream
there's no average there's no
two points that might have that you know
y might have two different answers
because
if the wind blows a certain way um in
the efficiency of your car maybe you
have a headwind so your car might alter
how efficient it is as it goes and so
there's going to be this variance on
here and this says no you can't have any
variance with you know the this is it's
going to be exactly this it can't be any
you can't be the same speed or the same
car and have a slightly different
efficiency
so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the prediction
so you'll get like a really high error
on here
and so uh reasons for overfitting
data used for training is not cleaned
and contains noise garbage values in it
you can spend so much time cleaning your
data
and it's so important it's so important
that if you have if you have some kind
of
something wrong with the data coming in
it needs to be addressed whether it's a
source of the data maybe they use in
medical different measuring tools
so you now have to adjust for data that
came in from hospital a versus hospital
b or even off of machine a and machine b
is testing something and those those
numbers are coming in wrong
the model has a high variance
again wind is a good example i was
talking about that with the car
you may have a hundred tests but because
the wind's blowing it's all over the
place
size of training data used is not enough
so a small amount of data is going to
also cause this problem you only have a
few points and you try to plot
everything
the model is too complex
this comes up a lot
we put too many pieces together and how
they interact can't even be tracked
and so you have to go back back break it
up and find out actually what correlates
and what doesn't
so
what is underfitting a scenario where
machine learning models
can either learn
the relationship between the data points
nor predict
or classify a new data point and you can
see here we have our efficiency of our
car and our line drawn and it's just
going to be way off for both the
training and the predicting data
as the model doesn't fully learn the
patterns it accepts every new data point
during the prediction
so instead of looking for a general
pattern we just kind of accept
everything
data used for training is not cleaned
and contains noise garbage and values
again under fitting and overfitting same
issue
you got to clean your data
the model has a high bias
we've seen this in all kinds of things
from
[Music]
uh
the mod the most common is the driving
cars to facial identification or
whatever it is the model itself when
they build it might have a bias towards
one thing and this would be an
underfitted model would have that bias
because it's averaged it out so if you
have five people from india and 10
people from
africa and 20 people from the us you
created a bias
because it's looking at the 20 people
and you only have a small amount of data
to work with
size of training data used is not enough
that goes with the size i was just
talking about
so we have a model with a high bias we
have size of training data used is not
enough the model is too simple
again this is one straight line through
all the data when it needs a slight
shift to it for other reasons
so what is a good fit
uh a linear curve that best fits the
data is neither overfitting or
underfitting models but is just right
and of course we have the nice examples
here where we have overfitting
lines going up and down every point is
trying to be include gluted underfitting
the line really is off from where the
data is and then a good fit is got to
get rid of that minimize that
error coming through regularization is
taking the guesswork out you're looking
at this graph and you're going oh which
one is that really over fit or is that
under fit that's pretty hard to tell
so we talk about regularization
regularization techniques are used to
calibrate the linear regression models
and to minimize the adjusted loss
function and prevent overfitting or
underfitting
so what that means uh in this case we're
going to go ahead and take a look at a
couple different things
we're going to look at regularization
which we'll start with a linear model
we'll look at the ridge regularization
and the lasso regularization
and these models are just like just like
we did the
mlp the multi-layered positron in the sk
learn module you could bring in the
ridge module and you can bring in the
lasso module
so when we talk about
ridge regression it modifies the
overfitted or underfitted models by
adding the penalty equivalent to the sum
of the squares of the magnitude of the
coefficients
and so we have a cost function equals
loss equals
lambda times the sum of w
squared or the absolute value of w
depending on how you're doing it now
remember we talked about error whether
we either square it or we absolute value
it
because that removes a plus or minus
sign on there and there's reasons to do
it either way but it is more common to
square the value
and then we have our um
in this case the lambda is going to be
the penalty for the errors we've thrown
in a greek character for you just to
confuse everybody
and w is the slope of the curve of the
line
so uh we're going to look at this and
we're going to draw a line this is going
to be like a linear regression model so
if you had in sklearn you could import
just a standard linear regression model
it would plot this line across whatever
data we're working on
and we look at this of course we're just
extrapolating this i know they use some
specific data but you don't want to get
into the actual domain
and so for a linear regression line
let's consider two points that are on
the line
and we'll go ahead and have a loss
equals zero
uh considering the two points on the
line we'll go ahead and do lambda equals
one we'll set our w is going to be one
point four
then the cost function equals zero plus
one times one point four squared which
equals 1.96
so
really don't get caught up too much in
the math on this other than
understanding
that this is something that's very easy
for a computer to calculate and if you
ever see the loss plus the plus the
lambda times w the sum of w squared
and then let's say we have a ridge
regression line and it does this we go
ahead and plot it and we do the
calculations on the data and for the
ridge regression let's assume a loss
equals 0.3 squared plus 0.2 squared
equals 0.13 so when they put all the
calculations through
of the two points we end up with the
0.0.62
so we've now had a linear regression
model we now had a ridge regression
model
and the ridge regression model plots a
little differently than the standard
linear regression model
and comparing the two models with all
the data points we can see that the
ridge regression line fits the model
more accurately than the linear
regression line
and i find this true on a lot of data i
work with i'll end up using either the
ridge regression model or the lasso mars
regression model for fitting especially
dealing with a lot of like stock markets
daily
setup they come out slightly better you
get a slightly better
fit
and so we have our lasso we just talked
about lasso coming in here
and the cost function equals
instead of doing a squared we're just
going to do the absolute value and so if
you remember this is where ridge
regression changes where's my ridge
regression model
we're squaring the value here
and if you look at this we're not
squaring the value we're just finding
the absolute value on here and so the
loss of this of the squared individuals
and here is our
lambda symbol again penalty for errors
and w equals the slope of the curve
and comparing the two models with all
the data points we can see that the
lasso regression line fits the model
more accurately than the linear
regression line
and this is like i said i use these two
models a lot uh the ridge and this is
important this is
this is kind of the meat of the matter
how do you know which one to use some of
it is you just do it a bunch of times
and then you figure it out
ridge regularization is useful when we
have many variables with relatively
smaller data samples
the model does not encourage convergence
towards zero but is likely to make them
closer to zero and prevent overfitting
the lasser regularization model is
preferred when we are fitting a linear
model with fewer variables
so in the la in the iris thing we had
four or five variables as you measure
the different leaf pieces uh you might
be doing the measurements on the cancer
project which has 36 different variables
so as we get down to the iris with four
variables
lasso lar will probably uh work pretty
good where you might use the ridge
regularization with more model with if
you have something significant larger
and it encourages the coefficients of
the variables to go towards zero because
of the shape of the constraint which is
an absolute value
and with any of this we want to go ahead
and
do a demo and lasso and ridge regression
so let's take a look and see what that
looks like in our code and bring up our
jupiter notebook
we'll start with our imports pandas is
pd import numpy is np import matplot
library as plt
sklearn we're going to import our data
sets it's kind of more generic
we usually just import one data set
instead of all of them but you know
quick and dirty when you're putting some
of these together
we have our sklearn model selection
we're going to import our train test
split for splitting our data up
and then we'll bring in our linear
regression model
and we'll go ahead and run these just to
load them up
and then load our data set we're just
talking about that
you could just
have imported the load boston and boston
data set in there instead of loading all
the data sets
and then once we've loaded our data set
we want to go ahead and take a look at
that data and see what we got here
let me just go and pop that down there
and
go and run it
and so we've gone ahead and taken our uh
boston uh data we're gonna look we put
it into our pandas data frame
um the boston data set and then the
boston columns we want to see what's
going on with them
we have our target
we have the house price
etc and so our x equals boston
eye location
now remember in pandas the new updates
to pandas they want eye location if
you're going to pull data we used to be
able to leave this off
but it does something different it
creates a slice versus a direct
setup so make sure using that eye
location and the i the output so this is
all just bringing our data together
and we can see here if we do we print
the boston
panda's head
we can see here all of our different
aspects we're looking for
and if you're following the x and the y
the x is
everything
except for the last column
where y is uh all the it's that's what
this means all the rows except for the
last column and then y is all the rows
but just the last column so y is our
house price
and the x is the
crimsian industry chas knox and all
these other different
statistics they've collected for house
sales in boston
there we go oops control
so we'll go ahead and split our data x
train and our x
test y train y test equals the train
test split which we imported
and we have our boston you could have
easily used the x and y on here as
opposed to boston
eye location
and we'll create our test size we're
going to take 25 of the data and put it
in as a test
and then we'll go ahead and run this
need an extra drink there
uh so we have our train and test and
then of course the print the train data
shape
i love doing this kind of thing whenever
i'm working with this data print out the
shape
make sure everything looks correct uh so
that we have 127 by 13 and 127 by one
379 by 13 they should match and if
they're if the the data sets are not
quite matching
then you know something's wrong and
you're going to get all those errors i
don't know how many times i've gone
through here and
it's dropped a row on one of them and
not on the other or something weird has
happened when i'm cleaning the data
this is pretty straightforward and
simple because the data comes in a nice
pre-package is all clean for you
so let's go ahead and apply apply the
multiple linear regression model
and we'll call this lreg reg linear
regression and we're going to go ahead
and fit that linear regression model to
x train and y train
then we'll generate the prediction on
the test set
so here's our l reg y predict
with our x test going into the
prediction
and let's calculate that mean square
error mse i told you you'll see mse used
a lot
people use it in variables and things
like that it's pretty common
and we get our mean squared error equals
this is just the basic formula we've
already been talking about what's the
difference squared
and then we look for the average of that
we'll go ahead and just run this
and you can see when we get through the
end of this we have our mean square
error on test
we have our total and then we have each
column coming down
and at this point unless you really know
the data you're working with it's not
going to mean a whole lot so if it's in
your domain you might be know what
you're looking at when you see these
kinds of numbers coming up
but if it's not it's just a bunch of
numbers and that's okay
at least that's okay for this demo uh
and then we're gonna go ahead and plot
these so we can see what's going on
and this is always kind of fun it's
always nice to have a nice visual of
what you're looking at and you can see
here
when we plot the coefficient scores on
here and we
the guys in the back did a great job
putting some pretty colors together
making it look nice and setting up the
columns
you can see here
uh your nox has like just a huge
coefficient
when i look at a table like this i look
for what has very little
different coefficients they're not using
a huge change and what has huge changes
and that flags you for all kinds of
things as you're working with the data
but it depends so much on the domain
you're working with
these are great things though as just a
quick look to see what's going on with
your data and what you're looking for
and of course once we look at this now
our motive is to reduce the coefficient
score so now we want to take these and
and
bring them down as much as we can
and for that we're going to work with
the ridge regression on here
so let's start by going we're going to
import our ridge model the red
regression from the sk learn library or
the scikit
and we're going to go ahead and train
the model so here's our ridge r equals
alpha equals 1. and i mentioned that
earlier
when i work with the ridge model
you'll see alpha equals one if you set
alpha equal to zero that's a standard
linear regression model so you have
alpha equals one two three four and you
usually use one two or three four and a
standard integer on there and we'll go
ahead and fit the ridge model on there
with our x train and our y train data
generate a prediction for that
for our x test
and we'll calculate the mean square
error
just like we did before this should all
look familiar
and we'll go ahead and print that out
and we'll look at the ridge coefficients
for our data and see what that looks
like
now
if i jump up and down between these two
you'll get a headache
[Laughter]
you'll still see the knox value let's
just look at the knocks because that was
the biggest value it's a minus nine here
and if we go back up here the nox value
is a minus 18. so right off the bat i'm
seeing a huge change
in the biggest coefficient there
uh so if we're going to do that nice
setup we want to go ahead and just print
it and see what that looks like
here we go and we've
set up our plot subplots and again the
team put together some nice colors so it
makes it look good
we're doing an x bar based on the
columns
and our
l regress coefficients color equals
color x spine bottom and so forth
so just put together a nice little graph
and you're starting to see
one this when you compare this if you
put on the same graph as this one up
here this is up here minus 18
this is at minus nine
and so this graph is half the size of
the graph above the same thing with
these values here
they might look the same but they're
actually all
almost half the value on here
and then finally you can do the same
thing for the lasso regression
this would all look
very similar as far as what we worked on
before
and i'm just going to print that on here
and run it
and again let's go up to
knox look where nox is it's all the way
down to zero
and if we look at our next biggest
coefficient it's minus 0.8 and really
here's our
22.73
let me go up here
um
16.7
and we go up here and we look at the
same number
uh 16.69
and so we look at this uh if i was
running this and doing uh working on a
project with this i would look at these
numbers i start with the 16.69
come down here and compare it to
uh
16.78 6 9 is better than 7 8.
so from the very beginning we might
start looking at this first model for
overall
predicting
but there's other factors involved we
might know that uh the nox value is
central and the other ones aren't quite
as good and so we might start looking at
just certain
setups like what is our what is this
particular coefficient because it might
have a certain meaning to us and so
forth and so you look at all those
different
items in there again but the bottom
dollar is our first model did better
than our other two models our mean
square error on the test set
continues to
come down on this
dimensionality reduction dimensionality
reduction refers to the technique that
reduces the number of input variables in
a data set
and so you can see on the table on the
right shows the orders made at an
automobile parts retailer
the retailer sells different automobile
parts from different companies and you
can see we have company b-packs iso max
and they have the item the tire the axle
an order id a price number and a
quantity in order to predict the future
cells
we find out that using correlation
analysis
that we just need three attributes
therefore we have reduced the number of
attributes from five to three
and clearly we don't really care about
the part number i don't think the part
number would have an effect on how many
tires are bought
and even the store who's buying them
probably does not have an effect on that
in this case that's what they've
actually done is remove those and we
just have the item the tire the price
and the quantity one of the things you
should be taking away from this is in
the scheme of things
we are in the descriptive phase we're
describing the data
and we're pre-processing the data what
can we do to clean it up
why dimensionality reduction
well number one less dimensions for a
given data set means less computation or
training time
that can be really important if you're
trying a number of different models
and you're re-running them over and over
again and even if you have seven
gigabytes of data that can start taking
days to go through all those different
models
so this is huge this is probably the
hugest part as far as reducing
our data set
redundancy is removed after removing
similar entries from the data set
again pre-processing some of our models
like a neural network if you put in two
of the same data it might give them a
higher weight than they would if it was
just once we want to get rid of that
redundancy
it also increases the processing time if
you have multiple
data coming in
space required to store the data is
reduced
so if we're committing this into a big
data
pool we might not send the company that
bought it why would we want to store two
whole extra columns when we added into
that pool of data
makes the data easy for plotting in 2d
and 3d plots this is my favorite part
very important you're in your
shareholder meeting
you want to be able to give them a
really good
clear
and simplified version you want to
reduce it down to something people can
take in
it helps to find out the most
significant features and skip the rest
which also comes in in post scribing uh
leads to better human interpretation
that kind of goes with number four it
makes data easy for plotting you have a
better interpretation when we're looking
at it principal component analysis
so what is it
principal component analysis is a
technique for reducing the
dimensionality of data sets increasing
interpretability but at the same time
minimizing information loss so we take
some very complex data set with lots of
variables we run it through the pca
we reduce the variables we end up with a
reduced variable setup
this is very confusing to look at
because
if you look at the end result
we have the different colors all lined
up so what we're going to take a look at
is let's say we have a picture here
let's say you are asked to take a
picture of some toddlers and you are
deciding which angle would be the best
to take the picture from so if we come
up here we look at this we say okay this
is you know one angle
we get the back of a lot of heads not
many faces
so we'll do it from here we might get
the one person up front smiling a lot of
the people in the class are missing so
we have a huge amount off to the right
of blank space maybe from up here again
we have the back of someone's head
and it turns out that the best angle to
click the picture from might be this
bottom left angle you look at it you say
hey that makes sense it's a good
configuration of all the people in the
picture now when we're talking about
data it's not
you really can't do it by what you think
is going to be the best we need to have
some kind of mathematical
formula so it's consistent and so it
makes sense in the back end
one of the projects i worked on many
years ago
has something similar to the iris if
you've ever done the iris data sets
probably one of the most common ones out
there where they have the flower
and they're measuring the stamen
in the petals and they have width and
they have length of the petal
instead of putting through the width and
the length of the petal we could just as
easily do the
width-to-length ratio we can divide the
width by the length and you get a single
number where you had two
that's the kind of idea that's going on
into this in pre-processing and looking
at what we can do to bring the data down
the very simplified example on my
iris
pedal example
when we look at the similarity in pca we
find the best picture or projection of
the data points
and so we look down at from one angle
we've drawn a line down there
we can see these data points based on in
this case just two variables now keep in
mind we're usually talking about 36 40
variables almost all of your
business models usually have about 26 to
27 different variables they're looking
at
same thing with like a bank loan model
we're talking 26 to 36 different
variables they're looking at that are
going in
so we want to do is we want to find the
best view in this case we're just
looking at the x y
we look down at it and we have our
second
idea pc2 and again we're looking at the
x i this x y this time from a different
direction
here for our ease we can consider that
we get two principal components namely
pc1 and pc2
comparing both the principal components
we find the data points are sufficiently
spaced in pc1
so if you look at what we got here we
have pc1 you can see along the line how
the data points are spaced versus the
spacing in pc2 and that's what they're
coming up with what is going to give us
the best look for these data points when
we combine them and we're looking at
them from just a single angle
whereas in pc2 they are less spaced
which makes the observation and further
calculations much more difficult
therefore we accept the pc1 and not the
pc2 as the data points are more spaced
now obviously the back end calculations
are a little bit more complicated when
we get into the math of how they decide
what is more valuable
this gives you an idea though that when
we're talking about this we're talking
about the perspective
which would help in understanding how
pca analysis works
we want to go ahead and do is dive into
the important terminologies under pca
and important terminologies are views
the perspective through which data
points are observed
and so you'll hear that if someone's
talking about a pca presentation and
they're not taking the time to reduce it
to something that the average person
shareholders can understand you might
hear them refer to it as the different
views what view are we taking
dimension number of columns in a data
set are called the dimensions of that
data set
and we talked about you'll hear features
dimensions
i was talking about features there's
usually when you're running a business
you're talking 25 26 27 different
features minimal and then you have the
principal component new variables that
are constructed as linear combinations
or mixtures of the initial variables
principal component is very important
it's a combination if you remember my
flower example it would be the
width over the length of the petal as
opposed to putting both width and length
in you just put in the ratio instead
which is a single number versus two
separate numbers projections
the perpendicular distance between the
principal component and the data points
and that goes to that line we had
earlier it's that right angle line of
where those point all those points fall
onto the line
important properties important
properties number of principal
components is always less than or equal
to the number of attributes
that just makes common sense you're not
going to do
10 principal properties with only three
features
you're trying to reduce them so it's
just kind of goofy but it is important
to remember that people will throw
weird code out there and just randomly
do stuff with instead of really thinking
it through principle components are
orthogonal
and this is what we're talking about
that right angle from the line
when we do pc1 we're looking at how
those points fall on to that line
same thing with pc2 we want to make sure
that pc1 does not equal pc2 we don't
want to have the same two principal
points
when we do two points
the priority of principal components
decreases as their numbers
increase
this is important to understand
if you're going to create
one principle
component
everything is summarized into that one
component as we go to two components the
priority
how much it holds value decreases as we
go down so if you have five different
points each one of those points is going
to have less value than just the one
point which has everything summarized in
it
how pca works
i said there was more in the back end we
talk about the math this is what we're
talking about is how does it actually
work
so now we have understanding that you're
looking at a perspective
now we want to see how that math side
works pca performs the following
operations in order to evaluate the
principal components for a given data
set
first we start with the standardization
then we have a covariance matrix
computation
and we use that to generate our i gene
vectors and i gene values
which is the feature vector and if you
remember the i gene vector is like a
translation for
moving the data
from x equals 1 to x equals 2 or
whatever altering it and the i gene
value is the final value that we
generate
when we talk about standardization the
main aim of this step is to standardize
the range of the attributes so that each
one of them lie within similar
boundaries
this process involves removal of the
mean from the variable values and
scaling the data with respect to the
standard deviation
and you can see here we have z equals
the variable values minus the mean
over the standard deviation
the covariance matrix computation
covariance matrix is used to express the
correlation between any two or more
attributes in multi-dimensional data set
the covariance matrix has the entries as
the variance and the covariance of the
tribute values the variance is denoted
by var and the covariance is denoted by
cov
on the right we can see the covariance
matrix for two attributes and their
values
when we do a hands-on and look at the
code we'll do a display of this so you
can see what we're talking about and
what that looks like
for now you can just notice that this is
a matrix that we're generating with the
variance and then the covariance of x to
y
on the right side we can see the
covariance table for more than two
attributes in a multi-dimensional data
set
this is what i was talking about we
usually are looking at not just one
feature two features
we're usually looking at 25 30 features
going on
and so if we do i was set up like this
we should see all those different
features as the different variables
covariance matrix tells us how the two
or more variables are related positive
covariance indicate that the value of
one variable is directly proportional to
the other variable
negative covariance indicate that the
value of one variable is inversely
proportional to the other variable that
is always important to note whenever
we're doing any of these matrixes that
we're going to be looking at that
positive and negative whether it's
inverted or not
and then we have the iogene values and
the i gene vectors
iogene values and hygiene vectors are
the mathematical value
that are extracted from the covariance
table
they are responsible for the generation
of a new set of variables from the old
set of variables which further lead to
the construction of the principal
components
igen vectors do not change directions
after linear transformation
i gene values are the scalars or the
magnitude of the i gene vectors
and again this is just chain
transforming that data so we're going to
change
the vector b
to the b prime as denoted b on the chart
and so when we have like multiple
variables how do we calculate that new
variable
and then we have feature vectors feature
vectors is simply a matrix that has igen
vectors of the components that we decide
to keep as the columns
here we decide whether we must keep or
discard the less significant principal
components that we have generated in the
above steps
this becomes really important as we
start looking at
the back end of this and we'll do this
in the demo
but one of the more important steps to
understand
and so we have the pca example consider
matrix x within rows or observations and
k columns or variables
now for this matrix we would construct a
variable space with as many dimensions
as the variable
but for our simplicity let's consider
this three dimensions for now
now each observation row of the matrix x
is placed in the k dimensional variable
space such that the rows in the data
table form a swarm of points in this
space
now we find the mean of all the
observations and then place it along the
data points on the plot
the first principal component is a line
that best accounts for the shape of the
point swarm it represents the maximum
variance direction in the data
each observation may be projected onto
this line in order to get a coordinate
value along the pc one this value is
known as a score
usually only one principal component is
insufficient to model the systematic
variation for a data set thus a second
principal axis is created
the second principle component is
oriented such that it reflects the
second largest source of variation in
the data while being orthogonal to pc1
pc2 also passes through the average
point
let's go ahead and pull this up and just
see what that means
inside our python scripting
i'm going to use the anaconda navigator
and i will be in python 3.6
for this example
i believe there's even like a 3.9 out
i tend to stay in 3.6 because a lot of
the models i use especially with the
neural networks are stable in three six
and then we open up our
jupiter i'm in chrome and go ahead and
create a new python three
and for ease of use our team in the back
was nice enough to put this together for
me
and we'll go and start with the
libraries the first thing i like to do
whenever i'm looking at
any new setup
well you know what let's do let's do the
libraries first we're going to do our
basic libraries which is matplot library
the plt from the matplot library pandas
our data frame
pd numpy our numbers array np
seaborn for graphing sns that goes with
the plot that actually sits on matplot
library so the seaborn sits on there
and then we have our amber sign because
we're in jupiter notebook map plot
library in line the newer version
actually doesn't require that but i put
it in there either anyway just because
i'm so used to it
and then we want to go ahead and take a
look at the data
and in this case we're going to pull in
certainly you can have lots of fun with
different data but we're going to use
the cancer data set
and one of the reasons a cancer data set
is it has like 36 35 different features
so it's kind of fun to use that as our
base for this and we'll go ahead and run
this and look at our keys
and the first thing we notice in our
keys for the cancer data set
is we have our data we have our target
our frame target names description
feature names and file name
so
what we're looking for in all this
is
let's take a look at the description
let's go in here and pull up the
description on here
i'm not going to spend a huge amount of
time on the description um
because this is we don't want to get
into a medical domain we want to focus
on our pca setup
what's important is you start looking at
what the different attributes are what
they mean
if you were in the medical field you'd
want to note all these different things
whether what they're measuring where
it's coming from
you can actually see the actual
different
measurements they're taking
no missing attributes
we page all the way to the bottom and
you're going to have your data in this
case our target
and if you dig deep enough to the target
let's actually do this
let's go ahead and print target names
real quick here i always like to just
take a look and see what's on the other
end of this
target
names
run that
yeah so the target name is is it
malignant or is it b9
so in other words is this
dangerous growth or is it something we
don't have to worry about that's the
bottom line with the cancer in this case
and then we can go ahead and load our
data
and you know let me go up uh just a
notch here for easy of reading
it's hard to get that just right that's
all you have to do
so let's go ahead and look at our data
uh our we're going to use our pandas
and we're going to go ahead and do our
data frame it's going to equal cancer
data
columns equals cancer feature equals
feature names so remember up here we
already loaded the the
names up of our of the features in there
what is going to come out of this let me
just see if we can get to that
it's at the top of target names
that's just this list of names here in
the setup
and we can go ahead and run this code
and i'll print the head and you can see
here we have the mean radius the mean
texture mean perimeter
i don't know about you this is a
wonderful data set if you're playing
with it because
like many of the data that most of the
data that comes in half the time we
don't even know we're looking at
we're just handed a bunch of stuff as a
data scientist going what the heck is
this
and so this is a good place to start
because this has a number of different
features in there we have no idea what
these feature means or where they come
from we want to just look at the data
and figure that out
and now we actually are getting into the
pca side of it as we've noticed before
is difficult to visualize high
dimensional data
we can use pca to find the first two
principal components and visualize the
data this new two-dimensional space with
a single scatter plot
before we do this we need to go ahead
and scale our data
now
i haven't run this to see if you really
have to scale the data on this
but as just a general
run time i almost do that as the first
step of any modeling even if it's
premodeling as we're doing here
in neural networks that is so important
with pca visualization it's already
going to scale it when we do the means
and deviation inside the pca
but just in case it's always good to
scale it
and then we're going to take our pca
with the site kit learn uses very
similar process to other pre-processing
functions that come with scikit-learn
we instantiate a pca object find the
principal components using the fit
method then apply the rotation and
dimensionality reduction by calling
transform we can also specify how many
components we want to
keep when creating the pca object
and so the code for this
oops getting a little bit ahead
let me go and run this code
uh so the code for this
is
from sklearn decomposition import pca
pca equals pca in components equals two
and that's really important to note that
because we're only going to want to look
at two components
i would never go over four components
especially if you're going to demo this
with somebody else if you're showing
this to the shareholders
the whole idea is to reduce it to
something people can see
and then the pca fit we're going to is
going to take the scaled data that we
generated up here
and then you can see we've created our
pca model with in components equals two
now whenever i use a new tool
i like to go in there and actually see
what i'm using so let's go to the scikit
webpage for the pca
and you can see in here here's our call
statement it describes what all the
different setups you have on there
probably the biggest one to look at
would be
well the biggest one is your components
how many components do you want
which you have to put in there pretty
much
and then you also might look at the svd
solver it's on auto right now but you
can override that and do different
things with it it does a pretty good job
as it is
and if we
go down all the way down to
there we go
to our methods
if you notice we have fit
we have fit transform
nowhere in here is predict because this
is not used for prediction
it's used to look at the data again
we're in the describe setup
we're fitting the data we're taking a
look at it we've already looked at our
minimum maximum we've already looked at
what's in each quarter we've done a full
description of the data this is part of
describing the data
that's the biggest thing i take away
when i come zooming in here and of
course i have examples of it down here
if you forget
and the biggest one of course is the
number of components
and then i mean the rest you can play
with
the actual solver whether you're doing a
full or randomize there's different
things it does pretty good on the auto
and now we can transform this data to
its first two principal components
and so we have our xpca
we're going to set that equal to pca
transform scaled data
so there we go there's our first
transformation
and let's just go ahead and print the
scaled data shape and the xpca data
shape
and the reason we want to do this is
just to show us
what's going on here we've taken 30
features i think i said 36 or something
like that but it's 30
and we've compressed it down to two
features and we decided we wanted two
features and that's where this comes
from
we spawn two features
so let's go ahead and
plot these and take a look and see
what's going on
and
we're just going to use our plt figure
we'll set the figure size on here here's
our scatter plot
xpca
x underscore pca of
of one these are two different
perceptions we're using uh and then
you'll see right here c for color cancer
equals target
and so remember we have zero we have one
and if i remember correctly zero was
malignant one was b9
so everything in the zero column is
going to be one color and the other
color is going to be one and then we're
going to use the plasma map just kind of
telling you what color it is add some
labels first principle component second
principal component and we'll go ahead
and run this
and you can see here instead of having a
chart one of those heat maps with 30
different
columns in it
we can look at this and say hey this one
actually did a pretty good job
of separating the data
and a couple things when i'm looking at
this that i notice is first
we have a very clear area where it's
clumped together
where it's going to be b9
and we have a huge area it's still
clumped together more spread out where
it's going to be malignant or i think i
had that backwards
and then in the middle
because we're dealing with something
in this particular case cancer we would
try to separate i would be exploring how
to separate this middle group out
in other words there's an area where
everything overlaps
and we're not going to have a clear
result on it
just because those are the people you
want to go in there and have extra tests
or treat it differently
versus going in and saying just cutting
into the can into the cancer so the body
absorbs it and it dissipates versus uh
actively going in there removing it
testing it going through chemo and all
the different things that's a big
difference you know as far as what's
going to happen here in that middle line
where the overlap is going to be huge
that's domain specific going back to the
data
we can see here
clearly by using these two components we
can easily separate these two classes
so the next step is what does that mean
interpreting the components
unfortunately with this great power of
dimensionality reduction comes the cost
of not being able to easily understand
what these components represent
i don't know what principle component
one licks work represents or second
principle
the components correspond to
combinations of original features the
components themselves are stored as an
attribute of the filtered pca object and
so we talk look at that we can go ahead
and do look at the pca components this
is in our model we built we've trained
it we can run that and you can see
here's the actual components uh it's the
two components have each have their own
array
and within the array you can see the
what the scores are using
and these actually give weight to what
features are doing what
so in this numpy matrix array each row
represents a principal component and
each column relates back to the original
features
what's really neat about this is we can
now go in reverse
and drop this onto a heat map
and start seeing
what this means and so let me go ahead
and just put this down upside down down
here
we'll go ahead and put this in here
we're going to use our
df comp data frame and we do our pca
components
and i want you to notice how easy this
is uh
we're going to set our columns equal to
cancer feature names
that just makes it really easy
and we're dumping it into a data frame
what's neat about a data frame is when
we get to seaborn it will pull that data
frame apart and
and set it up for us when we want and so
we're just going to do the cn the
seaborn heat map
of our data frame composition and we'll
use the plasma coloring
and it creates a nice little
color graph here
you can see we have the mean radius and
all the different features along the
bottom
on the right we have a scale so we can
see we have the dark colors all the way
to the really light colors which are
what's really shining there this is like
the primary stuff we want to look at
so this heat map and the color bar
basically represent the correlation
between the various features and the
principal component itself
so you know very powerful map to look at
and then you can go in here and we might
notice that the mean radius
look how how on the bottom of the map it
is
on some of this
so you have some interesting
correlations here that change the
variations on that and what means what
this is more when you get to a post
scribe you can also use this to try to
guess as what these things mean and what
you want to change to get a better
result in this session
we will briefly understand what
coronavirus really is and the various
symptoms of coronavirus
then we look at the global impact of
coronavirus in terms of the total cases
the deaths reported the fatality rate
and the tests carried out so far by
different countries after that we will
perform an analysis using svm and
polynomial regression in python to
predict the number of upcoming cases
from the 28th of april to the 17th of
may
the data we have taken is from the 22nd
of january to the 27th of april then we
will analyze the model using charts and
graphs and finally we will discuss the
various safety measures you can take to
ensure you are safe and not attacked by
coronavirus
so what is coronavirus coronavirus or
covad19 is an infectious disease caused
by a newly discovered coronavirus that
is believed to have emerged from a
seafood market in wuhan china during
december 2019.
it is zoonotic
so it's a disease that can be
transmitted from animals to people or
more specifically a disease that
normally exists in animals but that can
infect humans too
the virus that causes covid19 is mainly
transmitted through droplets generated
when an infected person coughs sneezes
or exhales
these droplets are too heavy to hang in
the air and quickly fall on floors or
surfaces
it appears that symptoms show up in
people within 14 days of exposure to the
virus
with that now let's look at the various
symptoms of covid19
a patient with coronavirus
can show generic symptoms such as cough
fever shortness of breath and muscle
pain
they can also have a sore throat
headache and loss of taste or smell
they are also expected to have middle
east respiratory syndrome or mars
and severe acute respiratory syndrome or
sars
middle east respiratory syndrome is a
viral respiratory illness caused by a
coronavirus
it is a contagious sometimes fatal
respiratory illness
it often spreads through close contact
with an infected person
sars is contagious and sometimes fatal
respiratory illness caused by a
coronavirus
it appeared in 2009 in china
it spread worldwide within a few months
although it was quickly contained sars
is a virus transmitted through droplets
that enters the air when someone with
the disease coughs sneezes or talks
now let's look at the impact of
coronavirus worldwide
the maps and the charts that i am going
to show you now have been taken from an
organization called our world in data
they largely focus on problems that the
world faces such as poverty different
diseases hunger climate change
existential crisis and inequality
their main goal is to research and use
data to make progress against the
world's largest problems
the map that you see on your screens
shows the total number of confirmed
coveted 19 cases till the 26th of april
below you can see the color scale
ranging from 0 to 1 million
the countries with the least number of
cases are marked in light orange color
the countries with cases between 5000 to
10 000 cases have been depicted using
orange color
the countries with cases above 1 lakh
are shown using red color
while those above 5 lakhs have a dark
red color
you can see from the map
african nations have fewer cases
compared to nations in asia europe and
america
china has close to 84 000 cases
india has around 26 500 cases as of
april 26th
iran has nearly 89 000 cases
now if you look at australia it has
cases of around 6700
in north america the united states has
the highest number of cases which is
actually the highest throughout the
world
in south america brazil has the maximum
number of cases
the next map shows the confirmed cases
of countries on the asian continent
china iran turkey india and saudi arabia
have the highest number of cases
then we have the map of europe
in europe countries such as italy france
germany the united kingdom spain and
russia have the highest number of cases
moving ahead you can see a graph of
certain selected countries and the total
confirmed cases
i have filtered out the total world
cases than countries like the united
states spain italy germany france china
and india
you will learn how to create a similar
graph when we will do our prediction
analysis
let's now have a look at the total
number of daily confirmed cases
so on the 26th of april the united
states had around 48 500 cases russia
had around 6000 cases brazil had nearly
5 500 cases followed by the united
kingdom with close to 5000 cases
the number of confirmed cases is lower
than the number of total cases
the reason being
limited testing in different countries
across the world
now looking at the total debts reported
across the world on this map the united
states has the highest number with over
53 000 deaths so far followed by italy
france the united kingdom and germany
next you can see a graph of a few
countries and their total death cases
till the 26th of april
you can see the united states spain
italy france china and india here
we will see how to build a similar graph
in our demo
up next on your screens is the map of
different countries and their fatality
rate
fatality rate is actually the ratio
between the confirmed deaths and
confirmed cases
france has the highest fatality rate
with over 18.22
followed by the united kingdom at 13.69
percent and italy at 13.51
moving ahead the next map shows the
total number of covet 19 test numbers
the united states has conducted over 5
million tests so far followed by russia
with over 2.8 million tests germany
turkey canada and india have been
conducting coveted 19 tests in large
numbers to deal with the situation
now each country is dealing with this
pandemic within its own capacity to make
sure the situation doesn't worsen most
affected countries have gone for a
lockdown
but does lockdown work well the research
says yes
on the screens you can see the number of
days some countries have enforced a
partial or full lockdown to curb the
spread of coronavirus
this information has been taken from one
of india's leading media groups india
today data intelligence unit
the orange color represents full
lockdown while the yellow color depicts
partial lockdown
countries like australia india belgium
the united kingdom china the united
states france spain and others already
have announced lockdown for more than 40
days
to make sure the lockdowns don't extend
further and the situation improves
people are being advised to stay at home
and avoid public gatherings
now let's look at where india stands
with its battle for corona virus
as per the press information bureau of
india or pib
here are the figures and the situation
report till 5 pm on the 26th of april
the states that have been affected the
most are maharashtra gujarat delhi
madhya pradesh uttar pradesh and tamil
nadu
maharashtra has over 7600 cases followed
by gujarat with over 3000 cases
maharashtra also has the highest number
of deaths reported which stands at 323
next is gujarat with 133 followed by
madhya pradesh at 99 deaths so far
there are still 312 cases yet to be
assigned to the states which are under
the process of contact tracing
you can see on the top there are more
than 27 000 confirmed cases in india out
of which 20 1777 are active cases
more than 6000 people have recovered
from corona virus while the death tally
stands at 872
the government is doing its best to
control the situation please follow the
rules and guidelines issued by the local
and central authorities this will help
us fight the situation together
effectively
now coming to the most important part of
this session which you guys have been
waiting for a while
the coronavirus outbreak prediction
analysis
we will analyze the outbreak of
coronavirus in the coming days across
various countries worldwide visualize
them using charts and graphs and predict
the number of upcoming cases for the
next 20 days using polynomial regression
and support vector machines model in
python
our data has information from the 22nd
of january to the 28th of april
the data sets we will be using are taken
from the repository operated by the
johns hopkins university center of
system science and engineering
let me show you where you can locate the
data sets
so here is the github repository where
you can find the data sets and a few
other resources
it is being updated on a regular basis
i would encourage you to go through this
link once
now
let me take you to the jupiter notebook
where i have already implemented the
course i'll run through it and explain
each cell of code to make you understand
what it does
so this is my jupiter notebook
first
we will import all the libraries
necessary for this analysis
we are importing the numpy and pandas
that are used for numerical computations
data manipulation and data analysis
then
i'm importing the matplotlib library for
creating data visualizations
next i'm also going to import random
math and time libraries
after that from the sklearn library i am
importing the linear regression model as
well as certain functions such as strain
test split and polynomial features to
build our polynomial regression
svr will help us create the support
vector machines model
we will also use sql one matrix to
import functions such as
mean squared error mean absolute error
and so on
this we will use for calculating the
accuracy of our model
let me now run this cell by hitting
shift plus enter
so you can see we have successfully run
the first cell
now
we will load our datasets
we'll be using three datasets for this
demo
the data sets are present in a github
repository maintained by johns hopkins
university to import the data sets i
have used the pandas library and read
underscore csv function
i provided the url location of the files
followed by the file name and the
extension of the file which is
dot csv
i have loaded these three datasets to
three variable names
now let's run the cells to import all
the three data sets
so first i have imported the confirmed
cases data set
now to display the data set i am using
the dot head function to see the top
five rows from the data
if i scroll down
you can see our confirmed underscore
cases data set
we are displaying the first five rows
from the data set
it has information like province of the
state country or the region the latitude
the longitude
and the date value starting from 22nd of
january to the 27th of
april
now let me go ahead
and import the debts underscore reported
data set
and now to display the data set i have
used the dotted function
so this is how
the depth data set looks like
let me now go ahead
and import the recovered underscore
cases data set
and now to display the head of the data
set
so here is how the recovered
data set looks like or the recoveries
data set looks like
finally i load the data set which is all
the latest updates from across the globe
regarding confirmed cases deaths
recovered and active cases
let's see the head of this latest data
set
as you can see it has information about
the province or the state
country or the region
when it was last updated
the latitude and the longitude of the
location
and it has data regarding the confirmed
cases the deaths reported
the recovered cases the active cases and
so on
after that i am extracting all the
column names from the confirmed cases
data frame using the dot keys function
if i run this
below you can see we have
all the column names
next
we are extracting only the date columns
that have information of confirmed cases
death cases and recovered cases using
the dot loc function
we are storing it in respective
variables
the first parameter that is colon
tells us we need all the rows from the
data
while the second parameter tells us we
need the data from the fourth column
till the last column
which are the date values
let me run it
let's check the confirmed variable and
see what it has
as you can see it has the date columns
from the 22nd of jan till the 27th of
april
in the next cell of code i am basically
creating multiple empty lists to find
the total cases worldwide the total
number of debts so far the mortality
rate the recovered cases and the total
active cases
i also want to find out the confirmed
cases debts and recoveries for few
countries such as china italy united
states spain france germany uk russia
and india
so i have created empty lists for these
countries as well
we will see how to append values to this
empty list in the next step
let me just run it
now using a for loop i am finding the
total sum of confirmed cases death cases
and recoveries made so far
these values i am appending to the empty
list world cases total debts
total recovered and total active cases
the total active cases are calculated as
the confirmed cases
minus the death cases minus the
recovered cases
now to calculate the mortality rate i am
using a simple formula which is the sum
of the total debts divided by the total
confirmed cases
the recovery rate is also calculated by
dividing the total recovered cases with
the total confirmed cases
then i am appending the values of
confirmed cases for each country using
the append function and selecting
different countries for which we had
created an empty list
similarly i am also appending the values
for debts and recoveries made for each
of these countries
let's run it
now you can check the world cases each
day
so
these are the world cases that have come
up each day
and you can see the number has reached
almost
30 lakhs now
then you can see the total death so far
globally
so these are the deaths
reported each day across the globe
you can see
there have been around
more than 2 lakhs casualties so far
let me collapse it
next i'm printing out the total sum of
confirmed cases
so you can see it stands at around 30
lakhs 41 764.
similarly let me print out the total
death sum
so it's around 2 lakhs 11 167
as of 27th of april
now also check the recovered sum
it's around 8 lakhs 93 967.
you can also have a look at the cases
based on individual countries
so here you can see the cases that have
come up so far in the united states
which is u.s underscore cases
so united states has nearly 9 lakhs 88
1997 cases
let me collapse it
now let's check for india as well
so in india the total cases
have reached 29
451.
let me collapse it further
you can also have a look at the
recoveries for each country here i am
printing out the recoveries made in
italy
next
i want to analyze the data for
understanding the daily increase in
cases across the world the daily
casualties and the daily recovery cases
for that
i am using a user defined function to
track the daily increase
let me run it
okay
now to find the total worldwide increase
in cases i am calling the above created
user defined function and assigning it
to world daily increase variable
similarly i am using the same method to
find the daily increase in cases for
different countries
let me show you the daily increase in
confirmed cases for spain
so yesterday that is 27th of april speed
had
2793 cases
now let's check for germany also
as you can see
yesterday germany had around 988 cases
in the next cell i am finding out the
daily death cases across the world as
well as for a few countries
let me run it
let's check the daily deaths that have
been reported in china
so these are the
values of daily deaths in china
you can see the numbers have
significantly reduced
also let's check for the united kingdom
as well
so on 27th of april united kingdom had
363 deaths reported
moving ahead i am also calculating the
daily recovery cases across the world
and for some countries
now let's go ahead and check the data
from india in terms of the daily
recovery cases
so these are the values
and if i scroll down you can see
yesterday that is 27th of april india
had 614 recovered cases
also let's have a look at the world
recovery cases
here you can see on 27th of april
nearly 28 234 people have recovered
across the globe
with that
the next step is to visualize our data
so first i am finding out the list of
unique countries using the list function
and passing the latest data variable to
filter the different uni countries
so here you can see the list of
different
countries
now the cell that you see on your
screens
we are using these lines of code to find
the country's confirmed cases
the death cases active cases recovery
cases and mortality rate
we are finding the sum of the cases and
appending it to empty list that we have
created
we are also removing the countries that
have no cases at all and finally we are
sorting the countries by the number of
confirmed cases
so let me run it
in this cell
you will be visualizing the data using a
gradient styled color map to see the
country name the number of confirmed
cases and the number of deaths the
number of recoveries and so on
i have taken blue as my gradient
let me run it
as you can see
we have an interesting table that looks
really attractive
on the left you can see the country
names
the table has been sorted in descending
order of confirmed cases
you can see the dark blue color depicts
the countries with the highest number of
cases
in this case it's united states
then we have spain italy and france
we also have germany and united kingdom
you can also see the number of deaths
and recoveries in the middle of this
table
and to the right you can see the number
of active cases and the mortality rate
you can see from the visual
italy
has the highest mortality rate
followed by the united kingdom
similarly
i am finding out the list of unique
provinces using the list function and
passing the latest data variable to
filter out different provinces
in the next cell
we are finding the confirmed cases the
death cases active cases recovery cases
and mortality rate for each province of
a particular country
we are also finding the sum of the cases
and appending it to empty list that we
have created
we are also removing the provinces that
have no cases at all
and finally we are sorting the provinces
by the number of confirmed cases
so let's run it
in this cell
we'll be visualizing the data of
different provinces using a
gradient-styled color map to see the
province or the state names the number
of confirmed cases the number of deaths
number of recoveries and so on
i have taken red as my gradient
let's run it
there you go
we have our visual here on the screen
new york state has the highest number of
confirmed cases
and the number of deaths as well
the hubei province in china
has had the highest recoveries which is
around
63 604
and to the extreme right you can see the
mortality rate
the next cell of code deals with
handling missing values or nan values
it is important to handle such values
otherwise your data would be noisy and
messy and it will lead to inappropriate
results
now
i am plotting a horizontal bar graph to
compare the number of cases in the
united states and outside of united
states
the outside united states cases have
been calculated by subtracting the
united states confirmed cases
from the country confirmed cases
worldwide
i have used plt dot bar h function to
plot my horizontal bars
we have also given a title to this plot
let me run it
so this is my bar graph
the blue bar represents the united
states confirmed cases
and the red bar represents the cases
outside of united states
now if you want to print the actual
values of the cases in the united states
and outside of united states you can do
that too
as you can see
there are nearly 19 lakhs 58 592 cases
outside of united states in united
states there are nearly
9 lakhs
3854 cases
and the total cases stand at
28 lakhs 96
746 cases
then i want to show only 10 countries
with the most confirmed cases
the rest i want to group it into the
others category
so using two empty lists i am going to
find out the top 10 countries with the
highest number of cases the remaining i
am categorizing under others
now to show these countries i have
defined a function
that will plot my horizontal bar graph
let me run it
now using plot underscore bar underscore
graphs function we will create the bar
graph
i have used the visual underscore unique
underscore countries and visual
underscore confirmed underscore cases
variables and given a title to the graph
so here is our horizontal bar graph
you can see the united states has the
highest number of cases
the next is spain
the united kingdom is at the 6th
position
russia is at the 10th place
and on the top you can see the others
category as well
now we will create a pie chart to plot
the same graph
for that i have created another function
called plot underscore pi underscore
charts
let me just run it
now using plot underscore pi underscore
charts function name we will create the
pie chart
i've again used visual underscore unique
underscore countries and visual
underscore confirmed underscore cases
variables and given a title to the graph
there you go
we have our pie chart on the screen
and on the right you can see the legend
which shows the different country names
the maximum area in the pie chart has
been occupied by united states
then we have the others category
followed by
spain italy and france
and follow the same procedure to show 10
provinces with the most confirmed cases
the rest i will group into the others
category for that i will be using
two empty lists
visual underscore unique underscore
provinces and visual underscore
confirmed underscore cases 2
let's plot the graph using plot
underscore bar underscore graphs
function
so here
you can see the bar graph
new york province or the new york state
has the highest number of cases
then we have new jersey
followed by uber provisions in china and
massachusetts
and on the top you can see the others
category
the next cell of code
looks pretty complicated
it is essentially trying to plot a pie
chart for different countries along with
its several states and provinces
here we want to show only the top 10
states with the highest number of cases
let me run it
now
i want to show the pie chart for the
united states and its top 10 states
let me do that
as you can see we have our pie chart and
on the right you can see we have the
legend for 10 states
so the maximum area in the pie chart has
been occupied by
new york and then
we have the other
states
next is new jersey followed by
massachusetts and california
let me also check the confirmed cases
for different states in france
i'll change this to
france
so here is our pie chart
which has
10 different states in france
with that let's now come to building our
model using polynomial regression and
support vector machines
in this step i am converting all the
dates and the cases in the form of a
numpy array using the np dot array
function
let's run it
since our prediction will be for the
next 20 days so i have created a
variable called days underscore future
and have assigned a value of 20
then i'm adding the last 20 days to the
total number of days we have
now let me show you the values of future
underscore forecast
so here
is a list of values basically is the
number of total days
next
we are converting all the integers into
date time values for better
visualization
now it's time to split our data into
training and testing sets
for that i am using the train underscore
test underscore split function
i have taken days since 22nd of jan and
the world cases as my parameters
i'll be using 75 percent of
data for training the model and 25
percent for testing the model
we will now transform our data for
polynomial regression
for that i will use the polynomial
features function and fit underscore
transform method to transform our
training testing and future forecast
data
next i will build the polynomial
regression model using the linear
regression function
and i have used the linear underscore
model dot fit function to fit the
training data
after that
i have used the predict function to
predict the test data set values
finally i am printing the mean absolute
error value and the mean squared error
value as well
so you can see the results here
let's now plot the graph between the
test data set and the values from the
predicted polynomial regression model
so below you can see the graph here
the blue line represents the test data
and the red line represents our
polynomial regression predictions
now let's start building our model using
a support vector machines algorithm
svm uses different parameters to build a
model these parameters are kernel c
gamma epsilon shrinking and svm
underscore grid
kernel specifies the kernel type to be
used in the algorithm
it must be one of linear poly rbf
sigmoid precomputed or callable
if nothing is given rbf will be used by
default
c is a regularization parameter
gamma is the kernel coefficient of rbf
poly and sigmoid
epsilon specifies the epsilon tube
within which no penalty is associated in
the training loss function
shrinking takes boolean values true or
false
svm grid has all the values parameters
passed to it
here we are creating the support vector
regressors and then using the fit
function to fit our training data set
finally we are predicting the values for
future data as well
let me now print the mae and the msc
values and display the graph of the
prediction
as you can see
the blue line represents the test data
and the red line represents the svm
predictions
now let's create some more
visualizations to understand the model
and our data better
here i am plotting the number of
coronavirus cases over time
i have used the adjusted dates and the
world cases to plot the graph
i've also assigned the x labels and the
y labels
let me run it
as you can see from the graph
the total number of cases
have reached close to
30 lakhs now
next in this graph we are showing the
total number of coronavirus deaths over
time
i've also used the x label and the y
label
you can see the total number of deaths
have reached over
2 lakhs
similarly in the next graph
we can see the total number of recovered
cases which are more than 8 lakhs
then
we are also looking at the total number
of active cases across the globe
so here is the graph for that
you can see the number of coronavirus
active cases over
time am now creating a bar graph
with adjusted dates and the world daily
increase as my variables
as you can see on some of the days the
cases have gone very high
reaching 100 000 in a day
the next graph we are plotting to show
the world daily increase in confirmed
death cases
so here is the
bar graph for that
you can see there are certain days that
have reported
more than 10 000 deaths in a day
the next graph shows the world daily
increase in confirmed recovery cases
so here is the graph for that
moving ahead i'll plot the graphs to
show the number of coronavirus cases
over time
let's now plot the actual graph
i have taken adjusted underscore dates
world cases linear underscore trade
variables as my parameter
let me run it
you can see this is our graph
the blue line represents the confirmed
we will check for the svm model as well
so below is the graph
again the
now
let me print out the predicted values
from the polynomial regression model as
a data frame
so you can see
we have our predicted
confirmed cases for the next 20 days
that is from the 28th of april to the
17th of may
likewise
let me also print out the predicted
values for the svm model as a data frame
so here are
our predicted values from the svm model
in the next graph we are plotting the
depths and the recoveries in the same
graph
so here is our line graph
you can see clearly the number of
recoveries are way higher than the
number of deaths which is actually a
good sign
moving forward
the next graph shows a line chart with
the number of coronavirus deaths versus
the number of coronavirus recoveries
so here is the graph
the x-axis has the number of recoveries
and the y-axis has the number of
total deaths
now this cell of code looks pretty huge
we want to plot the total number of
confirmed cases the daily increase in
confirmed cases the increase in debts
and the increase in recoveries for
different countries across the globe
let me run this cell
so using the country underscore plot
function that was defined above we will
be plotting the data for various
countries
let me start with china first
so you can see there are four graphs in
total if i scroll down the first you see
here is china confirmed cases
next is the china daily increase in
confirmed cases
so you can see in one of the days it had
a huge spike
in the increase in confirmed cases then
we have the china daily increase in
debts
and finally we have the daily increase
in recoveries
now let's check for italy
let me scroll down you can see the same
plots for italy as well the first we
have total
confirmed cases in italy
which is nearly around
2 lakhs
then we have the daily increase in
confirmed cases
next we have the italy
daily increase in deaths
and finally we have the increase in
recoveries
let's have a look at how the graphs for
india looks like
you see the total well
let's also see for the united states
so here you can see all the four graphs
for the united states
spain
france germany and and they have been
depicted using different colors
on the top left you can see the legend
now this graph looks similar to the one
that we saw in our slides
next i'm plotting the total coronavirus
depth
so here is our graph
finally let us look at the total
coronavirus cases in these countries
the last three graphs look similar to
what we have used in the slides from our
world in data
now it's my responsibility to call out
all the safety precautions that you
should take to make sure you are safe
from being attacked by coronavirus
first and foremost do not panic
during these unprecedented times if you
feel you are showing symptoms of
coronavirus do not wait please consult a
doctor immediately
stay at home to ensure you are not
exposed to the virus outside
maintain social distancing with people
if you are talking to someone outside
and if someone is sick
do not forget to wash your hands
regularly using soaps sanitizers and
other disinfectants
always remember to wear a mask while you
are going outside
make sure to practice respiratory
hygiene such as
cover your mouth while coughing and try
eating healthy food
all these precautions will help you
avoid coronavirus and make sure you are
safe and healthy
now moving on to the final section let
me tell you how simply learn can help
you start your career in machine
learning
so let me take you to our website first
okay so i am on the chrome browser
let me search for simplylearn.com
and here
under what you want to learn let me type
machine learning it will show me the
relevant courses that simply learn
offers
in the machine learning category
so you can see here there are multiple
courses
first let me open the
first link
and let's open the second link as well
so
so this is the post graduate program in
ai and machine learning which is in
collaboration with purdue university and
ibm
if i scroll down
you can see the key features of this
course
so we'll get purdue alumni association
membership
industry recognized ibm certificates
enrollment to simply launch job assist
there's 25 plus hands-on projects on gpu
enabled labs you have
450 plus hours of applied learning
capstone projects in three domains and
much more
and here you can see on the right this
is the certificate that you will get
after completing this program
and you will also get
certificates recognized by ibm
and another key feature of this course
is you can enroll to simply learn job
assist program so you will get im jobs
crew membership for six months resume
assistance and career monitoring design
to interview preparation and career
affairs
if i scroll further here you can see the
learning path
and you will learn about python for data
science
this machine learning
deep learning with tensorflow and carers
that's advanced deep learning and
computer vision you will also learn
about natural language processing that
is nlp and speech recognition
we have reinforcement learning
and you also have the opportunity to
select a few electives so we have ibm
watson for chatbots machine learning
with r there's git and github training
and two others
here you can see
the skills that will be covered so you
will learn about statistics which is a
core component of machine learning we
learn about python supervised learning
there's dance computer vision tensorflow
there's reinforcement learning speech
recognition you will also learn about
numpy pandas and other libraries
and here you can see these are some of
the tools that will be covered in this
course
if i scroll further now this is the
important section you can see the
industry projects that you will get to
work on
so this is
on twitter there's one on zumato one on
uber
and mercedes benz as well
they are our course advisors
so please go ahead and enroll to this
postgraduate program in ai machine
learning if you want to kick start your
career in
machine learning
now the next course we have is machine
learning certification course
if i scroll down you can see the details
of this course
so you will gain expertise with 25 plus
hands-on exercises you will get to work
on four real-life industry based
projects with integrated labs there will
be dedicated mentoring sessions from
industry experts 44 hours of instructed
led training with certification
here on the right you can see the skills
that will be covered
so we learn about time series modeling
linear and logistic regression we will
also learn about support vector machines
games clustering knife base decision
trees
random forest
and you learn the concepts of bagging
and boosting and deep learning
fundamentals as well
here you can see the
course content
and if i scroll further now these are
the projects that you will get to work
on
and finally you will receive this
certificate once you complete the course
thank you all for watching this full
course video tutorial on machine
learning with python i hope you liked it
if you have any questions then please
put them in the comments section our
team will help you solve your queries
thanks again stay tuned for more from
simply loan
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos to nerd up and get certified
click here
you
