hello everyone so i'm michael spence and
i'm the lead researcher on cder's
lighthouse project in self-supervised
and semi-supervised learning and in this
video i'm going to talk about some basic
semi-supervised learning methods and
some of their applications as well
so i'll start with just a brief recap of
semi-supervised learning so
semi-supervised learning is a paradigm
of machine learning which sits in
between supervised learning and
unsupervised learning so where
supervised learning uses only labeled
data and this is human annotated data
such as captioned images or things like
that
to train models and unsupervised
learning uses only unlabeled data
semi-supervised learning makes use of
both
when people talk about semi-supervised
learning they generally divide it into
two different categories so there's
inductive learning and transductive
learning
so inductive learning as you can see in
the diagram here it works quite
similarly to supervised learning you
take labeled and unlabeled data and
using a training process you end up with
a predictive model which can give you
predictions on future unseen data
this works for both classification and
regression problems
so transductive learning
works a bit differently you feed in
label data and unlabeled data
and through transductive learning
processes you get a bunch of predictions
returned rather than a predictive model
um so this has some limitations it means
you can't
easily genera generalize the information
you've learned to future unseen data
although there are ways of doing this
it's it's a bit more complicated than in
the inductive scenario
um so i'm going to talk next about some
concrete examples of these two
um semi-supervised paradigms the
simplest example of inductive learning
and possibly the simplest example of
semi-supervised learning in general is
what's known as self-training
so this is a procedure where you can
take any supervised method
for classification or regression
and modify it so that it works in a
semi-supervised manner taking advantage
of labeled and unlabeled data
so the procedure is quite simple you
begin with a small amount of label data
in this case it could be images
and you use this to train a base model
using normal supervised methods
and then what you do is you
use this base model to label your
unlabeled data
which is a process known as
pseudo-labeling
and you would then take your most
confident predictions
made with this base model
and you would add them into your label
data set
and then use that to train the next
iteration of your model
so i have some diagrams here which
should hopefully make the process a bit
clearer
so as i said you have your first
classifier which has just been trained
um on the original labeled images
you take these and you label your
unlabeled images
in a process called pseudo labeling
and then you take the most confident
predictions made with your model
you might say you want a confidence of
over 75 for example
and if any of the pseudo labels exceed
this confidence level
you would then add it into the into the
label data set so you would combine your
your most confident pseudo labels with
the original labels into a larger label
data set
and you would then train a new
classifier on that
um which
in the best case scenario would have
improved performance over the original
one
and you can generally iterate this
process every time adding more and more
pseudo labels you might do 10 iterations
for example is is a fairly standard
amount um
and hopefully if if your data sets well
suited for the process the performance
of the model should continue increasing
at each iteration
um so as an example of a successful use
of self training methods um
last year facebook made use of self
training to improve their speech
recognition models
so starting off with the base model
which was trained with 100 hours of uh
labeled data so
data you know which has been actually
humanly annotated
um they managed to increase their
performance of this by incorporating 500
hours of unlabeled speech data um and
using self-training and they find that
they had a decreased word error rate of
33.9 percent
which is obviously uh a really
significant improvement especially when
no extra effort was actually needed uh
for this unlabeled data
so this is a successful example of self
training being used
but it should be stressed that the
performance will vary like very much
from data set to data set and there's
plenty of cases where self turning might
actually
end up decreasing the the performance
um compared to just the supervised model
so self training was probably the
simplest example of an inductive method
of semi supervised learning
um i'll talk now about probably the
simplest method of transductive semi
supervised learning
so this is known as label propagation um
it's a graph based method of semi
supervised learning and what it allows
you to do is it allows you to spread uh
human annotated labels um within a
network made up of label data and
unlabeled data
so um
the mathematics can get a bit technical
but i'll just tell you now about the
general
um the general kind of intuition uh
behind this method
so if we look at the toy example here at
the bottom of the slide we have a
network of data points um most of which
are uncolored but four of which are
colored there's two red ones and two
green ones and we'd like to find a way
so that we can spread these colors
throughout the network
so one way of doing this
is we could pick a point in this case
four for example
and we could count up all the different
paths which travel to the network from
four to a colored node
um and if we do this we find here on the
right
these are the paths leading to red
points so four to seven to eight for
example
um there's five pass overall leading to
red points and if we count up the number
of paths leading to green points we find
there's only four
so in this sense we consider the point
four as being closer to red and green we
would then color it in as red
and we would repeat this process for
every point in the network
um until every point was labeled and
uncovered in another diagram which might
be useful um is displayed here so this
might help you understand label
propagation
um this is taken off the scikit-learn
user guide
um from one of their implementations of
label propagation
um so on the left here is the original
data set you can see it's mostly
unlabeled it's mostly orange
um but you have two label points one
light blue one dark blue
and if we apply the label propagation
algorithm you can see the end result is
that all points end up labeled which i
should hopefully provide a a good
demonstration of how it works and what
his end goal is so i hope you find this
quick run-through of uh two basic
methods of semi-supervised learning
useful
if you'd like to be more involved in
this project or would like to see any of
the future content i will be sharing
um feel free to get in touch with me um
there'll also be a link to the project
page in the description of the video
um so thanks very much for listening
