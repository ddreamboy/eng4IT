umap takes a big pile of data that you
can't graph and helps you graph it
hooray stat Quest hello I'm Josh starmer
and welcome to stat Quest today we're
going to talk about umap Dimension
reduction main
ideas this stack Quest is sponsored by
lightning and
grid. with lightning you can design
build and scale models with ease focus
on the business and research problems
that matter to you lightning takes care
of everything else and with grid you can
use the cloud to seamlessly train
hundreds of models from your laptop with
a single command no code changes
necessary for more details follow the
links in the pinned comment
below note this stat Quest focuses on
the main ideas of how umap Works however
if you're also interested in the map
mathematical details be sure to check
out the follow-up Quest umap
mathematical
details now imagine we collected weight
and height measurements from a bunch of
people and we plotted the people on a
two-dimensional graph like this where we
have weight on the x-axis the First
Dimension and height on the Y AIS the
second
dimension seeing the data can be useful
to identify outlin buers and to identify
clusters of similar people so looking at
data can be very useful and generally
speaking is one of the first steps in
any data
analysis however if we wanted to include
each person's age we would have to add a
third AIS and now our graph is
three-dimensional drawing a 3dimensional
graph on a two-dimensional computer
screen is awkward but
possible however if we wanted to include
a lot more features we'd need to draw a
four or more dimensional graph and
that's not possible so what do we do if
we have a lot of features and we want to
look at the data well one good option is
principal component analysis
PCA and if you're not already familiar
with PCA you definitely should be so
check out the quest anyway the problem
with PCA is that it only works well when
the first two principal components
account for most of the variation in the
data simply put if you have a really
complicated data set PCA may not work
very
well so if we can't use PCA what should
we do one option is called umap uniform
manifold approximation and
projection umap takes high-dimensional
data meaning data with three or more
features and outputs a low-dimensional
graph meaning a graph we can easily look
at umap is popular because it is
relatively fast even with large data
sets and similar samples tend to Cluster
together in the final output so it is
useful for identifying similarities and
outliers bam so we can see what umap
does and how it works we're going to
start with a very simple two-dimensional
graph and show how umap converts it to a
one-dimensional number line in other
words this two-dimensional graph will
represent the high dimensional data that
we reduce with um map to a single
Dimension that will be the low
dimensional
graph if we look at the high-dimensional
data we see that points a B and C are
relatively close to each other and form
a cluster and points d e and f are
relatively close to each other and form
another cluster
and both clusters are relatively far
from each other the goal of umap is to
create a low-dimensional graph of this
data that preserves these
high-dimensional clusters and their
relationship to each other the general
idea is to initialize the
low-dimensional points and then move the
low dimensional points around until they
form clusters that have the same
relationships we saw in the
high-dimensional
data in other words because points a B
and C are clustered together and are
relatively far from the other cluster
umap wants the low dimensional points a
b and c to be close to each other and
relatively far from the other
cluster so let's talk about the main
ideas of how umap makes this happen note
if umap simply projected all of the data
onto the
xaxis then instead of two distinct
clusters we'd see a mish mash of points
and if we had a slightly more complex
data set with a third cluster here then
projecting the data onto the Y AIS
wouldn't be any
better so what umap does is calculates
similarity scores to help identify
clustered points so it can try to
preserve that clustering in the
low-dimensional
graph the first thing that you map does
is calculate the distances between each
pair of high-dimensional points
note I'm only showing two of the
distances between the two clusters but
just know that we calculated all of them
and that the Clusters are way far
apart now to calculate the similarity
scores associated with point a we start
by putting point a on a
graph now because point a is 0.5 units
away from point B we put B 0.5 units
away from a on the graph and because
point a is 2.4 units away from C we put
C 2.4 units away from a on the graph and
because the remaining points d e and f
are all way far away from a we put them
way far away from a on the
graph now we draw a curve over the data
to calculate the similarity scores the
shape of this curve depends on the
number of high-dimensional neighbors
that you want each point to have a
common default value for the number of
high dimensional neighbors is 15 but in
this example where we have a tiny data
set and we know each cluster has three
points we'll set it to three note to be
clear the number of neighbors we want
each point to have includes the point
itself so by setting the number of
neighbors to three we actually only want
two other points as neighbors also later
in the quest we'll talk about what
happens when we change the number of
neighbors we want each point to have but
for now let's just set the value to
three now brace yourselves things are
about to get a little mathy the purpose
of this math is to get an understanding
of how the number of nearest neighbors
parameter which is arguably the most
important umap parameter you can adjust
works so that said umap takes the the
log base 2 of the number of high
dimensional neighbors you want each
point to have and since we set the
number of high dimensional neighbors to
three we get the log base 2 of 3 which
equals
1.6 and this value 1.6 is what defines
the shape of this curve the curve is
shaped in such a way that the Y AIS
coordinates for the nearest Neighbors
which in this case are points B and C
add up to the log base 2 of the number
of nearest neighbors you specified which
in this case is
1.6 in other words this curve is shaped
the way it is so that the y- AIS
coordinate for B is 1.0 and the y-axis
coordinate for C is
0.6 and the y- AIS coordinates for d e
and f are all pretty much zero adding
these scores together gives us 1.6 the
target number we are shooting for thus
plus the y-axis coordinates for points B
and C are their similarity scores
relative to a and since the scores for d
e and f are all zero we can ignore them
for
now so let's save the similarity scores
for B and C
bam now just like we did for point a
let's calculate the similarity scores
relative to point B first we put point B
on a graph and then then we add all the
other data at their respective
distances and then we draw a curve over
the data so that the sum of the
similarity scores the y-axis coordinates
for each point other than b equals
1.6 anyway just like before because
points d e and f are way far away their
scores are zero and we can ignore them
so we only need to save scores for a and
C relative to
B likewise we calculate and save the
nonzero scores for Point C bam note you
may have noticed that the similarity
score for C relative to B
0.6 is different from the similarity
score for B relative to C
1.0 the difference comes from the fact
that the curves for each point are
different umap scales the curve so that
regardless of how close or far the
neighboring points are are the sum of
the similarity scores will be equal to
the log base 2 of the number of nearest
neighbors that you specify and scaling
the curves ensures that every point is
similar to at least one other point in
the data
set however using different curves mean
the similarity scores are not
symmetrical so umap makes them
symmetrical using a method similar to
taking the average and this is one of
those details we'll discuss in the
follow-up stack Quest bam note in the
exact same way umap calculates the
similarity scores for points d e and
f
bam now umap initializes a
low-dimensional
graph but as we can see this
low-dimensional graph is not ideal
because point B needs to be closer to a
because they are in the same
high-dimensional cluster and point B
needs to be further from F because they
are in two different high-dimensional
clusters so to make this low-dimensional
graph show the same clusters we see in
the high-dimensional data umap picks two
low-dimensional points that it should
move closer together umap does this by
randomly selecting a pair of points in a
cluster proportionally to their
high-dimensional score in this case that
means there is a higher probability that
umap might randomly select points a and
B because their score is
1.0 and there is a lower probability
that umap might randomly select points A
and C because their score is
0.8 so for this example let's assume
that umap randomly selected points A and
B and that means umap wants to move
points A and B closer together then umap
flips a coin and decides to move point B
closer to a note it could have just as
easily decided to move point a closer to
B now that we know that we will move
point B closer to a umap picks a point
that b should move further from so umap
randomly picks one of the points that
are not in B's High dimensional cluster
however unlike before this time the high
dimensional scores do not influence
which point is picked instead each point
in a different cluster regardless of its
score has an equal chance of being
picked so let's imagine umap picked
Point
e now that umap has decided to move
point B closer to a because they should
be clustered together and move point B
further from E because they should be in
different clusters umap has to figure
out how much it should move point
B to do this umap calculates low
dimensional similarity scores Y axis
coordinates on a curve for points b and
a and for b and
e however now instead of using a variety
of Curves like we did for the high
dimensional data the low dimensional
similarity scores come from a fixed
bell-shaped curve that is derived from a
t
distribution note generally speaking a t
distribution curve is like a gussian
curve or normal distribution
except the T distribution tends to be
shorter and have fatter
Tails also note the low dimensional
curves are all the same
size now because points A and B are in
the same cluster or neighborhood umap
wants to move point B closer to a in
order to maximize this low-dimensional
score in contrast because points B and E
are in different clusters
umap wants to move point B further from
E so that it can minimize this low
dimensional
score so ultimately umap moves point B a
little closer to a and a little further
from
e note umap only moved point B a small
amount because when there is a ton of
data taking small steps each turn makes
it easier to get the low-dimensional
graph looking Just Right double
bam now umap picks another pair of
points to move together in this case
umap wants to move D closer to
e then um map randomly picks a point in
the other cluster to move D away
from in this case umap decides to move D
away from
C now umap calculates low dimensional
similarity scores to decide how to move
Point d
note if we move D closer to e the point
we want to get closer to then we will
also be moving D closer to C the point
we want to get further
from however this move barely increases
the score for D relative to
C in other words the score we want to
minimize will still be pretty small in
contrast the score we want to maximize
the score relative to e will get much
larger
thus umap moves d a little closer to
e likewise umap moves the points one
step at a time until we have two
low-dimensional clusters that are
relatively far from each other just like
we see in the high dimensional data
triple
bam note if you're familiar with how tne
works you may have noticed that umap is
very very similar
in terms of the main ideas of how umap
and tne work they are essentially the
same and most of the differences are
very subtle however there are two big
important
differences the first difference is that
tne always starts with a random
initialization of the low-dimensional
graph in other words tne might start out
with the low dimensional data looking
like this and then the next time you run
t- snake on the exact same data set it
might start out with a low dimensional
data looking like this and every time
you run tne on the same data set you
start with a different low-dimensional
graph of the
data in contrast umap uses something
called spectral embedding to initialize
the low-dimensional graph and what that
means is that every time you use UAP on
a specific data set you always start
with the exact same low dimensional
graph
bam the other difference is that tne
moves every single point a little bit
each
iteration in contrast umap can move just
one point or a small subset of points
each time and this helps it scale well
with super big data sets
bam lastly way back when we calculated
similarity scores for the high
dimensional data we said that the shape
of each curve was determined in part by
the userdefined parameter the number of
neighbors each point has now let's talk
about what happens when we change the
number of
neighbors generally speaking when you
have a lot of data a relatively low
value for the number of neighbors
results in small independent
clusters in some ways this is sort of
like seeing the details but not the big
picture in contrast a relatively large
value for the number of neighbors gives
you more of the big picture and less of
the details so it can be worth trying
different values to see what works best
with your own data
bam now it's time for some Shameless
self-promotion if you want to review
statistics and machine learning offline
check out the stat Quest study guides at
stack quest.org there's something for
everyone hooray we've made it to the end
of another exciting stat Quest if you
like this stat Quest and want to see
more please subscribe and if you want to
support stat Quest consider contributing
to my patreon campaign becoming a
channel member buying one or two of my
original songs or a t-shirt or a hoodie
or just donate the links are in the
description below all right until next
time Quest on
