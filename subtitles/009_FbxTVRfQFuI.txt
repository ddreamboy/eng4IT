hello and welcome to the session on deep
learning my name is mohan and in this
video we are going to talk about what
deep learning is all about some of you
may be already familiar with the image
recognition how does image recognition
work you can train this application or
your machine to recognize whether a
given image is a cat or a dog and this
is how it works at a very high level it
uses artificial neural network it is
trained with some known images and
during the training it is told if it is
recognizing correctly or not and then
when new images are submitted it
recognizes correctly based on the
accuracy of course so a little quick
understanding about artificial neural
networks so this is the way it does is
you provide a lot of training data also
known as labeled data for example in
this case these are the images of dogs
and the network extracts some features
that makes a dog a dog right so that is
known as feature extraction and based on
that when you submit a new image of dog
the basic features remain pretty much
the same it may be a completely
different image but the features of a
dog still remain pretty much the same in
various different images let's say
compared to a cat and that's the way
artificial neural network works we'll go
into details of this uh very shortly and
once the training is done with training
data we then test it with some test data
too which is basically completely new
data which the system has not seen
before unlike the training data and then
we find out whether it is predicting
correctly or not thereby we know whether
the training is complete or it needs
more training so that's not a very high
level artificial neural network works so
this is what we are going to talk about
today our agenda looks something like
this what is deep learning why do we
need deep learning and then what are the
applications of deep learning one of the
main components the secret sauce in deep
learning is neural networks so we're
going to talk about what is neural
network and
how it works and some of its components
like for example the activation function
the gradient descent and so on and so
forth so that as a part of working of a
neural network we will go into little
bit more details how this whole thing
works so without much further ado let's
get started so deep learning is
considered to be a part of machine
learning so this diagram very nicely
depicts what deep learning is at a very
high level you have the all-encompassing
artificial intelligence which is more a
concept rather than a technology or a
technical concept right so it is it's
more of a concept at a very high level
artificial intelligence under the herd
is actually machine learning and deep
learning and machine learning is a
broader concept you can say or a broader
technology and deep learning is a subset
of machine learning the primary
difference between machine learning and
deep learning is that deep learning uses
neural networks and it is suitable for
handling large amounts of unstructured
data and the last but not least one of
the major differences between machine
learning and deep learning is that in
machine learning the feature extraction
or the feature engineering is done by
the data scientists manually but in deep
learning since we use neural networks
the feature engineering happens
automatically so that's a little bit of
a quick difference between machine
learning and deep learning and this
diagram very nicely depicts the relation
between artificial intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time and it can do
a lot of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's the
reason deep learning is required
compared to let's say traditional
machine learning it can also perform
complex algorithms more complex than
let's say what machine learning can do
and it can achieve best performance with
the large amounts of data so the more
you have the data let's say reference
data or label data the better the system
will do because the training process
will be that much better and last but
not least with deep learning you can
really avoid the manual process of
feature extraction those are some of the
reasons why we need deep learning some
of the applications of deep learning
deep learning has made major inroads and
it is a major area in which deep
learning is applied is healthcare and
within healthcare particularly oncology
which is basically cancer related stuff
one of the issues with cancer is that a
lot of cancers today are curable they
can be cured they are detected early on
and the challenge with that is when a
diagnostics is performed let's say an
image has been taken of a patient to
detect whether there is cancer or not
you need a specialist to look at the
image and determine whether it is the
patient is fine or there is any onset of
cancer and the number of specialists are
limited so if we use deep learning if we
use automation here or if we use
artificial intelligence here then the
system can with a certain amount of the
good amount of accuracy determine
whether a particular patient is having
cancer or not so the prediction or the
detection process of a disease like
cancer can be expedited the detection
process can be expedited can be faster
without really waiting for a specialist
we can obviously then once the
application once the artificial
intelligence detects or predicts that
there is an onset of a cancer this can
be cross-checked by a doctor but at
least the initial screening process can
be automated and that is where the
current focus is with respect to deep
learning in healthcare what else
robotics is another area deep learning
is majorly used in robotics and you must
have seen nowadays robots are everywhere
humanoids the industrial robots which
are used for manufacturing process you
must have heard about sofia who got
citizenship with saudi arabia and so on
there are multiple such robots which are
knowledge oriented but there are also
industrial robots are used in industries
in the manufacturing process and
increasingly in security and also in
defense for example image processing
video is fed to them and they need to be
able to detect objects obstacles and so
on and so forth so that's where deep
learning is used they need to be able to
hear and make sense of the sounds that
they are hearing that needs deep
learning as well so robotics is a major
area where deep learning is applied then
we have self-driving cars or autonomous
cars you must have heard of google's
autonomous car which has been tested for
millions of miles and pretty much
incident free there were of course a
couple of incidents here and there but
it is uh considered to be fairly safe
and there are today a lot of automotive
companies in fact pretty much every
automotive company worth its name is
investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these will be in production and they
will be used extensively in real life
right now they are all in rnd and in
test phases but pretty soon these will
be on the road so this is another area
where deep learning is used and how is
it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars in the
site will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
wait so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens in a deep learning
environment so again a lot of scope here
to use deep learning a couple of other
applications are mission translations
today we have a lot of information and
very often this information is in one
particular language and more
specifically in english and people need
information in various parts of the
world it is pretty difficult for human
beings to translate each and every piece
of information or every document into
all possible languages there are
probably at least hundreds of languages
or if not more to translate each and
every document into every language is
pretty difficult therefore we can use
deep learning to do pretty much like a
real-time translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the fly for
example you go to somewhere like china
and you want to know what is written on
a signboard now it is impossible for
somebody to translate that and put it on
the web or something like that so you
have an application which is trained to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from chinese to english
that is one then there could be web
applications where there may be a
research document which is all in maybe
chinese or japanese and you want to
translate that to study that document or
in that case you need to translate so
therefore deep learning is used in such
situations as well and that is again on
demand so it is not like you have to
translate all these documents from other
languages into english and one shot and
keep it somewhere that is again pretty
much an impossible task but on a neat
basis so you have systems that are
trained to translate on the fly so
mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it needs a lot of
effort to train such neural net it has
been proved that it is possible so this
is a relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black and white
image this was colored by a neural net
or a deep learning application as you
can see it's done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning now one of the major secret
sauce of deep learning is neural network
deep learning works on neural network or
consists of neural network so let us see
what is neural network neural network or
artificial neural network is designed or
based on the human brain now human brain
consists of billions of small cells that
are known as neurons artificial neural
networks is in a way trying to simulate
the human brain so this is a quick
diagram of biological neuron a
biological neuron consists of the major
part which is the cell nucleus and then
it has some tentacles kind of stuff on
the top called dendrite and then there
is like a long tail which is known as
the axon further again at the end of
this action are what are known as
synapses these in turn are connected to
the dendrites of the next neuron and all
these neurons are interconnected with
each other therefore they are like
billions of them sitting in our brain
and they're all active they're working
they based on the signals they receive
signals as inputs from other neurons or
maybe from other parts of the body and
based on certain criteria they send
signals to the neurons at the other end
so they they get either activated or
they don't get activated based on so it
is like a binary gates so they get
activated or not activated based on the
inputs that they receive and so on so we
will see a little bit of those details
as we move forward in our artificial
neuron but this is a biological neuron
this is the structure of a biological
neuron and artificial neural network is
based on the human brain the smallest
component of artificial neural network
is an artificial neuron as shown here
sometimes is also referred to as
perceptron now this is a very high level
diagram the artificial neuron has a
small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as x1 x2 and so on each of the
inputs are multiplied by what is known
as weights which are represented as w1
w2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like x1
into w1 plus x2 into w2 and so on the
products are then added and then there
is a bias that is added to that in the
next slide we will see that passes
through an activation function and the
output comes as a y which is the output
and based on certain criteria the cell
gets either activated or not activated
so this output would be like a zero or a
one binary format okay so we will see
that in a little bit more detail but
let's do a quick comparison between
biological and artificial neurons just
like a biological neuron there are
dendrites and then there is a cell
nucleus and synapse and an axon
we have in the artificial neuron as well
these inputs come like the dead right if
you will act like the dendrites there is
a like a central unit which performs the
summation of these uh weighted inputs
which is basically w1 x1 w2 x2 and so on
and then our bias is added here and then
that passes through what is known as an
activation function okay so these are
known as the weights w1 w2 and then
there is a bias which will come out here
and that is added the bias is by the way
common for a particular neuron so there
won't be like b1 b2 b3 and so on only
weights will be one per input the bias
is common for the entire neuron it is
also common for or the value of the bias
remains the same for all the neurons in
a particular layer we will also see this
as we move forward and we see deep
neural network where there are multiple
neurons so that's the output now the
whole exercise of training the neuron is
about changing these weights and biases
as i mentioned artificial neural network
will consist of several such neurons and
as a part of the training process these
weights keep changing initially they are
assigned some random values through the
training process the weights the whole
process of training is to come up with
the optimum values of w1 w2 and wn and
then the b4 or the bias for this
particular neuron such that it gives an
accurate output as required so let's see
what exactly that means so the training
process this is how it happens it takes
the inputs each input is multiplied by a
weight and these weights during training
keep changing so initially they are
assigned some random values and based on
the output whether it is correct or
wrong there is a feedback coming back
and that will basically change these
weights until it starts giving the right
output that is represented in here as
sigma i going from 1 to n if there are n
inputs wi into x i so this is the
product of w1 x1 w2 x2 and so on right
and there is a bias that gets added here
and that entire thing goes to what is
known as an activation function so
essentially this is sigma of w i x i
plus a value of bias which is a b so
that entire thing goes as an input to an
activation function now this activation
function takes this as an input gives
the output as a binary output it could
be a zero or a one there are of course
to start with let's assume it's a binary
output later we will see that there are
different types of activation functions
so it need not always be binary output
but to start with let's keep simple so
it decides whether the neuron should be
fired or not so that is the output like
a binary output 0 or 1. all right so
again let me summarize this so it takes
the inputs so if you're processing an
image for example the inputs are the
pixel values of the image x1 x2 up to xn
there could be hundreds of these so all
of those are fed as so these are some
values and these pixel values again can
be from 0 to 56 each of those pixel
values are then multiplied with what is
known as a weight this is a numeric
value can be any value so this is a
number w1 similarly w2 is a number so
initially some random values will be
assigned and each of these weights are
multiplied with the input value and
their sum this is known as the weighted
sum so that is performed in this kind of
the central unit and then a bias is
added remember the bias is common for
each neuron so this is not the bias
value is not one
bias value for per input so just keep
that in mind the bias value there is one
bias per neuron so it is like this
summation plus bias is the output from
the section this is not the complete
output of the neuron but this is the
bias for output for step one that goes
as an input to what is known as
activation function and that activation
function results in an output usually a
binary output like a zero or a one which
is known as the firing of the neuron
okay good so we talked about activation
function so what is an activation
function an activation function
basically takes the weighted sum which
is we saw w1 x1 w2 x2 the sum of all
that plus the bias so it takes that as
an input and it generates a certain
output now there are different types of
activation functions and the output is
different for different types of
activation functions moreover why is an
activation function required it is
basically required to bring in
non-linearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today relu has kind of
taken over so relu is by far the most
popular activation function that is used
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0
and 1. threshold function is the value
will be
0 up to a certain value and beyond that
this is also known as a step function
and beyond that it will be 1. in case of
sigmoid there is a gradual increase but
in case of threshold it's like also
known as a step function there's a rapid
or instantaneous change from zero to one
whereas in sigmoid we will see in the
next slide there is a gradual increase
but the value in this case is between
zero and one as well now relu function
on the other hand it is equal to
basically if the input is 0 or less than
0 then the output is 0 whereas if the
input is greater than 0 then the output
is equal to the input i know it's a
little confusing but in the next slides
where we show the relu function it will
become clear similarly hyperbolic
tangent this is similar to sigmoid in
terms of the shape of the function
however while sigmoid goes from 0 to 1
hyperbolic tangent goes from -1 to 1 and
here again the increase or the change
from -1 to 1 is gradual and not like
threshold or step function where it
happens instantaneously so let's take a
little detailed look at some of these
functions so let's start with the
sigmoid function so this is the equation
of a sigmoid function which is 1 by 1
plus e to the power of minus x so x is
the value that is the input it goes from
0 to -1 so this is sigmoid function the
equation is phi x is equal to 1 by 1
plus e to the power of minus x and as
you can see here this is the input on
the x-axis as x is where the value is
coming from in fact it can also go
negative this is negative actually so
this is the zero so this is the negative
value of x so as x is coming from
negative value towards zero the value of
the output slowly as it is approaching
zero it it slowly and very gently
increases and actually at the point let
me just use a pen at the point here it
is it is 0.5 it is actually 0.5 okay and
slowly gradually it increases to 1 as
the value of x increases but then as the
value of x increases it tapers off it
doesn't go beyond one so that is the
speciality of sigmoid function so the
output value will remain between zero
and one it will never go below zero or
above one okay then so that is sigmoid
function now this is threshold function
or this is also referred to as a step
function and here we can also set the
threshold in this case it is that's why
it's called the threshold function
normally it is 0 but you can also set a
different value for the threshold now
the difference between this and the
sigmoid is that here the change is rapid
or instantaneous as the x value comes
from negative up to zero it remains zero
and at zero it pretty much immediately
increases to 1 okay so this is a
mathematical representation of threshold
function phi x is equal to 1 if x is
greater than equal to 0 and 0 if x is
less than 0. so for all negative values
it is 0 which since we have set the
threshold to be 0 so as soon as it
reaches 0 it becomes 1. you see the
difference between this and the previous
one which is basically the sigmoid where
the increase from 0 to 1 is gradual and
here it is instantaneous and that's why
this is also known as a step function
threshold function or step function this
is a relu a relu is one of the most
popular activation functions today this
is the definition of relu phi x is equal
to max of x comma zero what it says is
if the value of x is less than zero then
phi x is
zero the moment it increases goes beyond
zero the value of phi x is equal to x so
it doesn't stop at one actually it goes
all the way so as the value of x
increases the value of y will also
increase infinitely so there is no limit
here unlike your sigmoid or threshold or
the next one which is basically
hyperbolic tangent okay so in case of
relu remember there is no upper limit
the output is equal to either 0 in case
the value of x is negative or it is
equal to the value of x so for example
here if the value of x is 10 then the
value of y is also 10 right okay so that
is relu and there are several advantages
of relu and it is much more efficient
and provides much more accuracy compared
to other activation functions like
sigmoid and so on so that's the reason
it is very popular all right so this is
hyperbolic tangent activation function
the function looks similar to sigmoid
function the curve if you see the shape
it looks similar to sigmoid function but
the difference between hyperbolic
tangent and sigmoid function is that in
case of sigmoid the output goes from
zero to one whereas in case of
hyperbolic tangent it goes from -1 to 1
so that is the difference between
hyperbolic tangent and sigmoid function
otherwise the shape looks very similar
there is a gradual increase unlike the
step function where there was an instant
increase or instant change here again
very similar to sigmoid function the
value changes gradually from -1 to 1. so
this is the equation of hyperbolic
tangent activation function yeah so then
let's move on this is a diagrammatic
representation of the activation
function and how the overall data how
the overall progression happens from
input to the output so we get the input
from the input layer by the way the
neural network has three layers
typically there will be three layers
there is an input layer there is an
output layer and then you have the
hidden layer so the inputs come from the
input layer and they get processed in
the hidden layer and then you get the
output in the output layer so let's take
a little bit of a detailed look into the
working of a neural network so let's say
we want to classify some images between
dogs and cats how do we do this this is
known as a classification process and we
are trying to use neural networks and
deep learning to implement this
classification so how do we do that so
this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two hidden layers and what we
do is we provide labeled training data
which means these images are fed to the
network with the label saying that okay
this is a cat the neural network is
allowed to process it and come up with a
prediction saying whether it is a cat or
a dog and obviously in the beginning
there may be mistakes a cat may be
classified as a dog so we then say that
okay this is wrong this output is wrong
but every time it predicts correctly we
say yes this output is correct so that
learning process so it will go back make
some changes to its weights and biases
we again feed these inputs and it will
give us the output we will check whether
it is correct or not and so on so this
is a iterative process which is known as
the training process so we are training
the neural network and what happens in
the training process these weights and
biases you remember there were weights
like w1 w2 and so on so these weights
and biases keep changing every time you
feed these which is known as an epoch so
there are multiple iterations every
iteration is known as an epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so once again what
is the input this input could be like
1000 images of cats and dogs and they
are labeled because we know which is a
cat and which is a dog and we feed those
thousand images the neural network will
initially assign some weights and biases
for each neuron and it will try to
process extract the features from the
images and it will try to come up with a
prediction for each image and that
prediction that is calculated by the
network is compared with the actual
value whether it is a cat or a dog and
that's how the error is calculated so
let's say there are a thousand images
and in the first run only 500 of them
have been correctly classified that
means we are getting only 50 accuracy so
we feed that information back to the
network further update these weights and
biases for each of the neurons and we
run this these inputs once again it will
try to calculate extract the features
and it will try to predict which of
these is cats and dogs and this time
let's say out of thousand 700 of them
have been predicted correctly so that
means in the second iteration the
accuracy has increased from 50 to 70
percent all right then we go back again
we feed this maybe for a third iteration
fourth iteration and so on and slowly
and steadily the accuracy of this
network will keep increasing and it may
reach probably you never know 90 95
percent and there are several parameters
that are known as hyper parameters that
need to be changed and tweaked and that
is the overall training process and
ultimately at some point we say okay you
will probably never reach hundred
percent accuracy but then we set a limit
saying that okay if we receive 95
percent accuracy that is good enough for
our application and then we say okay our
training process is done so that is the
way training happens and once the
training is done now with the training
data set the system has let's say seen
all these thousand images therefore what
we do is the next step like in any
normal machine learning process we do
the testing where we take a fresh set of
images and we feed it to the network the
fresh set which it has not seen before
as a part of the training process and
this is again nothing new in deep
learning this was there in machine
learning as well so you feed the test
images and then find out whether we are
getting a similar accuracy or not so
maybe that accuracy may reduce a little
bit while training you may get 98
percent and then for test you may get 95
percent but there shouldn't be a drastic
drop like for example you get 98 in
training and then you get 50 or 40
percent with the test that means your
network has not learned you may have to
retrain your network so that is the way
neural network training works and
remember the whole process is about
changing these weights and biases and
coming up with the optimal values of
these weights and biases so that the
accuracy is the maximum possible all
right so a little bit more detail about
how this whole thing works so this is
known as forward propagation which is
the data or the information is going in
the forward direction the inputs are
taken weighted summation is done bias is
added here and then that is fed to the
activation function and then that is
that comes out as an output so that is
forward propagation and the output is
compared with the actual value and that
will give us the error the difference
between them is the error and in
technical terms that is also known as
our cost function and this is what we
would like to minimize there are
different ways of defining the cost
function but one of the simplest ways is
mean square error so it is nothing but
the square of the difference of the
errors or the sum of the squares of the
difference of the errors and this is
also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
square now there are different ways of
defining cost function it need not
always be the mean square error but the
most common one is this so you define
this cost function and you ask the
system to minimize this error so we use
what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate or the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate is very small
then it will probably take very long to
train whereas if the learning rate is
very high then it will appear to be
faster but then it will probably never
what is known as converge now what is
convergence now we are talking about a
few terms here convergence is like this
this is a representation of convergence
so the whole idea of gradient descent is
to optimize the cost function or
minimize the cost function in order to
do that we need to represent the cost
function as this curve we need to come
to this minimum value that is what is
known as the minimization of the cost
function now what happens if we have the
learning rate very small is that it will
take very long to come to this point on
the other hand if you have large higher
learning rate what will happen is
instead of stopping here it will cross
over because the learning rate is high
and then it has to come back so it will
result in what is known as like an
oscillation so it will never come to
this point which is known as convergence
instead it will go back and forth so
these are known as hyper parameters the
learning rate and so on and these have
to be those numbers or those values we
can determine typically using trial and
error out of experience we we try to
find out these values so that is the
gradient descent mechanism to optimize
the cost function and that is what is
used to train our neural network this is
another representation of how the
training process works and here in this
example we are trying to classify these
images whether they are cats or dogs and
as you can see actually each image is
fed in each time one image is fed rather
and these values of x1 x2 up to xn are
the pixel values within this image okay
so those values are then taken and for
each of those values a weight is
multiplied and then it goes to the next
layer and then to the next layer and so
on ultimately it comes as the output
layer and it gives an output as whether
it is a dog or a cat remember the output
will never be a named output so these
would be like a zero or a one and we say
okay zero corresponds to dogs and one
corresponds to catch so that is the way
it typically happens this is a binary
classification we have similar
situations where there can be multiple
classes which means that there will be
multiple more neurons in the output
layer okay so this is once again a quick
representation of how the forward
propagation and the backward propagation
works so the information is going
in this direction which is basically the
forward propagation and at the output
level
we find out what is the cost function
the difference is basically sent back as
part of the backward propagation and
gradient descent then adjust the weights
and biases for the next iteration this
happens iteratively till the cost
function is minimized and that is when
we say the whole the network has
converged or the training process has
converged and there can be situations
where convergence may not happen in rare
cases but by and large the network will
converge and after maybe a few
iterations it could be tens of
iterations or hundreds of iterations
depending on what exactly the number of
iterations can vary and then we say okay
we are getting a certain accuracy and we
say that is our threshold maybe 90
accuracy we stop at that and we say that
the system is trained the trained model
is then deployed for production and so
on so that is the way the neural network
training happens okay so that is the way
classification works in deep learning
using neural network and this slide is
an animation of this whole process as
you can see the forward propagation the
data is going forward from the input
layer to the output layer and there is
an output
and the error is calculated the cost
function is calculated and that is fed
back as a part of backward propagation
and that whole process repeats once
again okay so remember in neural
networks the training process is nothing
but the finding the best values of the
weights and biases for each and every
neuron in the network that's all
training of neural network consists of
finding the optimal values of the
weights and biases so that the accuracy
is maximum all right so with that we
come to the end of the session we all
have a great day thank you very much
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here
