[Music]
welcome back to deep learning and you
can see I have a couple of upgrades so
we have a much better recording quality
now and I hope you can also see that I
finally fix the sound problem so you
should be able to hear me much better
right now and we are back to a new
session we want to talk about a couple
of exciting topics so let's see what
I've got for you so today I want to
start discussing different architectures
and in particular in the first couple of
videos I want to talk a bit about the
early architectures the things that
we've seen in the very early days of
deep learning and we will follow them by
looking into deeper models in later
videos and in the end we want to talk
about learning architectures instead of
what humans might need just dozens of
examples these things will need millions
a lot of what we'll see in the next
couple of slides and videos have of
course being developed for image
recognition and object detection tasks
and in particular two data sets are very
important for these kinds of tarsus this
is the image net data set which you find
in reference 11 it has something like a
thousand classes more than 14 million
images and subsets have been used for
the image net large-scale visual
recognition challenges it contains
natural images of varying size so a lot
of these images have actually been
downloaded from the internet there's
also a smaller data sets if you don't
want to train with like millions of
images right away so there's also very
important the size our data sets so for
10 and sy for 100 which is 10 and 100
classes and there we only have
50k training and 10k testing images the
images have reduced size 32 by 32 in
order to very quickly be able to explore
different architectures pros and cons
and if you have these smaller data sets
then it also doesn't take so long for
training so this is also a very common
data set if you want to evaluate your
architecture okay so based on these
different data sets we then want to go
ahead and look into the early
architectures and I think one of the
most important ones is lunette which was
published in 1998 in reference number 9
and you can see this is essentially the
convolutional neural network as we have
been discussing so far it has been used
for example for letter recognition we
have the convolutional layers that we
have trainable kernel stem pooling
another set of convolutional layers and
another pooling operation and then
towards the end we are going into fully
connected layers where we then gradually
reduce and in the very end we have the
output layer that corresponds to the
number of classes we have been doing
this for millennia
so this is a very typical CN n type of
architecture and this kind of approach
has been used in many papers has
inspired a lot of work we have for every
architecture here key features and you
can see here most of the bullets are in
gray that means that most of these
features did not survive but of course
what survived here was convolution for
spatial features this is the main idea
that is still prevalent all the other
things like subsampling using average
pooling it still used a non-linearity
the tongue and super bowl eCos so it's a
not so deep model right then it had
sparse connectivity between s 2 and C 3
layers as you see here in the figure
so also not that common anymore the
multi-layer perceptrons final classifier
sought
something that we see no longer because
it has been removed for for example of
fully convolutional networks which is a
much better approach and also the
sequence of convolution pooling and
non-linearity is kind of fixed and today
we will do that in a much better way but
of course this architecture is
fundamental for many of the further
developments and I think it's really
important that we are also listing it
here the next milestone that I want to
talk about in this video is Alex net you
find the typical image by the way you
will find exactly this image also in the
original publication so Alex net is
consisting of those two branches that
you see here and you can see that even
in the original publication the top
branch is is cut in half so it's a kind
of artifact that you find in many
representations of Alex net when they
refer to this figure so they figure is
cut into parts but it's not that severe
because those two parts are essentially
identical and one of the reasons why it
was split into two sub networks you
could say is because Alex net has been
implemented on graphical processing
units so this is implemented on GPUs and
it actually was already multi-gpu so the
two branches that you see on the top
they have been implemented on two
different graphics processing units and
they could also be trained and then
synchronized using the software so the
GPU is of course a feature that is still
very prevalent
you know everybody today in deep
learning is very much relying on graphic
processing units as we've seen in
numerous occasions in this lecture it
had essentially eight layers so it's not
such a deep network
it had overlapping max pooling with a
stride of two size of three and it
introduced the Ray Lewis non-linearity
which is also very very commonly used
today so also very important feature and
of course it is the winner of the 2012
imagenet challenge which essentially cut
down the error rate into half so it's
really one of the milestones towards the
breakthrough of CNN's cool AI what else
do we have to combat overfitting in this
architecture already used drop out with
a probability of 0.5 in the first two
fully connected layers and deduced data
augmentation so there were random
transformations and random intensity
variations another key feature was that
it has been employing mini-batch
stochastic gradient descent with
momentum 0.9 and an l2 weight decay with
a parameter setting of 5 times 10 to the
minus 5 and it was using a rather simple
weight initialization just using a
normal distribution and a small standard
deviation which we have seen much better
approaches already in the previous
videos what else is important well we've
seen that the separation is a historical
reason the GPUs at the time but too
small to host the entire network so it
was split on two CPUs another key paper
I would say is the networking Network
paper where they essentially introduced
one-by-one filters this was originally
described as a network in network but
effectively we know it today as one by
one convolutions because they
essentially introduced fully connected
layers over the channels and we use this
recipe now a lot if you want to compress
channels and we fully connect over the
channel dimension so this is very nice
because we've seen already that this is
equivalent to a fully connected layer
and we can now integrate fully connected
layers in terms of one by one
convolution and this enables us this
very nice concept of the fully
convolutional Network
so it has very few parameters shared
across all the activations and then it
introduces this global spatial average
pooling as the last layer and this is
essentially the birth of fully
convolutional neural networks
another very important architecture is
the vgg network the visual geometry
group of the University of Oxford and
they introduced very small kernel sizes
in each convolution and the network is
also very common because it's available
for download so there's pre trained
models available and you can see that
the key feature that they have in this
network is that they essentially reduce
the spatial dimension and they increase
the channel they're mentioned so step by
step and this has this gradual
transformation from spatial domain into
a let's say for the classifier important
interpretation domain so we can see the
specialty dimension goes down at the
same time we go up with the channel
dimension and this allows us to
gradually convert from images and color
images towards meaning so I think the
small kernel sizes are the key feature
that are still used it was typically
used in 16 and 19 layers with max
pooling between some of the layers of
straight to size to learning procedure
was very similar to Aleks net but turn
out to be hard to train and practice you
needed pre-training with shallower
networks in order to construct this so
the network is not so great in terms of
performance has a lot of parameters but
it's pre trained and it's available and
therefore this has also caused the
community to adopt this quite widely so
you can see also when you work with open
source and accessible software this is
also a key feature that is important for
us in order to develop further concept
because parameters can be shared
training models can be shared source
code can be shared and this is why I
think this is a very important instance
of the net
to find in the deep learning landscape
another key network that we already seen
at quite some occasions in this lecture
is Google in it and here we have the
inception we've one version that you
find in reference and 14 I think the
main points that I want to highlight
here is that they had very good ideas in
order to save computations by using a
couple of tricks so they developed these
networks with embedded hardware in mind
and it also just features 1.5 billion
multiply add operations in the inference
time so this is pretty cool but what I
find even cooler
are these inception blocks so in total
it had 22 layers and the global average
pooling as a final layer but these
inception modules are really nice and we
will look at them in a little more
detail on the next couple of slides
because they essentially allow you to
let the network decide whether it wants
to pour or whether it wants to convolved
which is pretty cool and another trick
that is really nice are these auxilary
classifiers that they used in earlier
layers in order to stabilize the
gradient so the idea is that you plug in
your loss in to some of the more early
layers where you already try to figure
out a preliminary classification and
this helps relieve of building deeper
models because you can bring in the loss
at a rather early stage and you know the
deeper you go into the network the more
you go to the earlier layers the more
likely it is that you get a vanishing
gradient and with these auxilary
classifiers you can prevent it to some
extent and it's also quite useful if you
for example want to figure out how many
of those inceptions modules do you
really need then you can work with those
axillary classifiers so that's really a
very interesting content so let's talk a
bit about those inception modules and by
the way the inception modules are of
course something that has survived for
quite some time and it's still being
used in
any of the state-of-the-art deep
learning models so there's different
branches through this networks there's
like only a one by one convolution a one
by one convolution followed by a three
by three convolution or one by one
convolution followed by a five by five
convolution or max pooling followed by
one by one convolution so all of these
branches go in parallel and then you
concatenate the output of the branches
and offer it to the next layer so
essentially this allows then the network
to decide which of the branches it
trusts in the next layer and this way it
can somehow determine whether it wants
to pool or whether it wants to convulse
so you can essentially think about this
as an automatic routing that is
determined during the training also
interesting is that the one by one
filters serve as a kind of bottleneck
layer so you can use that in order to
compress the channels from the previous
layers and then you can compress and
then convolve still there's a lot of
computations if you were to implement it
exactly this way so the idea is then
that they use this bottleneck layer in
order to essentially compress the
correlations between different feature
Maps
so the idea is that you have this one by
one filters and what you do is instead
of running let's say 256 input feature
maps and 256 output feature maps were
free by free convolution this would
already mean that you have something
like 600,000 multiply add operations so
instead you use this bottleneck ideas so
you compress the channels from 256 but
by one by one convolution to 64 then you
do on the 64 channels the three by three
convolution and then you uncompress
essentially from the 64 channels again
to 256 and this saves a lot of
computations so in total you need
approximately
seventy thousand multiplied operations
here and if you look at the original
600,000 multiply add operations then you
can see that we already saved a lot of
computer also because I'm lazy okay so
these are essentially classical deep
learning architectures and we want to
talk about more sophisticated ones in
the second part and there I want to show
you how to go even deeper both deeper
layers and how you can do that
efficiently with for example other
versions of the inception module so
thank you very much for listening and
hope to see you in the next video
goodbye
[Music]
