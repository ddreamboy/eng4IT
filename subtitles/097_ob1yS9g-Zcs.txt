[Music]
neural networks typically used in deep
learning are computing systems inspired
by the structure and function of a human
brain it is used to find solutions to
those problems for which the algorithmic
method is expensive or does not exist
neural networks learn by example so we
do not need to program it in depth hi
guys welcome to the complete neural
network tutorial by simply learn but
before we begin make sure to subscribe
to our channel and hit the bell icon to
never miss an update first we will
understand the basics of neural networks
and see its practical implementation
then we learn how back propagation and
gradient descent works finally we'll
understand the concepts and practical
implementation of convolutional and
recurrent neural networks let's first
look at a small animated video to
understand neural networks better last
summer my family and i visited russia
even though none of us could read
russian we did not have any trouble in
figuring our way out all thanks to
google's real-time translation of
russian boards into english
this is just one of the several
applications of neural networks
neural networks form the base of deep
learning a subfield of machine learning
where the algorithms are inspired by the
structure of the human brain
neural networks take in data train
themselves to recognize the patterns in
this data and then predict the outputs
for a new set of similar data
let's understand how this is done
let's construct a neural network that
differentiates between a square circle
and triangle
neural networks are made up of layers of
neurons these neurons are the core
processing units of the network
first we have the input layer which
receives the input the output layer
predicts our final output
in between exists the hidden layers
which perform most of the computations
required by our network
here's an image of a circle
this image is composed of 28 by 28
pixels which make up for 784 pixels
each pixel is fed as input to each
neuron of the first layer
neurons of one layer are connected to
neurons of the next layer through
channels
each of these channels is assigned a
numerical value known as weight
the inputs are multiplied to the
corresponding weights and their sum is
sent as input to the neurons in the
hidden layer
each of these neurons is associated with
a numerical value called the bias which
is then added to the input sum
this value is then passed through a
threshold function called the activation
function
the result of the activation function
determines if the particular neuron will
get activated or not
an activated neuron transmits data to
the neurons of the next layer over the
channels
in this manner the data is propagated
through the network
this is called forward propagation
in the output layer the neuron with the
highest value fires and determines the
output the values are basically a
probability
for example here our neuron associated
with square has the highest probability
hence that's the output predicted by the
neural network
of course just by a look at it we know
our neural network has made a wrong
prediction but how does the network
figure this out
note that our network is yet to be
trained
during this training process along with
the input our network also has the
output fed to it
the predicted output is compared against
the actual output to realize the error
in prediction the magnitude of the error
indicates how wrong we are and the sign
suggests if our predicted values are
higher or lower than expected
the arrows here give an indication of
the direction and magnitude of change to
reduce the error
this information is then transferred
backward through our network this is
known as back propagation
now based on this information the
weights are adjusted
this cycle of forward propagation and
back propagation is iteratively
performed with multiple inputs
this process continues until our weights
are assigned such that the network can
predict the shapes correctly in most of
the cases
this brings our training process to an
end
you might wonder how long this training
process takes honestly neural networks
may take hours or even months to train
but time is a reasonable trade-off when
compared to its scope
let us look at some of the prime
applications of neural networks facial
recognition cameras on smartphones these
days can estimate the age of the person
based on their facial features this is
neural networks at play first
differentiating the face from the
background and then correlating the
lines and spots on your face to a
possible age forecasting neural networks
are trained to understand the patterns
and detect the possibility of rainfall
or rise in stock prices with high
accuracy music composition neural
networks can even learn patterns in
music and train itself enough to compose
a fresh tune so here's a question for
you which of the following statements
does not hold true a activation
functions are threshold functions b
error is calculated at each layer of the
neural network c both forward and back
propagation take place during the
training process of a neural network d
most of the data processing is carried
out in the hidden layers leave your
answers in the comments section below
with deep learning and neural networks
we are still taking baby steps the
growth in this field has been foreseen
by the big names companies such as
google amazon and nvidia have invested
in developing products such as libraries
predictive models and intuitive gpus
that support the implementation of
neural networks the question dividing
the visionaries is on the reach of
neural networks to what extent can we
replicate the human brain we'd have to
wait a few more years to give a definite
answer but if you enjoyed this video it
would only take a few seconds to like
and share it also if you haven't yet do
subscribe to our channel and hit the
bell icon
as we have a lot more exciting videos
coming up fun learning till then i hope
you're excited to learn more about
neural networks now we have richard who
will take you through the remaining part
of the course and make you understand
neural networks in detail with that over
to you richard
what is a neural network welcome to
today's lesson my name is richard
kirschner i'm with the simply learn team
today we want to discuss the very basics
of a neural network and what that is
so what's in it for you today what is
deep learning what is artificial neural
network how does neural network work
advantages of a neural network
applications of a neural network and the
future of neural networks let's start
with a brief history of the artificial
intelligence hello i am the human brain
this one's seeking enlightenment and has
sat and meditated i am the most complex
organ in the human body and i help you
to think understand and make decisions
and the secret behind all my power is a
neuron i'll get back to that in some
time ever since the 1950s scientists
have been trying to mimic the
functioning of a neuron and use it to
build smarter robots after a lot of
trial and error humans finally designed
a computer that can recognize human
speech it was only after 2000 that
humans were able to give birth to deep
learning that was able to see and
distinguish between different images and
videos
so looking at that let's dive into what
is deep learning now your first thought
might be is the opposite of shallow
learning no deep learning i like into a
magic box and let's go in there just
take a look as to why it's kind of a
magic box so what exactly is deep
learning these are the images of dogs
deep learning is a machine learning
technique that teaches computers to do
what comes naturally to humans learn by
example the robot gets trained with
photos as example now this is very
different than hardwiring a computer
program so that it recognizes something
it actually learns and that's where it's
a magic box because you don't really
control how it learns you control the
aspects that go in the computer comes
back and says wait i know what you are
looks at the photograph of the dog and
it's able to identify that it saw in the
images it says you are a doc woof woof
so that's an example of deep learning
you'll notice we didn't go in we'll go
into the actually how it works behind
the scenes for a neural network but
there's a bit of filling of magic and
that's where the term deep learning
comes in and that's also the term where
i like to call it a magic box you put
these things in here into the program
and it starts running the deep learning
and you have to understand those
settings but you don't have to follow
the exactly what's going on in the deep
learning model that brings us to the
question how does deep learning do it
remember the neuron scientists managed
to build an artificial form of it that
powers any deep learning based machine
so let's talk about artificial neural
networks what is an artificial neural
network to understand how an artificial
neuron works we need to understand how
the real one works first we have a
dendrite input to the neuron and you can
see these little hairs that come in and
they receive information then we have
the cell body information processing
happens here so it takes all these
different dendrites and information
coming in from the different dendrites
and it looks at that information and
then you have your axon which is the
output to the neuron so there goes your
axon and you see it goes all the way out
and at the very end it flanges out each
one of those little flanges connects to
the dendrite or the hairs on the next
one now let's see what an artificial
neural network looks like so an
artificial neural network we have an
input layer so that could be an array of
data each one of those white dots and
the yellow bar would represent say a
pixel in the picture then you have the
lines that connected to the hidden
layers which are your weights and they
add all those up on the hidden layers in
each one of those dots kind of like a
cell does something with all the inputs
and then it puts an output into the next
hidden layer and so on into the output
layer so information processing happens
here input to the neuron output to the
neuron so you can see how they are
similar we have an input which is our
yellow bar coming in and then you like
liken each of the hidden layers to being
a neuron and it passes it to the next
one and so on and then you have an
output to the next neuron or an output
to the real world a neural network is a
system of hardware and or software
pattern after the operation of neurons
in the human brain neural networks also
called artificial neural networks is a
way of achieving deep learning how does
artificial neural networks work let us
find out how does an artificial neural
network work hey siri what is the time
now it's 12 30 in the morning thanks
let's find out how she recognizes speech
here is a neural network in the
different layers on it so we have our
input layer our hidden layers and the
output layer this is the sentence that
needs to be recognized by the network
what is the time so when it comes in
each one comes in as a pattern of sound
so what is the time first let's consider
the word what and you have w-h-a-t and
you can see each one of those in the
sound bar probably looks a little
different than that just a
representation comes in as a different
pattern now we will split the sound wave
for the letter w into smaller segments
so we split off w and then we take w
when we analyze just w as the amplitude
is varying in the sound wave for w we
collect the values at different
intervals and form an array so we have
0.5 1.5 1.7 1.9 that might be the
different amplitudes coming in and we
feed the array of amplitudes to the
input layer so each one of those goes
into its own box on the input layer
random weights are assigned to each
interconnection between input and hidden
layer so remember all those little lines
i said those are special weights now
we're going to start by doing it
randomly we always start with randoms if
we start with some kind of preset
identical pattern like if you set them
all to three take forever to train it
and you're less likely to get a good
result where random works really well in
this the weights are multiplied with the
inputs and a bias is added to form the
transfer function so we make a sum of
all the weights times the value so you
take 0.5 which is your x coming in and
we're going to multiply that by w1 w2 w3
so on and then we get to the next level
we're going to add those together coming
in what's coming in so we add the weight
times x and there's always a bias added
in if you ever build your own neural
network don't forget to add the bias in
otherwise it tends to not work quite as
well you need that extra layer in there
to help it weights are assigned to the
interconnection between the hidden
layers the output of the transfer
function is fed as an input to the
activation function so the output from
one hidden layer becomes the input to
the next hidden layer acoustic model
contains the statistical representation
of each distinct sound that makes a word
and so we start building these
acoustical models and as these layers
separate them out they'll start learning
what the different models are for the
different letters lexicon contains the
data for different pronunciations of
every word so we have the lexicon at the
end where we end up with the a b c d and
it identifies the different letters in
there now the term acoustic model and
the term lexicon are specific to this
domain the domain of understanding
speech certainly when you're doing
photographs and other things you'll have
different labels on here but the process
is going to be the same and finally we
get our output later following the same
process for every word and letter the
neural network recognizes the sentence
you said what is the time so identify as
a w-h-a-t and then identifies that
that's one word what is the time and so
on it's 12 30. that way siri can look up
the time and read it back to you so
let's look at the advantages of an
artificial neural network so gentlemen
could you tell me the advantages of an
artificial neural network it's amazing
how many times i've been in that
situation where i have to explain to the
people making the decisions in the
company it's amazing how many times i've
been in that space where i have to
explain to the owner of the company what
the artificial intelligence and the
neural network actually do what are the
advantages of an artificial neural
network and what it can do for them and
how it works so an artificial neural
network outputs aren't limited entirely
by inputs and results given to them
initially by an expert system this
ability comes in handy for robotics and
pattern recognition systems artificial
neural networks have the potential for
high fault tolerance artificial neural
networks are capable of debugging or
diagnosing a network on their own very
common use these days is to go through
all the log files and sort them out
thousands of log files if you're in
working as an admin non-linear systems
have the capability of finding shortcuts
to reach computational expensive
solutions so we see this in banking
where by hand they have an excel
spreadsheet and then they start building
codes around that excel spreadsheet and
over 20 years they might build a
repertoire of all these functions and
the neural network comes up with the
same answers done in days weeks or even
a month for a huge bank so let's take a
look a little bit more because i
mentioned a couple applications of
artificial intelligence there but let's
dig deeper into the applications of
artificial intelligence let's look at
some of the real life this is stuff
going on right now in our world and
we're in such an exciting time with the
neural networks and the machine learning
and the artificial intelligence
development so let's take a look at some
of the current applications going on in
real life and you can use your
imagination to dig for some new ones
that we don't have listed here because
it's so
limitless the amount of applications
that are being worked on right now or
being implemented handwriting
recognition neural network is used to
convert handwritten characters into
digital characters that the system can
recognize
stock exchange prediction if you've ever
worked with stock exchange which i have
it is so fickled to track i mean it is
really hard to understand there are many
factors that affect the stock market
neural network can examine a lot of
factors and predict the prices on a
daily basis helping the stock brokers so
right now it's still intro phase where
it helps them and they really have to
look closely at it when you realize that
we generate over three terabytes a day
just from the stock exchange here in the
united states that's a lot of data to
dig through and you have to sort it out
before you even start focusing on even
one stock traveling salesman problem it
refers to finding the optimal path to
travel between all cities in an area
neural network helps solve the problem
providing higher revenue at a minimal
cost logistics is huge just the
logistics that we talk about salesmen
traveling from town to town logistics
are used by amazon amazon loves to ship
their packages and they have empty space
on their trucks so they'll pre-ship
packages and fill that empty space on
who they think will buy it saves them a
lot of time and people are a lot happier
because they get that tomorrow instead
of having to wait three weeks image
compression idea behind data compression
neural network is to store encrypt and
recreate the actual image again so we
can optimize our compression and data
images are the biggest one but it's
using all kinds of data wonderful
application to save a hard drive and to
optimize being able to read it back out
again those are just a few and like i
said use your mind to dig deeper and
let's take it even further we're going
to go step further here and let's look
at the future of deep learning here we
are that's not me thank goodness
wonderful person there reading her
crystal ball i'll tell you what i see in
the future
more personalized choices for users and
customers all over the world i certainly
like that when i go in there and
whatever online ordering system starts
referring stuff to me local company here
where i live that uses this where you
can take a picture and it starts looking
for what you want based on your picture
so if you see a couch you like starts
looking for furniture like that or
clothing i think it's mainly clothing
hyper intelligent virtual assistants
will make life easier if you played with
google assistant or siri or any of those
you can see how they're slowly evolving
and they're just now getting over that
hump where a virtual assistant can do
all kinds of things even pre-write your
email response for you new forms of
algorithm for learning methods would be
discovered there's always something
rolling out they've had some really cool
research in this area again this stuff
is such a we're just in the infant stage
of artificial intelligence and neural
networks and actually applying them to
the real world wonderful time to jump in
neural networks will be a lot faster in
the future neural network tools will be
embedded in every design surface we
already see that that you can buy a
little mini neural network that plugs
into a really cheap processing board or
into your laptop so the hardware is
starting to come out that goes right in
there where you can dump it on there and
that makes it also faster so because
it's on the hardware instead of the
software side neural networks will be
used in the field of medicine
agriculture physics discoveries just
everything you can imagine we see this
today where it's going from a phd
student in medicine trying to understand
t cells and understand the statistic
analysis of that to cure people to help
keep them healthy to help find out how
we heal to something that anybody can go
access and process the data on they're
working on shared data systems this
concept of it being used in these
different fields and these different
domains is huge the world's wide open
for anybody jumping out there to start
exploring them and start learning neural
networks let's dive in and say how does
a neural network work so now we've come
far enough to understand how neural
network works let's go ahead and walk
through this a nice graphical
representation they usually describe a
neural network as having different
layers you'll see that we've identified
a green layer an orange layer and a red
layer the green layer is the input so
you have your data coming in it picks up
the input signals and passes them to the
next layer the next layer does all kinds
of calculations and feature extraction
it's called the hidden layer a lot of
times there's more than one hidden layer
we're only showing one in this picture
but we'll show you how it looks like in
a more detail a little bit and then
finally we have an output layer this
layer delivers the final result so the
only two things we see is the input
layer and the output layer now let's
make use of this neural network and see
how it works wonder how traffic cameras
identify vehicles registration plate on
the road to detect speeding vehicles and
those breaking the law they got me going
through a red light the other day well
last month that's like the horrible
thing they sent you this picture of you
and all your information because they
pulled it up off of your license plate
and your picture they shouldn't have
gone through the red light so here we
are and we have an image of a car you
can see the license plates on there so
let's consider the image of this vehicle
and find out what's on the number plate
the picture itself is 28 by 28 pixels
and the image is fed as an input to
identify the registration plate each
neuron has a number called activation
that represents the grayscale value of
the corresponding pixel range and we
range it from zero to one one for a
white pixel and zero for a black pixel
and you can see down here we have an
example where one of the pixels is
registered as like 0.82 meaning it's
probably pretty dark each neuron is lit
up when its activation is close to one
so as we get closer to black on white we
can really start seeing the details in
there and you can see again the pixel
shows this one up there it's like part
of the car and so it lights up so pixels
in the form of arrays are fed to the
input layer and so we see here the pixel
of a car image fed as an input and
you're going to see that the input layer
which is green is one dimension while
our image is two dimension now when we
look at our setup that we're programming
in python it has a cool feature that
automatically does the work for us if
you're working with an older neural
network pattern package you then convert
each one of those rows so it's all one
array so you'd have like row one and
then just tack row two on to the end you
can almost feed the image directly into
some of these neural networks the key is
though is that if you're using a 28 by
28 and you get a picture of this 30 by
30 shrink the 30 by 30 down to fit the
28 by 28 so you can't increase the
number of input in this case green dots
it's very important to remember when you
work on neural networks and let's name
the inputs x1 x2 x3 respectively so each
one of those represents one of the
pixels coming in and the input layer
passes it to the hidden layer and you
can see here we now have two hidden
layers in this image in the orange and
each one of those pixels connects to
each one of those hidden layers and the
interconnections are assigned weights at
random so they get these random weights
that come through that if x1 lights up
then it's going to be x1 times this
weight going into the hidden layer and
we sum those weights the weights are
multiplied with the input signal and a
bias is added to all of them so as you
can see here we have x1 comes in and it
actually goes to all the different
hidden layer nodes or in this case
whatever you want to call them network
setup the orange dots and so you take
the value of x1 you multiply it by the
weight for the next hidden layer so x1
goes to hidden layer one x1 goes to
hidden layer two x1 goes hidden layer
one node two hidden layer one node three
and so on and the bias a lot of times
they just put the bias in as like
another green dot or another orange dot
and they give the bias a value one and
then all the weights go in from the bias
into the next node so the bias can
change we always just remember that you
need to have that bias in there there's
things that can be done with it
generally most the packages out there
control that for you so you don't have
to worry about figuring out what the
bias is but if you ever dive deep into
neural networks you got to remember
there's a bias or the answer won't come
out correctly the weight weighted sum of
the input is fed as an input to the
activation function to decide which
nodes to fire and for feature extraction
as the signal flows within the hidden
layers the weighted sum of inputs is
calculated and is fed to the activation
function in each layer to decide which
nodes to fire so here's our feature
extraction of the number plate and you
can see these are still hidden nodes in
the middle and this becomes important
we're going to take a little detour here
and look at the activation function so
we're going to dive just a little bit
into the math so you can start to
understand where some of the games go on
when you're playing with neural networks
in your programming so let's look at the
different activation functions before we
move ahead here's our friendly red tag
shopping robot and so one is a sigmoid
function and the sigmoid function which
is one over one plus e to the minus x
takes the x value and you can see where
it generates almost a zero and almost a
one with a very small area in the middle
where it crosses over and we can use
that value to feed into another function
so if it's really uncertain it might
have a 0.1 or 0.2 or 0.3 but for the
most part it's going to be really close
to 1 and really close to this case 0 0
to 1. the threshold function so if you
don't want to worry about the
uncertainty in the middle you just say
oh if x is greater than or equal to zero
if not then x is zero so it's either
zero or one really straightforward
there's no in between in the middle and
then you have the what they call the
relu relu function and you can see here
where it puts out the value but then it
says well if it's over one it's going to
be one and if it's less than zero it's
zero so it kind of just dead ends it on
those two ends but allows all the values
in the middle and again this like the
sigmoid function allows that information
to go to the next level so it might be
important to know if it's a 0.1 or a
minus 0.1 the next hidden layer might
pick that up and say oh this piece of
information is uncertain or this value
has a very low certainty to it and then
the hyperbolic tangent function and you
can see here it's a 1 minus e to the
minus 2x over 1 plus e to the minus 2x
and it's very much along the same theme
a little bit different in here in that
it goes between one and one so you'll
see some of these they go zero to one
but this one goes minus one to one and
if it's less than zero it's you know it
doesn't fire and if it's over zero it
fires and it also still puts out a value
so you still have a value you can get
off of that just like you can with the
sigmoid function and the relu function
very similar in use and i believe the
original used to be everything was done
in the sigmoid function that was the
most commonly used and now they just
kind of use more the relu function the
reason is one it processes faster
because you already have the value and
you don't have to add another compute
the one over one plus e to the minus x
for each hidden node and the data coming
off works pretty good as far as putting
it into the next level if you want to
know just how close it is to zero how
close is it not to functioning you know
is it minus point one minus point two
usually they're float values you get
like minus point minus .00138 or
something so yeah important information
but the relu is most commonly used these
days as far as the setup we're using but
you'll also see the sigmoid function
very commonly used also now that you
know what an activation function is
let's get back to the neural network so
finally the model would predict the
outcome of applying a suitable
activation function to the output layer
so we go in here we look at this we have
the optical character recognition ocr is
used on the images to convert it into a
text in order to identify what's written
on the plate and as it comes out you'll
see the red node and the red node might
actually represent just a letter a so
there's usually a lot of outputs when
you're doing text identification we're
not going to show that on here but you
might have it even in the order it might
be what order the license plates in so
you might have a b c d e f g
you know the alphabet plus the numbers
and you might have the one two three
four five six seven eight nine ten
places so it's a very large array that
comes out it's not a small amount of you
know we show three dots coming in eight
hidden layer nodes you know two sets of
four we just saw one red coming out a
lot of times this is uh you know 28
times 28 if you did 30 times 30 that's
you know 900 nodes so 28 is a little bit
less than that just on the input and so
you can imagine the hidden layer is just
as big each hidden layer is just as big
if not bigger and the output is going to
be there's so many digits yeah it's a
lot there's it's a huge amount of input
and output but we're only showing you
just you know it would be hard to show
in one picture and so it comes up and
this is what it finally gets out on the
output as it identifies a number on the
plate and in this case we have 0 8
d
0 3 8 5 8. error in the output is back
propagated through the network and
weights are adjusted to minimize the
error rate this is calculated by a cost
function when we're training our data
this is what's used and we'll look at
that in the code when we do the data
training so we have stuff we know the
answer to and then we put the
information through and it says yes that
was correct or no because remember we
randomly set all the weights to begin
with and if it's wrong we take that
error how far off are you you know are
you off but is it if it was like minus
one you're just a little bit off if it's
like minus 300 was your output remember
when we're looking at those different
options you know hyperbolic or whatever
and we're looking at the rel the rel
doesn't have a limit on top or bottom it
actually just generates a number so if
it's way off you have to adjust those
weights a lot but if it's pretty close
you might adjust the relates just a
little bit and you keep adjusting the
weights until they fit all the different
training models you put in so you might
have 500 training models and those
weights will adjust using the back
propagation it sends the error backward
the output is compared with the original
result and multiple iterations are done
to get the maximum accuracy so not only
does it look at each one but it goes
through it and just keeps cycling
through these the data making small
changes in the network until it gets the
right answers with every iteration the
weights at every interconnection are
adjusted based on the error we're not
going to dive into that math because it
is a differential equation and it gets a
little complicated but i will talk a
little bit about some of the different
options they have when we look at the
code so we've explored a neural network
let's look at the different types of
artificial neural networks and this is
like the biggest area growing is how
these all come together let's see the
different types of neural network and
again we're comparing this to human
learning so
here's a human brain i feel sorry for
that poor guy so we have a feed for
forward neural network simplest form of
a they call it a n a neural network data
travels only in one direction input to
output this is what we just looked at so
as the data comes in all the weights are
added it goes to the hidden layer all
the weights are added it goes to the
next hidden layer all the weights are
added and it goes to the output the only
time you use the reverse propagation is
to train it so when you actually use it
it's very fast when you're training it
it takes a while because it has to
iterate through all your training data
and you start getting into big data
because you can train these with a huge
amount of data the more data you put in
the better train they get the
applications vision and speech
recognition actually they're pretty much
everything we talked about a lot almost
all of them used this form of neural
network at some level radial basis
function neural network this model
classifies the data point based on its
distance from a center point what that
means is that you might not have
training data so you want to group
things together and you create central
points and it looks for all the things
you know some of these things are just
like the other if you've ever watched a
sesame street as a kid that dates me so
it brings things together and this is a
great way if you don't have the right
training model you can start finding
things that are connected you might not
have noticed before applications power
restoration systems they try to figure
out what's connected and then based on
that they can fix the problem if you
have a huge power system cajon and
self-organizing neural network vectors
of random dimensions are input to
discrete map comprised of neurons so
they basically find a way to draw they
call them they say dimensions or vectors
or planes because they actually chop the
data in one dimension two dimension
three dimension four five six they keep
adding dimensions and finding ways to
separate the data and connect different
data pieces together applications used
to recognize patterns and data like in
medical analysis the hidden layer saves
its output to be used for future
prediction recurrent neural networks so
the hidden layers remember its output
from last time and that becomes part of
its new input you might use that
especially in robotics or flying a drone
you want to know what your last change
was and how fast it was going to help
predict what your next change you need
to make is to get to where the drone
wants to go applications text-to-speech
conversation model so you know i talked
about drones but you know just
identifying on lexis or google assistant
or any of these they're starting to add
in i'd like to play a song on my pandora
and i'd like it to be at volume 90
so you now can add different things in
there and it connects them together the
input features are taken in batches like
a filter this allows a network to
remember an image in parts convolution
neural network today's world in photo
identification and taking apart photos
and trying to you know you ever seen
that on google where you have five
people together this is the kind of
thing separates all those people so then
it can do a face recognition on each
person applications used in signal and
image processing in this case i use
facial images or google picture images
as one of the options modular neural
network it has a collection of different
neural networks working together to get
the output so wow we just went through
all these different types of neural
networks and the final one is to put
multiple neural networks together i
mentioned that a little bit when we
separated people in a larger photo in
individuals in the photo and then do the
facial recognition on each person so one
network is used to separate them and the
next network is then used to figure out
who they are and do the facial
recognition applications still
undergoing research this is a cutting
edge you hear the term pipeline and
there's actual in python code and in
almost all the different neural network
setups out there they now have a
pipeline feature usually and it just
means you take the data from one neural
network and maybe another neural network
or you put it into the next neural
network and then you take three or four
other neural networks and feed them into
another one so how we connect the neural
networks is really just cutting edge and
it's so experimental i mean it's almost
creative in its nature there's not
really a science to it because each
specific domain has different things
it's looking at so if you're in the
banking domain it's going to be
different than the medical domain than
the automatic car domain and suddenly
figuring out how those all fit together
is just a lot of fun and really cool so
we have our types of artificial neural
network we have our feed forward neural
network we have a radial basis function
neural network we have our cohenon
self-organizing neural network recurrent
neural network convolution neural
network and modular neural network where
it brings them all together and no the
colors on the brain do not match what
your brain actually does but they do
bring it out that most of these were
developed by understanding how humans
learn and as we understand more and more
of how humans learn we can build
something in the computer industry to
mimic that to reflect that and that's
how these were developed so exciting
part use case problem statement so this
is where we jump in this is my favorite
part let's use the system to identify
between a cat and a dog if you remember
correctly i said we're going to do some
python code and you can see over here my
hair is kind of sticking up over the
computer cup of coffee on one side and a
little bit of old school a pencil and a
pen on the other side yeah most people
now take notes
i love the stickies on the computer
that's great that's that is my computer
i have sticky notes on my computer in
different colors so not too far from
today's programmer so the problem is is
we want to classify photos of cats and
dogs using a neural network and you can
see over here we have quite a variety of
dogs in the pictures and cats and you
know just sorting out it is a cat it's
pretty amazing and why would anybody
want to even know the difference between
a cat and a dog okay you know why well i
have a cat door it'd be kind of fun that
instead of it identifying instead of
having like a little collar with a
magnet on it which is what my cat has
the door would be able to see oh that's
the cat that's our cat coming in oh
that's the dog we have a dog too that's
a dog i want to let in maybe i don't
want to let this other animal in because
it's a raccoon so you can see where you
could take this one step further and
actually apply this you could actually
start a little startup company idea
self-identifying door so this use case
will be implemented on python i am
actually in python
3.6 it's always nice to tell people the
version of python because it does affect
sometimes which modules you load and
everything and we're going to start by
importing the required packages i told
you we're going to do this in cross so
we're going to import from karas models
sequential from the cross layers
conversion 2d or conv 2d max pooling 2d
flatten and dense and we'll talk about
what each one of these do in just a
second but before we do that let's talk
a little bit about the environment we're
going to work in and you know in fact
let me go ahead and open a
the website cross's website so we can
learn a little bit more about cross so
here we are on the cross website and
it's a k-e-r-a-s dot io that's the
official website for karass and the
first thing you'll notice is that cross
runs on top of either tensorflow
cntk and i think it's pronounced thanos
or thiano what's important on here is
that tensorflow and the same is true for
all these but tensorflow is probably one
of the most widely used currently
packages out there with the cross and of
course you know tomorrow this is all
going to change it's all going to
disappear and they'll have something new
out there so make sure when you're
learning this code that you understand
what's going on and also know the code i
mean look when you look at the code it's
not as complicated once you understand
what's going on the code itself is
pretty straightforward and the reason we
like karas and the reason that people
are jumping on it right now is such a
big deal is if we come down here let me
just scroll down a little bit we talk
about user friendliness modularity easy
extensibility work with python python is
a big one because a lot of people in
data science now use python although you
can actually access cross other ways if
we continue down here is layers and this
is where it gets really cool when we're
working with cross you just add layers
on remember those hidden layers we were
talking about and we talked about the
relu
activation you can see right here let me
just up that a little bit in size there
we go that's big i can add in an relu
layer and then i can add in a soft max
layer the next instance we didn't talk
about soft max so you can do each layer
separate now if i'm working in some of
the other kits i use i take that and i
have one setup and then i feed the
output into the next one this one i can
just add hidden layer after hidden layer
with the different information in it
which makes it very powerful and very
fast to spin up and try different setups
and see how they work with the data
you're working on and we'll dig a little
bit deeper in here and a lot of this is
very much the same so when we get to
that part i'll point that out to you
also now just a quick side note i'm
using anaconda with python in it and i
went ahead and created my own package
and i called it the cross python36
because i'm in python36 anaconda is cool
that way you can create different
environments real easily if you're doing
a lot of different experimenting with
these different packages probably want
to create your own environment in there
and the first thing is you can see right
here there's a lot of dependencies a lot
of these you should recognize by now if
you've done any of these videos if not
kudos for you for jumping in today pip
install numpy scipy the scikit learn
pillow and h5py are both needed for the
tensorflow and then putting the cross on
there and then you'll see here and pip
is just a standard installer that you
use with python you'll see here that we
did pip install tensorflow since we're
going to do cross on top of tensorflow
and then pip install and i went ahead
and used the github so git plus get and
you'll see here github.com this is one
of their releases one of the most
current release on there that goes on
top of tensorflow you can look up these
instructions pretty much anywhere this
is for doing it on anaconda certainly
you'd want to install these if you're
doing it in ubuntu server setup you want
to get i don't think you need the h5py
and ubuntu but you do need the rest in
there because they are dependencies in
there and it's pretty straightforward
and that's actually in some of the
instructions they have on their website
so you don't have to initially go
through this just remember their website
on there and then when i'm under my
anaconda navigator which i like you'll
see where i have environments and on the
bottom i created a new environment and i
called it cross python 36 just to
separate everything you can say i have
python 30.5 and python 36. i used to
have a bunch of other ones but it kind
of cleaned house recently and of course
once i go in here i can launch my
jupiter notebook making sure i'm using
the right environment that i just set up
this of course opens up my in this case
i'm using google chrome and in here i
can go and just create a new document in
here and this is all in your browser
window when you use the anaconda do you
have to use anaconda and jupiter
notebook no you can use any kind of
python editor whatever setup you're
comfortable with and whatever you're
doing in there so let's go ahead and go
in here and paste the code in and we're
importing a number of different settings
in here we have import sequential that's
under the models because that's the
model we're going to use as far as our
neural network and then we have layers
and we have conversion 2d max pooling 2d
flatten dense and you can actually just
kind of guess at what these do we're
talking we're working in a 2d photograph
and if you remember correctly i talked
about how the actual input layer is a
single array it's not in two dimensions
it's one dimension all these do is these
are tools to help flatten the image so
it takes a two-dimensional image and
then it creates its own proper setup you
don't have to worry about any of that
you don't have to do anything special
with the photograph you let the cross do
it we're going to run this and you'll
see right here they have some stuff that
is going to be depreciated and changed
because that's what it does everything's
being changed as we go you don't have to
worry about that too much if you have
warnings if you run it a second time the
warning will disappear and this has just
imported these packages for us to use
jupiter is nice about this you can do
each thing step by step and i'll go
ahead and also zoom in there a little
control plus that's one of the nice
things about being in a browser
environment so here we are back another
sip of coffee
if you're familiar with my other videos
you notice i'm always sipping coffee i
always have a in my case latte next to
me an espresso so the next step is to go
ahead and initialize we're going to call
it the cnn or classifier neural network
and the reason we call it a classifier
is because it's going to classify it
between two things it's going to be cat
or dog so when you're doing
classification you're picking specific
objects you're specific it's a true or
false yes no it is something or it's not
so first thing we're going to create our
classifier and it's going to equal
sequential so their sequential setup is
the classifier that's the actual model
we're using that's the neural network so
we call it a classifier and the next
step is to add in our convolution and
let me just do a shrink that down in
size you can see the whole line and
let's talk a little bit about what's
going on here i have my classifier and i
add something what am i adding well i'm
adding my first layer this first layer
we're adding in is probably the one that
takes the most work to make sure you
have it set correct and the reason i say
that this is your actual input and we're
going to jump here to the part that says
input shape equals 64 by 64 by 3. what
does that mean well that means that our
pictures coming in and there's these
pictures remember we had like the
picture of the car was 128 by 128 pixels
well this one is 64 by 64 pixels and
each pixel has three values that's where
these numbers come from and it is so
important that this matches i mentioned
a little bit that if you have like a
larger picture you have to reformat it
to fit this shape if it comes in as
something larger there's no input notes
there's no input neural network there
that will handle that extra space so you
have to reshape your data to fit in here
now the first layer is the most
important because after that keras knows
what your shape is coming in here and it
knows what's coming out and so that
really sets the stage most important
thing is that input shape matches your
data coming in and you'll get a lot of
errors if it doesn't you'll go through
there and picture number 55 doesn't
match it correctly and guess what it
does it usually gives you an error and
then the activation if you remember we
talked about the different activations
on here we're using the relu model like
i said that is the most commonly used
now because one it's fast it doesn't
have the added calculations in it it
just says here's the value coming out
based on the weights and the value going
in and um from there you know it's uh if
it's over one then it's good or over
zero it's good if it's under zero then
it's considered not active and then we
have this conversion 2d what the heck is
conversion 2d i'm not going to go into
too much detail in this because this has
a couple of things it's doing in here a
little bit more in depth than we're
ready to cover in this tutorial but this
is used to convert from the photo
because we have 64 by 64 by three and
we're just converting it to two
dimensional kind of setup so it's very
aware that this is a photograph and that
different pieces are next to each other
and then we're going to add in a second
convolutional layer that's what the conv
stands for 2d so it's these are hidden
layers so we have our input layer and
our two hidden layers and they are two
dimensional because we're doing with a
two dimensional photograph and you'll
see down here that on the last one we
add a max pooling 2d and we put a pool
size equals 2 2. and so what this is is
that as you get to the end of these
layers one of the things you always want
to think of is what they call mapping
and then reducing wonderful terminology
from the big data we're mapping this
data through all these layers and now we
want to reduce it to only two sets in
this case it's already in two sets
because it's a 2d photograph but we had
you know two dimensions by we actually
have 64 by 64 by three so now we're just
getting it down to a two by two just the
two dimension two dimensional instead of
having the third dimension of colors and
we'll go ahead and run these we're not
really seeing anything in our run script
because we're just setting up this is
all set up and this is where you start
playing because maybe you'll add a
different layer in here to do something
else to see how it works and see what
your output is that's what makes cross
so nice is i can with just a couple
flips of code put in a whole new layer
that does a whole new processing and see
whether that improves my run or makes it
worse and finally we're going to do the
final setup which is to flatten
classifier add a flattened setup and
then we're going to also add a layer a
dense layer and then we're going to add
in another dense layer and then we're
going to build it we're going to compile
this whole thing together so let's flip
over and see what that looks like and
we've even numbered them for you so
we're going to do the flattening and
flatten is exactly what it sounds like
we've been working in a two-dimensional
array of picture which actually is in
three dimensions because the pixels the
pixels have a whole other dimension to
it of three different values and we've
kind of resized those down to two by two
but now we're just going to flatten it i
don't want to have multiple dimensions
being worked on by tensor and by keras i
want just a single array so it's
flattened out and then step four full
connection so we add in our final two
layers and you could actually do all
kinds of things with this you could
actually leave out this some of these
layers and play with them you do need to
flatten it that's very important then we
want to use the dents again we're taking
this and we're taking whatever came into
it so once we take all those different
the two dimensions or three dimensions
as they are and we flatten it to one
dimension we want to take that and we're
going to pull it into units of 128. they
got that you say where did they get 128
from you could actually play with that
number and get all kinds of weird
results but in this case we took the 64
plus 64 is 128. you could probably even
do this with 64 or 32. usually you want
to keep it in the same multiple whatever
the data shape you're already using is
in and we're using the activation the
relu just like we did before and then we
finally filter all that into a single
output and it has how many units one why
because we want to know whether true or
false it's either a dog or a cat you
could say one is dog zero is cat or
maybe you're a cat lover and it's one is
cat and zero is dog if you love both
dogs and cats you're gonna have to
choose and then we use the sigmoid
activation if you remember from before
we had the relu and there's also the
sigmoid the sigmoid just makes it clear
it's yes or no we don't want any kind of
in-between number coming out and we'll
go ahead and run this and you'll see
it's still all in setup and then finally
we want to go ahead and compile and
let's put the compiling our classifier
neural network and we're going to use
the optimizer atom and i hinted at this
just a little bit before where does adam
come in where does an optimizer come in
well the optimizer is the reverse
propagation when we're training it it
goes all the way through and says error
and then how does it readjust those
weights there are a number of them atom
is the most commonly used and it works
best on large data most people stick
with the atom because when they're
testing on smaller data see if their
model is going to go through and get all
their errors out before they run it on
larger data sets they're going to run it
on atom anyway so they just leave it on
atom most commonly used but there are
some other ones out there you should be
aware of that that you might try them if
you're stuck in a bind or you might blur
that in the future but usually atom is
just fine on there and then you have two
more settings you have loss and metrics
we're not going to dig too much into
loss or metrics these are things you
really have to explore cross because
there are so many choices this is how it
computes the error there's so many
different ways to on your back
propagation and your training so we're
using the atom model but you can compute
the error by standard deviation standard
deviation squared they use binary cross
entropy i'd have to look that up to even
know what that is there's so many of
these a lot of times you just start with
the ones that look correct that are most
commonly used and then you have to go
read the cross site and actually see
with these different losses and metrics
and what different options they have so
we're not going to get too much into
them other than to reference you over to
the cross website to explore them deeper
but we are going to go ahead and run
them and now we've set up our classifier
so we have an object classifier and if
you go back up here you'll see that
we've added in step one we added in our
layer for the input we added a layer
that comes in there and uses the relu
for activation and then it pulls the
data so this is even though these are
two layers the actual neural network
layer is up here and then it uses this
to pull the data into a two by two so
into a two-dimensional array from a
three-dimensional array with the colors
then we flatten it so there's our outer
flattened and then we add another dense
what they call dense layer this dense
layer goes in there and it downsizes it
to 128 it reduces it so you can look at
this as we're mapping all this data down
the two dimensional setup and then we
flatten it so we map it to a flattened
map and then we take it and reduce it
down to 128 and we use the relu again
and then finally we reduce that down to
just a single output and we use the
sigmoid to do that to figure out whether
it's yes no true false in this case cat
or dog and then finally once we put all
these layers together we compile them
that's what we've done here and we've
compiled them as far as how it trains to
use these settings for the training back
propagation so if you remember we talked
about training our setup and when we go
into this you'll see that we have two
data sets we have one called the
training set and the testing set and
that's very standard in any data
processing is you need to have that's
pretty common in any data processing is
you need to have a certain amount of
data to train it and then you got to
know whether it works or not is it any
good and that's why you have a separate
set of data for testing it we already
know the answer but you don't want to
use that as part of the training set so
in here we jump into part two fitting
the classifier neural network to the
images and then from cross and let me
just zoom in there i always love that
about working with jupiter notebooks you
can really see we're going to come in
here we do the cross pre-processing and
image and we import image data generator
so nice of cross it's such a high-end
product right now going out and since
images are so common they already have
all this stuff to help us process the
data which is great and so we come in
here we do train data gin and we're
going to create our object for helping
us train it for reshaping the data so
that it's going to work with our setup
and we use an image data generator and
we're going to rescale it and you'll see
here we have one point which tells us
it's a float value on the rescale over
255. where does 255 come from well
that's the scale in the colors of the
pictures we're using their value from 0
to 255. so we want to divide it by 255
and it'll generate a number between 0
and 1. they have shear range and zoom
range horizontal flip equals true and
this of course has to do with if the
photos are different shapes and sizes
like i said it's a wonderful package you
really need to dig in deep to see all
the different options you have for
setting up your images for right now
though we're going to stick with some
basic stuff here and let me go ahead and
run this code and again it doesn't
really do anything because we're still
setting up the pre-processing let's take
a look at this next set of code and this
one is just huge we're creating the
training set so the training set is
going to go in here and it's going to
use our train data gen we just created
dot flow from directory it's going to
access in this case the path data set
training set that's a folder so it's
going to pull all the images out of that
folder now i'm actually running this in
the folder that the data sets in so if
you're doing the same setup and you load
your data in there and you're doing this
make sure wherever your jupyter notebook
is saving things to that you create this
path or you can do the complete path if
you need to you know c colon slash etc
and the target size the batch size and
class mode is binary so the class is
we're switching everything to a binary
value batch size what the heck is batch
size well that's how many pictures we're
going to batch through the training each
time and the target size 64 by 64.
little confusing but you can see right
here that this is just a general
training and you can go in there and
look at all the different settings for
your training set and of course with
different data we're doing pictures
there's all kinds of different settings
depending on what you're working with
let's go ahead and run that and see what
happens and you'll see that it found 800
images belonging to one classes so we
have 800 images in the training set and
if we're going to do this with the
training set we also have to format the
pictures in the test set now we're not
actually doing any predictions we're not
actually programming the model yet
all we're doing is preparing the data so
we're going to prepare a training set
and the test set so any changes we make
to the training set at this point also
have to be made to the test set so we've
done this thing we've done a train data
generator we've done our training set
and then we also have remember our test
set of data so i'm going to do the same
thing with that i'm going to create a
test data gen and we're going to do this
image data generator we're going to
rescale 1 over 255 we don't need the
other settings just the single setting
for the test datagen and we're going to
create our test set we're going to do
the same thing we did with the test set
except that we're pulling it from the
test set folder and we'll run that and
you'll see in our test set we found 2
000 images that's about right we're
using 20 percent of the images as test
and 80 to train it and then finally
we've set up all our data we've set up
all our layers which is where all the
work is is cleaning up that data making
sure it's going in there correctly and
we are actually going to fit it we're
going to train our data set and let's
see what that looks like and here we go
let's put the information in here and
let's just take a quick look at what
we're looking at with our fit generator
we have our classifier dot fit generator
that's our back propagation so the
information goes through forward with a
picture and it says oh you're either
right or you're wrong and then the air
goes backward and reprograms all those
weights so we're training our neural
network and of course we're using the
training set remember we created the
training set up here and then we're
going steps per epic so it's 8 000 steps
epic means that that's how many times we
go through all the pictures so we're
going to rerun each of the pictures and
we're going to go through the whole data
set 25 times but we're going to look at
each picture during each epic 8 000
times so we're really programming the
heck out of this and going back over it
and then they have validation data
equals test set so we have our training
set and then we're gonna have our test
set to validate it so we're gonna do
this all in one shot and we're gonna
look at that and they're gonna do 200
steps for each validation and we'll see
what that looks like in just a minute
let's go ahead and run our training here
and we're going to fit our data and as
it goes it says epic one of 25 you start
realizing that this is going to take a
while on my older computer it takes
about 45 minutes i have a dual processor
we're processing uh 10 000 photos that's
not a small amount of photographs to
process so if you're on your laptop in
which i am it's going to take a while so
let's go ahead and go get our cup of
coffee and a sip and come back and see
what this looks like so i'm back you
didn't know i was gone that was actually
a lengthy pause there i made a couple
changes and let's discuss those changes
real quick and why i made them so the
first thing i'm going to do is i'm going
to go up here and insert a cell above
and let's paste the original code back
in there and you'll see that the
original thing was steps per epic eight
thousand twenty five epics and
validation steps two thousand and i
changed these to four thousand epics or
four thousand steps per epic
10 epics and just 10 validation steps
and this will cause problems if you're
doing this as a commercial release but
for demo purposes this should work and
if you remember our steps per epic
that's how many photos we're going to
process in fact let me go ahead and get
my drawing pin out and let's just
highlight that right here well we have 8
000 pictures we're going through so for
each epic i'm going to change this to 4
000 i'm going to cut that in half so
it's going to randomly pick 4 000
pictures each time it goes through an
epic and the epic is how many processes
so this is 25 and i'm just going to cut
that to 10. so instead of doing 25 runs
through 8 000 photos each which you can
do the math of 25 times 8 000. i'm only
going to do 10 through 4 000 so i'm
going to run this 40 000 times through
the processes and the next thing i know
that you'll you'll want to notice is
that i also change the validation step
and this would cause some major problems
in releasing because i dropped it all
the way down to 10. what the validation
step does is it says we have 2 000
photos in our trainings or in our
testing set and we're going to use that
for validation well i'm only going to
use a random 10 of those to validate so
not really the best settings but let me
show you why we did that
let's scroll down here just a little bit
and let's look at the output here and
see what that what's going on there so
i've got my drawing tool back on and
you'll see here it lists a run so each
time it goes through an epic it's going
to do 4 000 steps and this is where the
4 000 comes in so that's what we have we
have epic one of 10 4 000 steps it's
randomly picking half the pictures in
the file and going through them and then
we're going to look at this number right
here that is for the whole epic and
that's 2411
seconds and if you remember correctly
you divide that by 60 you get minutes if
you divide that by 60 you get hours or
you can just divide the whole thing by
60 times 60 which is 3600. if 3600 is an
hour this is roughly 45 minutes right
here and that's 45 minutes to process
half the pictures so if i was doing all
the pictures we're talking an hour and a
half per epic times 36 or no 25 they had
25 up above 25 so that's roughly a
couple days a couple days of processing
well for this demo we don't want to do
that i don't want to come back the next
day plus my computer did a reboot in the
middle of the night so we look at this
and we say okay let's we're just testing
this out my computer that i'm running
this on is a dual core processor runs
point nine gigahertz per second for a
laptop you know it's good about four
years ago but for running something like
this is probably a little slow so we cut
the times down and the last one was
validation we're only validating it on a
random 10 photos and this comes into
effect because you're going to see down
here where we have accuracy value loss
value accuracy and loss those are very
important numbers to look at so the 10
means i'm only validating across 10
pictures that is where here we have
value this is acc is for accuracy value
loss we're not going to worry about that
too much and accuracy now accuracy is
while it's running it's putting these
two numbers together that's what
accuracy is and value accuracy is at the
end of the epic what's our accuracy into
the epic what is it looking at in this
tutorial we're not going to go so deep
but these numbers are really important
when you start talking about
these two numbers reflect bias that is
really important i'll just put that up
there and bias is a little bit beyond
this tutorial but the short of it is is
if this accuracy which is being our
validation per step is going down and
the value accuracy continues to go up
that means there's a bias that means i'm
memorizing the photos i'm looking at i'm
not actually looking for what makes a
dog a dog what makes a cat a cat i'm
just memorizing them and so the more
this discrepancy grows the bigger the
bias is and that is really the beauty of
the cross
neural network is a lot of built-in
features like this that make that really
easy to track so let's go ahead and take
a look at the next set of code so here
we are into part three we're going to
make a new prediction and so we're going
to bring in a couple tools for that and
then we have to process the image coming
in and find out whether it's an actual
dog or cat we can actually use this to
identify it and of course the final step
of part three is to print prediction
we'll go ahead and combine these and of
course you can see me there adding more
sticky notes to my computer screen
hidden behind the screen and
last one was don't forget to feed the
cat and the dog
so let's go and take a look at that and
see what that looks like in code and put
that in our jupiter notebook all right
and let's paste that in here and we'll
start by importing numpy as np numpy is
a very common package i pretty much
import it on any python project i'm
working on another one i use regularly
is pandas they're just ways of
organizing the data and then np is
usually the standard in most machine
learning tools as a return for the data
array although you know you can use the
standard data array from python and we
have cross pre-processing import image
fish that all look familiar because
we're going to take a test image and
we're going to set that equal to in this
case cat or dog one as you can see over
here and you know let me get my drawing
tool back on so let's take a look at
this we have our test image we're
loading and in here we have test image
one and this one hasn't data hasn't seen
this one at all so this is all new oh
let me shrink the screen down let me
start that over so here we have my test
image and we went ahead and the cross
processing has this nice image set up so
we're going to load the image and we're
going to alter it to a 64 by 64 print so
right off the bat we're going to cross
as nice that way it automatically sets
it up for us so we don't have to redo
all our images and find a way to reset
those and then we use also to set the
image to an array so again we're all in
pre-processing the data just like we
pre-processed before with our test
information and our training data and
then we use the numpy here's our numpy
that's uh from our right up here
important numpy as in p expand the
dimensions test image axes equals 0. so
it puts it into a single array and then
finally
all that work all that pre-processing
and all we do is we run the result we
click on here we go result equals
classifier predict test image and then
we find out well what is the test image
and let's just take a quick look and
just see what that is and you can see
when i ran it it comes up dog and if we
look at those images there it is cat or
dog image number one that looks like a
nice floppy eared lab
friendly with his tongue hanging out
it's either that or a very floppy eared
cat i'm not sure which but according to
our software says it's a dog and uh we
have a second picture over here let's
just see what happens when we run the
second picture we can go up here and
change this uh from dog image one to two
we'll run that and it comes down here
and says cat you can see me highlighting
it down there as cat so our process
works we are able to label a dog a dog
and a cat a cat just from the pictures
there we go cleared my drawing tool and
the last thing i want you to notice when
we come back up here to when i ran it
you'll see that has an accuracy of 1 and
the value accuracy of 1. well the value
accuracy is the important one because
the value accuracy is what it actually
runs on the test data remember i'm only
testing it on i'm only validating it on
a random 10 photos and those 10 folders
just happened to come up one now when
they ran this on the server it actually
came up about 86 percent this is why
cutting these numbers down so far for a
commercial release is bad so you want to
make sure you're a little careful of
that when you're testing your stuff that
you change these numbers back when you
run it on a more enterprise computer
other than your old laptop that you're
just practicing on or messing with and
we come down here and again you know we
had the validation of cat and so we have
successfully built a neural network that
could distinguish between photos of a
cat and a dog imagine all the other
things you could distinguish imagine all
the different industries you could dive
into with that just being able to
understand those two difference of
pictures what about mosquitoes could you
find the mosquitoes that bite versus
mosquitoes that are friendly it turns
out the mosquitoes that bite us are only
four percent of the mosquito population
if even that maybe two percent there's
all kinds of industries that use this
and there's so many industries that are
just now realizing how powerful these
tools are just in the photos alone there
is a myriad of industries sprouting up
and i said it before i'll say it again
what an exciting time to live in with
these tools and that we get to play with
back propagation and gradient descent
we're talking neural networks so we talk
about the neural network this is a
simple neural network which must be
trained to recognize handwritten
alphabets a b and c and you can see here
we have our input coming in in this case
we'll look at the letter a written out
on a 28 by 28 pixels so the handwritten
outfits are presented as images of 28 by
28 pixels and that image comes in in
this case we have 784 neurons that's 28
times 28 and the initial prediction is
made using random weights assigned to
each channel and so we have our forward
propagation as you see here so each node
is then their values are added up and
added up and so on going across and our
network predicts the input to be b with
the probability of 0.5 the predicted
probabilities are compared against the
actual probabilities and the errors
calculated so the error is simply the
actual minus predicted and you can see
here where we know it's not c so it's a
minus 2 we know it's not b so it's a
minus 0.5 but we do know that it is a
a so we go ahead and adjust that by 0.6
the magnitude indicates the amount of
change while the sign indicates an
increase or decrease in the weights the
information is transmitted back through
the network so here comes our back
propagation weights throughout the
network are adjusted in order to reduce
the loss in prediction so if we look at
this setup right here here comes our
error in this case 0.6 minus 0.5 minus
0.2 that comes up and adjusts our 1.4.9
all our multipliers in this manner we
keep training the network with multiple
inputs until it is able to predict with
a high accuracy and you can see here we
have a different a quickly switches from
cursive a to maybe more elongated a
similarly our network is trained with
the images for b and c two so let's take
a look at this uh here's a
straightforward data sets let's build a
neural network to predict the outputs
given the inputs and so we have an input
zero we expect an output of zero we have
an input of one we expect six two equals
twelve three should come out as eighteen
and four as twenty-four and we're just
doing multiples of six if you take the
time to look at it so in our example we
have our input and it goes into our
neural network so this box represents
our neural network one of the cool
things about neural networks is there
always this little black box that you
kind of train to do what you want and
you really don't have to know exactly
what the weights are although there are
some very high end setups to start
looking at those weights and how they
work and what they do and then you get
your output which is going to be in this
case our input is going to be x and our
output is going to be y and w is the
weight so we have a value times the
weight so if we're doing in this case
just a single neuron going through we
have x times w the network starts
training itself by choosing a random
value for w we're going to guess that w
equals three just roll the dice randomly
generate the number three for w and then
we put w equals three in here we have
our input zero or output zero w equals
three equals zero so we have no error on
the first line that actually comes out
correct and then we have one times uh we
put the 1 in we're looking for a 6 but
we get a 3 instead when we put the 2 in
we're looking for 12 we get a 6 in set
so you can see here our predicted output
doesn't match the output we're looking
for and then we take this we have our w
equals three we come up with the second
model where the w equals six now we're
going to look at how we figure out w
equals six in just a minute that is part
of the math behind this but you can see
here we put in w equals six and we build
the w equals 6 chart we end up with 0 6
12 18 24 which is the output we're
looking for and in that manner we end up
with the correct answer but we'll go
ahead and put in a third model where w
equals 9. so at this point this is one
way of doing this is just to guess what
w equals and you can see with w equals 9
we get the incorrect answers we get 9 18
27 36 we as humans can know just by
taking a look at the data that our
weight should be six but how does the
machine come to this conclusion how do
we program the computer to learn instead
of waiting for us to tell it it has the
right answer what the correct answer is
and you can imagine this is a very
simple problem if we're doing guessing w
equals three we guess w equals six we
guess w equals nine and then we look at
our results and we go oh it's gotta be w
equals six that's the best result but as
humans we wanna take that element out
and have the computer do that for us so
with that we're gonna have a loss
function the loss function is a measure
of error which defines the precision
lost to comparing the predicted output
to the actual output and it's simply
loss equals actual output minus
predicted output and then we square the
whole thing so let's apply the loss
function to the input value 2 loss for
our actual output predicted output
squared and our last function for the
input of 2 we end up with an actual
output of 12 and you can see here with w
equals 3 12 minus 6 squared equals 36 so
we end up with a loss of 36 w equals 6
12 12 times 12 squared equals 12 minus
12 squared equals 0 or 12 minus 18
squared equals 36. and so you can see we
have a huge loss on w equals three and w
equals nine we now plot a graph for the
weight versus loss and it always helps
to have a nice visual of what's going on
here so this graphical method of finding
the minimal of a function is called
radiant descent and this is the logic
behind this you can see as we come in
here and we go ahead and graph the loss
we have 36 for 3 and 36 for 9. we happen
to guess 6 which was the correct answer
right in the middle and you can see
right here forms a nice little parabola
and you can see a nice mark right in the
middle and as a human being we can look
at that we go ah the answer is 6. a
random point on this curve is chosen and
the slope at this point is calculated so
now we're getting away from the human
aspect of just looking at it and saying
what the answer is and we look at what's
going on with the math and so if we have
a positive slope it indicates an
increase in the weight and a negative
slope indicates a decrease in weight
this time the slope is negative hence
another random point towards its left is
chosen and you can see here we're
actually kind of just playing a little
high-low game going back and forth with
the gradient descent we continue
checking slopes at various points in
this manner so we have our input actual
output w3 w6 w9 we found our positive
slope increase in increase in weight a
negative slope indicates a decrease in
weight a zero slope indicates the
appropriate weight so our aim is to
reach a point where the slope is zero
and when we talk about neural networks
you're usually processing a massive
amount of information and data so you're
not going to have all your data nice and
neat where it's just a multiple of six
it's gonna be messy and so we're gonna
keep approaching that number but you'll
never get everything to fit at 0. you're
going to get stuff all over the place
and so you're really looking for the
minimum value you're not looking for an
absolute zero because you're not going
to get it once we're talking about
gradient descent that's what we're
talking about on there is finding the
bottom of that curb even if it doesn't
go all the way to zero so how do we
apply that to our neural network well we
use back propagation back propagation is
a process of updating weights of the
network in order to reduce the error in
prediction and so the magnitude of loss
of any point on our graph combined with
the slope is fed back to the network and
you can see here here's our simple model
with just one node of x times w the
input comes in we have our x times w the
output and then we're going to propagate
that loss going the other way a random
point on the graph gives a loss value of
36 with a positive slope and we continue
checking slopes at various points in
this manner so a random point in the
graph gives a loss value of 36 with a
positive slope 36 is quite a large
number this means our current weight
needs to change by a large number a
positive slope indicates that the change
of the weight must be positive similarly
another random point on the graph gives
a loss value of 10 with a negative slope
10 is a small number hence the weight
requires to be tuned quite less a
negative slope indicates that the weight
needs to be reduced rather than
increased after multiple iterations of
back propagation our weights are
assigned the appropriate value you can
see here we have our input we just
looked at x times six in our output and
eventually we get it that the weight is
six for the single node problem that
we're working on right now at this point
our network is trained and can be used
to make predictions let's now get back
to our first example and see where the
back propagation and gradient descent
fall into place and you can see here
we're not looking at a single node
anymore now we have 28 by 28 grid or 784
inputs coming into the first level which
has 784 nodes depending on how you build
your neural network the next layer might
also have 784 nodes or it might
continually smallen depending on what
you need and what's needed for that to
work so as mentioned earlier predicted
output is compared against the actual
output and you can see our error over
here actual minus prediction and then we
go ahead and compute our loss so the
loss of a is 0.7 squared equals 0.49
loss of b is 0.5 squared or 0.25 and so
on so now we have our first iteration on
there so weights throughout the network
are adjusted in order to reduce the loss
in prediction now of course we do that
by doing a second iteration coming
through with our different losses on
there and then ways throughout the
network are just in order to reduce the
loss of prediction again underneath the
second and we do a third iteration and
we just keep doing these iterations
going back until we get the right value
now you got to remember that when we're
doing a reverse propagation we're not
looking at just one letter a we're
looking at hundreds of letter a's and
usually we propagate that loss going
backwards we only take a small piece of
it so our adjustments are very small
because one of them is not correct we
don't want to create a bias so we talk
about back propagation we're talking
about going through over and over and
over this data until we get minimal loss
for our letter a so let's focus on the
minimum loss for our variable a and you
can see here we look at that we end up
we'll assume for below to be our graph
for the loss of prediction what the
variable a as compared to the ways
continuing it from the second layer and
we have our loss of a for 49 for 16 for
0.04 and you can see it makes this nice
curve where we can guess where the
bottom of this curve is and i like it on
this graph that they show that the curve
doesn't rest yeah on the x axis it
doesn't rest at where y equals zero
because you usually don't get that you
don't get a perfect fit on anything or
very rarely do you ever get a perfect
fit and so the random points chosen on
the graph is now back propagated through
the network in order to adjust the
weights so we're able to go back through
the network and readjust those weights
until we find that minimal value the
network is run once again with the new
weights this process is repeated
multiple times till provides accurate
predictions the weights are further
justify adjusted to identify b and c
two and this is interesting because you
actually do them at the same time so as
the error goes back you kind of find the
overall error for all the inputs coming
in and then that's what gets propagated
going back or the overall loss kind of
have to step away from that word error
because it's not just about the error
it's about the loss the weights are
further adjusted to identify b and c2
and so a lot of times you actually do
them all at the same time but you'll
adjust those weights a b and c as i was
just saying thus through gradient
descent and back propagation our network
is completely trained and we've taught
it to identify a b and c coming forward
one of the interesting things about
neural networks is the training process
takes a lot longer than the predicting
process so you can plan one of these
training neural networks doing the back
propagation to
significantly longer because you're
going over thousands of data points and
then when you actually run it forward
it's very quick which makes these things
very useful and just really part of
today's world in computing today we're
going to be covering the convolutional
neural network tutorial do you know how
deep learning recognizes the objects in
an image and really this particular
neural network is how image recognition
works it's very central one of the
biggest building blocks for image
recognition it does it using convolution
neural network and we over here we have
the basic picture of a hummingbird
pixels of an image fed as input you have
your input layer coming in so it takes
that graphic and puts it into the input
layer you have all your hidden layers
and then you have your output layer and
your output layer one of those is going
to light up and say oh it's a bird we're
going to go into depth we're going to
actually go back and forth on this a
number of times today so if you're not
catching all the image don't worry we're
going to get into the details so we have
our input layer accepts the pixels of
the image as input in the form of arrays
and you can see up here where they've
actually labeled each block of the bird
in different arrays so we'll dive into
deep as to how that looks like and how
those matrixes are set up your hidden
layer carry out feature extraction by
performing certain calculations and
manipulation so this is the part that
kind of reorganizes that picture
multiple ways until we get some data
that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around and alter it and use that
operation to detect a new pattern there
are multiple hidden layers like
convolution layer rel u is how that is
pronounced when that's the rectified
linear unit that has to do with the
activation function that's used
pooling layer also uses multiple filters
to detect edges corners eyes feathers
beak etc and just like the term says
pooling is pulling information together
and we'll look into that a lot closer
here so if you're if it's a little
confusing now we'll dig in deep and try
to get you squared away with that and
then finally there is a fully connected
layer that identifies the object in the
image so we have these different layers
coming through in the hidden layers and
they come into the final area and that's
where we have a one node or one neural
network entity that lights up that says
it's a bird
what's in it for you we're going to
cover an introduction to the cnn what is
convolution neural network how cnn
recognizes images we're going to dig
deeper into that and really look at the
individual layers in the convolutional
neural network and finally we do a use
case implementation using the cnn we'll
begin our introduction to the cnn by
introducing the pioneer of convolutional
neural network jan lecun he was the
director of facebook ai research group
built the first convolutional neural
network called lynette in 1988 so these
have been around for a while and have
had a chance to mature over the years it
was used for character recognition tasks
like reading zip code digits imagine
processing mail and automating that
process
cnn is a feed forward neural network
that is generally used to analyze visual
images by producing data with a
grid-like topology a cnn is also known
as a convent and very key to this is we
are looking at images that was what this
was designed for and you'll see the
different layers as we dig in near some
of the other some of them are actually
now used since we're using tensorflow
and cross in our code later on you'll
see that some of those layers appear in
a lot of your other neural network
frameworks but in this case this is very
central to processing images and doing
so in a variety that captures multiple
images and really drills down into their
different features in this example here
you see flowers are two varieties orchid
and a rose i think the orchid is much
more dainty and beautiful and the rose
smells quite beautiful of a couple rose
bushes in my yard they go into the input
layer that data is then sent to all the
different nodes in the next layer one of
the hidden layers based on its different
weights and its setup it then comes out
and gives those a new value those values
then are multiplied by their weights and
go to the next hidden layer and so on
and then you have the output layer and
one of those notes comes out and says
it's an orchid and the other one comes
out and says it's a rose depending on
how it was well it was trained what
separates the cnn or the convolutional
neural network from other neural
networks is a convolutional operation
forms the basis of any convolutional
neural network in a cnn every image
image is represented in the form of
arrays of pixel values so here we have a
real image of the digit 8
that then gets put on to its pixel
values represented in the form of an
array in this case you have a two
dimensional array and then you can see
in the final in form we transform the
digit 8 into its representational form
of pixels of zeros and ones where the
ones represent in this case the black
part of the eight and the zeros
represent the white background to
understand the convolution neural
network or how that convolutional
operation works we're going to take a
side step and look at matrixes in this
case we're going to simplify it and
we're going to take two matrices a and b
of one dimension now kind of separate
this from your thinking as we learned
that you want to focus just on the
matrix aspect of this and then we'll
bring that back together and see what
that looks like when we put the pieces
for the convolutional operation here
we've set up two arrays we have in this
case are a single dimension matrix and
we have a equals five three seven five
nine seven and we have b equals one two
three so in the convolution as it comes
in there's gonna look at these two and
we're gonna start by doing multiplying
them a times b and so we multiply the
arrays element wise and we get five six
six
where five is the five times one six is
three times two and then the other six
is two times three and since the two
arrays aren't the same size they're not
the same setup we're going to just
truncate the first one and we're gonna
look at the second array multiplied just
by the first three elements of the first
array now that's going to be a little
confusing remember a computer gets to
repeat these processes hundreds of times
so we're not going to just forget those
other numbers later on we'll see we'll
bring those back in and then we have the
sum of the product in this case 5 plus 6
plus 6 equals 17. so in our a times b
our very first digit in that matrix of a
times b is 17. and if you remember i
said we're not going to forget the other
digits so we now have three two five we
move one set over and we take three two
five and we multiply that times b and
you'll see that three times one is three
two times two is four and so on and so
on we sum it up so now we have the
second digit of our a times b product in
the matrix and we continue on with that
same thing so on and so on so then we
would go from three seven five to seven
five nine to five nine seven this short
matrix that we have for a we've now
covered all the different entities in a
that match three different levels of b
now in a little bit we're going to cover
where we use this math at this
multiplying of matrixes and how that
works but it's important to understand
that we're going through the matrix of
multiplying the different parts to it to
match the smaller matrix with the larger
matrix i know a lot of people get lost
at is you know what's going on here with
these matrixes uh oh scary math not
really that scary when you break it down
we're looking at a section of a and
we're comparing it to b so when you
break that down your mind like that you
realize okay so i'm just taking these
two matrixes and comparing them and i'm
bringing the value down into one matrix
a times b we're deucing that information
in a way that will help the computer see
different aspects let's go ahead and
flip over again back to our images
here we are back to our images talking
about going to the most basic two
dimensional image you can get to
consider the following two images the
image for the symbol backslash when you
press the backslash the above image is
processed you can see there for the
image for the forward slash is the
opposite so we click the forward slash
button that flips very basic we have
four pixels going in can't get any more
basic than that here we have a little
bit more complicated picture we take a
real image of a smiley face
then we represent that in the form of
black and white pixels so if this was an
image in the computer it's black and
white and like we saw before we convert
this into the zeros and one so where the
other one would have just been a matrix
of just four dots now we have a
significantly larger image coming in so
don't worry we're going to bring this
all together here in just a little bit
layers in convolutional neural network
we're looking at this we have our
convolution layer and that really is the
central aspect of processing images in
the convolutional neural network that's
why we have it and then that's going to
be feeding in and you have your relu
layer which is you know as we talked
about the rectified linear unit we'll
talk about that a little bit later the
relu isn't how it act is how that layer
is activated is the math behind it what
makes the neurons fire you'll see that a
lot of other neural networks when you're
using it just by itself it's for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the relu layer works great you
have your pooling layer that's where
you're pulling the data together pooling
is a neural network term is very
commonly used i like to use the term
reduce so if you're coming from the map
and reduce side you'll see that we're
mapping all this data through all these
networks and then we're going to reduce
it we're going to pull it together and
then finally we have the fully connected
layer that's where our output is going
to come out so we have started to look
at matrixes we've started to look at the
convolutional layer where it fits in and
everything we've taken a look at images
so we're going to focus more on the
convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following five by five image whose pixel
values are only zero and one now
obviously when we're dealing with color
there's all kinds of things that come in
on color processing but we want to keep
it simple and just keep it black and
white and so we have our image pixels so
we're sliding the filter matrix over the
image and computing the dot product to
detect the patterns and right here
you're going to ask where does this
filter come from this is a bit confusing
because the filter
is going to be derived
later on we build the filters when we
program or train our model so you don't
need to worry what the filter actually
is but you do need to understand how a
convolution layer works is what is the
filter doing filter and you'll have mini
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the filter works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two dimensional matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here one times one one
times zero one times one we multiply
those all together then sum them and we
end up with the convolved feature of
four we're going to take that and
sliding the filter matrix over the image
and computing the dot product to detect
patterns so we're just going to slide
this so we're going to predict the first
one and slide it over one notch predict
the second one and so on and so on all
the way through until we have a new
matrix and this matrix which is the same
size as the filter has reduced the image
and whatever filter whatever that's
filtering out is going to be looking at
just those features reduced down to a
smaller matrix so once the feature maps
are extracted the next step is to move
them to the relu layer so the relu layer
the next step first is going to perform
an element-wise operation so each of
those maps coming in if there's negative
pixels so it says all the negative
pixels to zero
and you can see this nice graph where it
just zeros out the negatives and then
you have a value that goes from zero up
to whatever value is coming out of the
matrix this introduces non-linearity to
the network so up until now we have a we
say linearity we're talking about the
fact that the feature has a value so
it's a linear feature this feature
came up and has let's say the feature is
the edge of the beak you know it's like
or the backslash that we saw you'll look
at that and say okay this feature has a
value from negative 10 to 10 in this
case um if it was one it'd say yeah this
might be a beak it might not might be an
edge right there a minus five means no
we're not even going to look at it to
zero and so we end up with an output and
the output takes all these features all
these filtered features remember we're
not just running one filter on this
we're running a number of filters on
this image and so we end up with a
rectified feature map that is looking at
just the features coming through and how
they weigh in from our filters so here
we have an input of a looks like a
toucan bird
very exotic looking real image is
scanned in multiple convolution and the
relu layers for locating features and
you can see up here is turn it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
skip by two or three or four pixels to
speed up the process that's one of the
ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
the image you have multiple filters have
been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new matrix one more aspect to
note about the relu layer is we're not
just having one value coming in
so not only do we have multiple features
going through but we're generating
multiple relu layers for locating the
features that's very important to note
you know so we have a quite a bundle we
have multiple filters multiple rail u
which brings us to the next step
forward propagation now we're going to
look at the pooling layer the rectified
feature map now goes through a pooling
layer pooling is a down sampling
operation that reduces the
dimensionality of the feature map that's
all we're trying to do we're trying to
take a huge amount of information and
reduce it down to a single answer this
is a specific kind of bird this is an
iris this is a rose so you have a
rectified feature map and you see here
we have a rectified feature map coming
in
we set the max pooling with a 2 by 2
filters and a stride of two and if you
remember correctly i talked about not
going one pixel at a time uh well that's
where the stride comes in we end up with
a two by two pooled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a st we skip a few there we go by
two we skip every other pixel and we
just do every other one and this reduces
our rectified feature map which is you
can see over here 16 by 16 to a four by
four so we're continually trying to
filter and reduce our data so that we
can get to something we can manage and
over here you see that we have the max
3 4 1 and 2 and in the max pooling we're
looking for the max value a little bit
different than what we were looking at
before so coming from the rectified
feature we're now finding the max value
and then we're pulling those features
together so instead of think of this as
image of the map think of this as how
valuable is a feature in that area how
much of a feature value do we have we
just want to find the best or the
maximum feature for that area they might
have that one piece of the filter of the
beak said oh i see a one in this beak in
this image and then it skips over and
says i see a three in this image and
says oh this one is rated as a four we
don't want to sum it together because
then you know you might have like five
ones and it'll say ah five but you might
have uh four zeros and one ten and that
ten says well this is definitely a beak
where the ones will say probably not a
beak a little strange analogy since
we're looking at a bird but you can see
how that pulled feature map comes down
and we're just looking for the max value
in each one of those matrixes pooling
layer uses different filters to identify
different parts of the image like edges
corners body feathers eyes beak etc i
know i focus mainly on the beak but
obviously each feature could be each a
different part of the bird coming in so
let's take a look at what that looks
like structure of a convolution neural
network so far this is where we're at
right now we have our input image coming
in and then we use our filters and
there's multiple filters on there that
are being developed to kind of twist and
change that data and so we multiply the
matrixes we take that little filter
maybe it's a two by two we multiply it
by each piece of the image and if we
step two then it's every other piece of
the image that generates multiple
convolution layers so we have a number
of convolution layers we have
set up in there just looking at that
data we then take those convolution
layers we run them through the relu
setup and then once we've done through
the release setup and we have multiple
values going on multiple layers that are
relu then we're going to take those
multiple layers and we're going to be
pooling them so now we have the pooling
layers or multiple poolings going on up
until this point we're dealing with
sometimes multiple dimensions you can
have three dimensions some strange data
setups that aren't doing images but
looking at other things they can have
four five six seven dimensions uh so
right now we're looking at 2d image
dimensions coming in into the pooling
layer so the next step is we want to
reduce those dimensions or flatten them
so flattening flattening is a process of
converting all of the resultant
two-dimensional arrays from pooled
feature map into a single long
continuous linear vector so over here
you see where we have a pooled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 6847
or a single linear vector and we find
out that not only do we do each of the
pooled feature maps we do all of them
into one long linear vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we flatten them out and combine them
into a single linear vector going in so
after we've done the flattening we have
just a quick recap because we've covered
so much so it's important to go back and
take a look at each of the steps we've
gone through the structure of the
network so far we have our convolution
where we twist it and we filter it and
multiply the matrixes we end up with our
convolutional layer which uses the relu
to figure out the values going out into
the pooling as you have numerous
convolution layers that then create
numerous pooling layers pulling that
data together which is the max value
which one we want to send forward we
want to send the best value and then
we're going to take all of that from
each of the pooling layers and we're
going to flatten it and we're going to
combine them into a single input going
into the final layer once you get to
that step you might be looking at that
going boy that looks like the normal
intuit to most neural network and you're
correct it is so once we have the
flattened matrix from the pooling layer
that becomes our input so the pooling
layer is fed as an input to the fully
connected layer to classify the image
and so you can see as our flattened
matrix comes in in this case we have the
pixels from the flattened matrix fed as
an input back to our toucan or whatever
that kind of bird that is i need one of
these to identify what kind of bird that
is it comes into our ford propagation
network
and that will then have the different
weights coming down across and then
finally it selects that that's a bird
and that is not a dog or a cat in this
case
even though it's not labeled the final
layer there in red is our output layer
our final output layer that says bird
cat or dog
so quick recap of everything we've
covered so far we have our input image
which is twisted and multiplied the
filters are multiplied times the matrix
and the two matrixes multiplied all the
filters to create our convolution layer
our convolution layers there's multiple
layers in there because it's all
building multiple layers off the
different filters then goes through the
relu as this activation and that creates
our pooling and so once we get into the
pooling layer we then and the pooling
look for who's the best what's the max
value coming in from our convolution and
we take that layer and we flatten it and
then it goes into a fully connected
layer our fully connected neural network
and then to the output and here we can
see the entire process how the cnn
recognizes a bird this is kind of nice
because it's showing the little pixels
and where they're going you can see the
filter is generating this convolution
network and that filter shows up in the
bottom part of the convolution network
and then based on that it uses the relu
for the pooling the pooling then find
out which one's the best and so on all
the way to the fully connected layer at
the end or the classification in the
output layer so that'd be a
classification neural network at the end
so we covered a lot of theory up till
now and you can imagine each one of
these steps has to be broken down in
code so putting that together can be a
little complicated not that each step of
the process is overly complicated but
because we have so many steps we have
one two three four five different steps
going on here with sub steps in there
we're going to break that down and walk
through that in code so in our use case
implementation using the cnn we'll be
using the cfar10 dataset from canadian
institute for advanced research for
classifying images across 10 categories
unfortunately they don't let me know
whether it's going to be a toucan or
some other kind of bird but we do get to
find out whether it can categorize
between a ship a frog deer bird airplane
automobile cat dog horse truck so that's
a lot of fun and if you're looking
anything in the news at all of our
automated cars and everything else you
can see where this kind of processing is
so important in today's world and very
cutting edge as far as what's coming out
in the commercial deployment i mean this
is really cool stuff we're starting to
see this just about everywhere in
industry so great time to be playing
with this and figuring it all out let's
go ahead and dive into the code and see
what that looks like when we're actually
writing our script
before we go on let's do uh one more
quick look at what we have here let's
just take a look at data batch one keys
and remember in jupiter notebook i can
get by with not doing the print
statement if i put a variable down there
it'll just display the variable and you
can see under data batch one for the
keys since this is a dictionary we have
the batch one label data and file names
so you can actually see how it's broken
up in our data set so for the next step
or step four as we're calling it uh we
want to display the image using matte
plot library there's many ways to
display the images you can even well
there's other ways to drill into it but
matplot library is really good for this
and we'll also look at our first reshape
uh setup or shaping the data so you can
have a little glimpse into what that
means uh so we're gonna start by
importing our map plot and of course
since i am doing jupiter notebook i need
to do the matplot inline command so it
shows up on my page so here we go we're
going to import matplot library.pipelot
as plt and if you remember matplot
library the pie plot is like a canvas
that we paint stuff onto and there's my
percentage sign matplot library in line
so it's going to show up in my notebook
and then of course we're going to import
numpy as np for our numbers python array
setup and let's go ahead and set
x equals to data batch one so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data we need to go
a little bit of reshaping so in here we
have to go ahead and reshape the data we
have 10 000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to i had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32 by 32
picture and let me do this let me go
ahead and do a drawing pad on here uh so
we have 32 bits by 32 bits and it's in
color so there's three bits of color now
i don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're going to take uh the shape we're
going to take the data which is just a
long stream of information and we're
going to break it up into 10 000 pieces
and those 10 000 pieces then are broken
into three pieces each and those three
pieces then are 32 by 32. you can look
at this like an old-fashioned projector
where they have the red screen or the
red projector the blue projector and the
green projector and they add them all
together and each one of those is a 32
by 32 bit so that's probably how this
was originally formatted was in that
kind of ideal things have changed so
we're going to transpose it and we're
going to take the three which was here
and we're going to put it at the end so
the first part is reshaping the data
from a single line of bit data or
whatever format it is into 10 000 by 3
by 32 by 32 and then we're going to
transpose the color factor to the last
place so it's the image then the 32 by
32 in the middle that's this part right
here and then finally we're going to
take this which is three bits of data
and put it at the end so it's more like
we do process images now and then as
type this is really important that we're
gonna use an integer eight you can come
in here and you'll see a lot of these
they'll try to do this with a float or a
float 64. what you got to remember
though is a float uses a lot of memory
so once you switch this into uh
something that's not integer 8 which
goes up to 128 you are just going to the
the amount of ram let's just put that in
here is going to go way up the amount of
ram that it loads
so you want to go ahead and use this you
can try the other ones and see what
happens if you have a lot of ram on your
computer but for this exercise this will
work just fine and let's go ahead and
take that and run this so now our x
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops i
said 128 is 255. uh you'll see it
doesn't go over 255 because it's
basically an ascii character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus a float value which would
bring this up
exponentially in size and since we're
using the matplot library we can do oops
that's not what i wanted since we're
using the map plot library we can take
our canvas and just do a plt dot im for
image show and let's just take a look at
what x0 looks like and it comes in i'm
not sure what that is but you can see
it's a very low grade image uh broken
down to the minimal pixels on there and
if we did the same thing oh let's do uh
let's see what one looks like hopefully
it's a little easier to see run on there
not enter let's hit the run on that uh
and we can see this is probably a semi
truck that's a good guess on there and i
can just go back up here instead of
typing the same line in over and over
and we'll look at three uh that looks
like a dump truck unloading uh and so on
you can do any of the 10 000 images we
can just jump to 55 looks like some kind
of animal looking at us there probably a
dog and just for fun let's do just one
more uh
run on there and we can see a nice car
for our image number four uh so you can
see we pace through all the different
images it's very easy to look at them
and they've been reshaped to fit our
view and what the
matte plot library uses for its format
so the next step is we're going to start
creating some helper functions we'll
start by a one hot encoder to help us or
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
encoder to do that and then we'll also
create a class
cfar helper so it's going to have an
init and a setup for the images and then
finally we'll go ahead and run that code
so you can see what that looks like and
then we get into the fun part where
we're actually going to start creating
our model our actual neural network
model so let's start by creating our one
hot encoder we're going to create our
own here and it's going to return an out
and we'll have our vector coming in and
our values equal 10. what this means is
that we have the 10 values the 10
possible labels and remember we don't
look at the labels as a number because a
car isn't one more than a horse maybe
just kind of bizarre to have horse
equals zero car equals one plane equals
two cat equals three so a cat plus a car
equals what uh so instead we create
a numpy array of zeros and there's going
to be 10 values so we have a 10
different values in there so you have
0 or 1. 1 means it's a cat 0 means it's
not a cat
in the next line it might be that one
means it's a car zero means it's not a
car so instead of having one output with
a value of 0 to 10 you have 10 outputs
with the values of 0 to 1. that's what
the one hot encoder is doing here and
we're going to utilize this in code in
just a minute so let's go ahead and take
a look at the next helpers we have a few
of these helper functions we're going to
build and when you're working with a
very complicated python project dividing
it up into separate definitions and
classes is very important otherwise it
just becomes really ungainly to work
with so let's go ahead and put in our
next helper which is a class and this is
a lot in this class so we'll break it
down here and let's just start oops we
put a space right in there there we go
that was a little bit more readable at a
second space so we're going to create
our class the cipher helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the init part uh self dot i equals zero
i'll come in in a little bit we'll come
back to that in the lower part we want
to initialize our training batches so
when we went through this there was like
a meta batch we don't need the meta
batch but we do need the data batch one
two three four five and we do not want
the testing batch in here this is just
the self all train batches so we're
gonna come make an array of all those
different images and then of course we
left the test batch out so we have our
self.test batch
we're going to initialize the training
images and the training labels and also
the test images and the test labels so
these are just this is just to
initialize these variables in here then
we create another definition down here
and this is going to set up the images
let's just take a look and see what's
going on in there now we could have all
just put this as part of the
init part
since this is all just helper stuff but
breaking it up again makes it easier to
read it also makes it easier when we
start executing the different pieces to
see what's going on so that way we have
a nice print statement to say hey we're
now running this and this is what's
going on in here we're going to set up
the self training images at this point
and that's going to go to a numpy array
v stack and in there we're going to load
up
in this case the data for d itself all
train batches again that points right up
to here so we're going to go through
each one of these
files or each one of these data sets
because they're not a file anymore we've
brought them in data batch one points to
the actual data and so our self training
images is going to stack them all into
our into a numpy array and then it's
always nice to get the training length
and that's just a total number of self
training images in there and then we're
going to take the self training images
let me switch marker colors because i am
getting a little too too much on the
markers up here oops there we go bring
down our marker change
so we can see it a little better and at
this point this should look familiar
where did we see this well when we
wanted to uh look at this above and we
want to look at the images in the
matplot library we had to reshape it so
we're doing the same thing here we're
taking our self training images and
based on the training length total
number of images because we stacked them
all together so now it's just one large
file of images we're going to take and
look at it as our three video cameras
that are each displaying a 32 by 32
we're going to switch that around so
that now we have each of our images that
stays the same place and then we have
our 32 by 32 and then by our three our
last are three different values for the
color and of course we want to go ahead
and they run this where we say divide by
255 that was from earlier it just brings
all the data into zero to one that's
what this is doing so we're turning this
into a zero to one array which is uh all
the pictures 32 by 32 by three and then
we're going to take the self training
labels and we're going to pump those
through our one hot encoder we just made
and we're going to stack them together
and
again we're converting this into an
array that goes from uh instead of
having horse equals one dog equals two
and then horse plus dog would equal
three which would be cat
now it's going to be you know an array
of 10 where each one is 0 to 1. then we
want to go ahead and set up our test
images and labels and when we're doing
this you're going to see it's the same
thing we just did with the rest of them
we just changed colors right here this
is no different than what we were doing
up here with our training set uh we're
going to stack the different images uh
we're going to get the length of them so
we know how many images are in there you
certainly could add them by hand but
it's nice to let the computer do it
especially if it ever changes on the
other end and you're using other data
and again we reshape them and transpose
them and we also do the one hot encoder
same thing we just did on our training
images so now our test images are in the
same format so now we have a definition
which sets up all our images in there
and then the next step is to go ahead
and batch them or next batch and let's
do another breakout here for batches
because this is really important to
understand tends to throw me for a
little loop when i'm working with
tensorflow or cross or a lot of these we
have our data coming in if you remember
we had like 10 000 photos let me just
put 10 000 down here we don't want to
run all 10 000 at once so we want to
break this up into batch sizes and you
also remember that we had the number of
photos in this case a length of test or
whatever number is in there we also have
32 by 32
by 3. so when we're looking at the batch
size we want to change this from 10 000
to
a batch of in this case i think we're
going to do batches of a hundred so we
want to look at just 100 the first
hundred of the photos and if you
remember we set self-i equal to
zero uh so what we're looking at here is
we're going to create x we're gonna get
the next batch from the very initialize
we've already initialized it for zero so
we're gonna look at x from zero to batch
size which we set to one hundred so just
the first 100 images and then we're
going to reshape that into
and this is important to let the data
know that we're looking at 100 by 32 by
32 by 3. now we've already formatted it
to the 32 by 32 by 3. this just sets
everything up correctly so that x has
the data in there in the correct order
and the correct shape and then the y
just like the x
is our labels so our training labels
again they go from 0 to batch size in
this case they do sell fi plus batch
size because the self is going to keep
changing and then finally we increment
the self i because we have zero so we so
the next time we call it we're going to
get the next batch size and so basically
we have x and y x being the photograph
data coming in and y being the label and
that of course is labeled through one
hot encoder so if you remember correctly
if it was say horse is equal to zero it
would be um one for the zero position
since this is the horse and then
everything else would be zero in here
let me just put lines through there
there we go there's our array
hard to see that array so let's go ahead
and take that and we're going to finish
loading it since this is our class and
now we're armed with all this um uh our
setup over here let's go ahead and load
that up and so we're going to create a
variable ch with the cfar helper in it
and then we're going to do ch.setup
images
now we could have just put all the setup
images under the init but by breaking
this up into two parts it makes it much
more readable and also if you're doing
other work there's reasons to do that as
far as the setup let's go ahead and run
that and you can see where it says
setting up training images and labels
setting up test images and that's one of
the reasons we broke it up is so that if
you're testing this out you can actually
have print statements in there telling
you what's going on which is really nice
uh they did a good job with this setup i
like the way that it was broken up in
the back and then one quick note you
want to remember that batch to set up
the next batch because we have to run uh
batch equals ch next batch of 100
because we're going to use the 100 size
but we'll come back to that we're going
to use that just remember that that's
part of our code we're going to be using
in a minute from the definition we just
made so now we're ready to create our
model first thing we want to do is we
want to import our tensorflow as tf i'll
just go ahead and run that so it's
loaded up and you see we got a
warning here
that's because they're making some
changes it's always growing and they're
going to be depreciating one of the
values from float 64 to float type or is
treated as an np float 64. uh nothing to
really worry about this doesn't even
affect what we're working on because
we've set all of our stuff to a 255
value or zero to one and do keep in mind
that zero to one value that we converted
to 255 is still a float value but it'll
easily work with either the numpy float
64 or the numpy d type float it doesn't
matter which one it goes through so the
depreciation would not affect our code
as we have it and in our tensorflow uh
we'll go ahead and just increase the
size in there just a moment so you can
get better view of the um what we're
typing in uh we're going to set a couple
placeholders here and so we have we're
going to set x equals tf placeholder tf
float 32 we just talked about the float
64 versus the numpy float we're actually
just going to keep this at float32 more
than a significant number of decimals
for what we're working with and since
it's a placeholder we're going to set
the shape equal to and we've set it
equal to none
because at this point we're just holding
the place on there we'll be setting up
as we run the batches that's what that
first value is and then 32 by 32 by 3
that's what we reshaped our data to fit
in and then we have our y true equals
placeholder tf float 32 and the shape
equals none comma 10. 10 is the 10
different labels we have so it's an
array of 10. and then let's create one
more placeholder we'll call this a hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we call drop out
if you remember from our theory before
we drop out so many nodes is looking at
or the different values going through
which helps decrease bias so we need to
go ahead and put a placeholder for that
also and we'll run this so it's all
loaded up in there so we have our three
different placeholders and since we're
in tensorflow when you use keras it does
some of this automatically but we're in
tensorflow direct cross sits on
tensorflow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each layer has
to have a bias going in we're going to
go ahead and work on our conversional 2d
our max pool so we have our pooling
layer our convolutional layer and then
our normal full layer so we're going to
go ahead and put those all into
definitions and let's see what that
looks like in code and you can also grab
some of these helper functions from the
mnist the uh nist setup let me just put
that in there if you're under the
tensorflow so a lot of these already in
there but we're going to go ahead and do
our own and we're going to create our
init weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here now we're not going to be
doing that we're just going to recreate
them for our code and let's take a look
at this we have our weights and so it
comes in is going to be the shape and
what comes out is going to be a random
number so we're going to go ahead and
just knit some random numbers based on
the shape with a standard deviation of
0.1 kind of a fun way to do that and
then the tf variable
init random distribution so we're just
creating a random distribution on there
that's all that is for the weights now
you might change that you might have a
higher standard deviation in some cases
you actually load preset weights that's
pretty rare usually you're testing that
against another model or something like
that and you want to see how those
weights configure with each other now
remember we have our bias so we need to
go ahead and initialize the bias with a
constant
in this case we're using 0.1 a lot of
times the bias is just put in as one and
then you have your weights add on to
that but we're going to set this as
point one uh so we want to return a
convolutional 2d in this case a neural
network this is uh would be a layer on
here what's going on with the con 2d is
we're taking our data coming in
we're going to filter it strides if you
remember correctly strides came from
here's our image and then we only look
at this picture here and then maybe we
have a stride of one so we look at this
picture here and we continue to look at
the different filters going on there the
other thing this does is that we have
our data coming in as 32
by 32
by
3 and we want to change this so that
it's just this is three dimensions and
it's going to reformat this as just two
dimensions so it's going to take this
number here and combine it with the 32
by 32. so this is a very important layer
here because it's reducing our data down
using different means and it connects
down i'm just going to jump down one
here
it goes with the convolutional layer so
you have your your kind of your
preformatting and the setup and then you
have your actual convolution layer that
goes through on there and you can see
here we have init weights by the shape
and knit bias shape of three because we
have the three different here's our
three again and then we return the tfnn
relu with the convention 2d so this
convolutional has this feeding into it
right there it's using that as part of
it and of course the input is the x y
plus b the bias so that's quite a
mouthful but these two are the are the
keys here to creating the convolutional
layers there the convolutional 2d coming
in and then the convolutional layer
which then steps through and creates all
those filters we saw then of course we
have our pooling uh so after each time
we run it through the convectional layer
we want to pull the data if you remember
correctly on the on the pool side and
let me just get rid of all my marks it's
getting a little crazy there and in fact
let's go ahead and jump back to that
slide let's just take a look at that
slide over here uh so we have our image
coming in we create our convolutional
layer with all the filters remember the
filters go um you know the filter is
coming in here and it looks at these
four boxes and then if it's a step let's
say step two it then goes to these four
boxes and then the next step and so on
uh so we have our convolutional layer
that we generate or convolutional layers
they use the uh relu function there's
other functions out there for this
though the relu is the
most the one that works the best at
least so far i'm sure that will change
then we have our pooling now if you
remember correctly the pooling was max
so if we had the filter coming in and
they did the multiplication on there and
we have a one and maybe a two here and
another one here and a three here three
is the max and so out of all of these
you then create an array that would be
three and if the max is over here two or
whatever it is that's what goes into the
pooling of what's going on in our
pooling uh so again we're reducing that
data down reducing it down as small as
we can and then finally we're going to
flatten it out into a single array and
that goes into our fully connected layer
and you can see that here in the code
right here we're going to create our
normal full layer so at some point we're
going to take from our pooling layer
this will go into some kind of
flattening process and then that will be
fed into the full the different layers
going in down here and so we have our
input size you'll see our input layer
get shape which is just going to get the
shape for whatever's coming in uh and
then input size initial weights is also
based on the input layer coming in and
the input size down here is based on the
input layer shape so we're just going to
already use the shape and already have
our size coming in and of course uh you
have to make sure you knit the bias
always put your bias on there and we'll
do that based on the size so this will
return
tf.matimole
input layer w plus b this is just a
normal full layer that's what this means
right down here that's what we're going
to return so that was a lot of steps we
went through let's go ahead and run that
so those are all loaded in there and
let's go ahead and uh create the layers
let's see what that looks like
now that we've done all the heavy
lifting and everything uh we get to do
all the easy part let's go ahead and
create our layers we'll create a
convolution layer one and two two
different convolutional layers and then
we'll take that and we'll flatten that
out and create a reshape pooling in
there for our reshape and then we'll
have our full uh layer at the end so
let's start by creating our first
convolutional layer then we come in here
and let me just run that real quick and
i want you to notice on here the 3
and the 32 this is important because
coming into this convolutional layer we
have three different channels and 32
pixels each
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 4 4 is just
fine that should work pretty good for
what we're doing for the size of the
image and then of course at the end once
you have your convolutional layer set up
you also need to pull it and you'll see
that the pooling is automatically set up
so that it would see the different shape
based on what's coming in so here we
have max toolbar 2x2 and we put in the
convolutional 1 that we just created the
convolutional layer we just created goes
right back into it and that right up
here as you can see is the x that's
coming in from here so it knows to look
at the first model and set the the data
accordingly set that up so it matches
and we went ahead and ran this already i
think i read let me go and run it again
and if we're going to do one layer let's
go ahead and do a second layer down here
and it's uh we'll call it convo 2.
it's also a convolutional layer on this
and you'll see that we're feeding
convolutional one in the pooling so it
goes from convolutional one into
convolutional one pooling from
convolutional one pooling into
convolutional two and then from
convolutional two into convolutional 2
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flattened
layer
let's go ahead and we'll do since we
have 64 coming out of here and we have a
four by four going in let's do eight by
eight by 64. so let's do
4096. this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our
convo 2 pooling and that will feed into
here the combo 2 pulling and then we're
going to set it up as a single layer
that's 4096 in size that's what that
means there we'll go ahead and run this
so we've now created this variable the
convo too flat and then we have our
first full layer this is the final
neural network where the flat layer
going in and we're going to again use
the relu for our setup on there on a
neural network for evaluation and you'll
notice that we're going to create our
first full layer our normal full layer
that's our definition so we created that
that's creating the normal full layer
and our input for the data comes right
here from the this goes right into it
the convo too flat so this tells it how
big the data is and we're going to have
it come out it's going to have 10 24
that's how big the layer is coming out
we'll go ahead and run this so now we
have our full layer one and with the
full layer one we want to also define
the full one dropout to go with that so
our full layer one comes in
keep probability equals whole
probability remember we created that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and
finally we'll go ahead and create a y
predict which is going to equal the
normal full one drop out and 10 because
we have 10 labels in there now in this
neural network we could have added
additional layers that would be another
option to play with you can also play
with instead of 10 24 you can use other
numbers for the way that sets up what's
coming out going into the next one we're
only going to do just the one layer and
the one layer drop out and you can see
if we did another layer it'd be really
easy just to feed in the full one drop
out into full layer two and then full
layer two dropout would have full air 2
feed into it and then you'd switch that
here for the y prediction for right now
this is great this particular data set
is tried and true and we know that this
will work on it and if we just type in y
predict and we run that
we'll see that this is a tensor object
uh shape question mark 10 d type 32 a
quick way to double check what we're
working on so now we've got all of our
we've done a setup all the way to the y
predict which we just did we want to go
ahead and apply the loss function and
make sure that's set up in there create
the optimizer and then
trainer optimizer and create a variable
to initialize all the global tf
variables so before we dive in to the um
loss function let me point out one quick
thing or just kind of a rehab over a
couple things and that is when we're
playing with this these setups
we pointed out up here we can change the
4 4 and use different numbers there the
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here of the 1024 also
this is also another number that if you
continue to raise that number you'll get
possibly a better fit you might overfit
and if you lower that number you'll use
less resources and generally you want to
use this in
the exponential growth an exponential
being 2 4
8 16 and in this case the next one down
would be 5 12. you can use any number
there but those would be the ideal
numbers when you look at this data so
the next step in all this is we need to
also create a way of tracking how good
our model is and we're going to call
this a loss function and so we're going
to create a cross entropy loss function
and so before we discuss exactly what
that is let's take a look and see what
we're feeding it
we're going to feed it our labels and we
have our true labels and our prediction
labels so coming in here is where the
two different variables we're sending in
or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy
in information theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this
and so the next step is we're going to
take the error we want to know our cross
entropy or our loss function how much
loss we have that's going to be part of
how we train the model so when you know
what the loss is and we're training it
we feed that back into the back
propagation setup and so we want to go
ahead and optimize that here's our
optimizer we're going to create the
optimizer using an atom optimizer
remember there's a lot of different ways
of optimizing the data atoms are most
popular used uh so our optimizer is
going to equal the tf train atom
optimizer if you don't remember what the
learning rate is let me just pop this
back into here here's our learning rate
when you have your weights you have all
your weights and your different nodes
that are coming out here's our node
coming out and it has all its weights
and then the error is being prop sent
back through in reverse on our neural
network so we take this error we adjust
these weights based on the different
formulas in this case the atom formula
is what we're using we don't want to
just adjust them completely we don't
want to change this weight so it exactly
fits the data coming through because if
we made that kind of adjustment it's
going to be biased to whatever the last
data we sent through is instead we're
going to multiply that by .001 and make
a very small shift in this weight so our
delta w is only 0.001 of the actual
delta w of the full change we're going
to compute from the atom and then we
want to go ahead and train it so our
training or set up a training
variable or function and this is going
to equal our optimizer minimize cross
entropy and we make sure we go ahead and
run this
so it's loaded in there and then we're
almost ready to train our model but
before we do that we need to create one
more
variable in here and we're going to
create a variable to initialize all the
global tf variables and when we look at
this
the tf global variable initializer this
is a tensorflow
object it goes through there and it
looks at all our different setup that we
have going under our tensorflow and then
initializes those variables uh so it's
kind of like a magic wand because it's
all hidden in the back end of tensorflow
all you need to know about this is that
you have to have the initialization on
there which is an operation
and you have to run that once you have
your setup going so we'll go ahead and
run this piece of code and then we're
going to go ahead and train our data so
let me run this so it's loaded up there
and so now we're going to go ahead and
run the model by creating a graph
session graph session is a tensorflow
term so you'll see that coming up it's
one of the things that throws me because
i always think of graphics and spark and
graph as just general graphing uh but
they talk about a graph session so we're
going to go ahead and run the model and
let's go ahead and walk through this uh
what's going on here and let's paste
this data in here and here we go so
we're going to start off with it with a
tf session as cess so that's our actual
tf session we've created uh so we're
right here with the tf uh
session our session we're creating we're
going to run tf global variable
initializer so right off the bat we're
initializing our variables here and then
we have for i in range 500. so what's
going on here remember 500 we're going
to break the data up and we're going to
batch it in at 500 points each we've
created our session run so we're going
to do with tf session as session right
here we've created our variable session
and then we're going to run and we're
going to go ahead and initialize it so
we have our tf global variables
initializer that we created
that initializes our our session in here
the next thing we're going to do is
we're going to go for i in range of 500
batch equals ch.nextbatch so if you
remember correctly this is loading up um
100 pictures at a time and
this is going to loop through that 500
times so we are literally doing uh what
is that 500 times 100 is
50 000. so that's 50 000 pictures we're
going to process right there and the
first process is we're going to do a
session run we're going to take our
train we created our train variable or
optimizer in there we're going to feed
it the dictionary we had our feed
dictionary that we created and we have x
equals batch zero coming in y true batch
one
hold the probability point five
and then just so that we can keep track
of what's going on we're gonna every 100
steps we're going to run a print so
currently on step format
accuracy is
and we're going to look at matches
equals tf.equal tf argument y prediction
1 tf.arg max y true comma 1. so we're
going to look at this as how many
matches it has and here our acc uh all
we're doing here is we're going to take
the matches how many matches they have
it creates it generates a chart we're
going to convert that to float that's
what the tfcast does and then we just
want to know the average we just want to
know the average of the accuracy and
then we'll go ahead and print that out
print session run accuracy feed
dictionary so it takes all this and it
prints out our accuracy on there so
let's go ahead and take this oops
screen's there let's go ahead and take
this and let's run it and this is going
to take a little bit to run uh so let's
see what happens on my old laptop and
we'll see here that we have our current
uh we're currently on step zero it takes
a little bit to get through the accuracy
and this will take just a moment to run
we can see that on our step 0 it has an
accuracy of 0.1 or 0.1028
and as it's running we'll go ahead you
don't need to watch it run all the way
but this accuracy is going to change a
little bit up and down so we've actually
lost some accuracy during our step two
but we'll see how that comes out let's
come back after we run it all the way
through and see how the different steps
come out it's actually reading that
backwards
the way this works is the closer we get
to one the more accuracy we have so you
can see here we've gone from a point one
to a point three nine um and we'll go
ahead and pause this and come back and
see what happens when we're done with
the full run all right now that we've
prepared the meal got it in the oven and
pulled out my finished dish here if
you've ever watched any of the old
cooking shows let's discuss a little bit
about this accuracy going on here and
how do you interpret that we've done a
couple things first we've defined
accuracy um the reason i got it
backwards before is you have
loss or accuracy and with loss you'll
get a graph that looks like this it goes
oops that's an s by the way there we go
you get a graph that curves down like
this and with accuracy you get a graph
that curves up this is how good it's
doing now in this case uh one is
supposed to be really good accuracy that
means it gets close to one but it never
crosses one so if you have an accuracy
of one that is phenomenal in fact that's
pretty much you know unheard of and the
same thing with loss if you have a loss
of zero that's also unheard of and the
zero's actually on this this axis right
here as we go in there so how do we
interpret that because you know if i was
looking at this and i go oh 0.51 that's
uh 51 you're doing 50 50. no this is not
percentage let me just put that in there
it is not percentage uh this is log
rhythmic what that means is that 0.2 is
twice as good as point one and uh when
we see point four that's twice as good
as point two real way to convert this
into a percentage you really can't say
this is is a direct percentage
conversion what you can do though is in
your head if we were to give this a
percentage uh we might look at this as
uh 50
we're just guessing equals 0.1 and if
fifty percent roughly equals point one
that's we started up here at the top
remember at the top here here's our
point one oh two eight the accuracy of
fifty percent then seventy five percent
is about point two and so on and so on
don't quote those numbers because it
doesn't work that way they say that if
you have 0.95
that's pretty much saying 100
and if you have anywhere between you'd
have to go look this up let me go and
remove all my drawings there
so the magic number is 0.5 we really
want to be over a 0.5 in this whole
thing and we have
both 0.504 remember this is accuracy if
we were looking at loss then we'd be
looking the other way but 0.05 you know
instead of how high it is we want how
low it is uh but with accuracy being
over a 0.5 is pretty valid that means
this is pretty solid and if you get to a
0.95 then it's a direct correlation
that's what we're looking for here in
these numbers you can see we finished
with this model at 0.5135
so still good
and if we look at when they ran this in
the other end remember there's a lot of
randomness that goes into it when we see
the weights they got
.5251 so a little better than ours but
that's fine you'll find your own comes
up a little bit better or worse
depending on just that randomness and so
we've gone through the whole model we've
created we trained the model and we've
also gone through on every 100th run to
test the model to see how accurate it is
we will start with the course of
fundamentals what is a neural network in
popular neural networks it's important
to know the framework we're in and what
we're going to be looking at
specifically then we'll touch on why a
recurrent neural network what is a
recurrent neural network and how does an
rn in work
one of the big things about rnns is what
they call the vanishing and exploding
gradient problem so we'll look at that
and then we're going to be using a use
case uh study that's going to be in
cross on tensorflow cross is a python
module for doing neural networks in deep
learning and in there there's the what
they call long short term memory lstm
and then we'll use the use case to
implement our lstm on the keras so when
you see that lstm that is basically the
rnn network and we'll get into that the
use case is always my favorite part
before we dive into any of this we're
going to take a look at what is an rnn
or an introduction to the rnn do you
know how google's auto complete feature
predicts the rest of the words a user is
typing i love that autocomplete feature
as i'm typing away saves me a lot of
time i can just kind of hit the enter
key and it auto fills everything and i
don't have to type as much well first
there's a collection of large volumes of
most frequently occurring consecutive
words
this is fed into a recurrent neural
network analysis the data by finding the
sequence of words occurring frequently
and builds a model to predict the next
word in the sentence and then google
what is the best food to eat in loss i'm
guessing you're going to say los mexico
no it's going to be las vegas
so the google search will take a look at
that and say hey the most common
autocomplete is going to be vegas in
there it usually gives you three or four
different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a google
search or even in microsoft words has a
some people get very mad at it auto
fills with the wrong stuff but you know
you're typing away and it helps you
autofill i have that in a lot of my
different packages it's just a standard
feature that we're all used to now so
before we dive into the rnn and getting
into the depths let's go ahead and talk
about what is a neural network neural
networks used in deep learning consist
of different layers connected to each
other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking throughout deep
learning the only way we can evaluate an
artificial intelligence or anything like
that is to compare it to human function
very important note on there and it
learns from a huge volumes of data and
it uses complex algorithm to train a
neural net so in here we have image
pixels of two different breeds of dog
one looks like a nice floppy eared lab
and one a german shepherd you know both
wonderful breeds of animals that image
then goes into an input layer uh that
input layer might be formatted at some
point because you have to let it know
like you know different pictures are
going to be different sizes and
different color content then it'll feed
into hidden layers so each of those
pixels or each point of data goes in and
then splits into the hidden layer which
then goes into another hidden layer
which then goes to an output layer rnn
there's some changes in there which
we're going to get into so it's not just
a straightforward propagation of data
like we've covered in many other
tutorials and finally you have an output
layer and the output layer has two
outputs it has one that lights up if
it's a german shepherd and another that
lights up as if it's a labrador so
identify as a dog's breed set networks
do not require memorizing the past
output so our forward propagation is
just that it goes forward and doesn't
have to re-memorize stuff and you can
see there that's not actually me in the
picture dressed up in my
suit i haven't worn a suit in years so
as we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural networks first there's the feed
forward neural network used in general
regression and classification problems
and we have the convolution neural
network used for image recognition deep
neural network used for acoustic
modeling deep belief network used for
cancer detection and recurrent neural
network used for speech recognition now
taking a lot of these and mixed them
around a little bit so just because it's
used for one thing doesn't mean it can't
be used for other modeling but generally
this is where the field is and this is
how those models are generally being
used right now so we talk about a feed
forward neural network in a feed forward
neural network information flows only in
the forward direction from the input
nodes through the hidden layers if any
into the output nodes there are no
cycles or loops in the network and so
you can see here we have our input layer
i was talking about how it just goes
straight forward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output and the
refer to the input is x a lot of times
and the output as y decisions are based
on current input no memory about the
past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
issues is because it doesn't have a
scope of memory or time a feed forward
neural network doesn't know how to
handle sequential data it only considers
only the current input so if you have a
series of things and because three
points back affects what's happening now
and what your output affects what's
happening that's very important so
whatever i put as an output is going to
affect the next one a feed forward
doesn't look at any of that it just
looks at this is what's coming in and it
cannot memorize previous inputs so it
doesn't have that list of inputs coming
in solution to feed forward neural
network you'll see here where it says
recurrent neural network and we have our
x on the bottom going to h going to y
that's your feed forward but right in
the middle it has a value c so there's a
whole another process so it's memorizing
what's going on in the hidden layers and
the hidden layers as they produce data
feed into the next one so your hidden
layer might have an output that goes off
to y
but that output goes back into the next
prediction coming in what this does is
this allows it to handle sequential data
it considers the current input and also
the previously received inputs and if
we're going to look at general drawings
and solutions we should also look at
applications of the rnn image captioning
rnn is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair that's
very tough i mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using rnn and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the new york stock
exchange in the u.s produces somewhere
in the neighborhood if you count all the
individual trades and fluctuations by
the second
it's like three terabytes a day of data
so we're going to look at one stock just
analyzing one stock is really tricky in
here we'll give you a little jump on
that so that's exciting but don't expect
to get rich off of it immediately
another application of the rnn is
natural language processing text mining
and sentiment analysis can be carried
out using rnn for natural language
processing and you can see right here
the term natural language processing
when you stream those three words
together is very different than i if i
said processing language natural leap so
the time series is very important when
we're analyzing sentiments it can change
the whole value of a sentence just by
switching the words around or if you're
just counting the words you might get
one sentiment where if you actually look
at the order they're in you get a
completely different sentiment when it
rains look for rainbows when it's dark
look for stars both of these are
positive sentiments and they're based
upon the order of which the sentence is
going in machine translation given an
input in one language rnn can be used to
translate the input into a different
languages as output i myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking english you would say
big cat and if you're speaking spanish
you would say cat big so that
translation is really important to get
the right order to get there's all kinds
of parts of speech that are important to
know by the order of the words here this
person is speaking in english and
getting translated and you can see here
a person is speaking in english in this
little diagram i guess that's denoted by
the flags i have a flag i own it no um
but they're speaking in english and it's
getting translated into
chinese italian french german and
spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
challenged i can now travel into worlds
i would never think of because i can
have something translate my english back
and forth readily and i'm not stuck with
a communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recurrent neural network
we turn that on its side so here it is
and now our x comes up from the bottom
into the hidden layers into y and they
usually draw very simplified x to h with
c as a loop a to y where a b and c are
the perimeters a lot of times you'll see
this kind of drawing in here digging
closer and closer into the h and how it
works going from left to right you'll
see that the c goes in and then the x
goes in so the x is going upward bound
and c is going to the right a is going
out and c is also going out that's where
it gets a little confusing so here we
have x n
c n and then we have y out and c out and
c is based on h t minus 1. so our value
is based on the y and the h value are
connected to each other they're not
necessarily the same value because h can
be its own thing and usually we draw
this or we represent it as a function h
of t equals a function of c where h of t
minus 1 that's the last h output and x
of t going in so it's the last output of
h combined with the new input of x where
h t is the new state fc is a function
with the parameters c that's a common
way of denoting it h t minus 1 is the
old state coming out and then x of t is
an input vector at time of step t
well we need to cover types of recurrent
neural networks and so the first one is
the most common one which is a
one-to-one single output
one-to-one neural network is usually
known as a vanilla neural network used
for regular machine learning problems
why because vanilla is usually
considered kind of a just a real basic
flavor but because it's very basic a lot
of times they'll call it the vanilla
neural network which is not the common
term but it is kind of a slang term
people will know what you're talking
about usually if you say that then we
run one to mini so you have a single
input and you might have a multiple
outputs in this case
image captioning as we looked at earlier
where we have not just looking at it as
a dog but a dog catching a ball in the
air and then you have mini to one
network takes in a sequence of inputs
examples sentiment analysis where a
given sentence can be classified as
expressing positive or negative
sentiments and we looked at that as we
were discussing if it rains look for a
rainbow so positive sentiment where rain
might be a negative sentiment if you're
just adding up the words in there and
then the course if you're going to do a
one-to-one many-to-one one-to-many
there's many to many networks takes in a
sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in english and then
going out in all the different languages
you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize just
how difficult it is to translate into
different languages
one of the biggest things you need to
understand when we're working with this
neural network is what's called the
vanishing gradient problem while
training an rnn your slope can be either
too small or very large and this makes
training difficult when the slope is too
small the problem is known as vanishing
gradient and you'll see here they have a
nice uh image loss of information
through time so if you're pushing not
enough information forward that
information is lost and then when you go
to train it you start losing the third
word in the sentence or something like
that or it doesn't quite follow the full
logic of what you're working on
exploding gradient problem oh this is
one that runs into everybody when you're
working with this particular neural
network when the slope tends to grow
exponentially instead of decaying this
problem is called exploding gradient
issues in gradient problem long training
time poor performance bad accuracy and
i'll add one more in there your computer
if you're on a lower end computer
testing out a model will lock up and
give you the memory error explaining
gradient problem consider the following
two examples to understand what should
be the next word in the sequence
the person who took my bike and blank a
thief the students who got into
engineering with blank from asia and you
can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe we'll have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the rnn must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the rnn must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
asia it might be sometimes difficult for
the air to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a vanishing gradient the option
problem
we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation functions so we'll
look at that just minimally there's a
lot of choices out there and then
finally there's long short term memory
networks the lstms and we can make
adjustments to that so just like we can
clip the gradient as it comes out we can
also
expand on that we can increase the
memory network the size of it so it
handles more information and one of the
most common problems in today's
setup is what they call long-term
dependencies suppose we try to predict
the last word in the text the clouds are
in the and you probably said sky here we
do not need any further context it's
pretty clear that the last word is going
to be sky suppose we try to predict the
last word in the text i have been
staying in spain for the last 10 years i
can speak fluent maybe you said
portuguese or french no you probably
said spanish the word we predict will
depend on the previous few words in
context here we need the context of
spain to predict the last word in the
text it's possible that the gap between
the relevant information and the point
where it is needed to become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default behavior all recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent h
layer lstms's
also have a chain like structure but the
repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default behavior ls tmss also have
a chain-like structure but the repeating
module has a different structure instead
of having a single neural network layer
there are four interacting layers
communicating in a very special way as
you can see the deeper we dig into this
the more complicated the graphs get in
here i want you to note that you have x
of t minus 1 coming in you have x of t
coming in and you have x at t plus one
and you have h of t minus one and h of t
coming in and h of t plus one going out
and of course uh on the other side is
the output a
in the middle we have our tangent h but
it occurs in two different places so not
only when we're computing the x a t plus
one or we getting the tangent h from x
to t but we're also getting that value
coming in from the x at t minus one so
the short of it is as you look at these
layers not only does it does the
propagate through the first layer goes
into the second layer back into itself
but it's also going into the third layer
so now we're kind of stacking those up
and this can get very complicated as you
grow that in size it also grows in
memory too and in the amount of
resources it takes but it's a very
powerful tool to help us address the
problem of complicated long sequential
information coming in like we were just
looking at in the sentence and when
we're looking at our long short term
memory network uh there's three steps of
processing in the lstms that we look at
the first one is we want to forget
irrelevant parts of the previous state
you know a lot of times like you know is
as in a unless we're trying to look at
whether it's a plural noun or not they
don't really play a huge part in the
language so we want to get rid of them
then selectively update cell state
values so we only want to update the
cell state values that reflect what
we're working on and finally we want to
put only output certain parts of the
cell state so whatever's coming out we
want to limit what's going out too and
let's dig a little deeper into this
let's just see what this really looks
like
so step one decides how much of the past
it should remember first step in the
lstm is to decide which information to
be omitted in from the cell in that
particular time step it is decided by
the sigmoid function it looks at the
previous state h to t minus 1 and the
current input x of t and computes the
function so you can see over here we
have a function of t
equals the sigmoid function of the
weight of f the h at t minus 1 and then
x of t plus of course you have a bias in
there with any of our neural networks so
we have a bias function so f of t equals
forget gate decides which information to
delete that is not important from the
previous time step considering an stm is
fed with the following inputs from the
previous and present time step alice is
good in physics john on the other hand
is good in chemistry so previous output
john plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our current input
so as we look at this the first step is
the forget gate realizes there might be
a change in context after encountering
the first full stop compares with the
current input sentence of x a t so we're
looking at that full stop and then
compares it with the input of the new
sentence the next sentence talks about
john so the information on alice is
deleted okay that's important to know so
we have this input coming in and if
we're going to continue on with john
then that's going to be the primary
information we're looking at the
position of the subject is vacated and
is assigned to john and so in this one
we've seen that we've weeded out a whole
bunch of information and we're only
passing information on john since that's
now the new topic so step two is then to
decide how much should this unit add to
the current state in the second layer
there are two parts one is the sigmoid
function and the other is the tangent h
in the sigmoid function it decides which
values to let through zero or one
tangent h function gives a weightage to
the values which are passed deciding
their level of importance minus one to
one and you can see the two formulas
that come up the i of t equals the
sigmoid of the weight of i h to t minus
one x of t plus the bias of i and the c
of t equals the tangent of h of the
weight of c of h of t minus one x of t
plus the bias of c so our i of t equals
the input gate determines which
information to let through based on its
significance in the current time step if
this seems a little complicated don't
worry because a lot of the programming
is already done when we get to the case
study understanding though that this is
part of the program is important when
you're trying to figure out these what
to set your settings at you should also
note when you're looking at this it
should have some semblance to your
forward propagation neural networks
where we have a value assigned to a
weight plus a bias very important steps
than any of the neural network layers
whether we're propagating into them the
information from one to the next or
we're just doing a straightforward
neural network propagation let's take a
quick look at this what it looks like
from the human standpoint as i step out
of my suit again consider the current
input at x of t john plays football well
he told me yesterday over the phone that
he had served as a captain of his
college football team that's our input
input gate announces the important
information john plays football and he
was a captain of his college team is
important he told me over the phone
yesterday is less important hence it is
forgotten this process of adding some
new information can be done via the
input gate now this example is as a
human form and we'll look at training
this stuff in just a minute
but as a human being if i wanted to get
this information from a conversation
maybe it's a google voice listening in
on you or something like that um how do
we weed out the information that he was
talking to me on the phone yesterday
well i don't want to memorize that he
talked to me on the phone yesterday or
maybe that is important but in this case
it's not i want to know that he was the
captain of the football team i want to
know that he served i want to know that
john plays football and he was a captain
of the college football team those are
the two things that i want to take away
as a human being again we measure a lot
of this from the human viewpoint and
that's also how we try to train them so
we can understand these neural networks
finally we get to step three decides
what part of the current cell state
makes it to the output the third step is
to decide what will be our output first
we run a sigmoid layer which decides
what parts of the cell state make it to
the output then we put the cell state
through the tangent h to push the values
to be between -1 and 1 and multiply it
by the output of the sigmoid gate so
when we talk about the output of t we
set that equal to the sigmoid of the
weight of 0 of the h of t minus 1 back
one step in time by the x of t plus of
course the bias the h of t equals the
outer t times the tangent of the tangent
h of c of t so our o t equals the output
gate allows the passed in information to
impact the output in the current time
step let's consider the example to
predicting the next word in the sentence
john played tremendously well against
the opponent and won for his team for
his contributions brave blank was
awarded player of the match there could
be a lot of choices for the empty space
current input brave is an adjective
adjectives describe a noun john could be
the best output after brave thumbs up
for john awarded player of the match and
if you were to pull just the nouns out
of the sentence team doesn't look right
because that's not really the subject
we're talking about contributions you
know brave contributions or brave teen
brave player brave match
so you look at this and you can start to
train this these this neural network so
starts looking at and goes oh no john is
what we're talking about so brave is an
adjective
john's going to be the best output and
we give john a big thumbs up and then of
course we jump into my favorite part the
case study use case implementation of
lstm let's predict the prices of stocks
using the lstm network based on the
stock price data between 2012
2016. we're going to try to predict the
stock prices of 2017.
and this will be a narrow set of data
we're not going to do the whole stock
market it turns out that the new york
stock exchange generates roughly three
terabytes of data per day that's all the
different trades up and down of all the
different stocks going on in each
individual one
second to second or nanosecond to
nanosecond but we're going to limit that
to just some very basic fundamental
information so don't think you're going
to get rich off this today but at least
you can give an eye you can give a step
forward in how to start processing
something like stock prices a very valid
use for machine learning in today's
markets
use case implementation of lstm
let's dive in we're going to import our
libraries we're going to import the
training set and get the scaling going
now if you watch any of our other
tutorials a lot of these pieces just
start to look very familiar because it's
very similar setup let's take a look at
that and just reminder we're going to be
using anaconda the jupiter notebook so
here i have my anaconda navigator when
we go under environments i've actually
set up a cross python 36 i'm in python36
and
nice thing about anaconda especially the
newer version remember a year ago
messing with anaconda in different
versions of python in different
environments anaconda now has a nice
interface
and i have this installed both on a
ubuntu linux machine and on windows so
it works fine on there you can go in
here and open a terminal window and then
in here once you're in the terminal
window this is where you're going to
start
installing using pip to install your
different modules and everything now
we've already pre-installed them so we
don't need to do that in here but if you
don't have them installed in your
particular environment you'll need to do
that and of course you don't need to use
the anaconda or the jupiter you can use
whatever favorite python id you like i'm
just a big fan of this because it keeps
all my stuff separate you can see on
this machine i have specifically
installed one for cross since we're
going to be working with cross under
tensorflow we go back to home i've gone
up here to application and that's the
environment i've loaded on here and then
we'll click on the launch jupiter
notebook now i've already in my jupiter
notebook um have set up a lot of stuff
so that we're ready to go kind of like
uh martha stewart's in the old cooking
show so we want to make sure we have all
our tools for you so you're not waiting
for them to load and if we go up here to
where it says new you can see where you
can
create a new python 3. that's what we
did here underneath the setup so it
already has all the modules installed on
it and i'm actually rename this if you
go under file you can rename it we've
i'm calling it rnn stock and let's just
take a look and start diving into the
code let's get into the exciting part
now we've looked at the tool and of
course you might be using a different
tool which is fine let's start putting
that code in there and seeing what those
imports and uploading everything looks
like now first half is kind of boring
when we hit the run button because we're
going to be importing numpy as np that's
uh the number python which is your numpy
array and the matplot library because
we're going to do some plotting at the
end and our pandas for our data set our
pandas is pd and when i hit run it
really doesn't do anything except for
load those modules just a quick note let
me just do a quick draw here oops shift
alt there we go you'll notice when we're
doing this setup if i was to divide this
up oops i'm going to actually let's
overlap these here we go
this first part that we're going to do
is
our data
prep
a lot of prepping involved
um in fact depending on what your system
and since we're using karas i put an
overlap here uh but you'll find that
almost
maybe even half of the code we do is all
about the data prep and the reason i
overlapped this with uh cross let me
just put that down because that's what
we're working in uh is because cross has
like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple steps a lot of
times that are in the kara setup we'll
take a look at that to see what comes up
in our code as we go through and look at
stock and the last part is to evaluate
and if you're working with
shareholders or
classroom whatever it is you're working
with uh the evaluate is the next biggest
piece um so the actual code here crossed
is a little bit more but when you're
working with some of the other packages
you might have like three lines that
might be it all your stuff is in your
pre-processing and your data since cross
has is cutting edge and you load the
individual layers you'll see that
there's a few more lines here and
crosses a little bit more robust and
then you spend a lot of times like i
said with the evaluate you want to have
something you present to everybody else
and say hey this is what i did this is
what it looks like so let's go through
those steps this is like a kind of just
general overview and let's just take a
look and see what the next set of code
looks like and in here we have a data
set train and it's going to be read
using the pd or pandas dot read csv and
it's a google stock price train dot csv
and so under this we have training set
equals data set train dot i location and
we've kind of sorted out part of that so
what's going on here let's just take a
look at let's look at the actual file
and see what's going on there now if we
look at this
ignore all the extra files on this i
already have a train and a test set
where it's sorted out this is important
to notice because a lot of times we do
that as part of the pre-processing of
the data we take
20 percent of the data out so we can
test it and then we train the rest of it
that's what we use to create our neural
network that way we can find out how
good it is uh but let's go ahead and
just take a look and see what that looks
like as far as the file itself and i
went ahead and just opened this up in a
basic word pad and text editor just so
we can take a look at it certainly you
can open up an excel or any other kind
of spreadsheet
and we note that this is a comma
separated variables we have a date
open high low close volume this is the
standard stuff that we import into our
stock or the most basic set of
information you can look at in stock
it's all free to download in this case
we downloaded it from google that's why
we call it the google stock price
and this specifically is google this is
the google stock values from as you can
see here we started off at 1 3 2012. so
when we look at this first setup up here
we have a data set train equals pd
underscore csv and if you noticed on the
original frame
let me just go back there
they had it set to home ubuntu downloads
google stock price train i went ahead
and changed that because we're in the
same file where i'm running the code so
i've saved this particular python code
and i don't need to go through any
special paths or have the full path on
there and then of course we want to take
out
certain values in here and you're going
to notice that we're using
our data set
and we're now in pandas
so pandas basically it looks like a
spreadsheet
and in this case we're going to do eye
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
and the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we always start with zero which is the
date and we're going to be looking at
open
and high which would be one and two
we'll just label that right there so you
can see now
when you go back and do this you
certainly can extrapolate and do this on
all the columns
but for the example let's just limit a
little bit here so that we can focus on
just some
key aspects of stock
and then we'll go up here and run the
code and again i said the first half is
very boring whenever we hit the run
button it doesn't do anything because
we're still just loading the data and
setting it up
now that we've loaded our data we want
to go ahead and scale it we want to do
what they call feature scaling and in
here we're going to pull it up from the
sk learn or the sk kit pre-processing
import min max scalar and when you look
at this you got to remember that
biases in our data we want to get rid of
that so if you have something that's
like a really high value let's just draw
a quick graph
and i have something here like the maybe
the stock has a value one stock has a
value of a hundred and another stock has
a value of five
um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100 is going to
be the max
and 5 is going to be the min and then
everything else goes and then we change
this so we just squish it down
i like the word squish so it's between 1
and 0. so 100 equals one or one equals a
hundred and zero equals five and you can
just multiply it's usually just a simple
multiplication we're using uh
multiplication
so it's going to be uh minus five and
then 100 divided or 95 divided by 1 so
or whatever value is is divided by 95.
and once we've actually created our
scale we've telling is going to be from
0 to 1. we want to take our training set
and we're going to create a training set
scaled and we're going to use our scalar
sc and we're going to fit we're going to
fit and transform the training set uh so
we can now use the sc this this
particular object will use it later on
our testing set because remember we have
to also scale that when we go to test
our model and see how it works and we'll
go ahead and click on the run again it's
not going to have any output yet because
we're just setting up all the variables
okay so we paste the data in here and
we're going to create the data structure
with the 60 time steps and output
first note we're running 60 time steps
and that is where this value here also
comes in so the first thing we do is we
create our x train and y train variables
we set them to an empty python array
very important to remember what kind of
array we're in what we're working with
and then we're going to come in here
we're going to go for i in range 60 to
1258 there's our 60 60 time steps and
the reason we want to do this is as
we're adding the data in there's nothing
below the 60. so if we're going to use
60 time steps we have to start at point
60 because it includes everything
underneath of it otherwise you'll get a
pointer error and then we're going to
take our x train and we're going to
append training set scaled this is a
scaled value between 0 and 1
and then as i is equal to 60 this value
is going to be
60 minus 60 is 0. so this actually is 0
to i so it's going to be 0 is 60 1 to
61. let me just circle this part right
here 1 to 61
2 to 62 and so on and so on and if you
remember i said 0 to 60 that's incorrect
because it does not count remember it
starts at 0 so this is a count of 60. so
it's actually 59. important to remember
that as we're looking at this and then
the second part of this that we're
looking at so if you remember correctly
here we go we go from 0 to 59 of i and
then we have a comma a 0 right here and
so finally we're just going to look at
the open value now i know we did put it
in there for 1 to 2.
if you remember quickly it doesn't count
the second one so it's just the open
value we're looking at just open
um and then finally we have y train dot
append training set i to zero and if you
remember correctly i two or i comma zero
if you remember correctly this is 0 to
59 so there's 60 values in it uh so we
do i down here this is number 60. so
we're going to do this is we're creating
an array and we have 0
to 59
and over here we have number 60 which is
going into the y train it's being
appended on there and then this just
goes all the way up so this is down here
is a
0 to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to 12
58. that's where this value here comes
in that's the length of the data we're
loading
so we've loaded two arrays we've loaded
one array that has which is filled with
arrays from 0 to 59 and we loaded one
array which is just the value and what
we're looking at you want to think about
this as a time sequence
here's my open open open open openopen
what's the next one in the series so
we're looking at the google stock and
each time it opens we want to know what
the next one 0 through 59 what's 60 1
through 60 what's 61 2 through 62 what's
62 and so on and so on going up and then
once we've loaded those in our for loop
we go ahead and take x train and y train
equals
np.arrayxtrain.npraytrain we're just
converting this back into a numpy array
that way we can use all the cool tools
that we get with numpy array including
reshaping so if we take a look and see
what's going on here we're going to take
our x train
we're going to reshape it
wow what the heck does reshape mean
that means we have an array if you
remember correctly
so many numbers by 60.
that's how wide it is
and so we're when you when you do x
train dot shape
that gets one of the shapes and you get
x train dot shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um
in this case where's that value 60 pi
1199 1258 minus 60 11.99 and we're
making sure that that is shaped
correctly so the data is grouped into
11
99 by 60 different arrays and then the
one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the end layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more
and then you get the leaf np dot reshape
comes from and using the existing shapes
to form it we'll go ahead and run this
piece of code again there's no real
output and then we'll import our
different cross modules that we need so
from cross models we're going to import
the sequential model dealing with
sequential data we have our dense layers
we have actually three layers we're
going to bring in our dents our lstm
which is what we're focusing on and our
dropout and we'll discuss these three
layers more in just a moment but you do
need the with the lstm you do need the
drop out and then the final layer will
be the dents but let's go ahead and run
this and they'll bring port our modules
and you'll see we get an error on here
and if you read it closer it's not
actually an error it's a warning what
does this warning mean these things come
up all the time when you're working with
such cutting edge modules they're
completely being updated all the time
we're not going to worry too much about
the warning all it's saying is that the
h5py
module which is part of cross is going
to be updated at some point and if
you're running new stuff on cross and
you start updating your cross system you
better make sure that your h5 pi is
updated too otherwise you're going to
have an error later on and you can
actually just run an update on the h5 pi
now if you wanted to not a big deal
we're not going to worry about that
today and i said we were going to jump
in and start looking at what those
layers mean i meant that and we're going
to start off with initializing the rnn
and then we'll start adding those layers
in and you'll see that we have the lstm
and then the dropout lstm then dropout
lstm then dropout what the heck is that
doing so let's explore that we'll start
by initializing the rnn regressor equals
sequential because we're using the
sequential model and we'll run that and
load that up and then we're going to
start adding our lstm layer and some
dropout regularization and right there
should be the q dropout regularization
and if we go back here and remember our
exploding gradient well that's what
we're talking about the dropout drops
out unnecessary data so we're not just
shifting huge amounts of data through
the network so and so we go in here
let's just go ahead and add this in i'll
go ahead and run this and we had three
of them so let me go ahead and put all
three of them in and then we can go back
over them there's the second one and
let's put one more in let's put that in
and we'll go ahead and put two more in i
mean i said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time i run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50. the units is the positive integer
and it's the dimensionality of the
output space this is what's going out
into the next layer so we might have 60
coming in but we have 50 going out we
have a return sequence because it is a
sequence data so we want to keep that
true and then you have to tell it what
shape it's in well we already know the
shape by just going in here and looking
at x train shape so input shape equals
the x train shape of one comma one it
makes it really easy you don't have to
remember all the numbers and put in 60
or whatever else is in there you just
let it tell the regressor what model to
use and so we follow our stm with a
dropout layer now understanding the
dropout layer is kind of exciting
because one of the things that happens
is we can over train our network that
means that our neural network will
memorize such specific data that it has
trouble predicting anything that's not
in that specific realm to fix for that
each time we run through the training
mode we're going to take point two or
twenty percent of our neurons they just
turn them off so we're only gonna train
on the other ones and it's gonna be
random that way each time we pass
through this we don't over train these
nodes come back in in the next training
cycle we randomly pick a different 20.
and finally i see a big difference as we
go from the first to the second and
third and fourth the first thing is we
don't have to input the shape because
the shapes already the output units is
50 here this item the next step
automatically knows this layer is
putting out 50 and because it's the next
layer it automatically sets that and
says oh 50 is coming out from our last
layer that's coming up you know goes
into the regressor and of course we have
our dropout and that's what's coming
into this one and so on and so on and so
the next three layers we don't have to
let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
dropout and dents dense just says we're
going to bring this all down into one
output instead of putting out a sequence
we just know i want to know the answer
at this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so far we've brought in our way up
here we brought in our data we brought
in our different modules we formatted
the data for training it we set it up
you know we have our y x train and our y
train we have our source of data and the
answers where we know so far that we're
going to put in there we've reshaped
that we've come in and built our cross
we've imported our different layers and
we have in here if you look we have what
five total layers now cross is a little
different than a lot of other systems
there's a lot of other systems put this
all in one line and do it automatic but
they don't give you the options of how
those layers interface and they don't
give you the options of how the data
comes in cross is cutting edge for this
reason so even those a lot of extra
steps in building the model this has a
huge impact on the output and what we
can do with this these new models from
cross so we brought in our dense we have
our full model put together a regressor
so we need to go ahead and compile it
and then we're going to go ahead and fit
the data we're going to compile the
pieces so they all come together and
then we're going to run our training
data on there and actually recreate our
regressor so it's ready to be used so
let's go ahead and compile that and i'd
go ahead and run that and if you've been
looking at any of our other tutorials on
neural networks you'll see we're going
to use the optimizer atom atom is
optimized for big data there's a couple
other optimizers out there beyond the
scope of this tutorial but certainly
atom will work pretty good for this and
loss equals mean squared value so when
we're training it this is what we want
to base the loss on how bad is our error
we're going to use the mean squared
value for our error and the atom
optimizer for its differential equations
you don't have to know the math behind
them but certainly it helps to know what
they're doing and where they fit into
the bigger models and then finally we're
going to do our fit fitting the rnn to
the training set we have the
regressor.fit x train y train epics and
batch size so we know where this is this
is our data coming in for the x train
our y train is the answer we're looking
for of our data our sequential input
epics is how many times we're going to
go over the whole data set we created a
whole data set of x trains so this is
each each of those rows which includes a
time sequence of 60. and batch size
another one of those things where cross
really shines is if you were pulling the
save from a large file instead of trying
to load it all into ram it can now pick
smaller batches up and load those
indirectly we're not worried about
pulling them off a file today this isn't
big enough to cause a computer too much
of a problem to run not too straining on
the resources but as we run this you can
imagine what happened if i was doing a
lot more than just one column in one
set of stock in this case google stock
imagine if i was doing this across all
the stocks and i had instead of just the
open i had open close high low and you
can actually find yourself with about 13
different variables times 60 because
it's a time sequence suddenly you find
yourself with a gig of memory you're
loading into your ram which will just
completely you know if it's just if
you're not on multiple computers or
cluster you can start running into
resource problems but for this we don't
have to worry about that so let's go
ahead and run this and this will
actually take a little bit on my
computer it's an older laptop and give a
second to kick in there there we go all
right so we have epic so this is going
to tell me it's running the first run
through all the data and as it's going
through it's batching them in 32 pieces
so 32 lines each time and there's 1198 i
think i said 11.99 earlier but it's
11.98 i was off by one and each one of
these is 13 seconds so you can imagine
this is roughly 20 to 30 minutes run
time on this computer like i said it's
an older laptop running at 0.9 gigahertz
on a dual processor and that's fine what
we'll do is i'll go ahead and stop go
get a drink of coffee and come back and
let's see what happens at the end and
where this takes us and like any good
cooking show
i've gone and got in my latte i also had
some other stuff running in the
background so you'll see these numbers
jumped up to like 19 seconds 15 seconds
but you can scroll through and you can
see we've run it through 100 steps or
100 epics so the question is what does
all this mean one of the first things
you'll notice is that our loss can is
over here it kind of stopped at .014 but
you can see it kind of goes down until
we hit about 0.014 three times in a row
so we guessed our epic pretty close
since our losses remain the same on
there so to find out what we're looking
at we're going to go ahead and load up
our test data the test data that we
didn't process yet and a real stock
price data set test eye location this is
the same thing we did it when we prepped
the data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing that we
need to do is go ahead and read the data
in from our test csv you see i've
changed the path on it for my computer
and then we'll call it the real stock
price and again we're doing just the one
column here and the values from i
location so it's all the rows and just
the values from these that one location
that's the open stock open let's go
ahead and run that so that's loaded in
there and then let's go ahead and create
we have our inputs we're going to create
inputs here and this should all look
familiar because this is the same thing
we did before we're going to take our
data set total we're going to do a
little panda concat from the data state
train now remember the end of the data
set train is part of the data going in
and let's just visualize that just a
little bit here's our train data let me
just put tr for train and it went up to
this value here but each one of these
values generated a bunch of columns it
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data so to find out we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and a real stock
price data set test
this is the same thing we did when we
prepped the data in the first place so
let's go ahead and go through this code
and we can see we've labeled it part
three making the predictions and
visualizing the results so the first
thing we need to do is go ahead and read
the data in from our test csv you see
i've changed the path on it for my
computer and then we'll call it the real
stock price and again we're doing just
the one column here and the values from
i location so it's all the rows and just
the values from these that one location
that's the open stock open let's go
ahead and run that so that's loaded in
there and then let's go ahead and create
we have our inputs we're going to create
inputs here and this would all look
familiar because this is the same thing
we did before we're going to take our
data set total we're going to do a
little panda concat from the datastate
train now remember the end of the data
set train is part of the data going in
let's just visualize that just a little
bit here's our train data let me just
put tr for train and it went up to this
value here but each one of these values
generated a bunch of columns it was 60
across and this value here equals this
one and this value here equals this one
and this value here equals this one and
so we need these top 60 to go into our
new data because that's part of the next
data or it's actually the top 59. so
that's what this first setup is over
here is we're going in we're doing the
real stock price and we're going to just
take the data set test and we're going
to load that in and then the real stock
price is our data test test location so
we're just looking at that first column
the open price and then our data set
total we're going to take pandas and
we're going to concat and we're going to
take our data set train for the open and
our dataset test open and this is one
way you can reference these columns
we've referenced them a couple of
different ways we've referenced them up
here with the one two but we know it's
labeled as a panda set as open so pandas
is great that way lots of versatility
there and we'll go ahead and go back up
here and run this there we go and you'll
notice this is the same as what we did
before we have our open data set where
pended our two different or concatenated
our two data sets together we have our
inputs equals data set total length data
set total minus length of data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works because normally when
you're running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that we didn't train
it with to see how well it graphs and we
have our inputs equals inputs dot
reshapes or reshaping like we did before
we're transforming our inputs so if you
remember from the transform between zero
and one and uh finally we want to go
ahead and take our x test and we're
going to create that x test and for i in
range 60 to 80. so here's our x test and
we're appending our inputs i to 60 which
remember is 0 to 59 and i comma 0 on the
other side so it's just the first column
which is our open column and once again
we take our x test we convert it to a
numpy array we do the same reshape we
did before and then we get down to the
final two lines and here we have
something new right here on these last
two lines let me just highlight those or
or mark them predicted stock price
equals regressor dot predicts x test so
we're predicting all the stock including
both the training and the testing model
here and then we want to take this
prediction and we want to inverse the
transform so remember we put it between
0 and 1. well that's not going to mean
very much to me to look at a float
number between 0 and 1 i want the dollar
amount so i want to know what the cash
value is and we'll go ahead and run this
and you'll see it runs much quicker than
the training that's what's so wonderful
about these neural networks once you put
them together it takes just a second to
run the same neural network that took us
what a half hour to train add and plot
the data we're going to plot what we
think it's going to be and we're going
to plot it against the real data what
the google stock actually did so let's
go ahead and take a look at that in code
and let's uh pull this code up so we
have our plt that's our oh if you
remember from the very beginning let me
just go back up to the top we have our
matte plot library dot pi plot as plt
that's where that comes in and we come
down here we're going to plot let me get
my drawing thing out again we're going
to go ahead and plt is basically kind of
like an object it's one of the things
that always through me when i'm doing
graphs in python because i always think
you have to create an object and then it
loads that class in there well in this
case plt is like a canvas you're putting
stuff on so if you've done html5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real google stock
price and then we're going to do our
predicted stock we're going to do it in
blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your graph especially if you're going
to present this to somebody you know to
your shareholders in the office and the
x label is going to be time because it's
a time series and we didn't actually put
the actual date and times on here but
that's fine we just know they're
incremented by time and then of course
the y label is the actual stock price
plt.legend tells us to build the legend
on here so that the color red and
real google stock price show up on there
and then the plot shows us that actual
graph so let's go ahead and run this and
see what that looks like and you can see
here we have a nice graph and let's talk
just a little bit about this graph
before we wrap it up here's our legend i
was telling you about that's why we have
the legend to show the prices we have
our title and everything and you'll
notice on the bottom we have a time
sequence we didn't put the actual time
in here now we could have we could have
gone ahead and
plotted the x since we know what the
dates are and plotted this to dates but
we also know that it's only the last
piece of data that we're looking at so
last piece of data which in somewhere
probably around here on the graph i
think it's like about 20 percent of the
data probably less than that we have the
google price and the google price has
this little up jump and then down and
you'll see that the actual google
instead of a turn down here just didn't
go up as high and didn't load go down so
our prediction has the same pattern but
the overall value is pretty far off as
far as um stock but then again we're
only looking at one column we're only
looking at the open price we're not
looking at how many volumes were traded
like i was pointing out earlier we talk
about stock just right off the bat
there's six columns there's open high
low close volume then there's weather i
mean volume shares then there's the
adjusted open adjusted high adjusted low
adjusted close they have a special
formula to predict exactly what it would
really be worth based on the value of
the stock and then from there there's
all kinds of other stuff you can put in
here so we're only looking at one small
aspect the opening price of the stock
and as you can see here we did a pretty
good job this curve follows the curve
pretty well it has like a little jumps
on it bins they don't quite match up so
this bin here does not quite match up
with that bin there but it's pretty darn
close we have the basic shape of it and
the prediction isn't too far off and you
can imagine that as we add more data in
and look at different aspects in the
specific domain of stock we should be
able to get a better representation each
time we drill in deeper of course this
took a half hour for my program my
computer to train so you can imagine
that if i was running it across all
those different variables it might take
a little bit longer to train the data
not so good for doing a quick tutorial
like this with that we've reached the
end of this complete neural networks
tutorial i hope you enjoyed this video
do like and share it thank you for
watching and stay tuned for more from
simply learn
[Music]
