ensemble methods take multiple models
which we'll call weak learners
and combine them together to form a more
powerful model overall
ensemble models are often at the top of
the leaderboard for data science
competitions
such as those found on kaggle there are
several different ensembling techniques
and today we'll talk about bagging
boosting and stacking
bagging or bootstrap aggregating
combines homogeneous weak learners
when i say homogeneous what i mean here
is that the weak learners are all the
same type of model
for example perhaps they're all decision
trees
with bagging we train each of the weak
learners independently
which means we can parallelize the model
training across multiple cores or
computers
very easily to obtain predictions on
unseen data
we then aggregate the predictions from
each of the weak learners and perform
some type of averaging
so why is it called bootstrap
aggregating well
after we've split our data into a
training set and a test set
we create b different bootstrap samples
and train a weak learner on each of
these b
bootstrap samples and train a weak
learner on each one
to make predictions using the bagged
model we pass an observation through
each of the b
weak learners and then we aggregate the
predictions
if it's classification this usually
takes the form of a vote
if we have a continuous response
variable this is typically performed as
averaging so what's the difference
between bagging decision trees
and a random forest a random forest is a
bagging algorithm
but there's one main difference between
bagging decision trees in general
and the random forest algorithm with a
random forest we not only use a
different subset of observations to
train each of the individual decision
trees
but we also take a different subset of
the variables
to train each of the individual decision
trees
boosting is an ensembling method that
trains models sequentially or
iteratively
therefore the weak learners are not
independent
the training of the model at the current
step depends on previous models
one example of a boosting algorithm is
adaboost or
adaptive boost for binary classification
the adaboost algorithm works in the
following way each model in the sequence
is trained where higher importance is
given to the observations that were most
difficult to predict in the previous
step
specifically the weights of the
previously misclassified observations
are increased
at the end in order to form the final
strong learner from the weak learners
a weighted sum of the weak learners is
taken the weights for each of the weak
learners is dependent on the performance
of the weak learner
the weights for each of the weak
learners is dependent on the performance
of the weak learner
in other words the better the weak
learner performed the higher its weight
will be
another example of a boosting algorithm
is gradient boosting
with gradient boosting the final model
is also a weighted sum of the weak
learners
however gradient descent is used to
determine how to improve at
each step in the sequence gradient
boosting is a generalization of boosting
where optimization can be based on any
arbitrary differentiable
loss function gradient boosting is often
used for decision trees
and gradient boosted trees often
outperform random forests
stacking is an ensembling method that
combines heterogeneous weak learners
for example you might combine neural
networks with decision trees with glms
and so on
it's also important to note that it's
common to have bagged and boosted models
as weak learners in a stacked ensemble
for this reason
stacked models can be very very
difficult to interpret meaningfully
with that being said they are usually
top performing models
so if ensemble methods are not
interpretable why may we want to use
them
well often our priority is not
interpretability
there are situations where we want to
have the model with the highest possible
accuracy
in these cases ensemble methods are
highly desirable
